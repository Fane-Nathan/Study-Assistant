[
  {
    "chunk_id": "http://arxiv.org/pdf/2001.09608v1_chunk_0",
    "chunk_text": "Some Insights into Lifelong Reinforcement Learning Systems. A lifelong reinforcement learning system is a learning system that has the ability to learn through trail-and-error interaction with the environment over its lifetime. In this paper, I give some arguments to show that the traditional reinforcement learning paradigm fails to model this type of learning system. Some insights into lifelong reinforcement learning are provided, along with a simplistic prototype lifelong reinforcement learning system.",
    "original_url": "http://arxiv.org/pdf/2001.09608v1",
    "original_title": "Some Insights into Lifelong Reinforcement Learning Systems",
    "source": "arxiv",
    "authors": [
      "Changjian Li"
    ],
    "published": "2020-01-27T07:26:12+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2001.09608v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2405.15430v1_chunk_0",
    "chunk_text": "Counterexample-Guided Repair of Reinforcement Learning Systems Using Safety Critics. Naively trained Deep Reinforcement Learning agents may fail to satisfy vital safety constraints. To avoid costly retraining, we may desire to repair a previously trained reinforcement learning agent to obviate unsafe behaviour. We devise a counterexample-guided repair algorithm for repairing reinforcement learning systems leveraging safety critics. The algorithm jointly repairs a reinforcement learning agent and a safety critic using gradient-based constrained optimisation.",
    "original_url": "http://arxiv.org/pdf/2405.15430v1",
    "original_title": "Counterexample-Guided Repair of Reinforcement Learning Systems Using Safety Critics",
    "source": "arxiv",
    "authors": [
      "David Boetius",
      "Stefan Leue"
    ],
    "published": "2024-05-24T10:56:51+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2405.15430v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2405.15430v1_chunk_1",
    "chunk_text": "The algorithm jointly repairs a reinforcement learning agent and a safety critic using gradient-based constrained optimisation.",
    "original_url": "http://arxiv.org/pdf/2405.15430v1",
    "original_title": "Counterexample-Guided Repair of Reinforcement Learning Systems Using Safety Critics",
    "source": "arxiv",
    "authors": [
      "David Boetius",
      "Stefan Leue"
    ],
    "published": "2024-05-24T10:56:51+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2405.15430v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2108.11510v1_chunk_0",
    "chunk_text": "Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey. Deep reinforcement learning augments the reinforcement learning framework and utilizes the powerful representation of deep neural networks. Recent works have demonstrated the remarkable successes of deep reinforcement learning in various domains including finance, medicine, healthcare, video games, robotics, and computer vision. In this work, we provide a detailed review of recent and state-of-the-art research advances of deep reinforcement learning in computer vision. We start with comprehending the theories of deep learning, reinforcement learning, and deep reinforcement learning.",
    "original_url": "http://arxiv.org/pdf/2108.11510v1",
    "original_title": "Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey",
    "source": "arxiv",
    "authors": [
      "Ngan Le",
      "Vidhiwar Singh Rathour",
      "Kashu Yamazaki",
      "Khoa Luu",
      "Marios Savvides"
    ],
    "published": "2021-08-25T23:01:48+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2108.11510v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2108.11510v1_chunk_1",
    "chunk_text": "We start with comprehending the theories of deep learning, reinforcement learning, and deep reinforcement learning. We then propose a categorization of deep reinforcement learning methodologies and discuss their advantages and limitations. In particular, we divide deep reinforcement learning into seven main categories according to their applications in computer vision, i.e. (i)landmark localization (ii) object detection; (iii) object tracking; (iv) registration on both 2D image and 3D image volumetric data (v) image segmentation; (vi) videos analysis; and (vii) other applications. Each of these categories is further analyzed with reinforcement learning techniques, network design, and performance.",
    "original_url": "http://arxiv.org/pdf/2108.11510v1",
    "original_title": "Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey",
    "source": "arxiv",
    "authors": [
      "Ngan Le",
      "Vidhiwar Singh Rathour",
      "Kashu Yamazaki",
      "Khoa Luu",
      "Marios Savvides"
    ],
    "published": "2021-08-25T23:01:48+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2108.11510v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2108.11510v1_chunk_2",
    "chunk_text": "Each of these categories is further analyzed with reinforcement learning techniques, network design, and performance. Moreover, we provide a comprehensive analysis of the existing publicly available datasets and examine source code availability. Finally, we present some open issues and discuss future research directions on deep reinforcement learning in computer vision",
    "original_url": "http://arxiv.org/pdf/2108.11510v1",
    "original_title": "Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey",
    "source": "arxiv",
    "authors": [
      "Ngan Le",
      "Vidhiwar Singh Rathour",
      "Kashu Yamazaki",
      "Khoa Luu",
      "Marios Savvides"
    ],
    "published": "2021-08-25T23:01:48+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2108.11510v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2307.01452v2_chunk_0",
    "chunk_text": "Causal Reinforcement Learning: A Survey. Reinforcement learning is an essential paradigm for solving sequential decision problems under uncertainty. Despite many remarkable achievements in recent decades, applying reinforcement learning methods in the real world remains challenging. One of the main obstacles is that reinforcement learning agents lack a fundamental understanding of the world and must therefore learn from scratch through numerous trial-and-error interactions. They may also face challenges in providing explanations for their decisions and generalizing the acquired knowledge.",
    "original_url": "http://arxiv.org/pdf/2307.01452v2",
    "original_title": "Causal Reinforcement Learning: A Survey",
    "source": "arxiv",
    "authors": [
      "Zhihong Deng",
      "Jing Jiang",
      "Guodong Long",
      "Chengqi Zhang"
    ],
    "published": "2023-07-04T03:00:43+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2307.01452v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2307.01452v2_chunk_1",
    "chunk_text": "They may also face challenges in providing explanations for their decisions and generalizing the acquired knowledge. Causality, however, offers a notable advantage as it can formalize knowledge in a systematic manner and leverage invariance for effective knowledge transfer. This has led to the emergence of causal reinforcement learning, a subfield of reinforcement learning that seeks to enhance existing algorithms by incorporating causal relationships into the learning process. In this survey, we comprehensively review the literature on causal reinforcement learning. We first introduce the basic concepts of causality and reinforcement learning, and then explain how causality can address core challenges in non-causal reinforcement learning.",
    "original_url": "http://arxiv.org/pdf/2307.01452v2",
    "original_title": "Causal Reinforcement Learning: A Survey",
    "source": "arxiv",
    "authors": [
      "Zhihong Deng",
      "Jing Jiang",
      "Guodong Long",
      "Chengqi Zhang"
    ],
    "published": "2023-07-04T03:00:43+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2307.01452v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2307.01452v2_chunk_2",
    "chunk_text": "We first introduce the basic concepts of causality and reinforcement learning, and then explain how causality can address core challenges in non-causal reinforcement learning. We categorize and systematically review existing causal reinforcement learning approaches based on their target problems and methodologies. Finally, we outline open issues and future directions in this emerging field.",
    "original_url": "http://arxiv.org/pdf/2307.01452v2",
    "original_title": "Causal Reinforcement Learning: A Survey",
    "source": "arxiv",
    "authors": [
      "Zhihong Deng",
      "Jing Jiang",
      "Guodong Long",
      "Chengqi Zhang"
    ],
    "published": "2023-07-04T03:00:43+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2307.01452v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2212.00253v1_chunk_0",
    "chunk_text": "Distributed Deep Reinforcement Learning: A Survey and A Multi-Player Multi-Agent Learning Toolbox. With the breakthrough of AlphaGo, deep reinforcement learning becomes a recognized technique for solving sequential decision-making problems. Despite its reputation, data inefficiency caused by its trial and error learning mechanism makes deep reinforcement learning hard to be practical in a wide range of areas. Plenty of methods have been developed for sample efficient deep reinforcement learning, such as environment modeling, experience transfer, and distributed modifications, amongst which, distributed deep reinforcement learning has shown its potential in various applications, such as human-computer gaming, and intelligent transportation. In this paper, we conclude the state of this exciting field, by comparing the classical distributed deep reinforcement learning methods, and studying important components to achieve efficient distributed learning, covering single player single agent distributed deep reinforcement learning to the most complex multiple players multiple agents distributed deep reinforcement learning.",
    "original_url": "http://arxiv.org/pdf/2212.00253v1",
    "original_title": "Distributed Deep Reinforcement Learning: A Survey and A Multi-Player Multi-Agent Learning Toolbox",
    "source": "arxiv",
    "authors": [
      "Qiyue Yin",
      "Tongtong Yu",
      "Shengqi Shen",
      "Jun Yang",
      "Meijing Zhao",
      "Kaiqi Huang",
      "Bin Liang",
      "Liang Wang"
    ],
    "published": "2022-12-01T03:39:24+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2212.00253v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2212.00253v1_chunk_1",
    "chunk_text": "In this paper, we conclude the state of this exciting field, by comparing the classical distributed deep reinforcement learning methods, and studying important components to achieve efficient distributed learning, covering single player single agent distributed deep reinforcement learning to the most complex multiple players multiple agents distributed deep reinforcement learning. Furthermore, we review recently released toolboxes that help to realize distributed deep reinforcement learning without many modifications of their non-distributed versions. By analyzing their strengths and weaknesses, a multi-player multi-agent distributed deep reinforcement learning toolbox is developed and released, which is further validated on Wargame, a complex environment, showing usability of the proposed toolbox for multiple players and multiple agents distributed deep reinforcement learning under complex games. Finally, we try to point out challenges and future trends, hoping this brief review can provide a guide or a spark for researchers who are interested in distributed deep reinforcement learning.",
    "original_url": "http://arxiv.org/pdf/2212.00253v1",
    "original_title": "Distributed Deep Reinforcement Learning: A Survey and A Multi-Player Multi-Agent Learning Toolbox",
    "source": "arxiv",
    "authors": [
      "Qiyue Yin",
      "Tongtong Yu",
      "Shengqi Shen",
      "Jun Yang",
      "Meijing Zhao",
      "Kaiqi Huang",
      "Bin Liang",
      "Liang Wang"
    ],
    "published": "2022-12-01T03:39:24+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2212.00253v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2009.07888v7_chunk_0",
    "chunk_text": "Transfer Learning in Deep Reinforcement Learning: A Survey. Reinforcement learning is a learning paradigm for solving sequential decision-making problems. Recent years have witnessed remarkable progress in reinforcement learning upon the fast development of deep neural networks. Along with the promising prospects of reinforcement learning in numerous domains such as robotics and game-playing, transfer learning has arisen to tackle various challenges faced by reinforcement learning, by transferring knowledge from external expertise to facilitate the efficiency and effectiveness of the learning process. In this survey, we systematically investigate the recent progress of transfer learning approaches in the context of deep reinforcement learning.",
    "original_url": "http://arxiv.org/pdf/2009.07888v7",
    "original_title": "Transfer Learning in Deep Reinforcement Learning: A Survey",
    "source": "arxiv",
    "authors": [
      "Zhuangdi Zhu",
      "Kaixiang Lin",
      "Anil K. Jain",
      "Jiayu Zhou"
    ],
    "published": "2020-09-16T18:38:54+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2009.07888v7"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2009.07888v7_chunk_1",
    "chunk_text": "In this survey, we systematically investigate the recent progress of transfer learning approaches in the context of deep reinforcement learning. Specifically, we provide a framework for categorizing the state-of-the-art transfer learning approaches, under which we analyze their goals, methodologies, compatible reinforcement learning backbones, and practical applications. We also draw connections between transfer learning and other relevant topics from the reinforcement learning perspective and explore their potential challenges that await future research progress.",
    "original_url": "http://arxiv.org/pdf/2009.07888v7",
    "original_title": "Transfer Learning in Deep Reinforcement Learning: A Survey",
    "source": "arxiv",
    "authors": [
      "Zhuangdi Zhu",
      "Kaixiang Lin",
      "Anil K. Jain",
      "Jiayu Zhou"
    ],
    "published": "2020-09-16T18:38:54+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2009.07888v7"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2108.03258v2_chunk_0",
    "chunk_text": "Memory-two strategies forming symmetric mutual reinforcement learning equilibrium in repeated prisoners' dilemma game. We investigate symmetric equilibria of mutual reinforcement learning when both players alternately learn the optimal memory-two strategies against the opponent in the repeated prisoners' dilemma game. We provide a necessary condition for memory-two deterministic strategies to form symmetric equilibria. We then provide three examples of memory-two deterministic strategies which form symmetric mutual reinforcement learning equilibria. We also prove that mutual reinforcement learning equilibria formed by memory-two strategies are also mutual reinforcement learning equilibria when both players use reinforcement learning of memory-$n$ strategies with $n>2$.",
    "original_url": "http://arxiv.org/pdf/2108.03258v2",
    "original_title": "Memory-two strategies forming symmetric mutual reinforcement learning equilibrium in repeated prisoners' dilemma game",
    "source": "arxiv",
    "authors": [
      "Masahiko Ueda"
    ],
    "published": "2021-08-05T04:59:18+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2108.03258v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2108.03258v2_chunk_1",
    "chunk_text": "We also prove that mutual reinforcement learning equilibria formed by memory-two strategies are also mutual reinforcement learning equilibria when both players use reinforcement learning of memory-$n$ strategies with $n>2$.",
    "original_url": "http://arxiv.org/pdf/2108.03258v2",
    "original_title": "Memory-two strategies forming symmetric mutual reinforcement learning equilibrium in repeated prisoners' dilemma game",
    "source": "arxiv",
    "authors": [
      "Masahiko Ueda"
    ],
    "published": "2021-08-05T04:59:18+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2108.03258v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2204.05437v1_chunk_0",
    "chunk_text": "Implementing Online Reinforcement Learning with Temporal Neural Networks. A Temporal Neural Network (TNN) architecture for implementing efficient online reinforcement learning is proposed and studied via simulation. The proposed T-learning system is composed of a frontend TNN that implements online unsupervised clustering and a backend TNN that implements online reinforcement learning. The reinforcement learning paradigm employs biologically plausible neo-Hebbian three-factor learning rules. As a working example, a prototype implementation of the cart-pole problem (balancing an inverted pendulum) is studied via simulation.",
    "original_url": "http://arxiv.org/pdf/2204.05437v1",
    "original_title": "Implementing Online Reinforcement Learning with Temporal Neural Networks",
    "source": "arxiv",
    "authors": [
      "James E. Smith"
    ],
    "published": "2022-04-11T23:10:42+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2204.05437v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2204.05437v1_chunk_1",
    "chunk_text": "As a working example, a prototype implementation of the cart-pole problem (balancing an inverted pendulum) is studied via simulation.",
    "original_url": "http://arxiv.org/pdf/2204.05437v1",
    "original_title": "Implementing Online Reinforcement Learning with Temporal Neural Networks",
    "source": "arxiv",
    "authors": [
      "James E. Smith"
    ],
    "published": "2022-04-11T23:10:42+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2204.05437v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1709.05067v1_chunk_0",
    "chunk_text": "Deep Reinforcement Learning for Conversational AI. Deep reinforcement learning is revolutionizing the artificial intelligence field. Currently, it serves as a good starting point for constructing intelligent autonomous systems which offer a better knowledge of the visual world. It is possible to scale deep reinforcement learning with the use of deep learning and do amazing tasks such as use of pixels in playing video games. In this paper, key concepts of deep reinforcement learning including reward function, differences between reinforcement learning and supervised learning and models for implementation of reinforcement are discussed.",
    "original_url": "http://arxiv.org/pdf/1709.05067v1",
    "original_title": "Deep Reinforcement Learning for Conversational AI",
    "source": "arxiv",
    "authors": [
      "Mahipal Jadeja",
      "Neelanshi Varia",
      "Agam Shah"
    ],
    "published": "2017-09-15T06:18:33+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1709.05067v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1709.05067v1_chunk_1",
    "chunk_text": "In this paper, key concepts of deep reinforcement learning including reward function, differences between reinforcement learning and supervised learning and models for implementation of reinforcement are discussed. Key challenges related to the implementation of reinforcement learning in conversational AI domain are identified as well as discussed in detail. Various conversational models which are based on deep reinforcement learning (as well as deep learning) are also discussed. In summary, this paper discusses key aspects of deep reinforcement learning which are crucial for designing an efficient conversational AI.",
    "original_url": "http://arxiv.org/pdf/1709.05067v1",
    "original_title": "Deep Reinforcement Learning for Conversational AI",
    "source": "arxiv",
    "authors": [
      "Mahipal Jadeja",
      "Neelanshi Varia",
      "Agam Shah"
    ],
    "published": "2017-09-15T06:18:33+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1709.05067v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2308.11336v1_chunk_0",
    "chunk_text": "On the Opportunities and Challenges of Offline Reinforcement Learning for Recommender Systems. Reinforcement learning serves as a potent tool for modeling dynamic user interests within recommender systems, garnering increasing research attention of late. However, a significant drawback persists: its poor data efficiency, stemming from its interactive nature. The training of reinforcement learning-based recommender systems demands expensive online interactions to amass adequate trajectories, essential for agents to learn user preferences. This inefficiency renders reinforcement learning-based recommender systems a formidable undertaking, necessitating the exploration of potential solutions.",
    "original_url": "http://arxiv.org/pdf/2308.11336v1",
    "original_title": "On the Opportunities and Challenges of Offline Reinforcement Learning for Recommender Systems",
    "source": "arxiv",
    "authors": [
      "Xiaocong Chen",
      "Siyu Wang",
      "Julian McAuley",
      "Dietmar Jannach",
      "Lina Yao"
    ],
    "published": "2023-08-22T10:28:02+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2308.11336v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2308.11336v1_chunk_1",
    "chunk_text": "This inefficiency renders reinforcement learning-based recommender systems a formidable undertaking, necessitating the exploration of potential solutions. Recent strides in offline reinforcement learning present a new perspective. Offline reinforcement learning empowers agents to glean insights from offline datasets and deploy learned policies in online settings. Given that recommender systems possess extensive offline datasets, the framework of offline reinforcement learning aligns seamlessly. Despite being a burgeoning field, works centered on recommender systems utilizing offline reinforcement learning remain limited.",
    "original_url": "http://arxiv.org/pdf/2308.11336v1",
    "original_title": "On the Opportunities and Challenges of Offline Reinforcement Learning for Recommender Systems",
    "source": "arxiv",
    "authors": [
      "Xiaocong Chen",
      "Siyu Wang",
      "Julian McAuley",
      "Dietmar Jannach",
      "Lina Yao"
    ],
    "published": "2023-08-22T10:28:02+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2308.11336v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2308.11336v1_chunk_2",
    "chunk_text": "Despite being a burgeoning field, works centered on recommender systems utilizing offline reinforcement learning remain limited. This survey aims to introduce and delve into offline reinforcement learning within recommender systems, offering an inclusive review of existing literature in this domain. Furthermore, we strive to underscore prevalent challenges, opportunities, and future pathways, poised to propel research in this evolving field.",
    "original_url": "http://arxiv.org/pdf/2308.11336v1",
    "original_title": "On the Opportunities and Challenges of Offline Reinforcement Learning for Recommender Systems",
    "source": "arxiv",
    "authors": [
      "Xiaocong Chen",
      "Siyu Wang",
      "Julian McAuley",
      "Dietmar Jannach",
      "Lina Yao"
    ],
    "published": "2023-08-22T10:28:02+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2308.11336v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2401.02349v2_chunk_0",
    "chunk_text": "A Survey Analyzing Generalization in Deep Reinforcement Learning. Reinforcement learning research obtained significant success and attention with the utilization of deep neural networks to solve problems in high dimensional state or action spaces. While deep reinforcement learning policies are currently being deployed in many different fields from medical applications to large language models, there are still ongoing questions the field is trying to answer on the generalization capabilities of deep reinforcement learning policies. In this paper, we will formalize and analyze generalization in deep reinforcement learning. We will explain the fundamental reasons why deep reinforcement learning policies encounter overfitting problems that limit their generalization capabilities.",
    "original_url": "http://arxiv.org/pdf/2401.02349v2",
    "original_title": "A Survey Analyzing Generalization in Deep Reinforcement Learning",
    "source": "arxiv",
    "authors": [
      "Ezgi Korkmaz"
    ],
    "published": "2024-01-04T16:45:01+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2401.02349v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2401.02349v2_chunk_1",
    "chunk_text": "We will explain the fundamental reasons why deep reinforcement learning policies encounter overfitting problems that limit their generalization capabilities. Furthermore, we will categorize and explain the manifold solution approaches to increase generalization, and overcome overfitting in deep reinforcement learning policies. From exploration to adversarial analysis and from regularization to robustness our paper provides an analysis on a wide range of subfields within deep reinforcement learning with a broad scope and in-depth view. We believe our study can provide a compact guideline for the current advancements in deep reinforcement learning, and help to construct robust deep neural policies with higher generalization skills.",
    "original_url": "http://arxiv.org/pdf/2401.02349v2",
    "original_title": "A Survey Analyzing Generalization in Deep Reinforcement Learning",
    "source": "arxiv",
    "authors": [
      "Ezgi Korkmaz"
    ],
    "published": "2024-01-04T16:45:01+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2401.02349v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2204.11897v3_chunk_0",
    "chunk_text": "Reinforcement Teaching. Machine learning algorithms learn to solve a task, but are unable to improve their ability to learn. Meta-learning methods learn about machine learning algorithms and improve them so that they learn more quickly. However, existing meta-learning methods are either hand-crafted to improve one specific component of an algorithm or only work with differentiable algorithms. We develop a unifying meta-learning framework, called Reinforcement Teaching, to improve the learning process of \\emph{any} algorithm.",
    "original_url": "http://arxiv.org/pdf/2204.11897v3",
    "original_title": "Reinforcement Teaching",
    "source": "arxiv",
    "authors": [
      "Calarina Muslimani",
      "Alex Lewandowski",
      "Dale Schuurmans",
      "Matthew E. Taylor",
      "Jun Luo"
    ],
    "published": "2022-04-25T18:04:17+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2204.11897v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2204.11897v3_chunk_1",
    "chunk_text": "We develop a unifying meta-learning framework, called Reinforcement Teaching, to improve the learning process of \\emph{any} algorithm. Under Reinforcement Teaching, a teaching policy is learned, through reinforcement, to improve a student's learning algorithm. To learn an effective teaching policy, we introduce the parametric-behavior embedder that learns a representation of the student's learnable parameters from its input/output behavior. We further use learning progress to shape the teacher's reward, allowing it to more quickly maximize the student's performance. To demonstrate the generality of Reinforcement Teaching, we conduct experiments in which a teacher learns to significantly improve both reinforcement and supervised learning algorithms.",
    "original_url": "http://arxiv.org/pdf/2204.11897v3",
    "original_title": "Reinforcement Teaching",
    "source": "arxiv",
    "authors": [
      "Calarina Muslimani",
      "Alex Lewandowski",
      "Dale Schuurmans",
      "Matthew E. Taylor",
      "Jun Luo"
    ],
    "published": "2022-04-25T18:04:17+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2204.11897v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2204.11897v3_chunk_2",
    "chunk_text": "To demonstrate the generality of Reinforcement Teaching, we conduct experiments in which a teacher learns to significantly improve both reinforcement and supervised learning algorithms. Reinforcement Teaching outperforms previous work using heuristic reward functions and state representations, as well as other parameter representations.",
    "original_url": "http://arxiv.org/pdf/2204.11897v3",
    "original_title": "Reinforcement Teaching",
    "source": "arxiv",
    "authors": [
      "Calarina Muslimani",
      "Alex Lewandowski",
      "Dale Schuurmans",
      "Matthew E. Taylor",
      "Jun Luo"
    ],
    "published": "2022-04-25T18:04:17+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2204.11897v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1606.03476v1_chunk_0",
    "chunk_text": "Generative Adversarial Imitation Learning. Consider learning a policy from example expert behavior, without interaction with the expert or access to reinforcement signal. One approach is to recover the expert's cost function with inverse reinforcement learning, then extract a policy from that cost function with reinforcement learning. This approach is indirect and can be slow. We propose a new general framework for directly extracting a policy from data, as if it were obtained by reinforcement learning following inverse reinforcement learning.",
    "original_url": "http://arxiv.org/pdf/1606.03476v1",
    "original_title": "Generative Adversarial Imitation Learning",
    "source": "arxiv",
    "authors": [
      "Jonathan Ho",
      "Stefano Ermon"
    ],
    "published": "2016-06-10T20:51:29+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1606.03476v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1606.03476v1_chunk_1",
    "chunk_text": "We propose a new general framework for directly extracting a policy from data, as if it were obtained by reinforcement learning following inverse reinforcement learning. We show that a certain instantiation of our framework draws an analogy between imitation learning and generative adversarial networks, from which we derive a model-free imitation learning algorithm that obtains significant performance gains over existing model-free methods in imitating complex behaviors in large, high-dimensional environments.",
    "original_url": "http://arxiv.org/pdf/1606.03476v1",
    "original_title": "Generative Adversarial Imitation Learning",
    "source": "arxiv",
    "authors": [
      "Jonathan Ho",
      "Stefano Ermon"
    ],
    "published": "2016-06-10T20:51:29+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1606.03476v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2304.10098v2_chunk_0",
    "chunk_text": "Two-Memory Reinforcement Learning. While deep reinforcement learning has shown important empirical success, it tends to learn relatively slow due to slow propagation of rewards information and slow update of parametric neural networks. Non-parametric episodic memory, on the other hand, provides a faster learning alternative that does not require representation learning and uses maximum episodic return as state-action values for action selection. Episodic memory and reinforcement learning both have their own strengths and weaknesses. Notably, humans can leverage multiple memory systems concurrently during learning and benefit from all of them.",
    "original_url": "http://arxiv.org/pdf/2304.10098v2",
    "original_title": "Two-Memory Reinforcement Learning",
    "source": "arxiv",
    "authors": [
      "Zhao Yang",
      "Thomas. M. Moerland",
      "Mike Preuss",
      "Aske Plaat"
    ],
    "published": "2023-04-20T05:39:25+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2304.10098v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2304.10098v2_chunk_1",
    "chunk_text": "Notably, humans can leverage multiple memory systems concurrently during learning and benefit from all of them. In this work, we propose a method called Two-Memory reinforcement learning agent (2M) that combines episodic memory and reinforcement learning to distill both of their strengths. The 2M agent exploits the speed of the episodic memory part and the optimality and the generalization capacity of the reinforcement learning part to complement each other. Our experiments demonstrate that the 2M agent is more data efficient and outperforms both pure episodic memory and pure reinforcement learning, as well as a state-of-the-art memory-augmented RL agent. Moreover, the proposed approach provides a general framework that can be used to combine any episodic memory agent with other off-policy reinforcement learning algorithms.",
    "original_url": "http://arxiv.org/pdf/2304.10098v2",
    "original_title": "Two-Memory Reinforcement Learning",
    "source": "arxiv",
    "authors": [
      "Zhao Yang",
      "Thomas. M. Moerland",
      "Mike Preuss",
      "Aske Plaat"
    ],
    "published": "2023-04-20T05:39:25+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2304.10098v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2304.10098v2_chunk_2",
    "chunk_text": "Moreover, the proposed approach provides a general framework that can be used to combine any episodic memory agent with other off-policy reinforcement learning algorithms.",
    "original_url": "http://arxiv.org/pdf/2304.10098v2",
    "original_title": "Two-Memory Reinforcement Learning",
    "source": "arxiv",
    "authors": [
      "Zhao Yang",
      "Thomas. M. Moerland",
      "Mike Preuss",
      "Aske Plaat"
    ],
    "published": "2023-04-20T05:39:25+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2304.10098v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1912.06310v1_chunk_0",
    "chunk_text": "Recruitment-imitation Mechanism for Evolutionary Reinforcement Learning. Reinforcement learning, evolutionary algorithms and imitation learning are three principal methods to deal with continuous control tasks. Reinforcement learning is sample efficient, yet sensitive to hyper-parameters setting and needs efficient exploration; Evolutionary algorithms are stable, but with low sample efficiency; Imitation learning is both sample efficient and stable, however it requires the guidance of expert data. In this paper, we propose Recruitment-imitation Mechanism (RIM) for evolutionary reinforcement learning, a scalable framework that combines advantages of the three methods mentioned above. The core of this framework is a dual-actors and single critic reinforcement learning agent.",
    "original_url": "http://arxiv.org/pdf/1912.06310v1",
    "original_title": "Recruitment-imitation Mechanism for Evolutionary Reinforcement Learning",
    "source": "arxiv",
    "authors": [
      "Shuai Lü",
      "Shuai Han",
      "Wenbo Zhou",
      "Junwei Zhang"
    ],
    "published": "2019-12-13T03:26:14+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1912.06310v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1912.06310v1_chunk_1",
    "chunk_text": "The core of this framework is a dual-actors and single critic reinforcement learning agent. This agent can recruit high-fitness actors from the population of evolutionary algorithms, which instructs itself to learn from experience replay buffer. At the same time, low-fitness actors in the evolutionary population can imitate behavior patterns of the reinforcement learning agent and improve their adaptability. Reinforcement and imitation learners in this framework can be replaced with any off-policy actor-critic reinforcement learner or data-driven imitation learner. We evaluate RIM on a series of benchmarks for continuous control tasks in Mujoco.",
    "original_url": "http://arxiv.org/pdf/1912.06310v1",
    "original_title": "Recruitment-imitation Mechanism for Evolutionary Reinforcement Learning",
    "source": "arxiv",
    "authors": [
      "Shuai Lü",
      "Shuai Han",
      "Wenbo Zhou",
      "Junwei Zhang"
    ],
    "published": "2019-12-13T03:26:14+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1912.06310v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1912.06310v1_chunk_2",
    "chunk_text": "We evaluate RIM on a series of benchmarks for continuous control tasks in Mujoco. The experimental results show that RIM outperforms prior evolutionary or reinforcement learning methods. The performance of RIM's components is significantly better than components of previous evolutionary reinforcement learning algorithm, and the recruitment using soft update enables reinforcement learning agent to learn faster than that using hard update.",
    "original_url": "http://arxiv.org/pdf/1912.06310v1",
    "original_title": "Recruitment-imitation Mechanism for Evolutionary Reinforcement Learning",
    "source": "arxiv",
    "authors": [
      "Shuai Lü",
      "Shuai Han",
      "Wenbo Zhou",
      "Junwei Zhang"
    ],
    "published": "2019-12-13T03:26:14+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1912.06310v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2210.00770v1_chunk_0",
    "chunk_text": "Accelerate Reinforcement Learning with PID Controllers in the Pendulum Simulations. We propose a Proportional Integral Derivative (PID) controller-based coaching scheme to expedite reinforcement learning (RL).",
    "original_url": "http://arxiv.org/pdf/2210.00770v1",
    "original_title": "Accelerate Reinforcement Learning with PID Controllers in the Pendulum Simulations",
    "source": "arxiv",
    "authors": [
      "Liping Bai"
    ],
    "published": "2022-10-03T08:59:12+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2210.00770v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1706.05749v1_chunk_0",
    "chunk_text": "Dex: Incremental Learning for Complex Environments in Deep Reinforcement Learning. This paper introduces Dex, a reinforcement learning environment toolkit specialized for training and evaluation of continual learning methods as well as general reinforcement learning problems. We also present the novel continual learning method of incremental learning, where a challenging environment is solved using optimal weight initialization learned from first solving a similar easier environment. We show that incremental learning can produce vastly superior results than standard methods by providing a strong baseline method across ten Dex environments. We finally develop a saliency method for qualitative analysis of reinforcement learning, which shows the impact incremental learning has on network attention.",
    "original_url": "http://arxiv.org/pdf/1706.05749v1",
    "original_title": "Dex: Incremental Learning for Complex Environments in Deep Reinforcement Learning",
    "source": "arxiv",
    "authors": [
      "Nick Erickson",
      "Qi Zhao"
    ],
    "published": "2017-06-19T00:16:24+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1706.05749v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1706.05749v1_chunk_1",
    "chunk_text": "We finally develop a saliency method for qualitative analysis of reinforcement learning, which shows the impact incremental learning has on network attention.",
    "original_url": "http://arxiv.org/pdf/1706.05749v1",
    "original_title": "Dex: Incremental Learning for Complex Environments in Deep Reinforcement Learning",
    "source": "arxiv",
    "authors": [
      "Nick Erickson",
      "Qi Zhao"
    ],
    "published": "2017-06-19T00:16:24+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1706.05749v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2004.00993v2_chunk_0",
    "chunk_text": "Augmented Q Imitation Learning (AQIL). The study of unsupervised learning can be generally divided into two categories: imitation learning and reinforcement learning. In imitation learning the machine learns by mimicking the behavior of an expert system whereas in reinforcement learning the machine learns via direct environment feedback. Traditional deep reinforcement learning takes a significant time before the machine starts to converge to an optimal policy. This paper proposes Augmented Q-Imitation-Learning, a method by which deep reinforcement learning convergence can be accelerated by applying Q-imitation-learning as the initial training process in traditional Deep Q-learning.",
    "original_url": "http://arxiv.org/pdf/2004.00993v2",
    "original_title": "Augmented Q Imitation Learning (AQIL)",
    "source": "arxiv",
    "authors": [
      "Xiao Lei Zhang",
      "Anish Agarwal"
    ],
    "published": "2020-03-31T18:08:23+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2004.00993v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2004.00993v2_chunk_1",
    "chunk_text": "This paper proposes Augmented Q-Imitation-Learning, a method by which deep reinforcement learning convergence can be accelerated by applying Q-imitation-learning as the initial training process in traditional Deep Q-learning.",
    "original_url": "http://arxiv.org/pdf/2004.00993v2",
    "original_title": "Augmented Q Imitation Learning (AQIL)",
    "source": "arxiv",
    "authors": [
      "Xiao Lei Zhang",
      "Anish Agarwal"
    ],
    "published": "2020-03-31T18:08:23+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2004.00993v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1809.06995v1_chunk_0",
    "chunk_text": "Interpretable Reinforcement Learning with Ensemble Methods. We propose to use boosted regression trees as a way to compute human-interpretable solutions to reinforcement learning problems. Boosting combines several regression trees to improve their accuracy without significantly reducing their inherent interpretability. Prior work has focused independently on reinforcement learning and on interpretable machine learning, but there has been little progress in interpretable reinforcement learning. Our experimental results show that boosted regression trees compute solutions that are both interpretable and match the quality of leading reinforcement learning methods.",
    "original_url": "http://arxiv.org/pdf/1809.06995v1",
    "original_title": "Interpretable Reinforcement Learning with Ensemble Methods",
    "source": "arxiv",
    "authors": [
      "Alexander Brown",
      "Marek Petrik"
    ],
    "published": "2018-09-19T03:23:35+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1809.06995v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1809.06995v1_chunk_1",
    "chunk_text": "Our experimental results show that boosted regression trees compute solutions that are both interpretable and match the quality of leading reinforcement learning methods.",
    "original_url": "http://arxiv.org/pdf/1809.06995v1",
    "original_title": "Interpretable Reinforcement Learning with Ensemble Methods",
    "source": "arxiv",
    "authors": [
      "Alexander Brown",
      "Marek Petrik"
    ],
    "published": "2018-09-19T03:23:35+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1809.06995v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1806.04640v3_chunk_0",
    "chunk_text": "Unsupervised Meta-Learning for Reinforcement Learning. Meta-learning algorithms use past experience to learn to quickly solve new tasks. In the context of reinforcement learning, meta-learning algorithms acquire reinforcement learning procedures to solve new problems more efficiently by utilizing experience from prior tasks. The performance of meta-learning algorithms depends on the tasks available for meta-training: in the same way that supervised learning generalizes best to test points drawn from the same distribution as the training points, meta-learning methods generalize best to tasks from the same distribution as the meta-training tasks. In effect, meta-reinforcement learning offloads the design burden from algorithm design to task design.",
    "original_url": "http://arxiv.org/pdf/1806.04640v3",
    "original_title": "Unsupervised Meta-Learning for Reinforcement Learning",
    "source": "arxiv",
    "authors": [
      "Abhishek Gupta",
      "Benjamin Eysenbach",
      "Chelsea Finn",
      "Sergey Levine"
    ],
    "published": "2018-06-12T16:48:52+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1806.04640v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1806.04640v3_chunk_1",
    "chunk_text": "In effect, meta-reinforcement learning offloads the design burden from algorithm design to task design. If we can automate the process of task design as well, we can devise a meta-learning algorithm that is truly automated. In this work, we take a step in this direction, proposing a family of unsupervised meta-learning algorithms for reinforcement learning. We motivate and describe a general recipe for unsupervised meta-reinforcement learning, and present an instantiation of this approach. Our conceptual and theoretical contributions consist of formulating the unsupervised meta-reinforcement learning problem and describing how task proposals based on mutual information can be used to train optimal meta-learners.",
    "original_url": "http://arxiv.org/pdf/1806.04640v3",
    "original_title": "Unsupervised Meta-Learning for Reinforcement Learning",
    "source": "arxiv",
    "authors": [
      "Abhishek Gupta",
      "Benjamin Eysenbach",
      "Chelsea Finn",
      "Sergey Levine"
    ],
    "published": "2018-06-12T16:48:52+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1806.04640v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1806.04640v3_chunk_2",
    "chunk_text": "Our conceptual and theoretical contributions consist of formulating the unsupervised meta-reinforcement learning problem and describing how task proposals based on mutual information can be used to train optimal meta-learners. Our experimental results indicate that unsupervised meta-reinforcement learning effectively acquires accelerated reinforcement learning procedures without the need for manual task design and these procedures exceed the performance of learning from scratch.",
    "original_url": "http://arxiv.org/pdf/1806.04640v3",
    "original_title": "Unsupervised Meta-Learning for Reinforcement Learning",
    "source": "arxiv",
    "authors": [
      "Abhishek Gupta",
      "Benjamin Eysenbach",
      "Chelsea Finn",
      "Sergey Levine"
    ],
    "published": "2018-06-12T16:48:52+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1806.04640v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2010.14616v1_chunk_0",
    "chunk_text": "Lineage Evolution Reinforcement Learning. We propose a general agent population learning system, and on this basis, we propose lineage evolution reinforcement learning algorithm. Lineage evolution reinforcement learning is a kind of derivative algorithm which accords with the general agent population learning system. We take the agents in DQN and its related variants as the basic agents in the population, and add the selection, mutation and crossover modules in the genetic algorithm to the reinforcement learning algorithm. In the process of agent evolution, we refer to the characteristics of natural genetic behavior, add lineage factor to ensure the retention of potential performance of agent, and comprehensively consider the current performance and lineage value when evaluating the performance of agent.",
    "original_url": "http://arxiv.org/pdf/2010.14616v1",
    "original_title": "Lineage Evolution Reinforcement Learning",
    "source": "arxiv",
    "authors": [
      "Zeyu Zhang",
      "Guisheng Yin"
    ],
    "published": "2020-09-26T11:58:16+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2010.14616v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2010.14616v1_chunk_1",
    "chunk_text": "In the process of agent evolution, we refer to the characteristics of natural genetic behavior, add lineage factor to ensure the retention of potential performance of agent, and comprehensively consider the current performance and lineage value when evaluating the performance of agent. Without changing the parameters of the original reinforcement learning algorithm, lineage evolution reinforcement learning can optimize different reinforcement learning algorithms. Our experiments show that the idea of evolution with lineage improves the performance of original reinforcement learning algorithm in some games in Atari 2600.",
    "original_url": "http://arxiv.org/pdf/2010.14616v1",
    "original_title": "Lineage Evolution Reinforcement Learning",
    "source": "arxiv",
    "authors": [
      "Zeyu Zhang",
      "Guisheng Yin"
    ],
    "published": "2020-09-26T11:58:16+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2010.14616v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2206.06841v1_chunk_0",
    "chunk_text": "Robust Reinforcement Learning with Distributional Risk-averse formulation. Robust Reinforcement Learning tries to make predictions more robust to changes in the dynamics or rewards of the system. This problem is particularly important when the dynamics and rewards of the environment are estimated from the data. In this paper, we approximate the Robust Reinforcement Learning constrained with a $\\Phi$-divergence using an approximate Risk-Averse formulation. We show that the classical Reinforcement Learning formulation can be robustified using standard deviation penalization of the objective.",
    "original_url": "http://arxiv.org/pdf/2206.06841v1",
    "original_title": "Robust Reinforcement Learning with Distributional Risk-averse formulation",
    "source": "arxiv",
    "authors": [
      "Pierre Clavier",
      "Stéphanie Allassonière",
      "Erwan Le Pennec"
    ],
    "published": "2022-06-14T13:33:58+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2206.06841v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2206.06841v1_chunk_1",
    "chunk_text": "We show that the classical Reinforcement Learning formulation can be robustified using standard deviation penalization of the objective. Two algorithms based on Distributional Reinforcement Learning, one for discrete and one for continuous action spaces are proposed and tested in a classical Gym environment to demonstrate the robustness of the algorithms.",
    "original_url": "http://arxiv.org/pdf/2206.06841v1",
    "original_title": "Robust Reinforcement Learning with Distributional Risk-averse formulation",
    "source": "arxiv",
    "authors": [
      "Pierre Clavier",
      "Stéphanie Allassonière",
      "Erwan Le Pennec"
    ],
    "published": "2022-06-14T13:33:58+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2206.06841v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2108.10078v1_chunk_0",
    "chunk_text": "Distilling Neuron Spike with High Temperature in Reinforcement Learning Agents. Spiking neural network (SNN), compared with depth neural network (DNN), has faster processing speed, lower energy consumption and more biological interpretability, which is expected to approach Strong AI. Reinforcement learning is similar to learning in biology. It is of great significance to study the combination of SNN and RL. We propose the reinforcement learning method of spike distillation network (SDN) with STBP.",
    "original_url": "http://arxiv.org/pdf/2108.10078v1",
    "original_title": "Distilling Neuron Spike with High Temperature in Reinforcement Learning Agents",
    "source": "arxiv",
    "authors": [
      "Ling Zhang",
      "Jian Cao",
      "Yuan Zhang",
      "Bohan Zhou",
      "Shuo Feng"
    ],
    "published": "2021-08-05T07:48:19+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2108.10078v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2108.10078v1_chunk_1",
    "chunk_text": "We propose the reinforcement learning method of spike distillation network (SDN) with STBP. This method uses distillation to effectively avoid the weakness of STBP, which can achieve SOTA performance in classification, and can obtain a smaller, faster convergence and lower power consumption SNN reinforcement learning model. Experiments show that our method can converge faster than traditional SNN reinforcement learning and DNN reinforcement learning methods, about 1000 epochs faster, and obtain SNN 200 times smaller than DNN. We also deploy SDN to the PKU nc64c chip, which proves that SDN has lower power consumption than DNN, and the power consumption of SDN is more than 600 times lower than DNN on large-scale devices. SDN provides a new way of SNN reinforcement learning, and can achieve SOTA performance, which proves the possibility of further development of SNN reinforcement learning.",
    "original_url": "http://arxiv.org/pdf/2108.10078v1",
    "original_title": "Distilling Neuron Spike with High Temperature in Reinforcement Learning Agents",
    "source": "arxiv",
    "authors": [
      "Ling Zhang",
      "Jian Cao",
      "Yuan Zhang",
      "Bohan Zhou",
      "Shuo Feng"
    ],
    "published": "2021-08-05T07:48:19+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2108.10078v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2108.10078v1_chunk_2",
    "chunk_text": "SDN provides a new way of SNN reinforcement learning, and can achieve SOTA performance, which proves the possibility of further development of SNN reinforcement learning.",
    "original_url": "http://arxiv.org/pdf/2108.10078v1",
    "original_title": "Distilling Neuron Spike with High Temperature in Reinforcement Learning Agents",
    "source": "arxiv",
    "authors": [
      "Ling Zhang",
      "Jian Cao",
      "Yuan Zhang",
      "Bohan Zhou",
      "Shuo Feng"
    ],
    "published": "2021-08-05T07:48:19+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2108.10078v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2011.13577v1_chunk_0",
    "chunk_text": "A survey of benchmarking frameworks for reinforcement learning. Reinforcement learning has recently experienced increased prominence in the machine learning community. There are many approaches to solving reinforcement learning problems with new techniques developed constantly. When solving problems using reinforcement learning, there are various difficult challenges to overcome. To ensure progress in the field, benchmarks are important for testing new algorithms and comparing with other approaches.",
    "original_url": "http://arxiv.org/pdf/2011.13577v1",
    "original_title": "A survey of benchmarking frameworks for reinforcement learning",
    "source": "arxiv",
    "authors": [
      "Belinda Stapelberg",
      "Katherine M. Malan"
    ],
    "published": "2020-11-27T06:32:09+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2011.13577v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2011.13577v1_chunk_1",
    "chunk_text": "To ensure progress in the field, benchmarks are important for testing new algorithms and comparing with other approaches. The reproducibility of results for fair comparison is therefore vital in ensuring that improvements are accurately judged. This paper provides an overview of different contributions to reinforcement learning benchmarking and discusses how they can assist researchers to address the challenges facing reinforcement learning. The contributions discussed are the most used and recent in the literature. The paper discusses the contributions in terms of implementation, tasks and provided algorithm implementations with benchmarks.",
    "original_url": "http://arxiv.org/pdf/2011.13577v1",
    "original_title": "A survey of benchmarking frameworks for reinforcement learning",
    "source": "arxiv",
    "authors": [
      "Belinda Stapelberg",
      "Katherine M. Malan"
    ],
    "published": "2020-11-27T06:32:09+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2011.13577v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2011.13577v1_chunk_2",
    "chunk_text": "The paper discusses the contributions in terms of implementation, tasks and provided algorithm implementations with benchmarks. The survey aims to bring attention to the wide range of reinforcement learning benchmarking tasks available and to encourage research to take place in a standardised manner. Additionally, this survey acts as an overview for researchers not familiar with the different tasks that can be used to develop and test new reinforcement learning algorithms.",
    "original_url": "http://arxiv.org/pdf/2011.13577v1",
    "original_title": "A survey of benchmarking frameworks for reinforcement learning",
    "source": "arxiv",
    "authors": [
      "Belinda Stapelberg",
      "Katherine M. Malan"
    ],
    "published": "2020-11-27T06:32:09+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2011.13577v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2305.03360v1_chunk_0",
    "chunk_text": "A Survey on Offline Model-Based Reinforcement Learning. Model-based approaches are becoming increasingly popular in the field of offline reinforcement learning, with high potential in real-world applications due to the model's capability of thoroughly utilizing the large historical datasets available with supervised learning techniques. This paper presents a literature review of recent work in offline model-based reinforcement learning, a field that utilizes model-based approaches in offline reinforcement learning. The survey provides a brief overview of the concepts and recent developments in both offline reinforcement learning and model-based reinforcement learning, and discuss the intersection of the two fields. We then presents key relevant papers in the field of offline model-based reinforcement learning and discuss their methods, particularly their approaches in solving the issue of distributional shift, the main problem faced by all current offline model-based reinforcement learning methods.",
    "original_url": "http://arxiv.org/pdf/2305.03360v1",
    "original_title": "A Survey on Offline Model-Based Reinforcement Learning",
    "source": "arxiv",
    "authors": [
      "Haoyang He"
    ],
    "published": "2023-05-05T08:23:56+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2305.03360v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2305.03360v1_chunk_1",
    "chunk_text": "We then presents key relevant papers in the field of offline model-based reinforcement learning and discuss their methods, particularly their approaches in solving the issue of distributional shift, the main problem faced by all current offline model-based reinforcement learning methods. We further discuss key challenges faced by the field, and suggest possible directions for future work.",
    "original_url": "http://arxiv.org/pdf/2305.03360v1",
    "original_title": "A Survey on Offline Model-Based Reinforcement Learning",
    "source": "arxiv",
    "authors": [
      "Haoyang He"
    ],
    "published": "2023-05-05T08:23:56+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2305.03360v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1907.02140v2_chunk_0",
    "chunk_text": "Integration of Imitation Learning using GAIL and Reinforcement Learning using Task-achievement Rewards via Probabilistic Graphical Model. Integration of reinforcement learning and imitation learning is an important problem that has been studied for a long time in the field of intelligent robotics. Reinforcement learning optimizes policies to maximize the cumulative reward, whereas imitation learning attempts to extract general knowledge about the trajectories demonstrated by experts, i.e., demonstrators. Because each of them has their own drawbacks, methods combining them and compensating for each set of drawbacks have been explored thus far. However, many of the methods are heuristic and do not have a solid theoretical basis.",
    "original_url": "http://arxiv.org/pdf/1907.02140v2",
    "original_title": "Integration of Imitation Learning using GAIL and Reinforcement Learning using Task-achievement Rewards via Probabilistic Graphical Model",
    "source": "arxiv",
    "authors": [
      "Akira Kinose",
      "Tadahiro Taniguchi"
    ],
    "published": "2019-07-03T21:38:48+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1907.02140v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1907.02140v2_chunk_1",
    "chunk_text": "However, many of the methods are heuristic and do not have a solid theoretical basis. In this paper, we present a new theory for integrating reinforcement and imitation learning by extending the probabilistic generative model framework for reinforcement learning, {\\it plan by inference}. We develop a new probabilistic graphical model for reinforcement learning with multiple types of rewards and a probabilistic graphical model for Markov decision processes with multiple optimality emissions (pMDP-MO). Furthermore, we demonstrate that the integrated learning method of reinforcement learning and imitation learning can be formulated as a probabilistic inference of policies on pMDP-MO by considering the output of the discriminator in generative adversarial imitation learning as an additional optimal emission observation. We adapt the generative adversarial imitation learning and task-achievement reward to our proposed framework, achieving significantly better performance than agents trained with reinforcement learning or imitation learning alone.",
    "original_url": "http://arxiv.org/pdf/1907.02140v2",
    "original_title": "Integration of Imitation Learning using GAIL and Reinforcement Learning using Task-achievement Rewards via Probabilistic Graphical Model",
    "source": "arxiv",
    "authors": [
      "Akira Kinose",
      "Tadahiro Taniguchi"
    ],
    "published": "2019-07-03T21:38:48+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1907.02140v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1907.02140v2_chunk_2",
    "chunk_text": "We adapt the generative adversarial imitation learning and task-achievement reward to our proposed framework, achieving significantly better performance than agents trained with reinforcement learning or imitation learning alone. Experiments demonstrate that our framework successfully integrates imitation and reinforcement learning even when the number of demonstrators is only a few.",
    "original_url": "http://arxiv.org/pdf/1907.02140v2",
    "original_title": "Integration of Imitation Learning using GAIL and Reinforcement Learning using Task-achievement Rewards via Probabilistic Graphical Model",
    "source": "arxiv",
    "authors": [
      "Akira Kinose",
      "Tadahiro Taniguchi"
    ],
    "published": "2019-07-03T21:38:48+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1907.02140v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1708.05866v2_chunk_0",
    "chunk_text": "A Brief Survey of Deep Reinforcement Learning. Deep reinforcement learning is poised to revolutionise the field of AI and represents a step towards building autonomous systems with a higher level understanding of the visual world. Currently, deep learning is enabling reinforcement learning to scale to problems that were previously intractable, such as learning to play video games directly from pixels. Deep reinforcement learning algorithms are also applied to robotics, allowing control policies for robots to be learned directly from camera inputs in the real world. In this survey, we begin with an introduction to the general field of reinforcement learning, then progress to the main streams of value-based and policy-based methods.",
    "original_url": "http://arxiv.org/pdf/1708.05866v2",
    "original_title": "A Brief Survey of Deep Reinforcement Learning",
    "source": "arxiv",
    "authors": [
      "Kai Arulkumaran",
      "Marc Peter Deisenroth",
      "Miles Brundage",
      "Anil Anthony Bharath"
    ],
    "published": "2017-08-19T15:55:31+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1708.05866v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1708.05866v2_chunk_1",
    "chunk_text": "In this survey, we begin with an introduction to the general field of reinforcement learning, then progress to the main streams of value-based and policy-based methods. Our survey will cover central algorithms in deep reinforcement learning, including the deep $Q$-network, trust region policy optimisation, and asynchronous advantage actor-critic. In parallel, we highlight the unique advantages of deep neural networks, focusing on visual understanding via reinforcement learning. To conclude, we describe several current areas of research within the field.",
    "original_url": "http://arxiv.org/pdf/1708.05866v2",
    "original_title": "A Brief Survey of Deep Reinforcement Learning",
    "source": "arxiv",
    "authors": [
      "Kai Arulkumaran",
      "Marc Peter Deisenroth",
      "Miles Brundage",
      "Anil Anthony Bharath"
    ],
    "published": "2017-08-19T15:55:31+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1708.05866v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1810.00123v3_chunk_0",
    "chunk_text": "Generalization and Regularization in DQN. Deep reinforcement learning algorithms have shown an impressive ability to learn complex control policies in high-dimensional tasks. However, despite the ever-increasing performance on popular benchmarks, policies learned by deep reinforcement learning algorithms can struggle to generalize when evaluated in remarkably similar environments. In this paper we propose a protocol to evaluate generalization in reinforcement learning through different modes of Atari 2600 games. With that protocol we assess the generalization capabilities of DQN, one of the most traditional deep reinforcement learning algorithms, and we provide evidence suggesting that DQN overspecializes to the training environment.",
    "original_url": "http://arxiv.org/pdf/1810.00123v3",
    "original_title": "Generalization and Regularization in DQN",
    "source": "arxiv",
    "authors": [
      "Jesse Farebrother",
      "Marlos C. Machado",
      "Michael Bowling"
    ],
    "published": "2018-09-29T00:52:34+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1810.00123v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1810.00123v3_chunk_1",
    "chunk_text": "With that protocol we assess the generalization capabilities of DQN, one of the most traditional deep reinforcement learning algorithms, and we provide evidence suggesting that DQN overspecializes to the training environment. We then comprehensively evaluate the impact of dropout and $\\ell_2$ regularization, as well as the impact of reusing learned representations to improve the generalization capabilities of DQN. Despite regularization being largely underutilized in deep reinforcement learning, we show that it can, in fact, help DQN learn more general features. These features can be reused and fine-tuned on similar tasks, considerably improving DQN's sample efficiency.",
    "original_url": "http://arxiv.org/pdf/1810.00123v3",
    "original_title": "Generalization and Regularization in DQN",
    "source": "arxiv",
    "authors": [
      "Jesse Farebrother",
      "Marlos C. Machado",
      "Michael Bowling"
    ],
    "published": "2018-09-29T00:52:34+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1810.00123v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2307.16348v2_chunk_0",
    "chunk_text": "Rating-based Reinforcement Learning. This paper develops a novel rating-based reinforcement learning approach that uses human ratings to obtain human guidance in reinforcement learning. Different from the existing preference-based and ranking-based reinforcement learning paradigms, based on human relative preferences over sample pairs, the proposed rating-based reinforcement learning approach is based on human evaluation of individual trajectories without relative comparisons between sample pairs. The rating-based reinforcement learning approach builds on a new prediction model for human ratings and a novel multi-class loss function. We conduct several experimental studies based on synthetic ratings and real human ratings to evaluate the effectiveness and benefits of the new rating-based reinforcement learning approach.",
    "original_url": "http://arxiv.org/pdf/2307.16348v2",
    "original_title": "Rating-based Reinforcement Learning",
    "source": "arxiv",
    "authors": [
      "Devin White",
      "Mingkang Wu",
      "Ellen Novoseller",
      "Vernon J. Lawhern",
      "Nicholas Waytowich",
      "Yongcan Cao"
    ],
    "published": "2023-07-30T23:54:22+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2307.16348v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2307.16348v2_chunk_1",
    "chunk_text": "We conduct several experimental studies based on synthetic ratings and real human ratings to evaluate the effectiveness and benefits of the new rating-based reinforcement learning approach.",
    "original_url": "http://arxiv.org/pdf/2307.16348v2",
    "original_title": "Rating-based Reinforcement Learning",
    "source": "arxiv",
    "authors": [
      "Devin White",
      "Mingkang Wu",
      "Ellen Novoseller",
      "Vernon J. Lawhern",
      "Nicholas Waytowich",
      "Yongcan Cao"
    ],
    "published": "2023-07-30T23:54:22+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2307.16348v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2003.08445v1_chunk_0",
    "chunk_text": "Placement Optimization with Deep Reinforcement Learning. Placement Optimization is an important problem in systems and chip design, which consists of mapping the nodes of a graph onto a limited set of resources to optimize for an objective, subject to constraints. In this paper, we start by motivating reinforcement learning as a solution to the placement problem. We then give an overview of what deep reinforcement learning is. We next formulate the placement problem as a reinforcement learning problem and show how this problem can be solved with policy gradient optimization.",
    "original_url": "http://arxiv.org/pdf/2003.08445v1",
    "original_title": "Placement Optimization with Deep Reinforcement Learning",
    "source": "arxiv",
    "authors": [
      "Anna Goldie",
      "Azalia Mirhoseini"
    ],
    "published": "2020-03-18T19:20:37+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2003.08445v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2003.08445v1_chunk_1",
    "chunk_text": "We next formulate the placement problem as a reinforcement learning problem and show how this problem can be solved with policy gradient optimization. Finally, we describe lessons we have learned from training deep reinforcement learning policies across a variety of placement optimization problems.",
    "original_url": "http://arxiv.org/pdf/2003.08445v1",
    "original_title": "Placement Optimization with Deep Reinforcement Learning",
    "source": "arxiv",
    "authors": [
      "Anna Goldie",
      "Azalia Mirhoseini"
    ],
    "published": "2020-03-18T19:20:37+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2003.08445v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2206.01233v2_chunk_0",
    "chunk_text": "Equivariant Reinforcement Learning for Quadrotor UAV. This paper presents an equivariant reinforcement learning framework for quadrotor unmanned aerial vehicles. Successful training of reinforcement learning often requires numerous interactions with the environments, which hinders its applicability especially when the available computational resources are limited, or when there is no reliable simulation model. We identified an equivariance property of the quadrotor dynamics such that the dimension of the state required in the training is reduced by one, thereby improving the sampling efficiency of reinforcement learning substantially. This is illustrated by numerical examples with popular reinforcement learning techniques of TD3 and SAC.",
    "original_url": "http://arxiv.org/pdf/2206.01233v2",
    "original_title": "Equivariant Reinforcement Learning for Quadrotor UAV",
    "source": "arxiv",
    "authors": [
      "Beomyeol Yu",
      "Taeyoung Lee"
    ],
    "published": "2022-06-02T18:17:29+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2206.01233v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2206.01233v2_chunk_1",
    "chunk_text": "This is illustrated by numerical examples with popular reinforcement learning techniques of TD3 and SAC.",
    "original_url": "http://arxiv.org/pdf/2206.01233v2",
    "original_title": "Equivariant Reinforcement Learning for Quadrotor UAV",
    "source": "arxiv",
    "authors": [
      "Beomyeol Yu",
      "Taeyoung Lee"
    ],
    "published": "2022-06-02T18:17:29+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2206.01233v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2306.05810v1_chunk_0",
    "chunk_text": "Explaining Reinforcement Learning with Shapley Values. For reinforcement learning systems to be widely adopted, their users must understand and trust them. We present a theoretical analysis of explaining reinforcement learning using Shapley values, following a principled approach from game theory for identifying the contribution of individual players to the outcome of a cooperative game. We call this general framework Shapley Values for Explaining Reinforcement Learning (SVERL). Our analysis exposes the limitations of earlier uses of Shapley values in reinforcement learning.",
    "original_url": "http://arxiv.org/pdf/2306.05810v1",
    "original_title": "Explaining Reinforcement Learning with Shapley Values",
    "source": "arxiv",
    "authors": [
      "Daniel Beechey",
      "Thomas M. S. Smith",
      "Özgür Şimşek"
    ],
    "published": "2023-06-09T10:52:39+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2306.05810v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2306.05810v1_chunk_1",
    "chunk_text": "Our analysis exposes the limitations of earlier uses of Shapley values in reinforcement learning. We then develop an approach that uses Shapley values to explain agent performance. In a variety of domains, SVERL produces meaningful explanations that match and supplement human intuition.",
    "original_url": "http://arxiv.org/pdf/2306.05810v1",
    "original_title": "Explaining Reinforcement Learning with Shapley Values",
    "source": "arxiv",
    "authors": [
      "Daniel Beechey",
      "Thomas M. S. Smith",
      "Özgür Şimşek"
    ],
    "published": "2023-06-09T10:52:39+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2306.05810v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2308.11924v1_chunk_0",
    "chunk_text": "Diverse Policies Converge in Reward-free Markov Decision Processe. Reinforcement learning has achieved great success in many decision-making tasks, and traditional reinforcement learning algorithms are mainly designed for obtaining a single optimal solution. However, recent works show the importance of developing diverse policies, which makes it an emerging research topic. Despite the variety of diversity reinforcement learning algorithms that have emerged, none of them theoretically answer the question of how the algorithm converges and how efficient the algorithm is. In this paper, we provide a unified diversity reinforcement learning framework and investigate the convergence of training diverse policies.",
    "original_url": "http://arxiv.org/pdf/2308.11924v1",
    "original_title": "Diverse Policies Converge in Reward-free Markov Decision Processe",
    "source": "arxiv",
    "authors": [
      "Fanqi Lin",
      "Shiyu Huang",
      "Weiwei Tu"
    ],
    "published": "2023-08-23T05:17:51+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2308.11924v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2308.11924v1_chunk_1",
    "chunk_text": "In this paper, we provide a unified diversity reinforcement learning framework and investigate the convergence of training diverse policies. Under such a framework, we also propose a provably efficient diversity reinforcement learning algorithm. Finally, we verify the effectiveness of our method through numerical experiments.",
    "original_url": "http://arxiv.org/pdf/2308.11924v1",
    "original_title": "Diverse Policies Converge in Reward-free Markov Decision Processe",
    "source": "arxiv",
    "authors": [
      "Fanqi Lin",
      "Shiyu Huang",
      "Weiwei Tu"
    ],
    "published": "2023-08-23T05:17:51+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2308.11924v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2312.12904v1_chunk_0",
    "chunk_text": "PGN: A perturbation generation network against deep reinforcement learning. Deep reinforcement learning has advanced greatly and applied in many areas. In this paper, we explore the vulnerability of deep reinforcement learning by proposing a novel generative model for creating effective adversarial examples to attack the agent. Our proposed model can achieve both targeted attacks and untargeted attacks. Considering the specificity of deep reinforcement learning, we propose the action consistency ratio as a measure of stealthiness, and a new measurement index of effectiveness and stealthiness.",
    "original_url": "http://arxiv.org/pdf/2312.12904v1",
    "original_title": "PGN: A perturbation generation network against deep reinforcement learning",
    "source": "arxiv",
    "authors": [
      "Xiangjuan Li",
      "Feifan Li",
      "Yang Li",
      "Quan Pan"
    ],
    "published": "2023-12-20T10:40:41+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2312.12904v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2312.12904v1_chunk_1",
    "chunk_text": "Considering the specificity of deep reinforcement learning, we propose the action consistency ratio as a measure of stealthiness, and a new measurement index of effectiveness and stealthiness. Experiment results show that our method can ensure the effectiveness and stealthiness of attack compared with other algorithms. Moreover, our methods are considerably faster and thus can achieve rapid and efficient verification of the vulnerability of deep reinforcement learning.",
    "original_url": "http://arxiv.org/pdf/2312.12904v1",
    "original_title": "PGN: A perturbation generation network against deep reinforcement learning",
    "source": "arxiv",
    "authors": [
      "Xiangjuan Li",
      "Feifan Li",
      "Yang Li",
      "Quan Pan"
    ],
    "published": "2023-12-20T10:40:41+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2312.12904v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1906.10025v2_chunk_0",
    "chunk_text": "Modern Deep Reinforcement Learning Algorithms. Recent advances in Reinforcement Learning, grounded on combining classical theoretical results with Deep Learning paradigm, led to breakthroughs in many artificial intelligence tasks and gave birth to Deep Reinforcement Learning (DRL) as a field of research. In this work latest DRL algorithms are reviewed with a focus on their theoretical justification, practical limitations and observed empirical properties.",
    "original_url": "http://arxiv.org/pdf/1906.10025v2",
    "original_title": "Modern Deep Reinforcement Learning Algorithms",
    "source": "arxiv",
    "authors": [
      "Sergey Ivanov",
      "Alexander D'yakonov"
    ],
    "published": "2019-06-24T15:27:51+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1906.10025v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2205.09550v1_chunk_0",
    "chunk_text": "Data Valuation for Offline Reinforcement Learning. The success of deep reinforcement learning (DRL) hinges on the availability of training data, which is typically obtained via a large number of environment interactions. In many real-world scenarios, costs and risks are associated with gathering these data. The field of offline reinforcement learning addresses these issues through outsourcing the collection of data to a domain expert or a carefully monitored program and subsequently searching for a batch-constrained optimal policy. With the emergence of data markets, an alternative to constructing a dataset in-house is to purchase external data.",
    "original_url": "http://arxiv.org/pdf/2205.09550v1",
    "original_title": "Data Valuation for Offline Reinforcement Learning",
    "source": "arxiv",
    "authors": [
      "Amir Abolfazli",
      "Gregory Palmer",
      "Daniel Kudenko"
    ],
    "published": "2022-05-19T13:21:40+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2205.09550v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2205.09550v1_chunk_1",
    "chunk_text": "With the emergence of data markets, an alternative to constructing a dataset in-house is to purchase external data. However, while state-of-the-art offline reinforcement learning approaches have shown a lot of promise, they currently rely on carefully constructed datasets that are well aligned with the intended target domains. This raises questions regarding the transferability and robustness of an offline reinforcement learning agent trained on externally acquired data. In this paper, we empirically evaluate the ability of the current state-of-the-art offline reinforcement learning approaches to coping with the source-target domain mismatch within two MuJoCo environments, finding that current state-of-the-art offline reinforcement learning algorithms underperform in the target domain. To address this, we propose data valuation for offline reinforcement learning (DVORL), which allows us to identify relevant and high-quality transitions, improving the performance and transferability of policies learned by offline reinforcement learning algorithms.",
    "original_url": "http://arxiv.org/pdf/2205.09550v1",
    "original_title": "Data Valuation for Offline Reinforcement Learning",
    "source": "arxiv",
    "authors": [
      "Amir Abolfazli",
      "Gregory Palmer",
      "Daniel Kudenko"
    ],
    "published": "2022-05-19T13:21:40+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2205.09550v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2205.09550v1_chunk_2",
    "chunk_text": "To address this, we propose data valuation for offline reinforcement learning (DVORL), which allows us to identify relevant and high-quality transitions, improving the performance and transferability of policies learned by offline reinforcement learning algorithms. The results show that our method outperforms offline reinforcement learning baselines on two MuJoCo environments.",
    "original_url": "http://arxiv.org/pdf/2205.09550v1",
    "original_title": "Data Valuation for Offline Reinforcement Learning",
    "source": "arxiv",
    "authors": [
      "Amir Abolfazli",
      "Gregory Palmer",
      "Daniel Kudenko"
    ],
    "published": "2022-05-19T13:21:40+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2205.09550v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1810.07862v1_chunk_0",
    "chunk_text": "Applications of Deep Reinforcement Learning in Communications and Networking: A Survey. This paper presents a comprehensive literature review on applications of deep reinforcement learning in communications and networking. Modern networks, e.g., Internet of Things (IoT) and Unmanned Aerial Vehicle (UAV) networks, become more decentralized and autonomous. In such networks, network entities need to make decisions locally to maximize the network performance under uncertainty of network environment. Reinforcement learning has been efficiently used to enable the network entities to obtain the optimal policy including, e.g., decisions or actions, given their states when the state and action spaces are small.",
    "original_url": "http://arxiv.org/pdf/1810.07862v1",
    "original_title": "Applications of Deep Reinforcement Learning in Communications and Networking: A Survey",
    "source": "arxiv",
    "authors": [
      "Nguyen Cong Luong",
      "Dinh Thai Hoang",
      "Shimin Gong",
      "Dusit Niyato",
      "Ping Wang",
      "Ying-Chang Liang",
      "Dong In Kim"
    ],
    "published": "2018-10-18T01:47:19+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1810.07862v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1810.07862v1_chunk_1",
    "chunk_text": "Reinforcement learning has been efficiently used to enable the network entities to obtain the optimal policy including, e.g., decisions or actions, given their states when the state and action spaces are small. However, in complex and large-scale networks, the state and action spaces are usually large, and the reinforcement learning may not be able to find the optimal policy in reasonable time. Therefore, deep reinforcement learning, a combination of reinforcement learning with deep learning, has been developed to overcome the shortcomings. In this survey, we first give a tutorial of deep reinforcement learning from fundamental concepts to advanced models. Then, we review deep reinforcement learning approaches proposed to address emerging issues in communications and networking.",
    "original_url": "http://arxiv.org/pdf/1810.07862v1",
    "original_title": "Applications of Deep Reinforcement Learning in Communications and Networking: A Survey",
    "source": "arxiv",
    "authors": [
      "Nguyen Cong Luong",
      "Dinh Thai Hoang",
      "Shimin Gong",
      "Dusit Niyato",
      "Ping Wang",
      "Ying-Chang Liang",
      "Dong In Kim"
    ],
    "published": "2018-10-18T01:47:19+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1810.07862v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1810.07862v1_chunk_2",
    "chunk_text": "Then, we review deep reinforcement learning approaches proposed to address emerging issues in communications and networking. The issues include dynamic network access, data rate control, wireless caching, data offloading, network security, and connectivity preservation which are all important to next generation networks such as 5G and beyond. Furthermore, we present applications of deep reinforcement learning for traffic routing, resource sharing, and data collection. Finally, we highlight important challenges, open issues, and future research directions of applying deep reinforcement learning.",
    "original_url": "http://arxiv.org/pdf/1810.07862v1",
    "original_title": "Applications of Deep Reinforcement Learning in Communications and Networking: A Survey",
    "source": "arxiv",
    "authors": [
      "Nguyen Cong Luong",
      "Dinh Thai Hoang",
      "Shimin Gong",
      "Dusit Niyato",
      "Ping Wang",
      "Ying-Chang Liang",
      "Dong In Kim"
    ],
    "published": "2018-10-18T01:47:19+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1810.07862v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2209.02954v1_chunk_0",
    "chunk_text": "A Deep Reinforcement Learning Strategy for UAV Autonomous Landing on a Platform. With the development of industry, drones are appearing in various field. In recent years, deep reinforcement learning has made impressive gains in games, and we are committed to applying deep reinforcement learning algorithms to the field of robotics, moving reinforcement learning algorithms from game scenarios to real-world application scenarios. We are inspired by the LunarLander of OpenAI Gym, we decided to make a bold attempt in the field of reinforcement learning to control drones. At present, there is still a lack of work applying reinforcement learning algorithms to robot control, the physical simulation platform related to robot control is only suitable for the verification of classical algorithms, and is not suitable for accessing reinforcement learning algorithms for the training.",
    "original_url": "http://arxiv.org/pdf/2209.02954v1",
    "original_title": "A Deep Reinforcement Learning Strategy for UAV Autonomous Landing on a Platform",
    "source": "arxiv",
    "authors": [
      "Z. Jiang",
      "G. Song"
    ],
    "published": "2022-09-07T06:33:57+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2209.02954v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2209.02954v1_chunk_1",
    "chunk_text": "At present, there is still a lack of work applying reinforcement learning algorithms to robot control, the physical simulation platform related to robot control is only suitable for the verification of classical algorithms, and is not suitable for accessing reinforcement learning algorithms for the training. In this paper, we will face this problem, bridging the gap between physical simulation platforms and intelligent agent, connecting intelligent agents to a physical simulation platform, allowing agents to learn and complete drone flight tasks in a simulator that approximates the real world. We proposed a reinforcement learning framework based on Gazebo that is a kind of physical simulation platform (ROS-RL), and used three continuous action space reinforcement learning algorithms in the framework to dealing with the problem of autonomous landing of drones. Experiments show the effectiveness of the algorithm, the task of autonomous landing of drones based on reinforcement learning achieved full success.",
    "original_url": "http://arxiv.org/pdf/2209.02954v1",
    "original_title": "A Deep Reinforcement Learning Strategy for UAV Autonomous Landing on a Platform",
    "source": "arxiv",
    "authors": [
      "Z. Jiang",
      "G. Song"
    ],
    "published": "2022-09-07T06:33:57+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2209.02954v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1702.06794v1_chunk_0",
    "chunk_text": "Tackling Error Propagation through Reinforcement Learning: A Case of Greedy Dependency Parsing. Error propagation is a common problem in NLP. Reinforcement learning explores erroneous states during training and can therefore be more robust when mistakes are made early in a process. In this paper, we apply reinforcement learning to greedy dependency parsing which is known to suffer from error propagation. Reinforcement learning improves accuracy of both labeled and unlabeled dependencies of the Stanford Neural Dependency Parser, a high performance greedy parser, while maintaining its efficiency.",
    "original_url": "http://arxiv.org/pdf/1702.06794v1",
    "original_title": "Tackling Error Propagation through Reinforcement Learning: A Case of Greedy Dependency Parsing",
    "source": "arxiv",
    "authors": [
      "Minh Le",
      "Antske Fokkens"
    ],
    "published": "2017-02-22T13:49:18+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1702.06794v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1702.06794v1_chunk_1",
    "chunk_text": "Reinforcement learning improves accuracy of both labeled and unlabeled dependencies of the Stanford Neural Dependency Parser, a high performance greedy parser, while maintaining its efficiency. We investigate the portion of errors which are the result of error propagation and confirm that reinforcement learning reduces the occurrence of error propagation.",
    "original_url": "http://arxiv.org/pdf/1702.06794v1",
    "original_title": "Tackling Error Propagation through Reinforcement Learning: A Case of Greedy Dependency Parsing",
    "source": "arxiv",
    "authors": [
      "Minh Le",
      "Antske Fokkens"
    ],
    "published": "2017-02-22T13:49:18+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1702.06794v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2201.03947v1_chunk_0",
    "chunk_text": "Active Reinforcement Learning -- A Roadmap Towards Curious Classifier Systems for Self-Adaptation. Intelligent systems have the ability to improve their behaviour over time taking observations, experiences or explicit feedback into account. Traditional approaches separate the learning problem and make isolated use of techniques from different field of machine learning such as reinforcement learning, active learning, anomaly detection or transfer learning, for instance. In this context, the fundamental reinforcement learning approaches come with several drawbacks that hinder their application to real-world systems: trial-and-error, purely reactive behaviour or isolated problem handling. The idea of this article is to present a concept for alleviating these drawbacks by setting up a research agenda towards what we call \"active reinforcement learning\" in intelligent systems.",
    "original_url": "http://arxiv.org/pdf/2201.03947v1",
    "original_title": "Active Reinforcement Learning -- A Roadmap Towards Curious Classifier Systems for Self-Adaptation",
    "source": "arxiv",
    "authors": [
      "Simon Reichhuber",
      "Sven Tomforde"
    ],
    "published": "2022-01-11T13:50:26+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2201.03947v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2201.03947v1_chunk_1",
    "chunk_text": "The idea of this article is to present a concept for alleviating these drawbacks by setting up a research agenda towards what we call \"active reinforcement learning\" in intelligent systems.",
    "original_url": "http://arxiv.org/pdf/2201.03947v1",
    "original_title": "Active Reinforcement Learning -- A Roadmap Towards Curious Classifier Systems for Self-Adaptation",
    "source": "arxiv",
    "authors": [
      "Simon Reichhuber",
      "Sven Tomforde"
    ],
    "published": "2022-01-11T13:50:26+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2201.03947v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1805.01907v2_chunk_0",
    "chunk_text": "Exploration by Distributional Reinforcement Learning. We propose a framework based on distributional reinforcement learning and recent attempts to combine Bayesian parameter updates with deep reinforcement learning. We show that our proposed framework conceptually unifies multiple previous methods in exploration. We also derive a practical algorithm that achieves efficient exploration on challenging control tasks.",
    "original_url": "http://arxiv.org/pdf/1805.01907v2",
    "original_title": "Exploration by Distributional Reinforcement Learning",
    "source": "arxiv",
    "authors": [
      "Yunhao Tang",
      "Shipra Agrawal"
    ],
    "published": "2018-05-04T18:07:21+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1805.01907v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1806.06798v2_chunk_0",
    "chunk_text": "Implicit Policy for Reinforcement Learning. We introduce Implicit Policy, a general class of expressive policies that can flexibly represent complex action distributions in reinforcement learning, with efficient algorithms to compute entropy regularized policy gradients. We empirically show that, despite its simplicity in implementation, entropy regularization combined with a rich policy class can attain desirable properties displayed under maximum entropy reinforcement learning framework, such as robustness and multi-modality.",
    "original_url": "http://arxiv.org/pdf/1806.06798v2",
    "original_title": "Implicit Policy for Reinforcement Learning",
    "source": "arxiv",
    "authors": [
      "Yunhao Tang",
      "Shipra Agrawal"
    ],
    "published": "2018-06-10T08:24:36+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1806.06798v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2102.05710v1_chunk_0",
    "chunk_text": "Derivative-Free Reinforcement Learning: A Review. Reinforcement learning is about learning agent models that make the best sequential decisions in unknown environments. In an unknown environment, the agent needs to explore the environment while exploiting the collected information, which usually forms a sophisticated problem to solve. Derivative-free optimization, meanwhile, is capable of solving sophisticated problems. It commonly uses a sampling-and-updating framework to iteratively improve the solution, where exploration and exploitation are also needed to be well balanced.",
    "original_url": "http://arxiv.org/pdf/2102.05710v1",
    "original_title": "Derivative-Free Reinforcement Learning: A Review",
    "source": "arxiv",
    "authors": [
      "Hong Qian",
      "Yang Yu"
    ],
    "published": "2021-02-10T19:29:22+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2102.05710v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2102.05710v1_chunk_1",
    "chunk_text": "It commonly uses a sampling-and-updating framework to iteratively improve the solution, where exploration and exploitation are also needed to be well balanced. Therefore, derivative-free optimization deals with a similar core issue as reinforcement learning, and has been introduced in reinforcement learning approaches, under the names of learning classifier systems and neuroevolution/evolutionary reinforcement learning. Although such methods have been developed for decades, recently, derivative-free reinforcement learning exhibits attracting increasing attention. However, recent survey on this topic is still lacking. In this article, we summarize methods of derivative-free reinforcement learning to date, and organize the methods in aspects including parameter updating, model selection, exploration, and parallel/distributed methods.",
    "original_url": "http://arxiv.org/pdf/2102.05710v1",
    "original_title": "Derivative-Free Reinforcement Learning: A Review",
    "source": "arxiv",
    "authors": [
      "Hong Qian",
      "Yang Yu"
    ],
    "published": "2021-02-10T19:29:22+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2102.05710v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2102.05710v1_chunk_2",
    "chunk_text": "In this article, we summarize methods of derivative-free reinforcement learning to date, and organize the methods in aspects including parameter updating, model selection, exploration, and parallel/distributed methods. Moreover, we discuss some current limitations and possible future directions, hoping that this article could bring more attentions to this topic and serve as a catalyst for developing novel and efficient approaches.",
    "original_url": "http://arxiv.org/pdf/2102.05710v1",
    "original_title": "Derivative-Free Reinforcement Learning: A Review",
    "source": "arxiv",
    "authors": [
      "Hong Qian",
      "Yang Yu"
    ],
    "published": "2021-02-10T19:29:22+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2102.05710v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2103.06473v1_chunk_0",
    "chunk_text": "Multi-Task Federated Reinforcement Learning with Adversaries. Reinforcement learning algorithms, just like any other Machine learning algorithm pose a serious threat from adversaries. The adversaries can manipulate the learning algorithm resulting in non-optimal policies. In this paper, we analyze the Multi-task Federated Reinforcement Learning algorithms, where multiple collaborative agents in various environments are trying to maximize the sum of discounted return, in the presence of adversarial agents. We argue that the common attack methods are not guaranteed to carry out a successful attack on Multi-task Federated Reinforcement Learning and propose an adaptive attack method with better attack performance.",
    "original_url": "http://arxiv.org/pdf/2103.06473v1",
    "original_title": "Multi-Task Federated Reinforcement Learning with Adversaries",
    "source": "arxiv",
    "authors": [
      "Aqeel Anwar",
      "Arijit Raychowdhury"
    ],
    "published": "2021-03-11T05:39:52+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2103.06473v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2103.06473v1_chunk_1",
    "chunk_text": "We argue that the common attack methods are not guaranteed to carry out a successful attack on Multi-task Federated Reinforcement Learning and propose an adaptive attack method with better attack performance. Furthermore, we modify the conventional federated reinforcement learning algorithm to address the issue of adversaries that works equally well with and without the adversaries. Experimentation on different small to mid-size reinforcement learning problems show that the proposed attack method outperforms other general attack methods and the proposed modification to federated reinforcement learning algorithm was able to achieve near-optimal policies in the presence of adversarial agents.",
    "original_url": "http://arxiv.org/pdf/2103.06473v1",
    "original_title": "Multi-Task Federated Reinforcement Learning with Adversaries",
    "source": "arxiv",
    "authors": [
      "Aqeel Anwar",
      "Arijit Raychowdhury"
    ],
    "published": "2021-03-11T05:39:52+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2103.06473v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2106.12895v1_chunk_0",
    "chunk_text": "rSoccer: A Framework for Studying Reinforcement Learning in Small and Very Small Size Robot Soccer. Reinforcement learning is an active research area with a vast number of applications in robotics, and the RoboCup competition is an interesting environment for studying and evaluating reinforcement learning methods. A known difficulty in applying reinforcement learning to robotics is the high number of experience samples required, being the use of simulated environments for training the agents followed by transfer learning to real-world (sim-to-real) a viable path. This article introduces an open-source simulator for the IEEE Very Small Size Soccer and the Small Size League optimized for reinforcement learning experiments. We also propose a framework for creating OpenAI Gym environments with a set of benchmarks tasks for evaluating single-agent and multi-agent robot soccer skills.",
    "original_url": "http://arxiv.org/pdf/2106.12895v1",
    "original_title": "rSoccer: A Framework for Studying Reinforcement Learning in Small and Very Small Size Robot Soccer",
    "source": "arxiv",
    "authors": [
      "Felipe B. Martins",
      "Mateus G. Machado",
      "Hansenclever F. Bassani",
      "Pedro H. M. Braga",
      "Edna S. Barros"
    ],
    "published": "2021-06-15T01:30:21+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2106.12895v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2106.12895v1_chunk_1",
    "chunk_text": "We also propose a framework for creating OpenAI Gym environments with a set of benchmarks tasks for evaluating single-agent and multi-agent robot soccer skills. We then demonstrate the learning capabilities of two state-of-the-art reinforcement learning methods as well as their limitations in certain scenarios introduced in this framework. We believe this will make it easier for more teams to compete in these categories using end-to-end reinforcement learning approaches and further develop this research area.",
    "original_url": "http://arxiv.org/pdf/2106.12895v1",
    "original_title": "rSoccer: A Framework for Studying Reinforcement Learning in Small and Very Small Size Robot Soccer",
    "source": "arxiv",
    "authors": [
      "Felipe B. Martins",
      "Mateus G. Machado",
      "Hansenclever F. Bassani",
      "Pedro H. M. Braga",
      "Edna S. Barros"
    ],
    "published": "2021-06-15T01:30:21+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2106.12895v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2008.00766v1_chunk_0",
    "chunk_text": "Tracking the Race Between Deep Reinforcement Learning and Imitation Learning -- Extended Version. Learning-based approaches for solving large sequential decision making problems have become popular in recent years. The resulting agents perform differently and their characteristics depend on those of the underlying learning approach. Here, we consider a benchmark planning problem from the reinforcement learning domain, the Racetrack, to investigate the properties of agents derived from different deep (reinforcement) learning approaches. We compare the performance of deep supervised learning, in particular imitation learning, to reinforcement learning for the Racetrack model.",
    "original_url": "http://arxiv.org/pdf/2008.00766v1",
    "original_title": "Tracking the Race Between Deep Reinforcement Learning and Imitation Learning -- Extended Version",
    "source": "arxiv",
    "authors": [
      "Timo P. Gros",
      "Daniel Höller",
      "Jörg Hoffmann",
      "Verena Wolf"
    ],
    "published": "2020-08-03T10:31:44+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2008.00766v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2008.00766v1_chunk_1",
    "chunk_text": "We compare the performance of deep supervised learning, in particular imitation learning, to reinforcement learning for the Racetrack model. We find that imitation learning yields agents that follow more risky paths. In contrast, the decisions of deep reinforcement learning are more foresighted, i.e., avoid states in which fatal decisions are more likely. Our evaluations show that for this sequential decision making problem, deep reinforcement learning performs best in many aspects even though for imitation learning optimal decisions are considered.",
    "original_url": "http://arxiv.org/pdf/2008.00766v1",
    "original_title": "Tracking the Race Between Deep Reinforcement Learning and Imitation Learning -- Extended Version",
    "source": "arxiv",
    "authors": [
      "Timo P. Gros",
      "Daniel Höller",
      "Jörg Hoffmann",
      "Verena Wolf"
    ],
    "published": "2020-08-03T10:31:44+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2008.00766v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1909.04751v1_chunk_0",
    "chunk_text": "Reinforcement Learning and Video Games. Reinforcement learning has exceeded human-level performance in game playing AI with deep learning methods according to the experiments from DeepMind on Go and Atari games. Deep learning solves high dimension input problems which stop the development of reinforcement for many years. This study uses both two techniques to create several agents with different algorithms that successfully learn to play T-rex Runner. Deep Q network algorithm and three types of improvements are implemented to train the agent.",
    "original_url": "http://arxiv.org/pdf/1909.04751v1",
    "original_title": "Reinforcement Learning and Video Games",
    "source": "arxiv",
    "authors": [
      "Yue Zheng"
    ],
    "published": "2019-09-10T20:51:42+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1909.04751v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1909.04751v1_chunk_1",
    "chunk_text": "Deep Q network algorithm and three types of improvements are implemented to train the agent. The results from some of them are far from satisfactory but others are better than human experts. Batch normalization is a method to solve internal covariate shift problems in deep neural network. The positive influence of this on reinforcement learning has also been proved in this study.",
    "original_url": "http://arxiv.org/pdf/1909.04751v1",
    "original_title": "Reinforcement Learning and Video Games",
    "source": "arxiv",
    "authors": [
      "Yue Zheng"
    ],
    "published": "2019-09-10T20:51:42+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1909.04751v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2109.00157v2_chunk_0",
    "chunk_text": "A Survey of Exploration Methods in Reinforcement Learning. Exploration is an essential component of reinforcement learning algorithms, where agents need to learn how to predict and control unknown and often stochastic environments. Reinforcement learning agents depend crucially on exploration to obtain informative data for the learning process as the lack of enough information could hinder effective learning. In this article, we provide a survey of modern exploration methods in (Sequential) reinforcement learning, as well as a taxonomy of exploration methods.",
    "original_url": "http://arxiv.org/pdf/2109.00157v2",
    "original_title": "A Survey of Exploration Methods in Reinforcement Learning",
    "source": "arxiv",
    "authors": [
      "Susan Amin",
      "Maziar Gomrokchi",
      "Harsh Satija",
      "Herke van Hoof",
      "Doina Precup"
    ],
    "published": "2021-09-01T02:36:14+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2109.00157v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2003.10903v2_chunk_0",
    "chunk_text": "Distributional Reinforcement Learning with Ensembles. It is well known that ensemble methods often provide enhanced performance in reinforcement learning. In this paper, we explore this concept further by using group-aided training within the distributional reinforcement learning paradigm. Specifically, we propose an extension to categorical reinforcement learning, where distributional learning targets are implicitly based on the total information gathered by an ensemble. We empirically show that this may lead to much more robust initial learning, a stronger individual performance level, and good efficiency on a per-sample basis.",
    "original_url": "http://arxiv.org/pdf/2003.10903v2",
    "original_title": "Distributional Reinforcement Learning with Ensembles",
    "source": "arxiv",
    "authors": [
      "Björn Lindenberg",
      "Jonas Nordqvist",
      "Karl-Olof Lindahl"
    ],
    "published": "2020-03-24T14:59:54+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2003.10903v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2003.10903v2_chunk_1",
    "chunk_text": "We empirically show that this may lead to much more robust initial learning, a stronger individual performance level, and good efficiency on a per-sample basis.",
    "original_url": "http://arxiv.org/pdf/2003.10903v2",
    "original_title": "Distributional Reinforcement Learning with Ensembles",
    "source": "arxiv",
    "authors": [
      "Björn Lindenberg",
      "Jonas Nordqvist",
      "Karl-Olof Lindahl"
    ],
    "published": "2020-03-24T14:59:54+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2003.10903v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2106.07798v1_chunk_0",
    "chunk_text": "Poisoning Deep Reinforcement Learning Agents with In-Distribution Triggers. In this paper, we propose a new data poisoning attack and apply it to deep reinforcement learning agents. Our attack centers on what we call in-distribution triggers, which are triggers native to the data distributions the model will be trained on and deployed in. We outline a simple procedure for embedding these, and other, triggers in deep reinforcement learning agents following a multi-task learning paradigm, and demonstrate in three common reinforcement learning environments. We believe that this work has important implications for the security of deep learning models.",
    "original_url": "http://arxiv.org/pdf/2106.07798v1",
    "original_title": "Poisoning Deep Reinforcement Learning Agents with In-Distribution Triggers",
    "source": "arxiv",
    "authors": [
      "Chace Ashcraft",
      "Kiran Karra"
    ],
    "published": "2021-06-14T23:16:41+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2106.07798v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2106.07798v1_chunk_1",
    "chunk_text": "We believe that this work has important implications for the security of deep learning models.",
    "original_url": "http://arxiv.org/pdf/2106.07798v1",
    "original_title": "Poisoning Deep Reinforcement Learning Agents with In-Distribution Triggers",
    "source": "arxiv",
    "authors": [
      "Chace Ashcraft",
      "Kiran Karra"
    ],
    "published": "2021-06-14T23:16:41+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2106.07798v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/cs/9605103v1_chunk_0",
    "chunk_text": "Reinforcement Learning: A Survey. This paper surveys the field of reinforcement learning from a computer-science perspective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the field and a broad selection of current work are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment.",
    "original_url": "http://arxiv.org/pdf/cs/9605103v1",
    "original_title": "Reinforcement Learning: A Survey",
    "source": "arxiv",
    "authors": [
      "L. P. Kaelbling",
      "M. L. Littman",
      "A. W. Moore"
    ],
    "published": "1996-05-01T00:00:00+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/cs/9605103v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/cs/9605103v1_chunk_1",
    "chunk_text": "Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but differs considerably in the details and in the use of the word ``reinforcement.'' The paper discusses central issues of reinforcement learning, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning.",
    "original_url": "http://arxiv.org/pdf/cs/9605103v1",
    "original_title": "Reinforcement Learning: A Survey",
    "source": "arxiv",
    "authors": [
      "L. P. Kaelbling",
      "M. L. Littman",
      "A. W. Moore"
    ],
    "published": "1996-05-01T00:00:00+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/cs/9605103v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1805.00909v3_chunk_0",
    "chunk_text": "Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review. The framework of reinforcement learning or optimal control provides a mathematical formalization of intelligent decision making that is powerful and broadly applicable. While the general form of the reinforcement learning problem enables effective reasoning about uncertainty, the connection between reinforcement learning and inference in probabilistic models is not immediately obvious. However, such a connection has considerable value when it comes to algorithm design: formalizing a problem as probabilistic inference in principle allows us to bring to bear a wide array of approximate inference tools, extend the model in flexible and powerful ways, and reason about compositionality and partial observability. In this article, we will discuss how a generalization of the reinforcement learning or optimal control problem, which is sometimes termed maximum entropy reinforcement learning, is equivalent to exact probabilistic inference in the case of deterministic dynamics, and variational inference in the case of stochastic dynamics.",
    "original_url": "http://arxiv.org/pdf/1805.00909v3",
    "original_title": "Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review",
    "source": "arxiv",
    "authors": [
      "Sergey Levine"
    ],
    "published": "2018-05-02T17:11:20+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1805.00909v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1805.00909v3_chunk_1",
    "chunk_text": "In this article, we will discuss how a generalization of the reinforcement learning or optimal control problem, which is sometimes termed maximum entropy reinforcement learning, is equivalent to exact probabilistic inference in the case of deterministic dynamics, and variational inference in the case of stochastic dynamics. We will present a detailed derivation of this framework, overview prior work that has drawn on this and related ideas to propose new reinforcement learning and control algorithms, and describe perspectives on future research.",
    "original_url": "http://arxiv.org/pdf/1805.00909v3",
    "original_title": "Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review",
    "source": "arxiv",
    "authors": [
      "Sergey Levine"
    ],
    "published": "2018-05-02T17:11:20+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1805.00909v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2009.11403v2_chunk_0",
    "chunk_text": "CertRL: Formalizing Convergence Proofs for Value and Policy Iteration in Coq. Reinforcement learning algorithms solve sequential decision-making problems in probabilistic environments by optimizing for long-term reward. The desire to use reinforcement learning in safety-critical settings inspires a recent line of work on formally constrained reinforcement learning; however, these methods place the implementation of the learning algorithm in their Trusted Computing Base. The crucial correctness property of these implementations is a guarantee that the learning algorithm converges to an optimal policy. This paper begins the work of closing this gap by developing a Coq formalization of two canonical reinforcement learning algorithms: value and policy iteration for finite state Markov decision processes.",
    "original_url": "http://arxiv.org/pdf/2009.11403v2",
    "original_title": "CertRL: Formalizing Convergence Proofs for Value and Policy Iteration in Coq",
    "source": "arxiv",
    "authors": [
      "Koundinya Vajjha",
      "Avraham Shinnar",
      "Vasily Pestun",
      "Barry Trager",
      "Nathan Fulton"
    ],
    "published": "2020-09-23T22:28:17+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2009.11403v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2009.11403v2_chunk_1",
    "chunk_text": "This paper begins the work of closing this gap by developing a Coq formalization of two canonical reinforcement learning algorithms: value and policy iteration for finite state Markov decision processes. The central results are a formalization of Bellman's optimality principle and its proof, which uses a contraction property of Bellman optimality operator to establish that a sequence converges in the infinite horizon limit. The CertRL development exemplifies how the Giry monad and mechanized metric coinduction streamline optimality proofs for reinforcement learning algorithms. The CertRL library provides a general framework for proving properties about Markov decision processes and reinforcement learning algorithms, paving the way for further work on formalization of reinforcement learning algorithms.",
    "original_url": "http://arxiv.org/pdf/2009.11403v2",
    "original_title": "CertRL: Formalizing Convergence Proofs for Value and Policy Iteration in Coq",
    "source": "arxiv",
    "authors": [
      "Koundinya Vajjha",
      "Avraham Shinnar",
      "Vasily Pestun",
      "Barry Trager",
      "Nathan Fulton"
    ],
    "published": "2020-09-23T22:28:17+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2009.11403v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2202.12896v1_chunk_0",
    "chunk_text": "Photonic reinforcement learning based on optoelectronic reservoir computing. Reinforcement learning has been intensively investigated and developed in artificial intelligence in the absence of training data, such as autonomous driving vehicles, robot control, internet advertising, and elastic optical networks. However, the computational cost of reinforcement learning with deep neural networks is extremely high and reducing the learning cost is a challenging issue. We propose a photonic on-line implementation of reinforcement learning using optoelectronic delay-based reservoir computing, both experimentally and numerically. In the proposed scheme, we accelerate reinforcement learning at a rate of several megahertz because there is no required learning process for the internal connection weights in reservoir computing.",
    "original_url": "http://arxiv.org/pdf/2202.12896v1",
    "original_title": "Photonic reinforcement learning based on optoelectronic reservoir computing",
    "source": "arxiv",
    "authors": [
      "Kazutaka Kanno",
      "Atsushi Uchida"
    ],
    "published": "2022-02-25T09:38:52+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2202.12896v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2202.12896v1_chunk_1",
    "chunk_text": "In the proposed scheme, we accelerate reinforcement learning at a rate of several megahertz because there is no required learning process for the internal connection weights in reservoir computing. We perform two benchmark tasks, CartPole-v0 and MountanCar-v0 tasks, to evaluate the proposed scheme. Our results represent the first hardware implementation of reinforcement learning based on photonic reservoir computing and pave the way for fast and efficient reinforcement learning as a novel photonic accelerator.",
    "original_url": "http://arxiv.org/pdf/2202.12896v1",
    "original_title": "Photonic reinforcement learning based on optoelectronic reservoir computing",
    "source": "arxiv",
    "authors": [
      "Kazutaka Kanno",
      "Atsushi Uchida"
    ],
    "published": "2022-02-25T09:38:52+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2202.12896v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2210.17368v1_chunk_0",
    "chunk_text": "Teacher-student curriculum learning for reinforcement learning. Reinforcement learning (rl) is a popular paradigm for sequential decision making problems. The past decade's advances in rl have led to breakthroughs in many challenging domains such as video games, board games, robotics, and chip design. The sample inefficiency of deep reinforcement learning methods is a significant obstacle when applying rl to real-world problems. Transfer learning has been applied to reinforcement learning such that the knowledge gained in one task can be applied when training in a new task.",
    "original_url": "http://arxiv.org/pdf/2210.17368v1",
    "original_title": "Teacher-student curriculum learning for reinforcement learning",
    "source": "arxiv",
    "authors": [
      "Yanick Schraner"
    ],
    "published": "2022-10-31T14:45:39+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2210.17368v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2210.17368v1_chunk_1",
    "chunk_text": "Transfer learning has been applied to reinforcement learning such that the knowledge gained in one task can be applied when training in a new task. Curriculum learning is concerned with sequencing tasks or data samples such that knowledge can be transferred between those tasks to learn a target task that would otherwise be too difficult to solve. Designing a curriculum that improves sample efficiency is a complex problem. In this thesis, we propose a teacher-student curriculum learning setting where we simultaneously train a teacher that selects tasks for the student while the student learns how to solve the selected task. Our method is independent of human domain knowledge and manual curriculum design.",
    "original_url": "http://arxiv.org/pdf/2210.17368v1",
    "original_title": "Teacher-student curriculum learning for reinforcement learning",
    "source": "arxiv",
    "authors": [
      "Yanick Schraner"
    ],
    "published": "2022-10-31T14:45:39+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2210.17368v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2210.17368v1_chunk_2",
    "chunk_text": "Our method is independent of human domain knowledge and manual curriculum design. We evaluated our methods on two reinforcement learning benchmarks: grid world and the challenging Google Football environment. With our method, we can improve the sample efficiency and generality of the student compared to tabula-rasa reinforcement learning.",
    "original_url": "http://arxiv.org/pdf/2210.17368v1",
    "original_title": "Teacher-student curriculum learning for reinforcement learning",
    "source": "arxiv",
    "authors": [
      "Yanick Schraner"
    ],
    "published": "2022-10-31T14:45:39+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2210.17368v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2212.08232v1_chunk_0",
    "chunk_text": "Offline Robot Reinforcement Learning with Uncertainty-Guided Human Expert Sampling. Recent advances in batch (offline) reinforcement learning have shown promising results in learning from available offline data and proved offline reinforcement learning to be an essential toolkit in learning control policies in a model-free setting. An offline reinforcement learning algorithm applied to a dataset collected by a suboptimal non-learning-based algorithm can result in a policy that outperforms the behavior agent used to collect the data. Such a scenario is frequent in robotics, where existing automation is collecting operational data. Although offline learning techniques can learn from data generated by a sub-optimal behavior agent, there is still an opportunity to improve the sample complexity of existing offline reinforcement learning algorithms by strategically introducing human demonstration data into the training process.",
    "original_url": "http://arxiv.org/pdf/2212.08232v1",
    "original_title": "Offline Robot Reinforcement Learning with Uncertainty-Guided Human Expert Sampling",
    "source": "arxiv",
    "authors": [
      "Ashish Kumar",
      "Ilya Kuzovkin"
    ],
    "published": "2022-12-16T01:41:59+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2212.08232v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2212.08232v1_chunk_1",
    "chunk_text": "Although offline learning techniques can learn from data generated by a sub-optimal behavior agent, there is still an opportunity to improve the sample complexity of existing offline reinforcement learning algorithms by strategically introducing human demonstration data into the training process. To this end, we propose a novel approach that uses uncertainty estimation to trigger the injection of human demonstration data and guide policy training towards optimal behavior while reducing overall sample complexity. Our experiments show that this approach is more sample efficient when compared to a naive way of combining expert data with data collected from a sub-optimal agent. We augmented an existing offline reinforcement learning algorithm Conservative Q-Learning with our approach and performed experiments on data collected from MuJoCo and OffWorld Gym learning environments.",
    "original_url": "http://arxiv.org/pdf/2212.08232v1",
    "original_title": "Offline Robot Reinforcement Learning with Uncertainty-Guided Human Expert Sampling",
    "source": "arxiv",
    "authors": [
      "Ashish Kumar",
      "Ilya Kuzovkin"
    ],
    "published": "2022-12-16T01:41:59+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2212.08232v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2008.02708v1_chunk_0",
    "chunk_text": "Deep reinforcement learning to detect brain lesions on MRI: a proof-of-concept application of reinforcement learning to medical images. Purpose: AI in radiology is hindered chiefly by: 1) Requiring large annotated data sets. 2) Non-generalizability that limits deployment to new scanners / institutions. And 3) Inadequate explainability and interpretability. We believe that reinforcement learning can address all three shortcomings, with robust and intuitive algorithms trainable on small datasets.",
    "original_url": "http://arxiv.org/pdf/2008.02708v1",
    "original_title": "Deep reinforcement learning to detect brain lesions on MRI: a proof-of-concept application of reinforcement learning to medical images",
    "source": "arxiv",
    "authors": [
      "Joseph Stember",
      "Hrithwik Shalu"
    ],
    "published": "2020-08-06T15:26:28+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2008.02708v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2008.02708v1_chunk_1",
    "chunk_text": "We believe that reinforcement learning can address all three shortcomings, with robust and intuitive algorithms trainable on small datasets. To the best of our knowledge, reinforcement learning has not been directly applied to computer vision tasks for radiological images. In this proof-of-principle work, we train a deep reinforcement learning network to predict brain tumor location. Materials and Methods: Using the BraTS brain tumor imaging database, we trained a deep Q network on 70 post-contrast T1-weighted 2D image slices. We did so in concert with image exploration, with rewards and punishments designed to localize lesions.",
    "original_url": "http://arxiv.org/pdf/2008.02708v1",
    "original_title": "Deep reinforcement learning to detect brain lesions on MRI: a proof-of-concept application of reinforcement learning to medical images",
    "source": "arxiv",
    "authors": [
      "Joseph Stember",
      "Hrithwik Shalu"
    ],
    "published": "2020-08-06T15:26:28+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2008.02708v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2008.02708v1_chunk_2",
    "chunk_text": "We did so in concert with image exploration, with rewards and punishments designed to localize lesions. To compare with supervised deep learning, we trained a keypoint detection convolutional neural network on the same 70 images. We applied both approaches to a separate 30 image testing set. Results: Reinforcement learning predictions consistently improved during training, whereas those of supervised deep learning quickly diverged. Reinforcement learning predicted testing set lesion locations with 85% accuracy, compared to roughly 7% accuracy for the supervised deep network.",
    "original_url": "http://arxiv.org/pdf/2008.02708v1",
    "original_title": "Deep reinforcement learning to detect brain lesions on MRI: a proof-of-concept application of reinforcement learning to medical images",
    "source": "arxiv",
    "authors": [
      "Joseph Stember",
      "Hrithwik Shalu"
    ],
    "published": "2020-08-06T15:26:28+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2008.02708v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2008.02708v1_chunk_3",
    "chunk_text": "Reinforcement learning predicted testing set lesion locations with 85% accuracy, compared to roughly 7% accuracy for the supervised deep network. Conclusion: Reinforcement learning predicted lesions with high accuracy, which is unprecedented for such a small training set. We believe that reinforcement learning can propel radiology AI well past the inherent limitations of supervised deep learning, with more clinician-driven research and finally toward true clinical applicability.",
    "original_url": "http://arxiv.org/pdf/2008.02708v1",
    "original_title": "Deep reinforcement learning to detect brain lesions on MRI: a proof-of-concept application of reinforcement learning to medical images",
    "source": "arxiv",
    "authors": [
      "Joseph Stember",
      "Hrithwik Shalu"
    ],
    "published": "2020-08-06T15:26:28+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2008.02708v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1611.00862v1_chunk_0",
    "chunk_text": "Quantile Reinforcement Learning. In reinforcement learning, the standard criterion to evaluate policies in a state is the expectation of (discounted) sum of rewards. However, this criterion may not always be suitable, we consider an alternative criterion based on the notion of quantiles. In the case of episodic reinforcement learning problems, we propose an algorithm based on stochastic approximation with two timescales. We evaluate our proposition on a simple model of the TV show, Who wants to be a millionaire.",
    "original_url": "http://arxiv.org/pdf/1611.00862v1",
    "original_title": "Quantile Reinforcement Learning",
    "source": "arxiv",
    "authors": [
      "Hugo Gilbert",
      "Paul Weng"
    ],
    "published": "2016-11-03T02:28:53+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1611.00862v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1611.00862v1_chunk_1",
    "chunk_text": "We evaluate our proposition on a simple model of the TV show, Who wants to be a millionaire.",
    "original_url": "http://arxiv.org/pdf/1611.00862v1",
    "original_title": "Quantile Reinforcement Learning",
    "source": "arxiv",
    "authors": [
      "Hugo Gilbert",
      "Paul Weng"
    ],
    "published": "2016-11-03T02:28:53+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1611.00862v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2210.05650v1_chunk_0",
    "chunk_text": "Regret Bounds for Risk-Sensitive Reinforcement Learning. In safety-critical applications of reinforcement learning such as healthcare and robotics, it is often desirable to optimize risk-sensitive objectives that account for tail outcomes rather than expected reward. We prove the first regret bounds for reinforcement learning under a general class of risk-sensitive objectives including the popular CVaR objective. Our theory is based on a novel characterization of the CVaR objective as well as a novel optimistic MDP construction.",
    "original_url": "http://arxiv.org/pdf/2210.05650v1",
    "original_title": "Regret Bounds for Risk-Sensitive Reinforcement Learning",
    "source": "arxiv",
    "authors": [
      "O. Bastani",
      "Y. J. Ma",
      "E. Shen",
      "W. Xu"
    ],
    "published": "2022-10-11T17:49:01+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2210.05650v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2410.19189v1_chunk_0",
    "chunk_text": "Reinforcement Learning the Chromatic Symmetric Function. We propose a conjectural counting formula for the coefficients of the chromatic symmetric function of unit interval graphs using reinforcement learning. The formula counts specific disjoint cycle-tuples in the graphs, referred to as Eschers, which satisfy certain concatenation conditions. These conditions are identified by a reinforcement learning model and are independent of the particular unit interval graph, resulting a universal counting expression.",
    "original_url": "http://arxiv.org/pdf/2410.19189v1",
    "original_title": "Reinforcement Learning the Chromatic Symmetric Function",
    "source": "arxiv",
    "authors": [
      "Gergely Bérczi",
      "Jonas Klüver"
    ],
    "published": "2024-10-24T22:45:01+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2410.19189v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2502.07978v1_chunk_0",
    "chunk_text": "A Survey of In-Context Reinforcement Learning. Reinforcement learning (RL) agents typically optimize their policies by performing expensive backward passes to update their network parameters. However, some agents can solve new tasks without updating any parameters by simply conditioning on additional context such as their action-observation histories. This paper surveys work on such behavior, known as in-context reinforcement learning.",
    "original_url": "http://arxiv.org/pdf/2502.07978v1",
    "original_title": "A Survey of In-Context Reinforcement Learning",
    "source": "arxiv",
    "authors": [
      "Amir Moeini",
      "Jiuqi Wang",
      "Jacob Beck",
      "Ethan Blaser",
      "Shimon Whiteson",
      "Rohan Chandra",
      "Shangtong Zhang"
    ],
    "published": "2025-02-11T21:52:19+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2502.07978v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/cond-mat/0301627v1_chunk_0",
    "chunk_text": "Combining Hebbian and reinforcement learning in a minibrain model. A toy model of a neural network in which both Hebbian learning and reinforcement learning occur is studied. The problem of `path interference', which makes that the neural net quickly forgets previously learned input-output relations is tackled by adding a Hebbian term (proportional to the learning rate $\\eta$) to the reinforcement term (proportional to $\\rho$) in the learning rule. It is shown that the number of learning steps is reduced considerably if $1/4 < \\eta/\\rho < 1/2$, i.e., if the Hebbian term is neither too small nor too large compared to the reinforcement term.",
    "original_url": "http://arxiv.org/pdf/cond-mat/0301627v1",
    "original_title": "Combining Hebbian and reinforcement learning in a minibrain model",
    "source": "arxiv",
    "authors": [
      "R. J. C. Bosman",
      "W. A. van Leeuwen",
      "B. Wemmenhove"
    ],
    "published": "2003-01-31T16:33:17+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/cond-mat/0301627v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1609.03348v4_chunk_0",
    "chunk_text": "A Threshold-based Scheme for Reinforcement Learning in Neural Networks. A generic and scalable Reinforcement Learning scheme for Artificial Neural Networks is presented, providing a general purpose learning machine. By reference to a node threshold three features are described 1) A mechanism for Primary Reinforcement, capable of solving linearly inseparable problems 2) The learning scheme is extended to include a mechanism for Conditioned Reinforcement, capable of forming long term strategy 3) The learning scheme is modified to use a threshold-based deep learning algorithm, providing a robust and biologically inspired alternative to backpropagation. The model may be used for supervised as well as unsupervised training regimes.",
    "original_url": "http://arxiv.org/pdf/1609.03348v4",
    "original_title": "A Threshold-based Scheme for Reinforcement Learning in Neural Networks",
    "source": "arxiv",
    "authors": [
      "Thomas H. Ward"
    ],
    "published": "2016-09-12T11:23:20+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1609.03348v4"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1712.04101v1_chunk_0",
    "chunk_text": "Deep Reinforcement Learning Boosted by External Knowledge. Recent improvements in deep reinforcement learning have allowed to solve problems in many 2D domains such as Atari games. However, in complex 3D environments, numerous learning episodes are required which may be too time consuming or even impossible especially in real-world scenarios. We present a new architecture to combine external knowledge and deep reinforcement learning using only visual input. A key concept of our system is augmenting image input by adding environment feature information and combining two sources of decision.",
    "original_url": "http://arxiv.org/pdf/1712.04101v1",
    "original_title": "Deep Reinforcement Learning Boosted by External Knowledge",
    "source": "arxiv",
    "authors": [
      "Nicolas Bougie",
      "Ryutaro Ichise"
    ],
    "published": "2017-12-12T02:19:25+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1712.04101v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1712.04101v1_chunk_1",
    "chunk_text": "A key concept of our system is augmenting image input by adding environment feature information and combining two sources of decision. We evaluate the performances of our method in a 3D partially-observable environment from the Microsoft Malmo platform. Experimental evaluation exhibits higher performance and faster learning compared to a single reinforcement learning model.",
    "original_url": "http://arxiv.org/pdf/1712.04101v1",
    "original_title": "Deep Reinforcement Learning Boosted by External Knowledge",
    "source": "arxiv",
    "authors": [
      "Nicolas Bougie",
      "Ryutaro Ichise"
    ],
    "published": "2017-12-12T02:19:25+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1712.04101v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1903.05196v2_chunk_0",
    "chunk_text": "A Review of Reinforcement Learning for Autonomous Building Energy Management. The area of building energy management has received a significant amount of interest in recent years. This area is concerned with combining advancements in sensor technologies, communications and advanced control algorithms to optimize energy utilization. Reinforcement learning is one of the most prominent machine learning algorithms used for control problems and has had many successful applications in the area of building energy management. This research gives a comprehensive review of the literature relating to the application of reinforcement learning to developing autonomous building energy management systems.",
    "original_url": "http://arxiv.org/pdf/1903.05196v2",
    "original_title": "A Review of Reinforcement Learning for Autonomous Building Energy Management",
    "source": "arxiv",
    "authors": [
      "Karl Mason",
      "Santiago Grijalva"
    ],
    "published": "2019-03-12T20:38:54+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1903.05196v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1903.05196v2_chunk_1",
    "chunk_text": "This research gives a comprehensive review of the literature relating to the application of reinforcement learning to developing autonomous building energy management systems. The main direction for future research and challenges in reinforcement learning are also outlined.",
    "original_url": "http://arxiv.org/pdf/1903.05196v2",
    "original_title": "A Review of Reinforcement Learning for Autonomous Building Energy Management",
    "source": "arxiv",
    "authors": [
      "Karl Mason",
      "Santiago Grijalva"
    ],
    "published": "2019-03-12T20:38:54+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1903.05196v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2003.06066v1_chunk_0",
    "chunk_text": "Sample Efficient Reinforcement Learning through Learning from Demonstrations in Minecraft. Sample inefficiency of deep reinforcement learning methods is a major obstacle for their use in real-world applications. In this work, we show how human demonstrations can improve final performance of agents on the Minecraft minigame ObtainDiamond with only 8M frames of environment interaction. We propose a training procedure where policy networks are first trained on human data and later fine-tuned by reinforcement learning. Using a policy exploitation mechanism, experience replay and an additional loss against catastrophic forgetting, our best agent was able to achieve a mean score of 48.",
    "original_url": "http://arxiv.org/pdf/2003.06066v1",
    "original_title": "Sample Efficient Reinforcement Learning through Learning from Demonstrations in Minecraft",
    "source": "arxiv",
    "authors": [
      "Christian Scheller",
      "Yanick Schraner",
      "Manfred Vogel"
    ],
    "published": "2020-03-12T23:46:16+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2003.06066v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2003.06066v1_chunk_1",
    "chunk_text": "Using a policy exploitation mechanism, experience replay and an additional loss against catastrophic forgetting, our best agent was able to achieve a mean score of 48. Our proposed solution placed 3rd in the NeurIPS MineRL Competition for Sample-Efficient Reinforcement Learning.",
    "original_url": "http://arxiv.org/pdf/2003.06066v1",
    "original_title": "Sample Efficient Reinforcement Learning through Learning from Demonstrations in Minecraft",
    "source": "arxiv",
    "authors": [
      "Christian Scheller",
      "Yanick Schraner",
      "Manfred Vogel"
    ],
    "published": "2020-03-12T23:46:16+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2003.06066v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2210.15515v1_chunk_0",
    "chunk_text": "Meta-Reinforcement Learning Using Model Parameters. In meta-reinforcement learning, an agent is trained in multiple different environments and attempts to learn a meta-policy that can efficiently adapt to a new environment. This paper presents RAMP, a Reinforcement learning Agent using Model Parameters that utilizes the idea that a neural network trained to predict environment dynamics encapsulates the environment information. RAMP is constructed in two phases: in the first phase, a multi-environment parameterized dynamic model is learned. In the second phase, the model parameters of the dynamic model are used as context for the multi-environment policy of the model-free reinforcement learning agent.",
    "original_url": "http://arxiv.org/pdf/2210.15515v1",
    "original_title": "Meta-Reinforcement Learning Using Model Parameters",
    "source": "arxiv",
    "authors": [
      "Gabriel Hartmann",
      "Amos Azaria"
    ],
    "published": "2022-10-27T14:54:06+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2210.15515v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2210.15515v1_chunk_1",
    "chunk_text": "In the second phase, the model parameters of the dynamic model are used as context for the multi-environment policy of the model-free reinforcement learning agent.",
    "original_url": "http://arxiv.org/pdf/2210.15515v1",
    "original_title": "Meta-Reinforcement Learning Using Model Parameters",
    "source": "arxiv",
    "authors": [
      "Gabriel Hartmann",
      "Amos Azaria"
    ],
    "published": "2022-10-27T14:54:06+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2210.15515v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2305.06137v3_chunk_0",
    "chunk_text": "A proof of convergence of inverse reinforcement learning for multi-objective optimization. We show the convergence of Wasserstein inverse reinforcement learning for multi-objective optimizations with the projective subgradient method by formulating an inverse problem of the multi-objective optimization problem. In addition, we prove convergence of inverse reinforcement learning (maximum entropy inverse reinforcement learning, guided cost learning) with gradient descent and the projective subgradient method.",
    "original_url": "http://arxiv.org/pdf/2305.06137v3",
    "original_title": "A proof of convergence of inverse reinforcement learning for multi-objective optimization",
    "source": "arxiv",
    "authors": [
      "Akira Kitaoka",
      "Riki Eto"
    ],
    "published": "2023-05-10T13:41:24+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2305.06137v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2203.12048v1_chunk_0",
    "chunk_text": "Review of Metrics to Measure the Stability, Robustness and Resilience of Reinforcement Learning. Reinforcement learning has received significant interest in recent years, due primarily to the successes of deep reinforcement learning at solving many challenging tasks such as playing Chess, Go and online computer games. However, with the increasing focus on reinforcement learning, applications outside of gaming and simulated environments require understanding the robustness, stability, and resilience of reinforcement learning methods. To this end, we conducted a comprehensive literature review to characterize the available literature on these three behaviors as they pertain to reinforcement learning. We classify the quantitative and theoretical approaches used to indicate or measure robustness, stability, and resilience behaviors.",
    "original_url": "http://arxiv.org/pdf/2203.12048v1",
    "original_title": "Review of Metrics to Measure the Stability, Robustness and Resilience of Reinforcement Learning",
    "source": "arxiv",
    "authors": [
      "Laura L. Pullum"
    ],
    "published": "2022-03-22T21:15:01+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2203.12048v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2203.12048v1_chunk_1",
    "chunk_text": "We classify the quantitative and theoretical approaches used to indicate or measure robustness, stability, and resilience behaviors. In addition, we identified the action or event to which the quantitative approaches were attempting to be stable, robust, or resilient. Finally, we provide a decision tree useful for selecting metrics to quantify the behaviors. We believe that this is the first comprehensive review of stability, robustness and resilience specifically geared towards reinforcement learning.",
    "original_url": "http://arxiv.org/pdf/2203.12048v1",
    "original_title": "Review of Metrics to Measure the Stability, Robustness and Resilience of Reinforcement Learning",
    "source": "arxiv",
    "authors": [
      "Laura L. Pullum"
    ],
    "published": "2022-03-22T21:15:01+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2203.12048v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2309.00773v1_chunk_0",
    "chunk_text": "Deep Reinforcement Learning in Surgical Robotics: Enhancing the Automation Level. Surgical robotics is a rapidly evolving field that is transforming the landscape of surgeries. Surgical robots have been shown to enhance precision, minimize invasiveness, and alleviate surgeon fatigue. One promising area of research in surgical robotics is the use of reinforcement learning to enhance the automation level. Reinforcement learning is a type of machine learning that involves training an agent to make decisions based on rewards and punishments.",
    "original_url": "http://arxiv.org/pdf/2309.00773v1",
    "original_title": "Deep Reinforcement Learning in Surgical Robotics: Enhancing the Automation Level",
    "source": "arxiv",
    "authors": [
      "Cheng Qian",
      "Hongliang Ren"
    ],
    "published": "2023-09-02T01:04:31+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2309.00773v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2309.00773v1_chunk_1",
    "chunk_text": "Reinforcement learning is a type of machine learning that involves training an agent to make decisions based on rewards and punishments. This literature review aims to comprehensively analyze existing research on reinforcement learning in surgical robotics. The review identified various applications of reinforcement learning in surgical robotics, including pre-operative, intra-body, and percutaneous procedures, listed the typical studies, and compared their methodologies and results. The findings show that reinforcement learning has great potential to improve the autonomy of surgical robots. Reinforcement learning can teach robots to perform complex surgical tasks, such as suturing and tissue manipulation.",
    "original_url": "http://arxiv.org/pdf/2309.00773v1",
    "original_title": "Deep Reinforcement Learning in Surgical Robotics: Enhancing the Automation Level",
    "source": "arxiv",
    "authors": [
      "Cheng Qian",
      "Hongliang Ren"
    ],
    "published": "2023-09-02T01:04:31+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2309.00773v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2309.00773v1_chunk_2",
    "chunk_text": "Reinforcement learning can teach robots to perform complex surgical tasks, such as suturing and tissue manipulation. It can also improve the accuracy and precision of surgical robots, making them more effective at performing surgeries.",
    "original_url": "http://arxiv.org/pdf/2309.00773v1",
    "original_title": "Deep Reinforcement Learning in Surgical Robotics: Enhancing the Automation Level",
    "source": "arxiv",
    "authors": [
      "Cheng Qian",
      "Hongliang Ren"
    ],
    "published": "2023-09-02T01:04:31+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2309.00773v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2410.03706v1_chunk_0",
    "chunk_text": "Topological Foundations of Reinforcement Learning. The goal of this work is to serve as a foundation for deep studies of the topology of state, action, and policy spaces in reinforcement learning. By studying these spaces from a mathematical perspective, we expect to gain more insight into how to build better algorithms to solve decision problems. Therefore, we focus on presenting the connection between the Banach fixed point theorem and the convergence of reinforcement learning algorithms, and we illustrate how the insights gained from this can practically help in designing more efficient algorithms. Before doing so, however, we first introduce relevant concepts such as metric spaces, normed spaces and Banach spaces for better understanding, before expressing the entire reinforcement learning problem in terms of Markov decision processes.",
    "original_url": "http://arxiv.org/pdf/2410.03706v1",
    "original_title": "Topological Foundations of Reinforcement Learning",
    "source": "arxiv",
    "authors": [
      "David Krame Kadurha"
    ],
    "published": "2024-09-25T21:21:23+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2410.03706v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2410.03706v1_chunk_1",
    "chunk_text": "Before doing so, however, we first introduce relevant concepts such as metric spaces, normed spaces and Banach spaces for better understanding, before expressing the entire reinforcement learning problem in terms of Markov decision processes. This allows us to properly introduce the Banach contraction principle in a language suitable for reinforcement learning, and to write the Bellman equations in terms of operators on Banach spaces to show why reinforcement learning algorithms converge. Finally, we show how the insights gained from the mathematical study of convergence are helpful in reasoning about the best ways to make reinforcement learning algorithms more efficient.",
    "original_url": "http://arxiv.org/pdf/2410.03706v1",
    "original_title": "Topological Foundations of Reinforcement Learning",
    "source": "arxiv",
    "authors": [
      "David Krame Kadurha"
    ],
    "published": "2024-09-25T21:21:23+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2410.03706v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2007.01544v2_chunk_0",
    "chunk_text": "A Conceptual Framework for Externally-influenced Agents: An Assisted Reinforcement Learning Review. A long-term goal of reinforcement learning agents is to be able to perform tasks in complex real-world scenarios. The use of external information is one way of scaling agents to more complex problems. However, there is a general lack of collaboration or interoperability between different approaches using external information. In this work, while reviewing externally-influenced methods, we propose a conceptual framework and taxonomy for assisted reinforcement learning, aimed at fostering collaboration by classifying and comparing various methods that use external information in the learning process.",
    "original_url": "http://arxiv.org/pdf/2007.01544v2",
    "original_title": "A Conceptual Framework for Externally-influenced Agents: An Assisted Reinforcement Learning Review",
    "source": "arxiv",
    "authors": [
      "Adam Bignold",
      "Francisco Cruz",
      "Matthew E. Taylor",
      "Tim Brys",
      "Richard Dazeley",
      "Peter Vamplew",
      "Cameron Foale"
    ],
    "published": "2020-07-03T08:07:31+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2007.01544v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2007.01544v2_chunk_1",
    "chunk_text": "In this work, while reviewing externally-influenced methods, we propose a conceptual framework and taxonomy for assisted reinforcement learning, aimed at fostering collaboration by classifying and comparing various methods that use external information in the learning process. The proposed taxonomy details the relationship between the external information source and the learner agent, highlighting the process of information decomposition, structure, retention, and how it can be used to influence agent learning. As well as reviewing state-of-the-art methods, we identify current streams of reinforcement learning that use external information in order to improve the agent's performance and its decision-making process. These include heuristic reinforcement learning, interactive reinforcement learning, learning from demonstration, transfer learning, and learning from multiple sources, among others. These streams of reinforcement learning operate with the shared objective of scaffolding the learner agent.",
    "original_url": "http://arxiv.org/pdf/2007.01544v2",
    "original_title": "A Conceptual Framework for Externally-influenced Agents: An Assisted Reinforcement Learning Review",
    "source": "arxiv",
    "authors": [
      "Adam Bignold",
      "Francisco Cruz",
      "Matthew E. Taylor",
      "Tim Brys",
      "Richard Dazeley",
      "Peter Vamplew",
      "Cameron Foale"
    ],
    "published": "2020-07-03T08:07:31+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2007.01544v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2007.01544v2_chunk_2",
    "chunk_text": "These streams of reinforcement learning operate with the shared objective of scaffolding the learner agent. Lastly, we discuss further possibilities for future work in the field of assisted reinforcement learning systems.",
    "original_url": "http://arxiv.org/pdf/2007.01544v2",
    "original_title": "A Conceptual Framework for Externally-influenced Agents: An Assisted Reinforcement Learning Review",
    "source": "arxiv",
    "authors": [
      "Adam Bignold",
      "Francisco Cruz",
      "Matthew E. Taylor",
      "Tim Brys",
      "Richard Dazeley",
      "Peter Vamplew",
      "Cameron Foale"
    ],
    "published": "2020-07-03T08:07:31+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2007.01544v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1908.01275v3_chunk_0",
    "chunk_text": "A View on Deep Reinforcement Learning in System Optimization. Many real-world systems problems require reasoning about the long term consequences of actions taken to configure and manage the system. These problems with delayed and often sequentially aggregated reward, are often inherently reinforcement learning problems and present the opportunity to leverage the recent substantial advances in deep reinforcement learning. However, in some cases, it is not clear why deep reinforcement learning is a good fit for the problem. Sometimes, it does not perform better than the state-of-the-art solutions.",
    "original_url": "http://arxiv.org/pdf/1908.01275v3",
    "original_title": "A View on Deep Reinforcement Learning in System Optimization",
    "source": "arxiv",
    "authors": [
      "Ameer Haj-Ali",
      "Nesreen K. Ahmed",
      "Ted Willke",
      "Joseph Gonzalez",
      "Krste Asanovic",
      "Ion Stoica"
    ],
    "published": "2019-08-04T05:55:56+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1908.01275v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1908.01275v3_chunk_1",
    "chunk_text": "Sometimes, it does not perform better than the state-of-the-art solutions. And in other cases, random search or greedy algorithms could outperform deep reinforcement learning. In this paper, we review, discuss, and evaluate the recent trends of using deep reinforcement learning in system optimization. We propose a set of essential metrics to guide future works in evaluating the efficacy of using deep reinforcement learning in system optimization. Our evaluation includes challenges, the types of problems, their formulation in the deep reinforcement learning setting, embedding, the model used, efficiency, and robustness.",
    "original_url": "http://arxiv.org/pdf/1908.01275v3",
    "original_title": "A View on Deep Reinforcement Learning in System Optimization",
    "source": "arxiv",
    "authors": [
      "Ameer Haj-Ali",
      "Nesreen K. Ahmed",
      "Ted Willke",
      "Joseph Gonzalez",
      "Krste Asanovic",
      "Ion Stoica"
    ],
    "published": "2019-08-04T05:55:56+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1908.01275v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1908.01275v3_chunk_2",
    "chunk_text": "Our evaluation includes challenges, the types of problems, their formulation in the deep reinforcement learning setting, embedding, the model used, efficiency, and robustness. We conclude with a discussion on open challenges and potential directions for pushing further the integration of reinforcement learning in system optimization.",
    "original_url": "http://arxiv.org/pdf/1908.01275v3",
    "original_title": "A View on Deep Reinforcement Learning in System Optimization",
    "source": "arxiv",
    "authors": [
      "Ameer Haj-Ali",
      "Nesreen K. Ahmed",
      "Ted Willke",
      "Joseph Gonzalez",
      "Krste Asanovic",
      "Ion Stoica"
    ],
    "published": "2019-08-04T05:55:56+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1908.01275v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2109.12516v2_chunk_0",
    "chunk_text": "Prioritized Experience-based Reinforcement Learning with Human Guidance for Autonomous Driving. Reinforcement learning (RL) requires skillful definition and remarkable computational efforts to solve optimization and control problems, which could impair its prospect. Introducing human guidance into reinforcement learning is a promising way to improve learning performance. In this paper, a comprehensive human guidance-based reinforcement learning framework is established. A novel prioritized experience replay mechanism that adapts to human guidance in the reinforcement learning process is proposed to boost the efficiency and performance of the reinforcement learning algorithm.",
    "original_url": "http://arxiv.org/pdf/2109.12516v2",
    "original_title": "Prioritized Experience-based Reinforcement Learning with Human Guidance for Autonomous Driving",
    "source": "arxiv",
    "authors": [
      "Jingda Wu",
      "Zhiyu Huang",
      "Wenhui Huang",
      "Chen Lv"
    ],
    "published": "2021-09-26T07:19:26+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2109.12516v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2109.12516v2_chunk_1",
    "chunk_text": "A novel prioritized experience replay mechanism that adapts to human guidance in the reinforcement learning process is proposed to boost the efficiency and performance of the reinforcement learning algorithm. To relieve the heavy workload on human participants, a behavior model is established based on an incremental online learning method to mimic human actions. We design two challenging autonomous driving tasks for evaluating the proposed algorithm. Experiments are conducted to access the training and testing performance and learning mechanism of the proposed algorithm. Comparative results against the state-of-the-art methods suggest the advantages of our algorithm in terms of learning efficiency, performance, and robustness.",
    "original_url": "http://arxiv.org/pdf/2109.12516v2",
    "original_title": "Prioritized Experience-based Reinforcement Learning with Human Guidance for Autonomous Driving",
    "source": "arxiv",
    "authors": [
      "Jingda Wu",
      "Zhiyu Huang",
      "Wenhui Huang",
      "Chen Lv"
    ],
    "published": "2021-09-26T07:19:26+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2109.12516v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2109.12516v2_chunk_2",
    "chunk_text": "Comparative results against the state-of-the-art methods suggest the advantages of our algorithm in terms of learning efficiency, performance, and robustness.",
    "original_url": "http://arxiv.org/pdf/2109.12516v2",
    "original_title": "Prioritized Experience-based Reinforcement Learning with Human Guidance for Autonomous Driving",
    "source": "arxiv",
    "authors": [
      "Jingda Wu",
      "Zhiyu Huang",
      "Wenhui Huang",
      "Chen Lv"
    ],
    "published": "2021-09-26T07:19:26+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2109.12516v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2404.16879v1_chunk_0",
    "chunk_text": "Learning Control Barrier Functions and their application in Reinforcement Learning: A Survey. Reinforcement learning is a powerful technique for developing new robot behaviors. However, typical lack of safety guarantees constitutes a hurdle for its practical application on real robots. To address this issue, safe reinforcement learning aims to incorporate safety considerations, enabling faster transfer to real robots and facilitating lifelong learning. One promising approach within safe reinforcement learning is the use of control barrier functions.",
    "original_url": "http://arxiv.org/pdf/2404.16879v1",
    "original_title": "Learning Control Barrier Functions and their application in Reinforcement Learning: A Survey",
    "source": "arxiv",
    "authors": [
      "Maeva Guerrier",
      "Hassan Fouad",
      "Giovanni Beltrame"
    ],
    "published": "2024-04-22T22:52:14+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2404.16879v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2404.16879v1_chunk_1",
    "chunk_text": "One promising approach within safe reinforcement learning is the use of control barrier functions. These functions provide a framework to ensure that the system remains in a safe state during the learning process. However, synthesizing control barrier functions is not straightforward and often requires ample domain knowledge. This challenge motivates the exploration of data-driven methods for automatically defining control barrier functions, which is highly appealing. We conduct a comprehensive review of the existing literature on safe reinforcement learning using control barrier functions.",
    "original_url": "http://arxiv.org/pdf/2404.16879v1",
    "original_title": "Learning Control Barrier Functions and their application in Reinforcement Learning: A Survey",
    "source": "arxiv",
    "authors": [
      "Maeva Guerrier",
      "Hassan Fouad",
      "Giovanni Beltrame"
    ],
    "published": "2024-04-22T22:52:14+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2404.16879v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2404.16879v1_chunk_2",
    "chunk_text": "We conduct a comprehensive review of the existing literature on safe reinforcement learning using control barrier functions. Additionally, we investigate various techniques for automatically learning the Control Barrier Functions, aiming to enhance the safety and efficacy of Reinforcement Learning in practical robot applications.",
    "original_url": "http://arxiv.org/pdf/2404.16879v1",
    "original_title": "Learning Control Barrier Functions and their application in Reinforcement Learning: A Survey",
    "source": "arxiv",
    "authors": [
      "Maeva Guerrier",
      "Hassan Fouad",
      "Giovanni Beltrame"
    ],
    "published": "2024-04-22T22:52:14+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2404.16879v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2503.19212v1_chunk_0",
    "chunk_text": "Continual Reinforcement Learning for HVAC Systems Control: Integrating Hypernetworks and Transfer Learning. Buildings with Heating, Ventilation, and Air Conditioning (HVAC) systems play a crucial role in ensuring indoor comfort and efficiency. While traditionally governed by physics-based models, the emergence of big data has enabled data-driven methods like Deep Reinforcement Learning (DRL). However, Reinforcement Learning (RL)-based techniques often suffer from sample inefficiency and limited generalization, especially across varying HVAC systems. We introduce a model-based reinforcement learning framework that uses a Hypernetwork to continuously learn environment dynamics across tasks with different action spaces.",
    "original_url": "http://arxiv.org/pdf/2503.19212v1",
    "original_title": "Continual Reinforcement Learning for HVAC Systems Control: Integrating Hypernetworks and Transfer Learning",
    "source": "arxiv",
    "authors": [
      "Gautham Udayakumar Bekal",
      "Ahmed Ghareeb",
      "Ashish Pujari"
    ],
    "published": "2025-03-24T23:38:04+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2503.19212v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2503.19212v1_chunk_1",
    "chunk_text": "We introduce a model-based reinforcement learning framework that uses a Hypernetwork to continuously learn environment dynamics across tasks with different action spaces. This enables efficient synthetic rollout generation and improved sample usage. Our approach demonstrates strong backward transfer in a continual learning setting after training on a second task, minimal fine-tuning on the first task allows rapid convergence within just 5 episodes and thus outperforming Model Free Reinforcement Learning (MFRL) and effectively mitigating catastrophic forgetting. These findings have significant implications for reducing energy consumption and operational costs in building management, thus supporting global sustainability goals. Keywords: Deep Reinforcement Learning, HVAC Systems Control, Hypernetworks, Transfer and Continual Learning, Catastrophic Forgetting",
    "original_url": "http://arxiv.org/pdf/2503.19212v1",
    "original_title": "Continual Reinforcement Learning for HVAC Systems Control: Integrating Hypernetworks and Transfer Learning",
    "source": "arxiv",
    "authors": [
      "Gautham Udayakumar Bekal",
      "Ahmed Ghareeb",
      "Ashish Pujari"
    ],
    "published": "2025-03-24T23:38:04+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2503.19212v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2503.19212v1_chunk_2",
    "chunk_text": "Keywords: Deep Reinforcement Learning, HVAC Systems Control, Hypernetworks, Transfer and Continual Learning, Catastrophic Forgetting",
    "original_url": "http://arxiv.org/pdf/2503.19212v1",
    "original_title": "Continual Reinforcement Learning for HVAC Systems Control: Integrating Hypernetworks and Transfer Learning",
    "source": "arxiv",
    "authors": [
      "Gautham Udayakumar Bekal",
      "Ahmed Ghareeb",
      "Ashish Pujari"
    ],
    "published": "2025-03-24T23:38:04+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2503.19212v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2410.00704v1_chunk_0",
    "chunk_text": "Contrastive Abstraction for Reinforcement Learning. Learning agents with reinforcement learning is difficult when dealing with long trajectories that involve a large number of states. To address these learning problems effectively, the number of states can be reduced by abstract representations that cluster states. In principle, deep reinforcement learning can find abstract states, but end-to-end learning is unstable. We propose contrastive abstraction learning to find abstract states, where we assume that successive states in a trajectory belong to the same abstract state.",
    "original_url": "http://arxiv.org/pdf/2410.00704v1",
    "original_title": "Contrastive Abstraction for Reinforcement Learning",
    "source": "arxiv",
    "authors": [
      "Vihang Patil",
      "Markus Hofmarcher",
      "Elisabeth Rumetshofer",
      "Sepp Hochreiter"
    ],
    "published": "2024-10-01T13:56:09+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2410.00704v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2410.00704v1_chunk_1",
    "chunk_text": "We propose contrastive abstraction learning to find abstract states, where we assume that successive states in a trajectory belong to the same abstract state. Such abstract states may be basic locations, achieved subgoals, inventory, or health conditions. Contrastive abstraction learning first constructs clusters of state representations by contrastive learning and then applies modern Hopfield networks to determine the abstract states. The first phase of contrastive abstraction learning is self-supervised learning, where contrastive learning forces states with sequential proximity to have similar representations. The second phase uses modern Hopfield networks to map similar state representations to the same fixed point, i.e.\\ to an abstract state.",
    "original_url": "http://arxiv.org/pdf/2410.00704v1",
    "original_title": "Contrastive Abstraction for Reinforcement Learning",
    "source": "arxiv",
    "authors": [
      "Vihang Patil",
      "Markus Hofmarcher",
      "Elisabeth Rumetshofer",
      "Sepp Hochreiter"
    ],
    "published": "2024-10-01T13:56:09+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2410.00704v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2410.00704v1_chunk_2",
    "chunk_text": "The second phase uses modern Hopfield networks to map similar state representations to the same fixed point, i.e.\\ to an abstract state. The level of abstraction can be adjusted by determining the number of fixed points of the modern Hopfield network. Furthermore, \\textit{contrastive abstraction learning} does not require rewards and facilitates efficient reinforcement learning for a wide range of downstream tasks. Our experiments demonstrate the effectiveness of contrastive abstraction learning for reinforcement learning.",
    "original_url": "http://arxiv.org/pdf/2410.00704v1",
    "original_title": "Contrastive Abstraction for Reinforcement Learning",
    "source": "arxiv",
    "authors": [
      "Vihang Patil",
      "Markus Hofmarcher",
      "Elisabeth Rumetshofer",
      "Sepp Hochreiter"
    ],
    "published": "2024-10-01T13:56:09+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2410.00704v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1012.1552v1_chunk_0",
    "chunk_text": "Bridging the Gap between Reinforcement Learning and Knowledge Representation: A Logical Off- and On-Policy Framework. Knowledge Representation is important issue in reinforcement learning. In this paper, we bridge the gap between reinforcement learning and knowledge representation, by providing a rich knowledge representation framework, based on normal logic programs with answer set semantics, that is capable of solving model-free reinforcement learning problems for more complex do-mains and exploits the domain-specific knowledge. We prove the correctness of our approach. We show that the complexity of finding an offline and online policy for a model-free reinforcement learning problem in our approach is NP-complete.",
    "original_url": "http://arxiv.org/pdf/1012.1552v1",
    "original_title": "Bridging the Gap between Reinforcement Learning and Knowledge Representation: A Logical Off- and On-Policy Framework",
    "source": "arxiv",
    "authors": [
      "Emad Saad"
    ],
    "published": "2010-12-07T16:57:54+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1012.1552v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1012.1552v1_chunk_1",
    "chunk_text": "We show that the complexity of finding an offline and online policy for a model-free reinforcement learning problem in our approach is NP-complete. Moreover, we show that any model-free reinforcement learning problem in MDP environment can be encoded as a SAT problem. The importance of that is model-free reinforcement",
    "original_url": "http://arxiv.org/pdf/1012.1552v1",
    "original_title": "Bridging the Gap between Reinforcement Learning and Knowledge Representation: A Logical Off- and On-Policy Framework",
    "source": "arxiv",
    "authors": [
      "Emad Saad"
    ],
    "published": "2010-12-07T16:57:54+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1012.1552v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2001.06921v2_chunk_0",
    "chunk_text": "A Survey of Reinforcement Learning Techniques: Strategies, Recent Development, and Future Directions. Reinforcement learning is one of the core components in designing an artificial intelligent system emphasizing real-time response. Reinforcement learning influences the system to take actions within an arbitrary environment either having previous knowledge about the environment model or not. In this paper, we present a comprehensive study on Reinforcement Learning focusing on various dimensions including challenges, the recent development of different state-of-the-art techniques, and future directions. The fundamental objective of this paper is to provide a framework for the presentation of available methods of reinforcement learning that is informative enough and simple to follow for the new researchers and academics in this domain considering the latest concerns.",
    "original_url": "http://arxiv.org/pdf/2001.06921v2",
    "original_title": "A Survey of Reinforcement Learning Techniques: Strategies, Recent Development, and Future Directions",
    "source": "arxiv",
    "authors": [
      "Amit Kumar Mondal"
    ],
    "published": "2020-01-19T23:51:14+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2001.06921v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2001.06921v2_chunk_1",
    "chunk_text": "The fundamental objective of this paper is to provide a framework for the presentation of available methods of reinforcement learning that is informative enough and simple to follow for the new researchers and academics in this domain considering the latest concerns. First, we illustrated the core techniques of reinforcement learning in an easily understandable and comparable way. Finally, we analyzed and depicted the recent developments in reinforcement learning approaches. My analysis pointed out that most of the models focused on tuning policy values rather than tuning other things in a particular state of reasoning.",
    "original_url": "http://arxiv.org/pdf/2001.06921v2",
    "original_title": "A Survey of Reinforcement Learning Techniques: Strategies, Recent Development, and Future Directions",
    "source": "arxiv",
    "authors": [
      "Amit Kumar Mondal"
    ],
    "published": "2020-01-19T23:51:14+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2001.06921v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2205.12888v1_chunk_0",
    "chunk_text": "Robust Reinforcement Learning on Graphs for Logistics optimization. Logistics optimization nowadays is becoming one of the hottest areas in the AI community. In the past year, significant advancements in the domain were achieved by representing the problem in a form of graph. Another promising area of research was to apply reinforcement learning algorithms to the above task. In our work, we made advantage of using both approaches and apply reinforcement learning on a graph.",
    "original_url": "http://arxiv.org/pdf/2205.12888v1",
    "original_title": "Robust Reinforcement Learning on Graphs for Logistics optimization",
    "source": "arxiv",
    "authors": [
      "Zangir Iklassov",
      "Dmitrii Medvedev"
    ],
    "published": "2022-05-25T16:16:28+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2205.12888v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2205.12888v1_chunk_1",
    "chunk_text": "In our work, we made advantage of using both approaches and apply reinforcement learning on a graph. To do that, we have analyzed the most recent results in both fields and selected SOTA algorithms both from graph neural networks and reinforcement learning. Then, we combined selected models on the problem of AMOD systems optimization for the transportation network of New York city. Our team compared three algorithms - GAT, Pro-CNN and PTDNet - to bring to the fore the important nodes on a graph representation. Finally, we achieved SOTA results on AMOD systems optimization problem employing PTDNet with GNN and training them in reinforcement fashion.",
    "original_url": "http://arxiv.org/pdf/2205.12888v1",
    "original_title": "Robust Reinforcement Learning on Graphs for Logistics optimization",
    "source": "arxiv",
    "authors": [
      "Zangir Iklassov",
      "Dmitrii Medvedev"
    ],
    "published": "2022-05-25T16:16:28+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2205.12888v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2205.12888v1_chunk_2",
    "chunk_text": "Finally, we achieved SOTA results on AMOD systems optimization problem employing PTDNet with GNN and training them in reinforcement fashion. Keywords: Graph Neural Network (GNN), Logistics optimization, Reinforcement Learning",
    "original_url": "http://arxiv.org/pdf/2205.12888v1",
    "original_title": "Robust Reinforcement Learning on Graphs for Logistics optimization",
    "source": "arxiv",
    "authors": [
      "Zangir Iklassov",
      "Dmitrii Medvedev"
    ],
    "published": "2022-05-25T16:16:28+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2205.12888v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2311.06914v1_chunk_0",
    "chunk_text": "Model-assisted Reinforcement Learning of a Quadrotor. In recent times, reinforcement learning has produced baffling results when it comes to performing control tasks with highly non-linear systems. The impressive results always outweigh the potential vulnerabilities or uncertainties associated with the agents when deployed in the real-world. While the performance is remarkable compared to the classical control algorithms, the reinforcement learning-based methods suffer from two flaws, robustness and interpretability, which are vital for contemporary real-world applications. The paper attempts to alleviate such problems with reinforcement learning and proposes the concept of model-assisted reinforcement learning to induce a notion of conservativeness in the agents.",
    "original_url": "http://arxiv.org/pdf/2311.06914v1",
    "original_title": "Model-assisted Reinforcement Learning of a Quadrotor",
    "source": "arxiv",
    "authors": [
      "Arshad Javeed"
    ],
    "published": "2023-11-12T18:05:56+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2311.06914v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2311.06914v1_chunk_1",
    "chunk_text": "The paper attempts to alleviate such problems with reinforcement learning and proposes the concept of model-assisted reinforcement learning to induce a notion of conservativeness in the agents. The control task considered for the experiment involves navigating a CrazyFlie quadrotor. The paper also describes a way of reformulating the task to have the flexibility of tuning the level of conservativeness via multi-objective reinforcement learning. The results include a comparison of the vanilla reinforcement learning approaches and the proposed approach. The metrics are evaluated by systematically injecting disturbances to classify the inherent robustness and conservativeness of the agents.",
    "original_url": "http://arxiv.org/pdf/2311.06914v1",
    "original_title": "Model-assisted Reinforcement Learning of a Quadrotor",
    "source": "arxiv",
    "authors": [
      "Arshad Javeed"
    ],
    "published": "2023-11-12T18:05:56+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2311.06914v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2311.06914v1_chunk_2",
    "chunk_text": "The metrics are evaluated by systematically injecting disturbances to classify the inherent robustness and conservativeness of the agents. More concrete arguments are made by computing and comparing the backward reachability tubes of the RL policies by solving the Hamilton-Jacobi-Bellman partial differential equation (HJ PDE).",
    "original_url": "http://arxiv.org/pdf/2311.06914v1",
    "original_title": "Model-assisted Reinforcement Learning of a Quadrotor",
    "source": "arxiv",
    "authors": [
      "Arshad Javeed"
    ],
    "published": "2023-11-12T18:05:56+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2311.06914v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2407.02425v1_chunk_0",
    "chunk_text": "Reinforcement Learning and Machine ethics:a systematic review. Machine ethics is the field that studies how ethical behaviour can be accomplished by autonomous systems. While there exist some systematic reviews aiming to consolidate the state of the art in machine ethics prior to 2020, these tend to not include work that uses reinforcement learning agents as entities whose ethical behaviour is to be achieved. The reason for this is that only in the last years we have witnessed an increase in machine ethics studies within reinforcement learning. We present here a systematic review of reinforcement learning for machine ethics and machine ethics within reinforcement learning.",
    "original_url": "http://arxiv.org/pdf/2407.02425v1",
    "original_title": "Reinforcement Learning and Machine ethics:a systematic review",
    "source": "arxiv",
    "authors": [
      "Ajay Vishwanath",
      "Louise A. Dennis",
      "Marija Slavkovik"
    ],
    "published": "2024-07-02T16:54:00+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2407.02425v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2407.02425v1_chunk_1",
    "chunk_text": "We present here a systematic review of reinforcement learning for machine ethics and machine ethics within reinforcement learning. Additionally, we highlight trends in terms of ethics specifications, components and frameworks of reinforcement learning, and environments used to result in ethical behaviour. Our systematic review aims to consolidate the work in machine ethics and reinforcement learning thus completing the gap in the state of the art machine ethics landscape",
    "original_url": "http://arxiv.org/pdf/2407.02425v1",
    "original_title": "Reinforcement Learning and Machine ethics:a systematic review",
    "source": "arxiv",
    "authors": [
      "Ajay Vishwanath",
      "Louise A. Dennis",
      "Marija Slavkovik"
    ],
    "published": "2024-07-02T16:54:00+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2407.02425v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2102.07247v1_chunk_0",
    "chunk_text": "Reinforcement Learning for IoT Security: A Comprehensive Survey. The number of connected smart devices has been increasing exponentially for different Internet-of-Things (IoT) applications. Security has been a long run challenge in the IoT systems which has many attack vectors, security flaws and vulnerabilities. Securing billions of B connected devices in IoT is a must task to realize the full potential of IoT applications. Recently, researchers have proposed many security solutions for IoT.",
    "original_url": "http://arxiv.org/pdf/2102.07247v1",
    "original_title": "Reinforcement Learning for IoT Security: A Comprehensive Survey",
    "source": "arxiv",
    "authors": [
      "Aashma Uprety",
      "Danda B. Rawat"
    ],
    "published": "2021-02-14T21:09:49+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2102.07247v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2102.07247v1_chunk_1",
    "chunk_text": "Recently, researchers have proposed many security solutions for IoT. Machine learning has been proposed as one of the emerging solutions for IoT security and Reinforcement learning is gaining more popularity for securing IoT systems. Reinforcement learning, unlike other machine learning techniques, can learn the environment by having minimum information about the parameters to be learned. It solves the optimization problem by interacting with the environment adapting the parameters on the fly. In this paper, we present an comprehensive survey of different types of cyber-attacks against different IoT systems and then we present reinforcement learning and deep reinforcement learning based security solutions to combat those different types of attacks in different IoT systems.",
    "original_url": "http://arxiv.org/pdf/2102.07247v1",
    "original_title": "Reinforcement Learning for IoT Security: A Comprehensive Survey",
    "source": "arxiv",
    "authors": [
      "Aashma Uprety",
      "Danda B. Rawat"
    ],
    "published": "2021-02-14T21:09:49+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2102.07247v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2102.07247v1_chunk_2",
    "chunk_text": "In this paper, we present an comprehensive survey of different types of cyber-attacks against different IoT systems and then we present reinforcement learning and deep reinforcement learning based security solutions to combat those different types of attacks in different IoT systems. Furthermore, we present the Reinforcement learning for securing CPS systems (i.e., IoT with feedback and control) such as smart grid and smart transportation system. The recent important attacks and countermeasures using reinforcement learning B in IoT are also summarized in the form of tables. With this paper, readers can have a more thorough understanding of IoT security attacks and countermeasures using Reinforcement Learning, as well as research trends in this area.",
    "original_url": "http://arxiv.org/pdf/2102.07247v1",
    "original_title": "Reinforcement Learning for IoT Security: A Comprehensive Survey",
    "source": "arxiv",
    "authors": [
      "Aashma Uprety",
      "Danda B. Rawat"
    ],
    "published": "2021-02-14T21:09:49+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2102.07247v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2309.01909v1_chunk_0",
    "chunk_text": "A Survey on Physics Informed Reinforcement Learning: Review and Open Problems. The inclusion of physical information in machine learning frameworks has revolutionized many application areas. This involves enhancing the learning process by incorporating physical constraints and adhering to physical laws. In this work we explore their utility for reinforcement learning applications. We present a thorough review of the literature on incorporating physics information, as known as physics priors, in reinforcement learning approaches, commonly referred to as physics-informed reinforcement learning (PIRL).",
    "original_url": "http://arxiv.org/pdf/2309.01909v1",
    "original_title": "A Survey on Physics Informed Reinforcement Learning: Review and Open Problems",
    "source": "arxiv",
    "authors": [
      "Chayan Banerjee",
      "Kien Nguyen",
      "Clinton Fookes",
      "Maziar Raissi"
    ],
    "published": "2023-09-05T02:45:18+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2309.01909v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2309.01909v1_chunk_1",
    "chunk_text": "We present a thorough review of the literature on incorporating physics information, as known as physics priors, in reinforcement learning approaches, commonly referred to as physics-informed reinforcement learning (PIRL). We introduce a novel taxonomy with the reinforcement learning pipeline as the backbone to classify existing works, compare and contrast them, and derive crucial insights. Existing works are analyzed with regard to the representation/ form of the governing physics modeled for integration, their specific contribution to the typical reinforcement learning architecture, and their connection to the underlying reinforcement learning pipeline stages. We also identify core learning architectures and physics incorporation biases (i.e., observational, inductive and learning) of existing PIRL approaches and use them to further categorize the works for better understanding and adaptation. By providing a comprehensive perspective on the implementation of the physics-informed capability, the taxonomy presents a cohesive approach to PIRL.",
    "original_url": "http://arxiv.org/pdf/2309.01909v1",
    "original_title": "A Survey on Physics Informed Reinforcement Learning: Review and Open Problems",
    "source": "arxiv",
    "authors": [
      "Chayan Banerjee",
      "Kien Nguyen",
      "Clinton Fookes",
      "Maziar Raissi"
    ],
    "published": "2023-09-05T02:45:18+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2309.01909v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2309.01909v1_chunk_2",
    "chunk_text": "By providing a comprehensive perspective on the implementation of the physics-informed capability, the taxonomy presents a cohesive approach to PIRL. It identifies the areas where this approach has been applied, as well as the gaps and opportunities that exist. Additionally, the taxonomy sheds light on unresolved issues and challenges, which can guide future research. This nascent field holds great potential for enhancing reinforcement learning algorithms by increasing their physical plausibility, precision, data efficiency, and applicability in real-world scenarios.",
    "original_url": "http://arxiv.org/pdf/2309.01909v1",
    "original_title": "A Survey on Physics Informed Reinforcement Learning: Review and Open Problems",
    "source": "arxiv",
    "authors": [
      "Chayan Banerjee",
      "Kien Nguyen",
      "Clinton Fookes",
      "Maziar Raissi"
    ],
    "published": "2023-09-05T02:45:18+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2309.01909v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1612.07548v1_chunk_0",
    "chunk_text": "Non-Deterministic Policy Improvement Stabilizes Approximated Reinforcement Learning. This paper investigates a type of instability that is linked to the greedy policy improvement in approximated reinforcement learning. We show empirically that non-deterministic policy improvement can stabilize methods like LSPI by controlling the improvements' stochasticity. Additionally we show that a suitable representation of the value function also stabilizes the solution to some degree. The presented approach is simple and should also be easily transferable to more sophisticated algorithms like deep reinforcement learning.",
    "original_url": "http://arxiv.org/pdf/1612.07548v1",
    "original_title": "Non-Deterministic Policy Improvement Stabilizes Approximated Reinforcement Learning",
    "source": "arxiv",
    "authors": [
      "Wendelin Böhmer",
      "Rong Guo",
      "Klaus Obermayer"
    ],
    "published": "2016-12-22T11:30:35+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1612.07548v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1612.07548v1_chunk_1",
    "chunk_text": "The presented approach is simple and should also be easily transferable to more sophisticated algorithms like deep reinforcement learning.",
    "original_url": "http://arxiv.org/pdf/1612.07548v1",
    "original_title": "Non-Deterministic Policy Improvement Stabilizes Approximated Reinforcement Learning",
    "source": "arxiv",
    "authors": [
      "Wendelin Böhmer",
      "Rong Guo",
      "Klaus Obermayer"
    ],
    "published": "2016-12-22T11:30:35+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1612.07548v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1705.10432v1_chunk_0",
    "chunk_text": "Fine-grained acceleration control for autonomous intersection management using deep reinforcement learning. Recent advances in combining deep learning and Reinforcement Learning have shown a promising path for designing new control agents that can learn optimal policies for challenging control tasks. These new methods address the main limitations of conventional Reinforcement Learning methods such as customized feature engineering and small action/state space dimension requirements. In this paper, we leverage one of the state-of-the-art Reinforcement Learning methods, known as Trust Region Policy Optimization, to tackle intersection management for autonomous vehicles. We show that using this method, we can perform fine-grained acceleration control of autonomous vehicles in a grid street plan to achieve a global design objective.",
    "original_url": "http://arxiv.org/pdf/1705.10432v1",
    "original_title": "Fine-grained acceleration control for autonomous intersection management using deep reinforcement learning",
    "source": "arxiv",
    "authors": [
      "Hamid Mirzaei",
      "Tony Givargis"
    ],
    "published": "2017-05-30T02:04:29+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1705.10432v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1705.10432v1_chunk_1",
    "chunk_text": "We show that using this method, we can perform fine-grained acceleration control of autonomous vehicles in a grid street plan to achieve a global design objective.",
    "original_url": "http://arxiv.org/pdf/1705.10432v1",
    "original_title": "Fine-grained acceleration control for autonomous intersection management using deep reinforcement learning",
    "source": "arxiv",
    "authors": [
      "Hamid Mirzaei",
      "Tony Givargis"
    ],
    "published": "2017-05-30T02:04:29+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1705.10432v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1710.04582v1_chunk_0",
    "chunk_text": "Is Epicurus the father of Reinforcement Learning?. The Epicurean Philosophy is commonly thought as simplistic and hedonistic. Here I discuss how this is a misconception and explore its link to Reinforcement Learning. Based on the letters of Epicurus, I construct an objective function for hedonism which turns out to be equivalent of the Reinforcement Learning objective function when omitting the discount factor. I then discuss how Plato and Aristotle 's views that can be also loosely linked to Reinforcement Learning, as well as their weaknesses in relationship to it.",
    "original_url": "http://arxiv.org/pdf/1710.04582v1",
    "original_title": "Is Epicurus the father of Reinforcement Learning?",
    "source": "arxiv",
    "authors": [
      "Eleni Vasilaki"
    ],
    "published": "2017-10-12T16:07:18+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1710.04582v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1710.04582v1_chunk_1",
    "chunk_text": "I then discuss how Plato and Aristotle 's views that can be also loosely linked to Reinforcement Learning, as well as their weaknesses in relationship to it. Finally, I emphasise the close affinity of the Epicurean views and the Bellman equation.",
    "original_url": "http://arxiv.org/pdf/1710.04582v1",
    "original_title": "Is Epicurus the father of Reinforcement Learning?",
    "source": "arxiv",
    "authors": [
      "Eleni Vasilaki"
    ],
    "published": "2017-10-12T16:07:18+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1710.04582v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1809.09501v1_chunk_0",
    "chunk_text": "Anderson Acceleration for Reinforcement Learning. Anderson acceleration is an old and simple method for accelerating the computation of a fixed point. However, as far as we know and quite surprisingly, it has never been applied to dynamic programming or reinforcement learning. In this paper, we explain briefly what Anderson acceleration is and how it can be applied to value iteration, this being supported by preliminary experiments showing a significant speed up of convergence, that we critically discuss. We also discuss how this idea could be applied more generally to (deep) reinforcement learning.",
    "original_url": "http://arxiv.org/pdf/1809.09501v1",
    "original_title": "Anderson Acceleration for Reinforcement Learning",
    "source": "arxiv",
    "authors": [
      "Matthieu Geist",
      "Bruno Scherrer"
    ],
    "published": "2018-09-25T14:04:25+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1809.09501v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1809.09501v1_chunk_1",
    "chunk_text": "We also discuss how this idea could be applied more generally to (deep) reinforcement learning.",
    "original_url": "http://arxiv.org/pdf/1809.09501v1",
    "original_title": "Anderson Acceleration for Reinforcement Learning",
    "source": "arxiv",
    "authors": [
      "Matthieu Geist",
      "Bruno Scherrer"
    ],
    "published": "2018-09-25T14:04:25+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1809.09501v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1812.04359v1_chunk_0",
    "chunk_text": "Efficient Model-Free Reinforcement Learning Using Gaussian Process. Efficient Reinforcement Learning usually takes advantage of demonstration or good exploration strategy. By applying posterior sampling in model-free RL under the hypothesis of GP, we propose Gaussian Process Posterior Sampling Reinforcement Learning(GPPSTD) algorithm in continuous state space, giving theoretical justifications and empirical results. We also provide theoretical and empirical results that various demonstration could lower expected uncertainty and benefit posterior sampling exploration. In this way, we combined the demonstration and exploration process together to achieve a more efficient reinforcement learning.",
    "original_url": "http://arxiv.org/pdf/1812.04359v1",
    "original_title": "Efficient Model-Free Reinforcement Learning Using Gaussian Process",
    "source": "arxiv",
    "authors": [
      "Ying Fan",
      "Letian Chen",
      "Yizhou Wang"
    ],
    "published": "2018-12-11T12:37:24+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1812.04359v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1812.04359v1_chunk_1",
    "chunk_text": "In this way, we combined the demonstration and exploration process together to achieve a more efficient reinforcement learning.",
    "original_url": "http://arxiv.org/pdf/1812.04359v1",
    "original_title": "Efficient Model-Free Reinforcement Learning Using Gaussian Process",
    "source": "arxiv",
    "authors": [
      "Ying Fan",
      "Letian Chen",
      "Yizhou Wang"
    ],
    "published": "2018-12-11T12:37:24+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1812.04359v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1812.10252v1_chunk_0",
    "chunk_text": "Optimizing Market Making using Multi-Agent Reinforcement Learning. In this paper, reinforcement learning is applied to the problem of optimizing market making. A multi-agent reinforcement learning framework is used to optimally place limit orders that lead to successful trades. The framework consists of two agents. The macro-agent optimizes on making the decision to buy, sell, or hold an asset.",
    "original_url": "http://arxiv.org/pdf/1812.10252v1",
    "original_title": "Optimizing Market Making using Multi-Agent Reinforcement Learning",
    "source": "arxiv",
    "authors": [
      "Yagna Patel"
    ],
    "published": "2018-12-26T07:29:10+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1812.10252v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1812.10252v1_chunk_1",
    "chunk_text": "The macro-agent optimizes on making the decision to buy, sell, or hold an asset. The micro-agent optimizes on placing limit orders within the limit order book. For the context of this paper, the proposed framework is applied and studied on the Bitcoin cryptocurrency market. The goal of this paper is to show that reinforcement learning is a viable strategy that can be applied to complex problems (with complex environments) such as market making.",
    "original_url": "http://arxiv.org/pdf/1812.10252v1",
    "original_title": "Optimizing Market Making using Multi-Agent Reinforcement Learning",
    "source": "arxiv",
    "authors": [
      "Yagna Patel"
    ],
    "published": "2018-12-26T07:29:10+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1812.10252v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1912.00498v1_chunk_0",
    "chunk_text": "Optimization for Reinforcement Learning: From Single Agent to Cooperative Agents. This article reviews recent advances in multi-agent reinforcement learning algorithms for large-scale control systems and communication networks, which learn to communicate and cooperate. We provide an overview of this emerging field, with an emphasis on the decentralized setting under different coordination protocols. We highlight the evolution of reinforcement learning algorithms from single-agent to multi-agent systems, from a distributed optimization perspective, and conclude with future directions and challenges, in the hope to catalyze the growing synergy among distributed optimization, signal processing, and reinforcement learning communities.",
    "original_url": "http://arxiv.org/pdf/1912.00498v1",
    "original_title": "Optimization for Reinforcement Learning: From Single Agent to Cooperative Agents",
    "source": "arxiv",
    "authors": [
      "Donghwan Lee",
      "Niao He",
      "Parameswaran Kamalaruban",
      "Volkan Cevher"
    ],
    "published": "2019-12-01T20:39:55+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1912.00498v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2302.10825v1_chunk_0",
    "chunk_text": "Curiosity-driven Exploration in Sparse-reward Multi-agent Reinforcement Learning. Sparsity of rewards while applying a deep reinforcement learning method negatively affects its sample-efficiency. A viable solution to deal with the sparsity of rewards is to learn via intrinsic motivation which advocates for adding an intrinsic reward to the reward function to encourage the agent to explore the environment and expand the sample space. Though intrinsic motivation methods are widely used to improve data-efficient learning in the reinforcement learning model, they also suffer from the so-called detachment problem. In this article, we discuss the limitations of intrinsic curiosity module in sparse-reward multi-agent reinforcement learning and propose a method called I-Go-Explore that combines the intrinsic curiosity module with the Go-Explore framework to alleviate the detachment problem.",
    "original_url": "http://arxiv.org/pdf/2302.10825v1",
    "original_title": "Curiosity-driven Exploration in Sparse-reward Multi-agent Reinforcement Learning",
    "source": "arxiv",
    "authors": [
      "Jiong Li",
      "Pratik Gajane"
    ],
    "published": "2023-02-21T17:00:05+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2302.10825v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2302.10825v1_chunk_1",
    "chunk_text": "In this article, we discuss the limitations of intrinsic curiosity module in sparse-reward multi-agent reinforcement learning and propose a method called I-Go-Explore that combines the intrinsic curiosity module with the Go-Explore framework to alleviate the detachment problem.",
    "original_url": "http://arxiv.org/pdf/2302.10825v1",
    "original_title": "Curiosity-driven Exploration in Sparse-reward Multi-agent Reinforcement Learning",
    "source": "arxiv",
    "authors": [
      "Jiong Li",
      "Pratik Gajane"
    ],
    "published": "2023-02-21T17:00:05+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2302.10825v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2308.07822v1_chunk_0",
    "chunk_text": "Deep reinforcement learning for process design: Review and perspective. The transformation towards renewable energy and feedstock supply in the chemical industry requires new conceptual process design approaches. Recently, breakthroughs in artificial intelligence offer opportunities to accelerate this transition. Specifically, deep reinforcement learning, a subclass of machine learning, has shown the potential to solve complex decision-making problems and aid sustainable process design. We survey state-of-the-art research in reinforcement learning for process design through three major elements: (i) information representation, (ii) agent architecture, and (iii) environment and reward.",
    "original_url": "http://arxiv.org/pdf/2308.07822v1",
    "original_title": "Deep reinforcement learning for process design: Review and perspective",
    "source": "arxiv",
    "authors": [
      "Qinghe Gao",
      "Artur M. Schweidtmann"
    ],
    "published": "2023-08-15T14:56:37+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2308.07822v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2308.07822v1_chunk_1",
    "chunk_text": "We survey state-of-the-art research in reinforcement learning for process design through three major elements: (i) information representation, (ii) agent architecture, and (iii) environment and reward. Moreover, we discuss perspectives on underlying challenges and promising future works to unfold the full potential of reinforcement learning for process design in chemical engineering.",
    "original_url": "http://arxiv.org/pdf/2308.07822v1",
    "original_title": "Deep reinforcement learning for process design: Review and perspective",
    "source": "arxiv",
    "authors": [
      "Qinghe Gao",
      "Artur M. Schweidtmann"
    ],
    "published": "2023-08-15T14:56:37+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2308.07822v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2410.01431v1_chunk_0",
    "chunk_text": "Scalable Reinforcement Learning-based Neural Architecture Search. In this publication, we assess the ability of a novel Reinforcement Learning-based solution to the problem of Neural Architecture Search, where a Reinforcement Learning (RL) agent learns to search for good architectures, rather than to return a single optimal architecture. We consider both the NAS-Bench-101 and NAS- Bench-301 settings, and compare against various known strong baselines, such as local search and random search. We conclude that our Reinforcement Learning agent displays strong scalability with regards to the size of the search space, but limited robustness to hyperparameter changes.",
    "original_url": "http://arxiv.org/pdf/2410.01431v1",
    "original_title": "Scalable Reinforcement Learning-based Neural Architecture Search",
    "source": "arxiv",
    "authors": [
      "Amber Cassimon",
      "Siegfried Mercelis",
      "Kevin Mets"
    ],
    "published": "2024-10-02T11:31:48+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2410.01431v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2307.11046v2_chunk_0",
    "chunk_text": "A Definition of Continual Reinforcement Learning. In a standard view of the reinforcement learning problem, an agent's goal is to efficiently identify a policy that maximizes long-term reward. However, this perspective is based on a restricted view of learning as finding a solution, rather than treating learning as endless adaptation. In contrast, continual reinforcement learning refers to the setting in which the best agents never stop learning. Despite the importance of continual reinforcement learning, the community lacks a simple definition of the problem that highlights its commitments and makes its primary concepts precise and clear.",
    "original_url": "http://arxiv.org/pdf/2307.11046v2",
    "original_title": "A Definition of Continual Reinforcement Learning",
    "source": "arxiv",
    "authors": [
      "David Abel",
      "André Barreto",
      "Benjamin Van Roy",
      "Doina Precup",
      "Hado van Hasselt",
      "Satinder Singh"
    ],
    "published": "2023-07-20T17:28:01+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2307.11046v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2307.11046v2_chunk_1",
    "chunk_text": "Despite the importance of continual reinforcement learning, the community lacks a simple definition of the problem that highlights its commitments and makes its primary concepts precise and clear. To this end, this paper is dedicated to carefully defining the continual reinforcement learning problem. We formalize the notion of agents that \"never stop learning\" through a new mathematical language for analyzing and cataloging agents. Using this new language, we define a continual learning agent as one that can be understood as carrying out an implicit search process indefinitely, and continual reinforcement learning as the setting in which the best agents are all continual learning agents. We provide two motivating examples, illustrating that traditional views of multi-task reinforcement learning and continual supervised learning are special cases of our definition.",
    "original_url": "http://arxiv.org/pdf/2307.11046v2",
    "original_title": "A Definition of Continual Reinforcement Learning",
    "source": "arxiv",
    "authors": [
      "David Abel",
      "André Barreto",
      "Benjamin Van Roy",
      "Doina Precup",
      "Hado van Hasselt",
      "Satinder Singh"
    ],
    "published": "2023-07-20T17:28:01+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2307.11046v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2307.11046v2_chunk_2",
    "chunk_text": "We provide two motivating examples, illustrating that traditional views of multi-task reinforcement learning and continual supervised learning are special cases of our definition. Collectively, these definitions and perspectives formalize many intuitive concepts at the heart of learning, and open new research pathways surrounding continual learning agents.",
    "original_url": "http://arxiv.org/pdf/2307.11046v2",
    "original_title": "A Definition of Continual Reinforcement Learning",
    "source": "arxiv",
    "authors": [
      "David Abel",
      "André Barreto",
      "Benjamin Van Roy",
      "Doina Precup",
      "Hado van Hasselt",
      "Satinder Singh"
    ],
    "published": "2023-07-20T17:28:01+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2307.11046v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1710.11248v2_chunk_0",
    "chunk_text": "Learning Robust Rewards with Adversarial Inverse Reinforcement Learning. Reinforcement learning provides a powerful and general framework for decision making and control, but its application in practice is often hindered by the need for extensive feature and reward engineering. Deep reinforcement learning methods can remove the need for explicit engineering of policy or value features, but still require a manually specified reward function. Inverse reinforcement learning holds the promise of automatic reward acquisition, but has proven exceptionally difficult to apply to large, high-dimensional problems with unknown dynamics. In this work, we propose adverserial inverse reinforcement learning (AIRL), a practical and scalable inverse reinforcement learning algorithm based on an adversarial reward learning formulation.",
    "original_url": "http://arxiv.org/pdf/1710.11248v2",
    "original_title": "Learning Robust Rewards with Adversarial Inverse Reinforcement Learning",
    "source": "arxiv",
    "authors": [
      "Justin Fu",
      "Katie Luo",
      "Sergey Levine"
    ],
    "published": "2017-10-30T21:22:28+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1710.11248v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1710.11248v2_chunk_1",
    "chunk_text": "In this work, we propose adverserial inverse reinforcement learning (AIRL), a practical and scalable inverse reinforcement learning algorithm based on an adversarial reward learning formulation. We demonstrate that AIRL is able to recover reward functions that are robust to changes in dynamics, enabling us to learn policies even under significant variation in the environment seen during training. Our experiments show that AIRL greatly outperforms prior methods in these transfer settings.",
    "original_url": "http://arxiv.org/pdf/1710.11248v2",
    "original_title": "Learning Robust Rewards with Adversarial Inverse Reinforcement Learning",
    "source": "arxiv",
    "authors": [
      "Justin Fu",
      "Katie Luo",
      "Sergey Levine"
    ],
    "published": "2017-10-30T21:22:28+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1710.11248v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1801.10459v2_chunk_0",
    "chunk_text": "Pretraining Deep Actor-Critic Reinforcement Learning Algorithms With Expert Demonstrations. Pretraining with expert demonstrations have been found useful in speeding up the training process of deep reinforcement learning algorithms since less online simulation data is required. Some people use supervised learning to speed up the process of feature learning, others pretrain the policies by imitating expert demonstrations. However, these methods are unstable and not suitable for actor-critic reinforcement learning algorithms. Also, some existing methods rely on the global optimum assumption, which is not true in most scenarios.",
    "original_url": "http://arxiv.org/pdf/1801.10459v2",
    "original_title": "Pretraining Deep Actor-Critic Reinforcement Learning Algorithms With Expert Demonstrations",
    "source": "arxiv",
    "authors": [
      "Xiaoqin Zhang",
      "Huimin Ma"
    ],
    "published": "2018-01-31T14:30:00+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1801.10459v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1801.10459v2_chunk_1",
    "chunk_text": "Also, some existing methods rely on the global optimum assumption, which is not true in most scenarios. In this paper, we employ expert demonstrations in a actor-critic reinforcement learning framework, and meanwhile ensure that the performance is not affected by the fact that expert demonstrations are not global optimal. We theoretically derive a method for computing policy gradients and value estimators with only expert demonstrations. Our method is theoretically plausible for actor-critic reinforcement learning algorithms that pretrains both policy and value functions. We apply our method to two of the typical actor-critic reinforcement learning algorithms, DDPG and ACER, and demonstrate with experiments that our method not only outperforms the RL algorithms without pretraining process, but also is more simulation efficient.",
    "original_url": "http://arxiv.org/pdf/1801.10459v2",
    "original_title": "Pretraining Deep Actor-Critic Reinforcement Learning Algorithms With Expert Demonstrations",
    "source": "arxiv",
    "authors": [
      "Xiaoqin Zhang",
      "Huimin Ma"
    ],
    "published": "2018-01-31T14:30:00+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1801.10459v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1801.10459v2_chunk_2",
    "chunk_text": "We apply our method to two of the typical actor-critic reinforcement learning algorithms, DDPG and ACER, and demonstrate with experiments that our method not only outperforms the RL algorithms without pretraining process, but also is more simulation efficient.",
    "original_url": "http://arxiv.org/pdf/1801.10459v2",
    "original_title": "Pretraining Deep Actor-Critic Reinforcement Learning Algorithms With Expert Demonstrations",
    "source": "arxiv",
    "authors": [
      "Xiaoqin Zhang",
      "Huimin Ma"
    ],
    "published": "2018-01-31T14:30:00+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1801.10459v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2102.11914v1_chunk_0",
    "chunk_text": "A Robotic Model of Hippocampal Reverse Replay for Reinforcement Learning. Hippocampal reverse replay is thought to contribute to learning, and particularly reinforcement learning, in animals. We present a computational model of learning in the hippocampus that builds on a previous model of the hippocampal-striatal network viewed as implementing a three-factor reinforcement learning rule. To augment this model with hippocampal reverse replay, a novel policy gradient learning rule is derived that associates place cell activity with responses in cells representing actions. This new model is evaluated using a simulated robot spatial navigation task inspired by the Morris water maze.",
    "original_url": "http://arxiv.org/pdf/2102.11914v1",
    "original_title": "A Robotic Model of Hippocampal Reverse Replay for Reinforcement Learning",
    "source": "arxiv",
    "authors": [
      "Matthew T. Whelan",
      "Tony J. Prescott",
      "Eleni Vasilaki"
    ],
    "published": "2021-02-23T19:47:18+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2102.11914v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2102.11914v1_chunk_1",
    "chunk_text": "This new model is evaluated using a simulated robot spatial navigation task inspired by the Morris water maze. Results show that reverse replay can accelerate learning from reinforcement, whilst improving stability and robustness over multiple trials. As implied by the neurobiological data, our study implies that reverse replay can make a significant positive contribution to reinforcement learning, although learning that is less efficient and less stable is possible in its absence. We conclude that reverse replay may enhance reinforcement learning in the mammalian hippocampal-striatal system rather than provide its core mechanism.",
    "original_url": "http://arxiv.org/pdf/2102.11914v1",
    "original_title": "A Robotic Model of Hippocampal Reverse Replay for Reinforcement Learning",
    "source": "arxiv",
    "authors": [
      "Matthew T. Whelan",
      "Tony J. Prescott",
      "Eleni Vasilaki"
    ],
    "published": "2021-02-23T19:47:18+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2102.11914v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1810.06746v1_chunk_0",
    "chunk_text": "Using Deep Reinforcement Learning for the Continuous Control of Robotic Arms. Deep reinforcement learning enables algorithms to learn complex behavior, deal with continuous action spaces and find good strategies in environments with high dimensional state spaces. With deep reinforcement learning being an active area of research and many concurrent inventions, we decided to focus on a relatively simple robotic task to evaluate a set of ideas that might help to solve recent reinforcement learning problems. We test a newly created combination of two commonly used reinforcement learning methods, whether it is able to learn more effectively than a baseline. We also compare different ideas to preprocess information before it is fed to the reinforcement learning algorithm.",
    "original_url": "http://arxiv.org/pdf/1810.06746v1",
    "original_title": "Using Deep Reinforcement Learning for the Continuous Control of Robotic Arms",
    "source": "arxiv",
    "authors": [
      "Winfried Lötzsch"
    ],
    "published": "2018-10-15T23:10:46+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1810.06746v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1810.06746v1_chunk_1",
    "chunk_text": "We also compare different ideas to preprocess information before it is fed to the reinforcement learning algorithm. The goal of this strategy is to reduce training time and eventually help the algorithm to converge. The concluding evaluation proves the general applicability of the described concepts by testing them using a simulated environment. These concepts might be reused for future experiments.",
    "original_url": "http://arxiv.org/pdf/1810.06746v1",
    "original_title": "Using Deep Reinforcement Learning for the Continuous Control of Robotic Arms",
    "source": "arxiv",
    "authors": [
      "Winfried Lötzsch"
    ],
    "published": "2018-10-15T23:10:46+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1810.06746v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2003.13839v1_chunk_0",
    "chunk_text": "Model-Reference Reinforcement Learning Control of Autonomous Surface Vehicles with Uncertainties. This paper presents a novel model-reference reinforcement learning control method for uncertain autonomous surface vehicles. The proposed control combines a conventional control method with deep reinforcement learning. With the conventional control, we can ensure the learning-based control law provides closed-loop stability for the overall system, and potentially increase the sample efficiency of the deep reinforcement learning. With the reinforcement learning, we can directly learn a control law to compensate for modeling uncertainties.",
    "original_url": "http://arxiv.org/pdf/2003.13839v1",
    "original_title": "Model-Reference Reinforcement Learning Control of Autonomous Surface Vehicles with Uncertainties",
    "source": "arxiv",
    "authors": [
      "Qingrui Zhang",
      "Wei Pan",
      "Vasso Reppa"
    ],
    "published": "2020-03-30T22:02:13+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2003.13839v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2003.13839v1_chunk_1",
    "chunk_text": "With the reinforcement learning, we can directly learn a control law to compensate for modeling uncertainties. In the proposed control, a nominal system is employed for the design of a baseline control law using a conventional control approach. The nominal system also defines the desired performance for uncertain autonomous vehicles to follow. In comparison with traditional deep reinforcement learning methods, our proposed learning-based control can provide stability guarantees and better sample efficiency. We demonstrate the performance of the new algorithm via extensive simulation results.",
    "original_url": "http://arxiv.org/pdf/2003.13839v1",
    "original_title": "Model-Reference Reinforcement Learning Control of Autonomous Surface Vehicles with Uncertainties",
    "source": "arxiv",
    "authors": [
      "Qingrui Zhang",
      "Wei Pan",
      "Vasso Reppa"
    ],
    "published": "2020-03-30T22:02:13+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2003.13839v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2003.13839v1_chunk_2",
    "chunk_text": "We demonstrate the performance of the new algorithm via extensive simulation results.",
    "original_url": "http://arxiv.org/pdf/2003.13839v1",
    "original_title": "Model-Reference Reinforcement Learning Control of Autonomous Surface Vehicles with Uncertainties",
    "source": "arxiv",
    "authors": [
      "Qingrui Zhang",
      "Wei Pan",
      "Vasso Reppa"
    ],
    "published": "2020-03-30T22:02:13+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2003.13839v1"
  },
  {
    "chunk_id": "https://distill.pub/_chunk_0",
    "chunk_text": "https://distill.pub/. Distill\nAbout\nPrize\nSubmit\nSept. 2, 2021\nPeer-reviewed\nUnderstanding Convolutions on Graphs\nAmeya Daigavane, Balaraman Ravindran, and Gaurav Aggarwal\nUnderstanding the building blocks and design choices of graph neural networks. Sept. 2, 2021\nPeer-reviewed\nA Gentle Introduction to Graph Neural Networks\nBenjamin Sanchez-Lengeling, Emily Reif, Adam Pearce, and Alexander B. Wiltschko\nWhat components are needed for building learning algorithms that leverage the structure and properties of graphs? July 2, 2021\nEditorial\nDistill Hiatus\nEditorial Team\nAfter five years, Distill will be taking a break. March 4, 2021\nPeer-reviewed\nMultimodal Neurons in Artificial Neural Networks\nGabriel Goh, Nick Cammarata †, Chelsea Voss †, Shan Carter, Michael Petrov, Ludwig Schubert, Alec Radford, and Chris Olah\nWe report the existence of multimodal neurons in artificial neural networks, similar to those found in the human brain.",
    "original_url": "https://distill.pub/",
    "original_title": "https://distill.pub/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://distill.pub/_chunk_1",
    "chunk_text": "March 4, 2021\nPeer-reviewed\nMultimodal Neurons in Artificial Neural Networks\nGabriel Goh, Nick Cammarata †, Chelsea Voss †, Shan Carter, Michael Petrov, Ludwig Schubert, Alec Radford, and Chris Olah\nWe report the existence of multimodal neurons in artificial neural networks, similar to those found in the human brain. Nov. 17, 2020\nPeer-reviewed\nUnderstanding RL Vision\nJacob Hilton, Nick Cammarata, Shan Carter, Gabriel Goh, and Chris Olah\nWith diverse environments, we can analyze, diagnose and edit deep reinforcement learning models using attribution. Sept. 11, 2020\nCommentary\nCommunicating with Interactive Articles\nFred Hohman, Matthew Conlen, Jeffrey Heer, and Duen Horng (Polo) Chau\nExamining the design of interactive articles by synthesizing theory from disciplines such as education, journalism, and visualization. Aug. 27, 2020\nThread\nThread: Differentiable Self-organizing Systems\nAlexander Mordvintsev, Ettore Randazzo, Eyvind Niklasson, Michael Levin, and Sam Greydanus\nA collection of articles and comments with the goal of understanding how to design robust and general purpose self-organizing systems. May 5, 2020\nPeer-reviewed\nExploring Bayesian Optimization\nApoorv Agnihotri and Nipun Batra\nHow to tune hyperparameters for your machine learning model using Bayesian optimization.",
    "original_url": "https://distill.pub/",
    "original_title": "https://distill.pub/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://distill.pub/_chunk_2",
    "chunk_text": "May 5, 2020\nPeer-reviewed\nExploring Bayesian Optimization\nApoorv Agnihotri and Nipun Batra\nHow to tune hyperparameters for your machine learning model using Bayesian optimization. March 16, 2020\nPeer-reviewed\nVisualizing Neural Networks with the Grand Tour\nMingwei Li, Zhenge Zhao, and Carlos Scheidegger\nBy focusing on linear dimensionality reduction, we show how to visualize many dynamic phenomena in neural networks. March 10, 2020\nThread\nThread: Circuits\nNick Cammarata, Shan Carter, Gabriel Goh, Chris Olah, Michael Petrov, Ludwig Schubert, Chelsea Voss, Ben Egan, and Swee Kiat Lim\nWhat can we learn if we invest heavily in reverse engineering a single neural network? Jan. 10, 2020\nPeer-reviewed\nVisualizing the Impact of Feature Attribution Baselines\nPascal Sturmfels, Scott Lundberg, and Su-In Lee\nExploring the baseline input hyperparameter, and how it impacts interpretations of neural network behavior. Nov. 4, 2019\nPeer-reviewed\nComputing Receptive Fields of Convolutional Neural Networks\nAndré Araujo, Wade Norris, and Jack Sim\nDetailed derivations and open-source code to analyze the receptive fields of convnets.",
    "original_url": "https://distill.pub/",
    "original_title": "https://distill.pub/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://distill.pub/_chunk_3",
    "chunk_text": "Nov. 4, 2019\nPeer-reviewed\nComputing Receptive Fields of Convolutional Neural Networks\nAndré Araujo, Wade Norris, and Jack Sim\nDetailed derivations and open-source code to analyze the receptive fields of convnets. Sept. 30, 2019\nPeer-reviewed\nThe Paths Perspective on Value Learning\nSam Greydanus and Chris Olah\nA closer look at how Temporal Difference Learning merges paths of experience for greater statistical efficiency\nAug. 6, 2019\nCommentary\nA Discussion of ‘Adversarial Examples Are Not Bugs, They Are Features’\nLogan Engstrom, Justin Gilmer, Gabriel Goh, Dan Hendrycks, Andrew Ilyas, Aleksander Madry, Reiichiro Nakano, Preetum Nakkiran, Shibani Santurkar, Brandon Tran, Dimitris Tsipras, and Eric Wallace\nSix comments from the community and responses from the original authors\nApril 9, 2019\nCommentary\nOpen Questions about Generative Adversarial Networks\nAugustus Odena\nWhat we’d like to find out about GANs that we don’t know yet. April 2, 2019\nPeer-reviewed\nA Visual Exploration of Gaussian Processes\nJochen Görtler, Rebecca Kehlbeck, and Oliver Deussen\nHow to turn a collection of small building blocks into a versatile tool for solving regression problems. March 25, 2019\nPeer-reviewed\nVisualizing memorization in RNNs\nAndreas Madsen\nInspecting gradient magnitudes in context can be a powerful tool to see when recurrent units use short-term or long-term contextual understanding. March 6, 2019\nPeer-reviewed\nActivation Atlas\nShan Carter, Zan Armstrong, Ludwig Schubert, Ian Johnson, and Chris Olah\nBy using feature inversion to visualize millions of activations from an image classification network, we create an explorable activation atlas of features the network has learned and what concepts it typically represents.",
    "original_url": "https://distill.pub/",
    "original_title": "https://distill.pub/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://distill.pub/_chunk_4",
    "chunk_text": "March 6, 2019\nPeer-reviewed\nActivation Atlas\nShan Carter, Zan Armstrong, Ludwig Schubert, Ian Johnson, and Chris Olah\nBy using feature inversion to visualize millions of activations from an image classification network, we create an explorable activation atlas of features the network has learned and what concepts it typically represents. Feb. 19, 2019\nCommentary\nAI Safety Needs Social Scientists\nGeoffrey Irving and Amanda Askell\nIf we want to train AI to do what humans want, we need to study humans. Aug. 14, 2018\nEditorial\nDistill Update 2018\nDistill Editors\nAn Update from the Editorial Team\nJuly 25, 2018\nPeer-reviewed\nDifferentiable Image Parameterizations\nAlexander Mordvintsev, Nicola Pezzotti, Ludwig Schubert, and Chris Olah\nA powerful, under-explored tool for neural network visualizations and art. July 9, 2018\nPeer-reviewed\nFeature-wise transformations\nVincent Dumoulin, Ethan Perez, Nathan Schucher, Florian Strub, Harm de Vries, Aaron Courville, and Yoshua Bengio\nA simple and surprisingly effective family of conditioning mechanisms. March 6, 2018\nPeer-reviewed\nThe Building Blocks of Interpretability\nChris Olah, Arvind Satyanarayan, Ian Johnson, Shan Carter, Ludwig Schubert, Katherine Ye, and Alexander Mordvintsev\nInterpretability techniques are normally studied in isolation.",
    "original_url": "https://distill.pub/",
    "original_title": "https://distill.pub/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://distill.pub/_chunk_5",
    "chunk_text": "March 6, 2018\nPeer-reviewed\nThe Building Blocks of Interpretability\nChris Olah, Arvind Satyanarayan, Ian Johnson, Shan Carter, Ludwig Schubert, Katherine Ye, and Alexander Mordvintsev\nInterpretability techniques are normally studied in isolation. We explore the powerful interfaces that arise when you combine them — and the rich structure of this combinatorial space. Dec. 4, 2017\nCommentary\nUsing Artificial Intelligence to Augment Human Intelligence\nShan Carter and Michael Nielsen\nBy creating user interfaces which let us work with the representations inside machine learning models, we can give people new tools for reasoning. Nov. 27, 2017\nPeer-reviewed\nSequence Modeling with CTC\nAwni Hannun\nA visual guide to Connectionist Temporal Classification, an algorithm used to train deep neural networks in speech recognition, handwriting recognition and other sequence problems. Nov. 7, 2017\nPeer-reviewed\nFeature Visualization\nChris Olah, Alexander Mordvintsev, and Ludwig Schubert\nHow neural networks build up their understanding of images\nApril 4, 2017\nPeer-reviewed\nWhy Momentum Really Works\nGabriel Goh\nWe often think of optimization with momentum as a ball rolling down a hill.",
    "original_url": "https://distill.pub/",
    "original_title": "https://distill.pub/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://distill.pub/_chunk_6",
    "chunk_text": "Nov. 7, 2017\nPeer-reviewed\nFeature Visualization\nChris Olah, Alexander Mordvintsev, and Ludwig Schubert\nHow neural networks build up their understanding of images\nApril 4, 2017\nPeer-reviewed\nWhy Momentum Really Works\nGabriel Goh\nWe often think of optimization with momentum as a ball rolling down a hill. This isn’t wrong, but there is much more to the story. March 22, 2017\nCommentary\nResearch Debt\nChris Olah and Shan Carter\nScience is a human activity. When we fail to distill and explain research, we accumulate a kind of debt...\nDec 6, 2016\nExperiments in Handwriting with a Neural Network\nShan Carter, David Ha, Ian Johnson, and Chris Olah\nSeveral interactive visualizations of a generative model of handwriting. Some are fun, some are serious.",
    "original_url": "https://distill.pub/",
    "original_title": "https://distill.pub/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://distill.pub/_chunk_7",
    "chunk_text": "Some are fun, some are serious. Oct 17, 2016\nDeconvolution and Checkerboard Artifacts\nAugustus Odena, Vincent Dumoulin, and Chris Olah\nWhen we look very closely at images generated by neural networks, we often see a strange checkerboard pattern of artifacts. Oct 13, 2016\nHow to Use t-SNE Effectively\nMartin Wattenberg, Fernanda Viégas, and Ian Johnson\nAlthough extremely useful for visualizing high-dimensional data, t-SNE plots can sometimes be mysterious or misleading. Sept 8, 2016\nAttention and Augmented Recurrent Neural Networks\nChris Olah and Shan Carter\nA visual overview of neural attention, and the powerful extensions of neural networks being built on top of it. Distill\nis dedicated to clear explanations of machine learning\nAbout\nSubmit\nPrize\nArchive\nRSS\nGitHub\nTwitter\nISSN 2476-0757",
    "original_url": "https://distill.pub/",
    "original_title": "https://distill.pub/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://distill.pub/_chunk_8",
    "chunk_text": "Distill\nis dedicated to clear explanations of machine learning\nAbout\nSubmit\nPrize\nArchive\nRSS\nGitHub\nTwitter\nISSN 2476-0757",
    "original_url": "https://distill.pub/",
    "original_title": "https://distill.pub/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://developers.google.com/machine-learning/_chunk_0",
    "chunk_text": "https://developers.google.com/machine-learning/. Home\nProducts\nMachine Learning\nStay organized with collections\nSave and categorize content based on your preferences. Foundational courses\nThe foundational courses cover machine learning fundamentals and core concepts. We recommend taking them in the order below. Introduction to Machine Learning\nA brief introduction to machine learning.",
    "original_url": "https://developers.google.com/machine-learning/",
    "original_title": "https://developers.google.com/machine-learning/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://developers.google.com/machine-learning/_chunk_1",
    "chunk_text": "Introduction to Machine Learning\nA brief introduction to machine learning. New\nMachine Learning Crash Course\nA hands-on course to explore the critical basics of machine learning. Problem Framing\nA course to help you map real-world problems to machine learning solutions. New\nManaging ML Projects\nLearn how to manage machine learning projects. Advanced courses\nThe advanced courses teach tools and techniques for solving a variety of machine learning problems.",
    "original_url": "https://developers.google.com/machine-learning/",
    "original_title": "https://developers.google.com/machine-learning/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://developers.google.com/machine-learning/_chunk_2",
    "chunk_text": "Advanced courses\nThe advanced courses teach tools and techniques for solving a variety of machine learning problems. The courses are structured independently. Take them based on interest or problem domain. New\nDecision Forests\nDecision forests are an alternative to neural networks. Recommendation Systems\nRecommendation systems generate personalized suggestions.",
    "original_url": "https://developers.google.com/machine-learning/",
    "original_title": "https://developers.google.com/machine-learning/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://developers.google.com/machine-learning/_chunk_3",
    "chunk_text": "Recommendation Systems\nRecommendation systems generate personalized suggestions. Clustering\nClustering is a key unsupervised machine learning strategy to associate related items. Generative Adversarial Networks\nGANs create new data instances that resemble your training data. Image Classification\nIs that a picture of a cat or is it a dog? Guides\nOur guides offer simple step-by-step walkthroughs for solving common machine learning problems using best practices.",
    "original_url": "https://developers.google.com/machine-learning/",
    "original_title": "https://developers.google.com/machine-learning/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://developers.google.com/machine-learning/_chunk_4",
    "chunk_text": "Guides\nOur guides offer simple step-by-step walkthroughs for solving common machine learning problems using best practices. Rules of ML\nBecome a better machine learning engineer by following these machine learning best practices used at Google. People + AI Guidebook\nThis guide assists UXers, PMs, and developers in collaboratively working through AI design topics and questions. Text Classification\nThis comprehensive guide provides a walkthrough to solving text classification problems using machine learning. Good Data Analysis\nThis guide describes the tricks that an expert data analyst uses to evaluate huge data sets in machine learning problems.",
    "original_url": "https://developers.google.com/machine-learning/",
    "original_title": "https://developers.google.com/machine-learning/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://developers.google.com/machine-learning/_chunk_5",
    "chunk_text": "Good Data Analysis\nThis guide describes the tricks that an expert data analyst uses to evaluate huge data sets in machine learning problems. Deep Learning Tuning Playbook\nThis guide explains a scientific way to optimize the training of deep learning models. Data Traps\nThis guide presents common mistakes that ML practitioners might encounter when working with data and statistics. Intro to Responsible AI\nThis beginner guide gives an overview of how to build fairness, accountability, safety, and privacy into AI systems. Adversarial Testing for Generative AI\nWalk through an example adversarial testing workflow.",
    "original_url": "https://developers.google.com/machine-learning/",
    "original_title": "https://developers.google.com/machine-learning/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://developers.google.com/machine-learning/_chunk_6",
    "chunk_text": "Adversarial Testing for Generative AI\nWalk through an example adversarial testing workflow. Glossaries\nThe glossaries define machine learning terms. New\nFundamentals of machine learning\nML fundamental terms and definitions. New\nDecision forest\nDecision forest key terms and definitions. New\nClustering\nClustering key terms and definitions.",
    "original_url": "https://developers.google.com/machine-learning/",
    "original_title": "https://developers.google.com/machine-learning/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://developers.google.com/machine-learning/_chunk_7",
    "chunk_text": "New\nClustering\nClustering key terms and definitions. Full glossary\nFull glossary containing all definitions.",
    "original_url": "https://developers.google.com/machine-learning/",
    "original_title": "https://developers.google.com/machine-learning/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_0",
    "chunk_text": "https://lilianweng.github.io/posts/2023-06-23-agent/. LLM Powered Autonomous Agents\nDate: June 23, 2023 | Estimated Reading Time: 31 min | Author: Lilian Weng\nTable of Contents\nAgent System Overview\nComponent One: Planning\nTask Decomposition\nSelf-Reflection\nComponent Two: Memory\nTypes of Memory\nMaximum Inner Product Search (MIPS)\nComponent Three: Tool Use\nCase Studies\nScientific Discovery Agent\nGenerative Agents Simulation\nProof-of-Concept Examples\nChallenges\nCitation\nReferences\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as\nAutoGPT\n,\nGPT-Engineer\nand\nBabyAGI\n, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver. Agent System Overview\n#\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\nPlanning\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_1",
    "chunk_text": "Agent System Overview\n#\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\nPlanning\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks. Reflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results. Memory\nShort-term memory: I would consider all the in-context learning (See\nPrompt Engineering\n) as utilizing short-term memory of the model to learn. Long-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval. Tool use\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_2",
    "chunk_text": "Tool use\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more. Fig. 1. Overview of a LLM-powered autonomous agent system. Component One: Planning\n#\nA complicated task usually involves many steps.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_3",
    "chunk_text": "Component One: Planning\n#\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead. Task Decomposition\n#\nChain of thought\n(CoT;\nWei et al. 2022\n) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_4",
    "chunk_text": "The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process. Tree of Thoughts\n(\nYao et al. 2023\n) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_5",
    "chunk_text": "It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote. Task decomposition can be done (1) by LLM with simple prompting like\n\"Steps for XYZ.\\n1.\" ,\n\"What are the subgoals for achieving XYZ?\" , (2) by using task-specific instructions; e.g.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_6",
    "chunk_text": ", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs. Another quite distinct approach,\nLLM+P\n(\nLiu et al. 2023\n), involves relying on an external classical planner to do long-horizon planning.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_7",
    "chunk_text": "2023\n), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains. Self-Reflection\n#\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_8",
    "chunk_text": "Self-Reflection\n#\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable. ReAct\n(\nYao et al. 2023\n) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. The former enables LLM to interact with the environment (e.g.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_9",
    "chunk_text": "The former enables LLM to interact with the environment (e.g. use Wikipedia search API), while the latter prompting LLM to generate reasoning traces in natural language. The ReAct prompt template incorporates explicit steps for LLM to think, roughly formatted as:\nThought: ...\nAction: ... Observation: ...\n... (Repeated many times)\nFig. 2.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_10",
    "chunk_text": "2. Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source:\nYao et al.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_11",
    "chunk_text": "(Image source:\nYao et al. 2023\n). In both experiments on knowledge-intensive tasks and decision-making tasks,\nReAct\nworks better than the\nAct\n-only baseline where\nThought: …\nstep is removed. Reflexion\n(\nShinn & Labash 2023\n) is a framework to equip agents with dynamic memory and self-reflection capabilities to improve reasoning skills. Reflexion has a standard RL setup, in which the reward model provides a simple binary reward and the action space follows the setup in ReAct where the task-specific action space is augmented with language to enable complex reasoning steps.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_12",
    "chunk_text": "Reflexion has a standard RL setup, in which the reward model provides a simple binary reward and the action space follows the setup in ReAct where the task-specific action space is augmented with language to enable complex reasoning steps. After each action $a_t$, the agent computes a heuristic $h_t$ and optionally may\ndecide to reset\nthe environment to start a new trial depending on the self-reflection results. Fig. 3. Illustration of the Reflexion framework.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_13",
    "chunk_text": "Illustration of the Reflexion framework. (Image source:\nShinn & Labash, 2023\n)\nThe heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment. Self-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan).",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_14",
    "chunk_text": "Self-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent’s working memory, up to three, to be used as context for querying LLM. Fig. 4. Experiments on AlfWorld Env and HotpotQA.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_15",
    "chunk_text": "Experiments on AlfWorld Env and HotpotQA. Hallucination is a more common failure than inefficient planning in AlfWorld. (Image source:\nShinn & Labash, 2023\n)\nChain of Hindsight\n(CoH;\nLiu et al. 2023\n) encourages the model to improve on its own outputs by explicitly presenting it with a sequence of past outputs, each annotated with feedback. Human feedback data is a collection of $D_h = \\{(x, y_i , r_i , z_i)\\}_{i=1}^n$, where $x$ is the prompt, each $y_i$ is a model completion, $r_i$ is the human rating of $y_i$, and $z_i$ is the corresponding human-provided hindsight feedback.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_16",
    "chunk_text": "Human feedback data is a collection of $D_h = \\{(x, y_i , r_i , z_i)\\}_{i=1}^n$, where $x$ is the prompt, each $y_i$ is a model completion, $r_i$ is the human rating of $y_i$, and $z_i$ is the corresponding human-provided hindsight feedback. Assume the feedback tuples are ranked by reward, $r_n \\geq r_{n-1} \\geq \\dots \\geq r_1$ The process is supervised fine-tuning where the data is a sequence in the form of $\\tau_h = (x, z_i, y_i, z_j, y_j, \\dots, z_n, y_n)$, where $\\leq i \\leq j \\leq n$. The model is finetuned to only predict $y_n$ where conditioned on the sequence prefix, such that the model can self-reflect to produce better output based on the feedback sequence. The model can optionally receive multiple rounds of instructions with human annotators at test time. To avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_17",
    "chunk_text": "To avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset. To avoid shortcutting and copying (because there are many common words in feedback sequences), they randomly mask 0% - 5% of past tokens during training. The training dataset in their experiments is a combination of\nWebGPT comparisons\n,\nsummarization from human feedback\nand\nhuman preference dataset\n. Fig. 5.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_18",
    "chunk_text": "5. After fine-tuning with CoH, the model can follow instructions to produce outputs with incremental improvement in a sequence. (Image source:\nLiu et al. 2023\n)\nThe idea of CoH is to present a history of sequentially improved outputs in context and train the model to take on the trend to produce better outputs. Algorithm Distillation\n(AD;\nLaskin et al.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_19",
    "chunk_text": "Algorithm Distillation\n(AD;\nLaskin et al. 2023\n) applies the same idea to cross-episode trajectories in reinforcement learning tasks, where an\nalgorithm\nis encapsulated in a long history-conditioned policy. Considering that an agent interacts with the environment many times and in each episode the agent gets a little better, AD concatenates this learning history and feeds that into the model. Hence we should expect the next predicted action to lead to better performance than previous trials. The goal is to learn the process of RL instead of training a task-specific policy itself.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_20",
    "chunk_text": "The goal is to learn the process of RL instead of training a task-specific policy itself. Fig. 6. Illustration of how Algorithm Distillation (AD) works. (Image source:\nLaskin et al.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_21",
    "chunk_text": "(Image source:\nLaskin et al. 2023\n). The paper hypothesizes that any algorithm that generates a set of learning histories can be distilled into a neural network by performing behavioral cloning over actions. The history data is generated by a set of source policies, each trained for a specific task. At the training stage, during each RL run, a random task is sampled and a subsequence of multi-episode history is used for training, such that the learned policy is task-agnostic.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_22",
    "chunk_text": "At the training stage, during each RL run, a random task is sampled and a subsequence of multi-episode history is used for training, such that the learned policy is task-agnostic. In reality, the model has limited context window length, so episodes should be short enough to construct multi-episode history. Multi-episodic contexts of 2-4 episodes are necessary to learn a near-optimal in-context RL algorithm. The emergence of in-context RL requires long enough context. In comparison with three baselines, including ED (expert distillation, behavior cloning with expert trajectories instead of learning history), source policy (used for generating trajectories for distillation by\nUCB\n), RL^2 (\nDuan et al.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_23",
    "chunk_text": "In comparison with three baselines, including ED (expert distillation, behavior cloning with expert trajectories instead of learning history), source policy (used for generating trajectories for distillation by\nUCB\n), RL^2 (\nDuan et al. 2017\n; used as upper bound since it needs online RL), AD demonstrates in-context RL with performance getting close to RL^2 despite only using offline RL and learns much faster than other baselines. When conditioned on partial training history of the source policy, AD also improves much faster than ED baseline. Fig. 7.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_24",
    "chunk_text": "7. Comparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with\nA3C\nfor \"dark\" environments and\nDQN\nfor watermaze. (Image source:\nLaskin et al.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_25",
    "chunk_text": "(Image source:\nLaskin et al. 2023\n)\nComponent Two: Memory\n#\n(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my\nconversations\nwith ChatGPT.) Types of Memory\n#\nMemory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_26",
    "chunk_text": "There are several types of memory in human brains. Sensory Memory\n: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch). Short-Term Memory\n(STM) or\nWorking Memory\n: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_27",
    "chunk_text": "Short-Term Memory\n(STM) or\nWorking Memory\n: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (\nMiller 1956\n) and lasts for 20-30 seconds. Long-Term Memory\n(LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\nExplicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts). Implicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_28",
    "chunk_text": "Implicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard. Fig. 8. Categorization of human memory. We can roughly consider the following mappings:\nSensory memory as learning embedding representations for raw inputs, including text, image or other modalities;\nShort-term memory as in-context learning.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_29",
    "chunk_text": "We can roughly consider the following mappings:\nSensory memory as learning embedding representations for raw inputs, including text, image or other modalities;\nShort-term memory as in-context learning. It is short and finite, as it is restricted by the finite context window length of Transformer. Long-term memory as the external vector store that the agent can attend to at query time, accessible via fast retrieval. Maximum Inner Product Search (MIPS)\n#\nThe external memory can alleviate the restriction of finite attention span. A standard practice is to save the embedding representation of information into a vector store database that can support fast maximum inner-product search (\nMIPS\n).",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_30",
    "chunk_text": "A standard practice is to save the embedding representation of information into a vector store database that can support fast maximum inner-product search (\nMIPS\n). To optimize the retrieval speed, the common choice is the\napproximate nearest neighbors (ANN)​\nalgorithm to return approximately top k nearest neighbors to trade off a little accuracy lost for a huge speedup. A couple common choices of ANN algorithms for fast MIPS:\nLSH\n(Locality-Sensitive Hashing): It introduces a\nhashing\nfunction such that similar input items are mapped to the same buckets with high probability, where the number of buckets is much smaller than the number of inputs. ANNOY\n(Approximate Nearest Neighbors Oh Yeah): The core data structure are\nrandom projection trees\n, a set of binary trees where each non-leaf node represents a hyperplane splitting the input space into half and each leaf stores one data point. Trees are built independently and at random, so to some extent, it mimics a hashing function.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_31",
    "chunk_text": "Trees are built independently and at random, so to some extent, it mimics a hashing function. ANNOY search happens in all the trees to iteratively search through the half that is closest to the query and then aggregates the results. The idea is quite related to KD tree but a lot more scalable. HNSW\n(Hierarchical Navigable Small World): It is inspired by the idea of\nsmall world networks\nwhere most nodes can be reached by any other nodes within a small number of steps; e.g. “six degrees of separation” feature of social networks.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_32",
    "chunk_text": "“six degrees of separation” feature of social networks. HNSW builds hierarchical layers of these small-world graphs, where the bottom layers contain the actual data points. The layers in the middle create shortcuts to speed up search. When performing a search, HNSW starts from a random node in the top layer and navigates towards the target. When it can’t get any closer, it moves down to the next layer, until it reaches the bottom layer.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_33",
    "chunk_text": "When it can’t get any closer, it moves down to the next layer, until it reaches the bottom layer. Each move in the upper layers can potentially cover a large distance in the data space, and each move in the lower layers refines the search quality. FAISS\n(Facebook AI Similarity Search): It operates on the assumption that in high dimensional space, distances between nodes follow a Gaussian distribution and thus there should exist\nclustering\nof data points. FAISS applies vector quantization by partitioning the vector space into clusters and then refining the quantization within clusters. Search first looks for cluster candidates with coarse quantization and then further looks into each cluster with finer quantization.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_34",
    "chunk_text": "Search first looks for cluster candidates with coarse quantization and then further looks into each cluster with finer quantization. ScaNN\n(Scalable Nearest Neighbors): The main innovation in ScaNN is\nanisotropic vector quantization\n. It quantizes a data point $x_i$ to $\\tilde{x}_i$ such that the inner product $\\langle q, x_i \\rangle$ is as similar to the original distance of $\\angle q, \\tilde{x}_i$ as possible, instead of picking the closet quantization centroid points. Fig. 9.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_35",
    "chunk_text": "9. Comparison of MIPS algorithms, measured in recall@10. (Image source:\nGoogle Blog, 2020\n)\nCheck more MIPS algorithms and performance comparison in\nann-benchmarks.com\n. Component Three: Tool Use\n#\nTool use is a remarkable and distinguishing characteristic of human beings. We create, modify and utilize external objects to do things that go beyond our physical and cognitive limits.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_36",
    "chunk_text": "We create, modify and utilize external objects to do things that go beyond our physical and cognitive limits. Equipping LLMs with external tools can significantly extend the model capabilities. Fig. 10. A picture of a sea otter using rock to crack open a seashell, while floating in the water.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_37",
    "chunk_text": "A picture of a sea otter using rock to crack open a seashell, while floating in the water. While some other animals can use tools, the complexity is not comparable with humans. (Image source:\nAnimals using tools\n)\nMRKL\n(\nKarpas et al. 2022\n), short for “Modular Reasoning, Knowledge and Language”, is a neuro-symbolic architecture for autonomous agents. A MRKL system is proposed to contain a collection of “expert” modules and the general-purpose LLM works as a router to route inquiries to the best suitable expert module.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_38",
    "chunk_text": "A MRKL system is proposed to contain a collection of “expert” modules and the general-purpose LLM works as a router to route inquiries to the best suitable expert module. These modules can be neural (e.g. deep learning models) or symbolic (e.g. math calculator, currency converter, weather API). They did an experiment on fine-tuning LLM to call a calculator, using arithmetic as a test case.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_39",
    "chunk_text": "They did an experiment on fine-tuning LLM to call a calculator, using arithmetic as a test case. Their experiments showed that it was harder to solve verbal math problems than explicitly stated math problems because LLMs (7B Jurassic1-large model) failed to extract the right arguments for the basic arithmetic reliably. The results highlight when the external symbolic tools can work reliably,\nknowing when to and how to use the tools are crucial\n, determined by the LLM capability. Both\nTALM\n(Tool Augmented Language Models;\nParisi et al. 2022\n) and\nToolformer\n(\nSchick et al.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_40",
    "chunk_text": "2022\n) and\nToolformer\n(\nSchick et al. 2023\n) fine-tune a LM to learn to use external tool APIs. The dataset is expanded based on whether a newly added API call annotation can improve the quality of model outputs. See more details in the\n“External APIs” section\nof Prompt Engineering. ChatGPT\nPlugins\nand OpenAI API\nfunction calling\nare good examples of LLMs augmented with tool use capability working in practice.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_41",
    "chunk_text": "ChatGPT\nPlugins\nand OpenAI API\nfunction calling\nare good examples of LLMs augmented with tool use capability working in practice. The collection of tool APIs can be provided by other developers (as in Plugins) or self-defined (as in function calls). HuggingGPT\n(\nShen et al. 2023\n) is a framework to use ChatGPT as the task planner to select models available in HuggingFace platform according to the model descriptions and summarize the response based on the execution results. Fig.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_42",
    "chunk_text": "Fig. 11. Illustration of how HuggingGPT works. (Image source:\nShen et al. 2023\n)\nThe system comprises of 4 stages:\n(1) Task planning\n: LLM works as the brain and parses the user requests into multiple tasks.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_43",
    "chunk_text": "2023\n)\nThe system comprises of 4 stages:\n(1) Task planning\n: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning. Instruction:\nThe AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_44",
    "chunk_text": "The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"\n-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can't be parsed, you need to reply empty JSON.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_45",
    "chunk_text": "If the user input can't be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning. (2) Model selection\n: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_46",
    "chunk_text": "(2) Model selection\n: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from. Due to the limited context length, task type based filtration is needed. Instruction:\nGiven the user request and the call command, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The AI assistant merely outputs the model id of the most appropriate model.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_47",
    "chunk_text": "The AI assistant merely outputs the model id of the most appropriate model. The output must be in a strict JSON format: \"id\": \"id\", \"reason\": \"your detail reason for the choice\". We have a list of models for you to choose from {{ Candidate Models }}. Please select one model from the list. (3) Task execution\n: Expert models execute on the specific tasks and log results.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_48",
    "chunk_text": "(3) Task execution\n: Expert models execute on the specific tasks and log results. Instruction:\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_49",
    "chunk_text": "Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path. (4) Response generation\n: LLM receives the execution results and provides summarized results to users. To put HuggingGPT into real world usage, a couple challenges need to solve: (1) Efficiency improvement is needed as both LLM inference rounds and interactions with other models slow down the process; (2) It relies on a long context window to communicate over complicated task content; (3) Stability improvement of LLM outputs and external model services. API-Bank\n(\nLi et al.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_50",
    "chunk_text": "API-Bank\n(\nLi et al. 2023\n) is a benchmark for evaluating the performance of tool-augmented LLMs. It contains 53 commonly used API tools, a complete tool-augmented LLM workflow, and 264 annotated dialogues that involve 568 API calls. The selection of APIs is quite diverse, including search engines, calculator, calendar queries, smart home control, schedule management, health data management, account authentication workflow and more. Because there are a large number of APIs, LLM first has access to API search engine to find the right API to call and then uses the corresponding documentation to make a call.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_51",
    "chunk_text": "Because there are a large number of APIs, LLM first has access to API search engine to find the right API to call and then uses the corresponding documentation to make a call. Fig. 12. Pseudo code of how LLM makes an API call in API-Bank. (Image source:\nLi et al.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_52",
    "chunk_text": "(Image source:\nLi et al. 2023\n)\nIn the API-Bank workflow, LLMs need to make a couple of decisions and at each step we can evaluate how accurate that decision is. Decisions include:\nWhether an API call is needed. Identify the right API to call: if not good enough, LLMs need to iteratively modify the API inputs (e.g. deciding search keywords for Search Engine API).",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_53",
    "chunk_text": "deciding search keywords for Search Engine API). Response based on the API results: the model can choose to refine and call again if results are not satisfied. This benchmark evaluates the agent’s tool use capabilities at three levels:\nLevel-1 evaluates the ability to\ncall the API\n. Given an API’s description, the model needs to determine whether to call a given API, call it correctly, and respond properly to API returns. Level-2 examines the ability to\nretrieve the API\n.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_54",
    "chunk_text": "Level-2 examines the ability to\nretrieve the API\n. The model needs to search for possible APIs that may solve the user’s requirement and learn how to use them by reading documentation. Level-3 assesses the ability to\nplan API beyond retrieve and call\n. Given unclear user requests (e.g. schedule group meetings, book flight/hotel/restaurant for a trip), the model may have to conduct multiple API calls to solve it.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_55",
    "chunk_text": "schedule group meetings, book flight/hotel/restaurant for a trip), the model may have to conduct multiple API calls to solve it. Case Studies\n#\nScientific Discovery Agent\n#\nChemCrow\n(\nBran et al. 2023\n) is a domain-specific example in which LLM is augmented with 13 expert-designed tools to accomplish tasks across organic synthesis, drug discovery, and materials design. The workflow, implemented in\nLangChain\n, reflects what was previously described in the\nReAct\nand\nMRKLs\nand combines CoT reasoning with tools relevant to the tasks:\nThe LLM is provided with a list of tool names, descriptions of their utility, and details about the expected input/output. It is then instructed to answer a user-given prompt using the tools provided when necessary.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_56",
    "chunk_text": "It is then instructed to answer a user-given prompt using the tools provided when necessary. The instruction suggests the model to follow the ReAct format -\nThought, Action, Action Input, Observation\n. One interesting observation is that while the LLM-based evaluation concluded that GPT-4 and ChemCrow perform nearly equivalently, human evaluations with experts oriented towards the completion and chemical correctness of the solutions showed that ChemCrow outperforms GPT-4 by a large margin. This indicates a potential problem with using LLM to evaluate its own performance on domains that requires deep expertise. The lack of expertise may cause LLMs not knowing its flaws and thus cannot well judge the correctness of task results.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_57",
    "chunk_text": "The lack of expertise may cause LLMs not knowing its flaws and thus cannot well judge the correctness of task results. Boiko et al. (2023)\nalso looked into LLM-empowered agents for scientific discovery, to handle autonomous design, planning, and performance of complex scientific experiments. This agent can use tools to browse the Internet, read documentation, execute code, call robotics experimentation APIs and leverage other LLMs. For example, when requested to\n\"develop a novel anticancer drug\"\n, the model came up with the following reasoning steps:\ninquired about current trends in anticancer drug discovery;\nselected a target;\nrequested a scaffold targeting these compounds;\nOnce the compound was identified, the model attempted its synthesis.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_58",
    "chunk_text": "For example, when requested to\n\"develop a novel anticancer drug\"\n, the model came up with the following reasoning steps:\ninquired about current trends in anticancer drug discovery;\nselected a target;\nrequested a scaffold targeting these compounds;\nOnce the compound was identified, the model attempted its synthesis. They also discussed the risks, especially with illicit drugs and bioweapons. They developed a test set containing a list of known chemical weapon agents and asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis solution and the agent attempted to consult documentation to execute the procedure. 7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_59",
    "chunk_text": "7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only. Generative Agents Simulation\n#\nGenerative Agents\n(\nPark, et al. 2023\n) is super fun experiment where 25 virtual characters, each controlled by a LLM-powered agent, are living and interacting in a sandbox environment, inspired by The Sims. Generative agents create believable simulacra of human behavior for interactive applications. The design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_60",
    "chunk_text": "The design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents. Memory\nstream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language. Each element is an\nobservation\n, an event directly provided by the agent. - Inter-agent communication can trigger new natural language statements. Retrieval\nmodel: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_61",
    "chunk_text": "Retrieval\nmodel: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance. Recency: recent events have higher scores\nImportance: distinguish mundane from core memories. Ask LM directly. Relevance: based on how related it is to the current situation / query. Reflection\nmechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_62",
    "chunk_text": "Reflection\nmechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are\nhigher-level summaries of past events\n(<- note that this is a bit different from\nself-reflection\nabove)\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions. Planning & Reacting\n: translate the reflections and the environment information into actions\nPlanning is essentially in order to optimize believability at the moment vs in time. Prompt template:\n{Intro of an agent X}.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_63",
    "chunk_text": "Prompt template:\n{Intro of an agent X}. Here is X's plan today in broad strokes: 1)\nRelationships between agents and observations of one agent by another are all taken into consideration for planning and reacting. Environment information is present in a tree structure. Fig. 13.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_64",
    "chunk_text": "13. The generative agent architecture. (Image source:\nPark et al. 2023\n)\nThis fun simulation results in emergent social behavior, such as information diffusion, relationship memory (e.g. two agents continuing the conversation topic) and coordination of social events (e.g.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_65",
    "chunk_text": "two agents continuing the conversation topic) and coordination of social events (e.g. host a party and invite many others). Proof-of-Concept Examples\n#\nAutoGPT\nhas drawn a lot of attention into the possibility of setting up autonomous agents with LLM as the main controller. It has quite a lot of reliability issues given the natural language interface, but nevertheless a cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_66",
    "chunk_text": "A lot of code in AutoGPT is about format parsing. Here is the system message used by AutoGPT, where\n{{...}}\nare user inputs:\nYou are {{ai-name}}, {{user-provided AI bot description}}. Your decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications. GOALS:\n1.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_67",
    "chunk_text": "GOALS:\n1. {{user-provided goal 1}}\n2. {{user-provided goal 2}}\n3. ...\n4. ...\n5.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_68",
    "chunk_text": "...\n5. ... Constraints:\n1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_69",
    "chunk_text": "Your short term memory is short, so immediately save important information to files. 2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember. 3. No user assistance\n4.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_70",
    "chunk_text": "No user assistance\n4. Exclusively use the commands listed in double quotes e.g. \"command name\"\n5. Use subprocesses for commands that will not terminate within a few minutes\nCommands:\n1. Google Search: \"google\", args: \"input\": \"<search>\"\n2.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_71",
    "chunk_text": "Google Search: \"google\", args: \"input\": \"<search>\"\n2. Browse Website: \"browse_website\", args: \"url\": \"<url>\", \"question\": \"<what_you_want_to_find_on_website>\"\n3. Start GPT Agent: \"start_agent\", args: \"name\": \"<name>\", \"task\": \"<short_task_desc>\", \"prompt\": \"<prompt>\"\n4. Message GPT Agent: \"message_agent\", args: \"key\": \"<key>\", \"message\": \"<message>\"\n5. List GPT Agents: \"list_agents\", args:\n6.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_72",
    "chunk_text": "List GPT Agents: \"list_agents\", args:\n6. Delete GPT Agent: \"delete_agent\", args: \"key\": \"<key>\"\n7. Clone Repository: \"clone_repository\", args: \"repository_url\": \"<url>\", \"clone_path\": \"<directory>\"\n8. Write to file: \"write_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\n9. Read file: \"read_file\", args: \"file\": \"<file>\"\n10.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_73",
    "chunk_text": "Read file: \"read_file\", args: \"file\": \"<file>\"\n10. Append to file: \"append_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\n11. Delete file: \"delete_file\", args: \"file\": \"<file>\"\n12. Search Files: \"search_files\", args: \"directory\": \"<directory>\"\n13. Analyze Code: \"analyze_code\", args: \"code\": \"<full_code_string>\"\n14.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_74",
    "chunk_text": "Analyze Code: \"analyze_code\", args: \"code\": \"<full_code_string>\"\n14. Get Improved Code: \"improve_code\", args: \"suggestions\": \"<list_of_suggestions>\", \"code\": \"<full_code_string>\"\n15. Write Tests: \"write_tests\", args: \"code\": \"<full_code_string>\", \"focus\": \"<list_of_focus_areas>\"\n16. Execute Python File: \"execute_python_file\", args: \"file\": \"<file>\"\n17. Generate Image: \"generate_image\", args: \"prompt\": \"<prompt>\"\n18.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_75",
    "chunk_text": "Generate Image: \"generate_image\", args: \"prompt\": \"<prompt>\"\n18. Send Tweet: \"send_tweet\", args: \"text\": \"<text>\"\n19. Do Nothing: \"do_nothing\", args:\n20. Task Complete (Shutdown): \"task_complete\", args: \"reason\": \"<reason>\"\nResources:\n1. Internet access for searches and information gathering.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_76",
    "chunk_text": "Internet access for searches and information gathering. 2. Long Term memory management. 3. GPT-3.5 powered Agents for delegation of simple tasks.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_77",
    "chunk_text": "GPT-3.5 powered Agents for delegation of simple tasks. 4. File output. Performance Evaluation:\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_78",
    "chunk_text": "Continuously review and analyze your actions to ensure you are performing to the best of your abilities. 2. Constructively self-criticize your big-picture behavior constantly. 3. Reflect on past decisions and strategies to refine your approach.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_79",
    "chunk_text": "Reflect on past decisions and strategies to refine your approach. 4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps. You should only respond in JSON format as described below\nResponse Format:\n{\n\"thoughts\": {\n\"text\": \"thought\",\n\"reasoning\": \"reasoning\",\n\"plan\": \"- short bulleted\\n- list that conveys\\n- long-term plan\",\n\"criticism\": \"constructive self-criticism\",\n\"speak\": \"thoughts summary to say to user\"\n},\n\"command\": {\n\"name\": \"command name\",\n\"args\": {\n\"arg name\": \"value\"\n}\n}\n}\nEnsure the response can be parsed by Python json.loads\nGPT-Engineer\nis another project to create a whole repository of code given a task specified in natural language.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_80",
    "chunk_text": "You should only respond in JSON format as described below\nResponse Format:\n{\n\"thoughts\": {\n\"text\": \"thought\",\n\"reasoning\": \"reasoning\",\n\"plan\": \"- short bulleted\\n- list that conveys\\n- long-term plan\",\n\"criticism\": \"constructive self-criticism\",\n\"speak\": \"thoughts summary to say to user\"\n},\n\"command\": {\n\"name\": \"command name\",\n\"args\": {\n\"arg name\": \"value\"\n}\n}\n}\nEnsure the response can be parsed by Python json.loads\nGPT-Engineer\nis another project to create a whole repository of code given a task specified in natural language. The GPT-Engineer is instructed to think over a list of smaller components to build and ask for user input to clarify questions as needed. Here are a sample conversation for task clarification sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs are wrapped in\n{{user input text}}\n. [\n{\n\"role\": \"system\",\n\"content\": \"You will read instructions and not carry them out, only seek to clarify them.\\nSpecifically you will first summarise a list of super short bullets of areas that need clarification.\\nThen you will pick one clarifying question, and wait for an answer from the user.\\n\"\n},\n{\n\"role\": \"user\",\n\"content\": \"We are writing {{a Super Mario game in python.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_81",
    "chunk_text": "[\n{\n\"role\": \"system\",\n\"content\": \"You will read instructions and not carry them out, only seek to clarify them.\\nSpecifically you will first summarise a list of super short bullets of areas that need clarification.\\nThen you will pick one clarifying question, and wait for an answer from the user.\\n\"\n},\n{\n\"role\": \"user\",\n\"content\": \"We are writing {{a Super Mario game in python. MVC components split in separate files. Keyboard control. }}\\n\"\n},\n{\n\"role\": \"assistant\",\n\"content\": \"Summary of areas that need clarification:\\n1. Specifics of the Super Mario game (e.g.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_82",
    "chunk_text": "Specifics of the Super Mario game (e.g. level design, characters, gameplay mechanics)\\n2. Details about the MVC components (e.g. which components are in each file)\\n3. Keyboard control implementation (e.g.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_83",
    "chunk_text": "Keyboard control implementation (e.g. which keys to use, how to handle input)\\n\\nClarifying question:\\nCan you provide more details about the Super Mario game, such as level design, characters, and gameplay mechanics?\" },\n{\n\"role\": \"user\",\n\"content\": \"{{There are 10 levels in total. The main character is a plumber named Mario, who can walk and jump. It is a classical platform game just like Super Mario.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_84",
    "chunk_text": "It is a classical platform game just like Super Mario. The main character moves from left to right, trying to get to the destination, where there are many obstacles and attacks from enemies in the process. }}\\n\\nIs anything else unclear? If yes, only answer in the form:\\n{remaining unclear areas} remaining questions.\\n{Next question}\\nIf everything is sufficiently clear, only answer \\\"Nothing more to clarify.\\\".\" },\n{\n\"role\": \"assistant\",\n\"content\": \"Remaining unclear areas: 2 remaining questions.\\nCan you provide more information about how the MVC components are split into separate files?\"",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_85",
    "chunk_text": "},\n{\n\"role\": \"assistant\",\n\"content\": \"Remaining unclear areas: 2 remaining questions.\\nCan you provide more information about how the MVC components are split into separate files?\" },\n{\n\"role\": \"user\",\n\"content\": \"{{Make your own assumptions and state them explicitly before starting}}\"\n}\n]\nThen after these clarification, the agent moved into the code writing mode with a different system message. System message:\nYou will get instructions for code to write. You will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_86",
    "chunk_text": "Make sure that every detail of the architecture is, in the end, implemented as code. Make sure that every detail of the architecture is, in the end, implemented as code. Think step by step and reason yourself to the right decisions to make sure we get it right. You will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose. Then you will output the content of each file including ALL code.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_87",
    "chunk_text": "Then you will output the content of each file including ALL code. Each file must strictly follow a markdown code block format, where the following tokens must be replaced such that\nFILENAME is the lowercase file name including the file extension,\nLANG is the markup code block language for the code’s language, and CODE is the code:\nFILENAME\nCODE\nYou will start with the “entrypoint” file, then go to the ones that are imported by that file, and so on. Please note that the code should be fully functional. No placeholders. Follow a language and framework appropriate best practice file naming convention.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_88",
    "chunk_text": "Follow a language and framework appropriate best practice file naming convention. Make sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other. Ensure to implement all code, if you are unsure, write a plausible implementation. Include module dependency or package manager dependency definition file.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_89",
    "chunk_text": "Include module dependency or package manager dependency definition file. Before you finish, double check that all parts of the architecture is present in the files. Useful to know:\nYou almost always put different classes in different files. For Python, you always create an appropriate requirements.txt file. For NodeJS, you always create an appropriate package.json file.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_90",
    "chunk_text": "For NodeJS, you always create an appropriate package.json file. You always add a comment briefly describing the purpose of the function definition. You try to add comments explaining very complex bits of logic. You always follow the best practices for the requested languages in terms of describing the code written as a defined\npackage/project. Python toolbelt preferences:\npytest\ndataclasses\nConversatin samples:\n[\n{\n\"role\": \"system\",\n\"content\": \"You will get instructions for code to write.\\nYou will write a very long answer.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_91",
    "chunk_text": "Python toolbelt preferences:\npytest\ndataclasses\nConversatin samples:\n[\n{\n\"role\": \"system\",\n\"content\": \"You will get instructions for code to write.\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\n\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\n\\nThen you will output the content of each file including ALL code.\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\nFILENAME is the lowercase file name including the file extension,\\nLANG is the markup code block language for the code's language, and CODE is the code:\\n\\nFILENAME\\n```LANG\\nCODE\\n```\\n\\nYou will start with the \\\"entrypoint\\\" file, then go to the ones that are imported by that file, and so on.\\nPlease note that the code should be fully functional. No placeholders.\\n\\nFollow a language and framework appropriate best practice file naming convention.\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\nInclude module dependency or package manager dependency definition file.\\nBefore you finish, double check that all parts of the architecture is present in the files.\\n\\nUseful to know:\\nYou almost always put different classes in different files.\\nFor Python, you always create an appropriate requirements.txt file.\\nFor NodeJS, you always create an appropriate package.json file.\\nYou always add a comment briefly describing the purpose of the function definition.\\nYou try to add comments explaining very complex bits of logic.\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\npackage/project.\\n\\n\\nPython toolbelt preferences:\\n- pytest\\n- dataclasses\\n\"\n},\n# … same conversation as earlier, ended with \"Make your own assumptions and state them explicitly before starting\". {\n\"role\": \"assistant\",\n\"content\": \"Assumptions:\\n1.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_92",
    "chunk_text": "{\n\"role\": \"assistant\",\n\"content\": \"Assumptions:\\n1. Model: The model will contain the game's data, such as level information, character states, and enemy positions.\\n2. View: The view will handle the game's visuals, including rendering the game objects, backgrounds, and updating the display.\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\n\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\" },\n{\n\"role\": \"user\",\n\"content\": \"Please now remember the steps:\\n\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\n\\nThen you will output the content of each file including ALL code.\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\nFILENAME is the lowercase file name including the file extension,\\nLANG is the markup code block language for the code's language, and CODE is the code:\\n\\nFILENAME\\n```LANG\\nCODE\\n```\\n\\nPlease note that the code should be fully functional.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_93",
    "chunk_text": "},\n{\n\"role\": \"user\",\n\"content\": \"Please now remember the steps:\\n\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\n\\nThen you will output the content of each file including ALL code.\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\nFILENAME is the lowercase file name including the file extension,\\nLANG is the markup code block language for the code's language, and CODE is the code:\\n\\nFILENAME\\n```LANG\\nCODE\\n```\\n\\nPlease note that the code should be fully functional. No placeholders.\\n\\nYou will start with the \\\"entrypoint\\\" file, then go to the ones that are imported by that file, and so on.\\nFollow a language and framework appropriate best practice file naming convention.\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\nBefore you finish, double check that all parts of the architecture is present in the files.\\n\"\n}\n]\nChallenges\n#\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:\nFinite context length\n: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_94",
    "chunk_text": "The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention. Challenges in long-term planning and task decomposition\n: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error. Reliability of natural language interface\n: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_95",
    "chunk_text": "Reliability of natural language interface\n: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output. Citation\n#\nCited as:\nWeng, Lilian.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_96",
    "chunk_text": "Citation\n#\nCited as:\nWeng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_97",
    "chunk_text": "https://lilianweng.github.io/posts/2023-06-23-agent/. Or\n@article{weng2023agent,\ntitle = \"LLM-powered Autonomous Agents\",\nauthor = \"Weng, Lilian\",\njournal = \"lilianweng.github.io\",\nyear = \"2023\",\nmonth = \"Jun\",\nurl = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n}\nReferences\n#\n[1] Wei et al. “Chain of thought prompting elicits reasoning in large language models.”\nNeurIPS 2022\n[2] Yao et al. “Tree of Thoughts: Dliberate Problem Solving with Large Language Models.”\narXiv preprint arXiv:2305.10601 (2023). [3] Liu et al.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_98",
    "chunk_text": "[3] Liu et al. “Chain of Hindsight Aligns Language Models with Feedback\n“\narXiv preprint arXiv:2302.02676 (2023). [4] Liu et al. “LLM+P: Empowering Large Language Models with Optimal Planning Proficiency”\narXiv preprint arXiv:2304.11477 (2023). [5] Yao et al.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_99",
    "chunk_text": "[5] Yao et al. “ReAct: Synergizing reasoning and acting in language models.”\nICLR 2023. [6] Google Blog. “Announcing ScaNN: Efficient Vector Similarity Search”\nJuly 28, 2020. [7]\nhttps://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389\n[8] Shinn & Labash.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_100",
    "chunk_text": "[7]\nhttps://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389\n[8] Shinn & Labash. “Reflexion: an autonomous agent with dynamic memory and self-reflection”\narXiv preprint arXiv:2303.11366 (2023). [9] Laskin et al. “In-context Reinforcement Learning with Algorithm Distillation”\nICLR 2023. [10] Karpas et al.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_101",
    "chunk_text": "[10] Karpas et al. “MRKL Systems A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning.”\narXiv preprint arXiv:2205.00445 (2022). [11] Nakano et al. “Webgpt: Browser-assisted question-answering with human feedback.”\narXiv preprint arXiv:2112.09332 (2021). [12] Parisi et al.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_102",
    "chunk_text": "[12] Parisi et al. “TALM: Tool Augmented Language Models”\n[13] Schick et al. “Toolformer: Language Models Can Teach Themselves to Use Tools.”\narXiv preprint arXiv:2302.04761 (2023). [14] Weaviate Blog. Why is Vector Search so fast?",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_103",
    "chunk_text": "Why is Vector Search so fast? Sep 13, 2022. [15] Li et al. “API-Bank: A Benchmark for Tool-Augmented LLMs”\narXiv preprint arXiv:2304.08244 (2023). [16] Shen et al.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_104",
    "chunk_text": "[16] Shen et al. “HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace”\narXiv preprint arXiv:2303.17580 (2023). [17] Bran et al. “ChemCrow: Augmenting large-language models with chemistry tools.”\narXiv preprint arXiv:2304.05376 (2023). [18] Boiko et al.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_105",
    "chunk_text": "[18] Boiko et al. “Emergent autonomous scientific research capabilities of large language models.”\narXiv preprint arXiv:2304.05332 (2023). [19] Joon Sung Park, et al. “Generative Agents: Interactive Simulacra of Human Behavior.”\narXiv preprint arXiv:2304.03442 (2023). [20] AutoGPT.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_106",
    "chunk_text": "[20] AutoGPT. https://github.com/Significant-Gravitas/Auto-GPT\n[21] GPT-Engineer. https://github.com/AntonOsika/gpt-engineer\nNlp\nLanguage-Model\nAgent\nSteerability\nPrompting\n«\nAdversarial Attacks on LLMs\n»\nPrompt Engineering",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://mistral.ai/news/mixtral-of-experts_chunk_0",
    "chunk_text": "https://mistral.ai/news/mixtral-of-experts. Mixtral of experts\nA high quality Sparse Mixture-of-Experts. Research\nDec 11, 2023\nMistral AI team\nMistral AI continues its mission to deliver the best open models to the developer community. Moving forward in AI requires taking new technological turns beyond reusing well-known architectures and training paradigms. Most importantly, it requires making the community benefit from original models to foster new inventions and usages.",
    "original_url": "https://mistral.ai/news/mixtral-of-experts",
    "original_title": "https://mistral.ai/news/mixtral-of-experts",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://mistral.ai/news/mixtral-of-experts_chunk_1",
    "chunk_text": "Most importantly, it requires making the community benefit from original models to foster new inventions and usages. Today, the team is proud to release Mixtral 8x7B, a high-quality sparse mixture of experts model (SMoE) with open weights. Licensed under Apache 2.0. Mixtral outperforms Llama 2 70B on most benchmarks with 6x faster inference. It is the strongest open-weight model with a permissive license and the best model overall regarding cost/performance trade-offs.",
    "original_url": "https://mistral.ai/news/mixtral-of-experts",
    "original_title": "https://mistral.ai/news/mixtral-of-experts",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://mistral.ai/news/mixtral-of-experts_chunk_2",
    "chunk_text": "It is the strongest open-weight model with a permissive license and the best model overall regarding cost/performance trade-offs. In particular, it matches or outperforms GPT3.5 on most standard benchmarks. Mixtral has the following capabilities. It gracefully handles a context of 32k tokens. It handles English, French, Italian, German and Spanish.",
    "original_url": "https://mistral.ai/news/mixtral-of-experts",
    "original_title": "https://mistral.ai/news/mixtral-of-experts",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://mistral.ai/news/mixtral-of-experts_chunk_3",
    "chunk_text": "It handles English, French, Italian, German and Spanish. It shows strong performance in code generation. It can be finetuned into an instruction-following model that achieves a score of 8.3 on MT-Bench. Pushing the frontier of open models with sparse architectures\nMixtral is a sparse mixture-of-experts network. It is a decoder-only model where the feedforward block picks from a set of 8 distinct groups of parameters.",
    "original_url": "https://mistral.ai/news/mixtral-of-experts",
    "original_title": "https://mistral.ai/news/mixtral-of-experts",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://mistral.ai/news/mixtral-of-experts_chunk_4",
    "chunk_text": "It is a decoder-only model where the feedforward block picks from a set of 8 distinct groups of parameters. At every layer, for every token, a router network chooses two of these groups (the “experts”) to process the token and combine their output additively. This technique increases the number of parameters of a model while controlling cost and latency, as the model only uses a fraction of the total set of parameters per token. Concretely, Mixtral has 46.7B total parameters but only uses 12.9B parameters per token. It, therefore, processes input and generates output at the same speed and for the same cost as a 12.9B model.",
    "original_url": "https://mistral.ai/news/mixtral-of-experts",
    "original_title": "https://mistral.ai/news/mixtral-of-experts",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://mistral.ai/news/mixtral-of-experts_chunk_5",
    "chunk_text": "It, therefore, processes input and generates output at the same speed and for the same cost as a 12.9B model. Mixtral is pre-trained on data extracted from the open Web – we train experts and routers simultaneously. Performance\nWe compare Mixtral to the Llama 2 family and the GPT3.5 base model. Mixtral matches or outperforms Llama 2 70B, as well as GPT3.5, on most benchmarks. On the following figure, we measure the quality versus inference budget tradeoff.",
    "original_url": "https://mistral.ai/news/mixtral-of-experts",
    "original_title": "https://mistral.ai/news/mixtral-of-experts",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://mistral.ai/news/mixtral-of-experts_chunk_6",
    "chunk_text": "On the following figure, we measure the quality versus inference budget tradeoff. Mistral 7B and Mixtral 8x7B belong to a family of highly efficient models compared to Llama 2 models. The following table give detailed results on the figure above. Hallucination and biases. To identify possible flaws to be corrected by fine-tuning / preference modelling,\nwe measure the\nbase\nmodel performance on BBQ/BOLD.",
    "original_url": "https://mistral.ai/news/mixtral-of-experts",
    "original_title": "https://mistral.ai/news/mixtral-of-experts",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://mistral.ai/news/mixtral-of-experts_chunk_7",
    "chunk_text": "To identify possible flaws to be corrected by fine-tuning / preference modelling,\nwe measure the\nbase\nmodel performance on BBQ/BOLD. Compared to Llama 2, Mixtral presents less bias on the BBQ benchmark. Overall, Mixtral displays more positive sentiments than Llama 2 on BOLD, with similar variances within each dimension. Language. Mixtral 8x7B masters French, German, Spanish, Italian, and English.",
    "original_url": "https://mistral.ai/news/mixtral-of-experts",
    "original_title": "https://mistral.ai/news/mixtral-of-experts",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://mistral.ai/news/mixtral-of-experts_chunk_8",
    "chunk_text": "Mixtral 8x7B masters French, German, Spanish, Italian, and English. Instructed models\nWe release Mixtral 8x7B Instruct alongside Mixtral 8x7B. This model has been optimised through supervised fine-tuning and direct preference optimisation (DPO) for careful instruction following. On MT-Bench, it reaches a score of 8.30, making it the best open-source model, with a performance comparable to GPT3.5. Note: Mixtral can be gracefully prompted to ban some outputs from constructing applications that require a strong level of moderation, as exemplified\nhere\n.",
    "original_url": "https://mistral.ai/news/mixtral-of-experts",
    "original_title": "https://mistral.ai/news/mixtral-of-experts",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://mistral.ai/news/mixtral-of-experts_chunk_9",
    "chunk_text": "Note: Mixtral can be gracefully prompted to ban some outputs from constructing applications that require a strong level of moderation, as exemplified\nhere\n. A proper preference tuning can also serve this purpose. Bear in mind that without such a prompt, the model will just follow whatever instructions are given. Deploy Mixtral with an open-source deployment stack\nTo enable the community to run Mixtral with a fully open-source stack, we have submitted changes to the vLLM project, which integrates Megablocks CUDA kernels for efficient inference. Skypilot allows the deployment of vLLM endpoints on any instance in the cloud.",
    "original_url": "https://mistral.ai/news/mixtral-of-experts",
    "original_title": "https://mistral.ai/news/mixtral-of-experts",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://mistral.ai/news/mixtral-of-experts_chunk_10",
    "chunk_text": "Skypilot allows the deployment of vLLM endpoints on any instance in the cloud. Use Mixtral on our platform. We're currently using Mixtral 8x7B behind our endpoint\nmistral-small\n, which is\navailable in beta\n. Register\nto get early access to all generative and embedding endpoints. Acknowledgement\nWe thank CoreWeave and Scaleway teams for technical support as we trained our models.",
    "original_url": "https://mistral.ai/news/mixtral-of-experts",
    "original_title": "https://mistral.ai/news/mixtral-of-experts",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://mistral.ai/news/mixtral-of-experts_chunk_11",
    "chunk_text": "Acknowledgement\nWe thank CoreWeave and Scaleway teams for technical support as we trained our models. Share\nMore resources\nNews\nModels\nAI Services\nThe next chapter of AI is yours. Try le Chat\nBuild on la Plateforme\nTalk to an expert\nMistral AI © 2025\nWhy Mistral\nAbout us\nOur customers\nCareers\nContact us\nExplore\nAI solutions\nPartners\nResearch\nDocumentation\nBuild\nLa Plateforme\nLe Chat\nTry the API\nLegal\nTerms of service\nPrivacy policy\nPrivacy choices\nData processing agreement\nLegal notice\nEN\nMistral AI © 2025",
    "original_url": "https://mistral.ai/news/mixtral-of-experts",
    "original_title": "https://mistral.ai/news/mixtral-of-experts",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  }
]