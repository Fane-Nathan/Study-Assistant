[
  {
    "chunk_id": "http://arxiv.org/pdf/2410.15578v1_chunk_0",
    "chunk_text": "Generalized Probabilistic Attention Mechanism in Transformers\n\nThe Transformer architecture has become widely adopted due to its demonstrated success, attributed to the attention mechanism at its core. Despite these successes, the attention mechanism of Transformers is associated with two well-known issues: rank-collapse and gradient vanishing. In this paper, we present a theoretical analysis that it is inherently difficult to address both issues simultaneously in the conventional attention mechanism. To handle these issues, we introduce a novel class of attention mechanism, referred to as generalized probabilistic attention mechanism (GPAM), and its dual-attention implementation within the Transformer architecture. Unlike conventional attention mechanisms, GPAM allows for negative attention scores while preserving a fixed total sum.",
    "original_url": "http://arxiv.org/pdf/2410.15578v1",
    "original_title": "Generalized Probabilistic Attention Mechanism in Transformers",
    "source": "arxiv",
    "authors": [
      "DongNyeong Heo",
      "Heeyoul Choi"
    ],
    "published": "2024-10-21T01:55:52+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2410.15578v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2410.15578v1_chunk_1",
    "chunk_text": "Unlike conventional attention mechanisms, GPAM allows for negative attention scores while preserving a fixed total sum. We provide theoretical evidence that the proposed dual-attention GPAM (daGPAM) effectively mitigates both the rank-collapse and gradient vanishing issues which are difficult to resolve simultaneously with the conventional attention mechanisms. Furthermore, we empirically validate this theoretical evidence, demonstrating the superiority of daGPAM compared to other alternative attention mechanisms that were proposed to address the same issues. Additionally, we demonstrate the practical benefits of GPAM in natural language processing tasks, such as language modeling and neural machine translation.",
    "original_url": "http://arxiv.org/pdf/2410.15578v1",
    "original_title": "Generalized Probabilistic Attention Mechanism in Transformers",
    "source": "arxiv",
    "authors": [
      "DongNyeong Heo",
      "Heeyoul Choi"
    ],
    "published": "2024-10-21T01:55:52+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2410.15578v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2209.15176v1_chunk_0",
    "chunk_text": "Adaptive Sparse and Monotonic Attention for Transformer-based Automatic Speech Recognition\n\nThe Transformer architecture model, based on self-attention and multi-head attention, has achieved remarkable success in offline end-to-end Automatic Speech Recognition (ASR). However, self-attention and multi-head attention cannot be easily applied for streaming or online ASR. For self-attention in Transformer ASR, the softmax normalization function-based attention mechanism makes it impossible to highlight important speech information. For multi-head attention in Transformer ASR, it is not easy to model monotonic alignments in different heads. To overcome these two limits, we integrate sparse attention and monotonic attention into Transformer-based ASR.",
    "original_url": "http://arxiv.org/pdf/2209.15176v1",
    "original_title": "Adaptive Sparse and Monotonic Attention for Transformer-based Automatic Speech Recognition",
    "source": "arxiv",
    "authors": [
      "Chendong Zhao",
      "Jianzong Wang",
      "Wen qi Wei",
      "Xiaoyang Qu",
      "Haoqian Wang",
      "Jing Xiao"
    ],
    "published": "2022-09-30T01:55:57+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2209.15176v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2209.15176v1_chunk_1",
    "chunk_text": "To overcome these two limits, we integrate sparse attention and monotonic attention into Transformer-based ASR. The sparse mechanism introduces a learned sparsity scheme to enable each self-attention structure to fit the corresponding head better. The monotonic attention deploys regularization to prune redundant heads for the multi-head attention structure. The experiments show that our method can effectively improve the attention mechanism on widely used benchmarks of speech recognition.",
    "original_url": "http://arxiv.org/pdf/2209.15176v1",
    "original_title": "Adaptive Sparse and Monotonic Attention for Transformer-based Automatic Speech Recognition",
    "source": "arxiv",
    "authors": [
      "Chendong Zhao",
      "Jianzong Wang",
      "Wen qi Wei",
      "Xiaoyang Qu",
      "Haoqian Wang",
      "Jing Xiao"
    ],
    "published": "2022-09-30T01:55:57+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2209.15176v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2303.01542v1_chunk_0",
    "chunk_text": "Self-attention in Vision Transformers Performs Perceptual Grouping, Not Attention\n\nRecently, a considerable number of studies in computer vision involves deep neural architectures called vision transformers. Visual processing in these models incorporates computational models that are claimed to implement attention mechanisms. Despite an increasing body of work that attempts to understand the role of attention mechanisms in vision transformers, their effect is largely unknown. Here, we asked if the attention mechanisms in vision transformers exhibit similar effects as those known in human visual attention. To answer this question, we revisited the attention formulation in these models and found that despite the name, computationally, these models perform a special class of relaxation labeling with similarity grouping effects.",
    "original_url": "http://arxiv.org/pdf/2303.01542v1",
    "original_title": "Self-attention in Vision Transformers Performs Perceptual Grouping, Not Attention",
    "source": "arxiv",
    "authors": [
      "Paria Mehrani",
      "John K. Tsotsos"
    ],
    "published": "2023-03-02T19:18:11+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2303.01542v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2303.01542v1_chunk_1",
    "chunk_text": "To answer this question, we revisited the attention formulation in these models and found that despite the name, computationally, these models perform a special class of relaxation labeling with similarity grouping effects. Additionally, whereas modern experimental findings reveal that human visual attention involves both feed-forward and feedback mechanisms, the purely feed-forward architecture of vision transformers suggests that attention in these models will not have the same effects as those known in humans. To quantify these observations, we evaluated grouping performance in a family of vision transformers. Our results suggest that self-attention modules group figures in the stimuli based on similarity in visual features such as color. Also, in a singleton detection experiment as an instance of saliency detection, we studied if these models exhibit similar effects as those of feed-forward visual salience mechanisms utilized in human visual attention.",
    "original_url": "http://arxiv.org/pdf/2303.01542v1",
    "original_title": "Self-attention in Vision Transformers Performs Perceptual Grouping, Not Attention",
    "source": "arxiv",
    "authors": [
      "Paria Mehrani",
      "John K. Tsotsos"
    ],
    "published": "2023-03-02T19:18:11+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2303.01542v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2303.01542v1_chunk_2",
    "chunk_text": "Also, in a singleton detection experiment as an instance of saliency detection, we studied if these models exhibit similar effects as those of feed-forward visual salience mechanisms utilized in human visual attention. We found that generally, the transformer-based attention modules assign more salience either to distractors or the ground. Together, our study suggests that the attention mechanisms in vision transformers perform similarity grouping and not attention.",
    "original_url": "http://arxiv.org/pdf/2303.01542v1",
    "original_title": "Self-attention in Vision Transformers Performs Perceptual Grouping, Not Attention",
    "source": "arxiv",
    "authors": [
      "Paria Mehrani",
      "John K. Tsotsos"
    ],
    "published": "2023-03-02T19:18:11+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2303.01542v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2108.01778v1_chunk_0",
    "chunk_text": "Armour: Generalizable Compact Self-Attention for Vision Transformers\n\nAttention-based transformer networks have demonstrated promising potential as their applications extend from natural language processing to vision. However, despite the recent improvements, such as sub-quadratic attention approximation and various training enhancements, the compact vision transformers to date using the regular attention still fall short in comparison with its convnet counterparts, in terms of \\textit{accuracy,} \\textit{model size}, \\textit{and} \\textit{throughput}. This paper introduces a compact self-attention mechanism that is fundamental and highly generalizable. The proposed method reduces redundancy and improves efficiency on top of the existing attention optimizations. We show its drop-in applicability for both the regular attention mechanism and some most recent variants in vision transformers.",
    "original_url": "http://arxiv.org/pdf/2108.01778v1",
    "original_title": "Armour: Generalizable Compact Self-Attention for Vision Transformers",
    "source": "arxiv",
    "authors": [
      "Lingchuan Meng"
    ],
    "published": "2021-08-03T22:33:58+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2108.01778v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2108.01778v1_chunk_1",
    "chunk_text": "We show its drop-in applicability for both the regular attention mechanism and some most recent variants in vision transformers. As a result, we produced smaller and faster models with the same or better accuracies.",
    "original_url": "http://arxiv.org/pdf/2108.01778v1",
    "original_title": "Armour: Generalizable Compact Self-Attention for Vision Transformers",
    "source": "arxiv",
    "authors": [
      "Lingchuan Meng"
    ],
    "published": "2021-08-03T22:33:58+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2108.01778v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2103.15679v1_chunk_0",
    "chunk_text": "Generic Attention-model Explainability for Interpreting Bi-Modal and Encoder-Decoder Transformers\n\nTransformers are increasingly dominating multi-modal reasoning tasks, such as visual question answering, achieving state-of-the-art results thanks to their ability to contextualize information using the self-attention and co-attention mechanisms. These attention modules also play a role in other computer vision tasks including object detection and image segmentation. Unlike Transformers that only use self-attention, Transformers with co-attention require to consider multiple attention maps in parallel in order to highlight the information that is relevant to the prediction in the model's input. In this work, we propose the first method to explain prediction by any Transformer-based architecture, including bi-modal Transformers and Transformers with co-attentions. We provide generic solutions and apply these to the three most commonly used of these architectures: (i) pure self-attention, (ii) self-attention combined with co-attention, and (iii) encoder-decoder attention.",
    "original_url": "http://arxiv.org/pdf/2103.15679v1",
    "original_title": "Generic Attention-model Explainability for Interpreting Bi-Modal and Encoder-Decoder Transformers",
    "source": "arxiv",
    "authors": [
      "Hila Chefer",
      "Shir Gur",
      "Lior Wolf"
    ],
    "published": "2021-03-29T15:03:11+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2103.15679v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2103.15679v1_chunk_1",
    "chunk_text": "We provide generic solutions and apply these to the three most commonly used of these architectures: (i) pure self-attention, (ii) self-attention combined with co-attention, and (iii) encoder-decoder attention. We show that our method is superior to all existing methods which are adapted from single modality explainability.",
    "original_url": "http://arxiv.org/pdf/2103.15679v1",
    "original_title": "Generic Attention-model Explainability for Interpreting Bi-Modal and Encoder-Decoder Transformers",
    "source": "arxiv",
    "authors": [
      "Hila Chefer",
      "Shir Gur",
      "Lior Wolf"
    ],
    "published": "2021-03-29T15:03:11+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2103.15679v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2112.14000v1_chunk_0",
    "chunk_text": "Pale Transformer: A General Vision Transformer Backbone with Pale-Shaped Attention\n\nRecently, Transformers have shown promising performance in various vision tasks. To reduce the quadratic computation complexity caused by the global self-attention, various methods constrain the range of attention within a local region to improve its efficiency. Consequently, their receptive fields in a single attention layer are not large enough, resulting in insufficient context modeling. To address this issue, we propose a Pale-Shaped self-Attention (PS-Attention), which performs self-attention within a pale-shaped region. Compared to the global self-attention, PS-Attention can reduce the computation and memory costs significantly.",
    "original_url": "http://arxiv.org/pdf/2112.14000v1",
    "original_title": "Pale Transformer: A General Vision Transformer Backbone with Pale-Shaped Attention",
    "source": "arxiv",
    "authors": [
      "Sitong Wu",
      "Tianyi Wu",
      "Haoru Tan",
      "Guodong Guo"
    ],
    "published": "2021-12-28T05:37:24+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2112.14000v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2112.14000v1_chunk_1",
    "chunk_text": "Compared to the global self-attention, PS-Attention can reduce the computation and memory costs significantly. Meanwhile, it can capture richer contextual information under the similar computation complexity with previous local self-attention mechanisms. Based on the PS-Attention, we develop a general Vision Transformer backbone with a hierarchical architecture, named Pale Transformer, which achieves 83.4%, 84.3%, and 84.9% Top-1 accuracy with the model size of 22M, 48M, and 85M respectively for 224 ImageNet-1K classification, outperforming the previous Vision Transformer backbones. For downstream tasks, our Pale Transformer backbone performs better than the recent state-of-the-art CSWin Transformer by a large margin on ADE20K semantic segmentation and COCO object detection & instance segmentation. The code will be released on https://github.com/BR-IDL/PaddleViT.",
    "original_url": "http://arxiv.org/pdf/2112.14000v1",
    "original_title": "Pale Transformer: A General Vision Transformer Backbone with Pale-Shaped Attention",
    "source": "arxiv",
    "authors": [
      "Sitong Wu",
      "Tianyi Wu",
      "Haoru Tan",
      "Guodong Guo"
    ],
    "published": "2021-12-28T05:37:24+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2112.14000v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2112.14000v1_chunk_2",
    "chunk_text": "The code will be released on https://github.com/BR-IDL/PaddleViT.",
    "original_url": "http://arxiv.org/pdf/2112.14000v1",
    "original_title": "Pale Transformer: A General Vision Transformer Backbone with Pale-Shaped Attention",
    "source": "arxiv",
    "authors": [
      "Sitong Wu",
      "Tianyi Wu",
      "Haoru Tan",
      "Guodong Guo"
    ],
    "published": "2021-12-28T05:37:24+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2112.14000v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2407.01548v1_chunk_0",
    "chunk_text": "From Cognition to Computation: A Comparative Review of Human Attention and Transformer Architectures\n\nAttention is a cornerstone of human cognition that facilitates the efficient extraction of information in everyday life. Recent developments in artificial intelligence like the Transformer architecture also incorporate the idea of attention in model designs. However, despite the shared fundamental principle of selectively attending to information, human attention and the Transformer model display notable differences, particularly in their capacity constraints, attention pathways, and intentional mechanisms. Our review aims to provide a comparative analysis of these mechanisms from a cognitive-functional perspective, thereby shedding light on several open research questions. The exploration encourages interdisciplinary efforts to derive insights from human attention mechanisms in the pursuit of developing more generalized artificial intelligence.",
    "original_url": "http://arxiv.org/pdf/2407.01548v1",
    "original_title": "From Cognition to Computation: A Comparative Review of Human Attention and Transformer Architectures",
    "source": "arxiv",
    "authors": [
      "Minglu Zhao",
      "Dehong Xu",
      "Tao Gao"
    ],
    "published": "2024-04-25T05:13:38+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2407.01548v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2407.01548v1_chunk_1",
    "chunk_text": "The exploration encourages interdisciplinary efforts to derive insights from human attention mechanisms in the pursuit of developing more generalized artificial intelligence.",
    "original_url": "http://arxiv.org/pdf/2407.01548v1",
    "original_title": "From Cognition to Computation: A Comparative Review of Human Attention and Transformer Architectures",
    "source": "arxiv",
    "authors": [
      "Minglu Zhao",
      "Dehong Xu",
      "Tao Gao"
    ],
    "published": "2024-04-25T05:13:38+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2407.01548v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1907.06607v1_chunk_0",
    "chunk_text": "Agglomerative Attention\n\nNeural networks using transformer-based architectures have recently demonstrated great power and flexibility in modeling sequences of many types. One of the core components of transformer networks is the attention layer, which allows contextual information to be exchanged among sequence elements. While many of the prevalent network structures thus far have utilized full attention -- which operates on all pairs of sequence elements -- the quadratic scaling of this attention mechanism significantly constrains the size of models that can be trained. In this work, we present an attention model that has only linear requirements in memory and computation time. We show that, despite the simpler attention model, networks using this attention mechanism can attain comparable performance to full attention networks on language modeling tasks.",
    "original_url": "http://arxiv.org/pdf/1907.06607v1",
    "original_title": "Agglomerative Attention",
    "source": "arxiv",
    "authors": [
      "Matthew Spellings"
    ],
    "published": "2019-07-15T17:11:05+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1907.06607v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1907.06607v1_chunk_1",
    "chunk_text": "We show that, despite the simpler attention model, networks using this attention mechanism can attain comparable performance to full attention networks on language modeling tasks.",
    "original_url": "http://arxiv.org/pdf/1907.06607v1",
    "original_title": "Agglomerative Attention",
    "source": "arxiv",
    "authors": [
      "Matthew Spellings"
    ],
    "published": "2019-07-15T17:11:05+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1907.06607v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2405.04515v2_chunk_0",
    "chunk_text": "A Transformer with Stack Attention\n\nNatural languages are believed to be (mildly) context-sensitive. Despite underpinning remarkably capable large language models, transformers are unable to model many context-free language tasks. In an attempt to address this limitation in the modeling power of transformer-based language models, we propose augmenting them with a differentiable, stack-based attention mechanism. Our stack-based attention mechanism can be incorporated into any transformer-based language model and adds a level of interpretability to the model. We show that the addition of our stack-based attention mechanism enables the transformer to model some, but not all, deterministic context-free languages.",
    "original_url": "http://arxiv.org/pdf/2405.04515v2",
    "original_title": "A Transformer with Stack Attention",
    "source": "arxiv",
    "authors": [
      "Jiaoda Li",
      "Jennifer C. White",
      "Mrinmaya Sachan",
      "Ryan Cotterell"
    ],
    "published": "2024-05-07T17:47:57+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2405.04515v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2405.04515v2_chunk_1",
    "chunk_text": "We show that the addition of our stack-based attention mechanism enables the transformer to model some, but not all, deterministic context-free languages.",
    "original_url": "http://arxiv.org/pdf/2405.04515v2",
    "original_title": "A Transformer with Stack Attention",
    "source": "arxiv",
    "authors": [
      "Jiaoda Li",
      "Jennifer C. White",
      "Mrinmaya Sachan",
      "Ryan Cotterell"
    ],
    "published": "2024-05-07T17:47:57+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2405.04515v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1904.05873v1_chunk_0",
    "chunk_text": "An Empirical Study of Spatial Attention Mechanisms in Deep Networks\n\nAttention mechanisms have become a popular component in deep neural networks, yet there has been little examination of how different influencing factors and methods for computing attention from these factors affect performance. Toward a better general understanding of attention mechanisms, we present an empirical study that ablates various spatial attention elements within a generalized attention formulation, encompassing the dominant Transformer attention as well as the prevalent deformable convolution and dynamic convolution modules. Conducted on a variety of applications, the study yields significant findings about spatial attention in deep networks, some of which run counter to conventional understanding. For example, we find that the query and key content comparison in Transformer attention is negligible for self-attention, but vital for encoder-decoder attention. A proper combination of deformable convolution with key content only saliency achieves the best accuracy-efficiency tradeoff in self-attention.",
    "original_url": "http://arxiv.org/pdf/1904.05873v1",
    "original_title": "An Empirical Study of Spatial Attention Mechanisms in Deep Networks",
    "source": "arxiv",
    "authors": [
      "Xizhou Zhu",
      "Dazhi Cheng",
      "Zheng Zhang",
      "Stephen Lin",
      "Jifeng Dai"
    ],
    "published": "2019-04-11T17:58:37+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1904.05873v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1904.05873v1_chunk_1",
    "chunk_text": "A proper combination of deformable convolution with key content only saliency achieves the best accuracy-efficiency tradeoff in self-attention. Our results suggest that there exists much room for improvement in the design of attention mechanisms.",
    "original_url": "http://arxiv.org/pdf/1904.05873v1",
    "original_title": "An Empirical Study of Spatial Attention Mechanisms in Deep Networks",
    "source": "arxiv",
    "authors": [
      "Xizhou Zhu",
      "Dazhi Cheng",
      "Zheng Zhang",
      "Stephen Lin",
      "Jifeng Dai"
    ],
    "published": "2019-04-11T17:58:37+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1904.05873v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2201.13027v2_chunk_0",
    "chunk_text": "BOAT: Bilateral Local Attention Vision Transformer\n\nVision Transformers achieved outstanding performance in many computer vision tasks. Early Vision Transformers such as ViT and DeiT adopt global self-attention, which is computationally expensive when the number of patches is large. To improve efficiency, recent Vision Transformers adopt local self-attention mechanisms, where self-attention is computed within local windows. Despite the fact that window-based local self-attention significantly boosts efficiency, it fails to capture the relationships between distant but similar patches in the image plane. To overcome this limitation of image-space local attention, in this paper, we further exploit the locality of patches in the feature space.",
    "original_url": "http://arxiv.org/pdf/2201.13027v2",
    "original_title": "BOAT: Bilateral Local Attention Vision Transformer",
    "source": "arxiv",
    "authors": [
      "Tan Yu",
      "Gangming Zhao",
      "Ping Li",
      "Yizhou Yu"
    ],
    "published": "2022-01-31T07:09:50+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2201.13027v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2201.13027v2_chunk_1",
    "chunk_text": "To overcome this limitation of image-space local attention, in this paper, we further exploit the locality of patches in the feature space. We group the patches into multiple clusters using their features, and self-attention is computed within every cluster. Such feature-space local attention effectively captures the connections between patches across different local windows but still relevant. We propose a Bilateral lOcal Attention vision Transformer (BOAT), which integrates feature-space local attention with image-space local attention. We further integrate BOAT with both Swin and CSWin models, and extensive experiments on several benchmark datasets demonstrate that our BOAT-CSWin model clearly and consistently outperforms existing state-of-the-art CNN models and vision Transformers.",
    "original_url": "http://arxiv.org/pdf/2201.13027v2",
    "original_title": "BOAT: Bilateral Local Attention Vision Transformer",
    "source": "arxiv",
    "authors": [
      "Tan Yu",
      "Gangming Zhao",
      "Ping Li",
      "Yizhou Yu"
    ],
    "published": "2022-01-31T07:09:50+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2201.13027v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2201.13027v2_chunk_2",
    "chunk_text": "We further integrate BOAT with both Swin and CSWin models, and extensive experiments on several benchmark datasets demonstrate that our BOAT-CSWin model clearly and consistently outperforms existing state-of-the-art CNN models and vision Transformers.",
    "original_url": "http://arxiv.org/pdf/2201.13027v2",
    "original_title": "BOAT: Bilateral Local Attention Vision Transformer",
    "source": "arxiv",
    "authors": [
      "Tan Yu",
      "Gangming Zhao",
      "Ping Li",
      "Yizhou Yu"
    ],
    "published": "2022-01-31T07:09:50+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2201.13027v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2410.02703v2_chunk_0",
    "chunk_text": "Selective Attention Improves Transformer\n\nUnneeded elements in the attention's context degrade performance. We introduce Selective Attention, a simple parameter-free change to the standard attention mechanism which reduces attention to unneeded elements. Selective attention consistently improves language modeling and downstream task performance in a variety of model sizes and context lengths. For example, transformers trained with the language modeling objective on C4 with selective attention perform language modeling equivalently to standard transformers with ~2X more heads and parameters in their attention modules. Selective attention also allows decreasing the size of the attention's context buffer, leading to meaningful reductions in the memory and compute requirements during inference.",
    "original_url": "http://arxiv.org/pdf/2410.02703v2",
    "original_title": "Selective Attention Improves Transformer",
    "source": "arxiv",
    "authors": [
      "Yaniv Leviathan",
      "Matan Kalman",
      "Yossi Matias"
    ],
    "published": "2024-10-03T17:27:30+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2410.02703v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2410.02703v2_chunk_1",
    "chunk_text": "Selective attention also allows decreasing the size of the attention's context buffer, leading to meaningful reductions in the memory and compute requirements during inference. For example, transformers trained on C4 with context sizes of 512, 1,024, and 2,048 need 16X, 25X, and 47X less memory for their attention module, respectively, when equipped with selective attention, as those without selective attention, with the same validation perplexity.",
    "original_url": "http://arxiv.org/pdf/2410.02703v2",
    "original_title": "Selective Attention Improves Transformer",
    "source": "arxiv",
    "authors": [
      "Yaniv Leviathan",
      "Matan Kalman",
      "Yossi Matias"
    ],
    "published": "2024-10-03T17:27:30+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2410.02703v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2307.04723v3_chunk_0",
    "chunk_text": "Quark/Gluon Discrimination and Top Tagging with Dual Attention Transformer\n\nJet tagging is a crucial classification task in high energy physics. Recently the performance of jet tagging has been significantly improved by the application of deep learning techniques. In this study, we introduce a new architecture for jet tagging: the Particle Dual Attention Transformer (P-DAT). This novel transformer architecture stands out by concurrently capturing both global and local information, while maintaining computational efficiency. Regarding the self attention mechanism, we have extended the established attention mechanism between particles to encompass the attention mechanism between particle features.",
    "original_url": "http://arxiv.org/pdf/2307.04723v3",
    "original_title": "Quark/Gluon Discrimination and Top Tagging with Dual Attention Transformer",
    "source": "arxiv",
    "authors": [
      "Minxuan He",
      "Daohan Wang"
    ],
    "published": "2023-07-10T17:33:54+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2307.04723v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2307.04723v3_chunk_1",
    "chunk_text": "Regarding the self attention mechanism, we have extended the established attention mechanism between particles to encompass the attention mechanism between particle features. The particle attention module computes particle level interactions across all the particles, while the channel attention module computes attention scores between particle features, which naturally captures jet level interactions by taking all particles into account. These two kinds of attention mechanisms can complement each other. Further, we incorporate both the pairwise particle interactions and the pairwise jet feature interactions in the attention mechanism. We demonstrate the effectiveness of the P-DAT architecture in classic top tagging and quark-gluon discrimination tasks, achieving competitive performance compared to other benchmark strategies.",
    "original_url": "http://arxiv.org/pdf/2307.04723v3",
    "original_title": "Quark/Gluon Discrimination and Top Tagging with Dual Attention Transformer",
    "source": "arxiv",
    "authors": [
      "Minxuan He",
      "Daohan Wang"
    ],
    "published": "2023-07-10T17:33:54+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2307.04723v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2307.04723v3_chunk_2",
    "chunk_text": "We demonstrate the effectiveness of the P-DAT architecture in classic top tagging and quark-gluon discrimination tasks, achieving competitive performance compared to other benchmark strategies.",
    "original_url": "http://arxiv.org/pdf/2307.04723v3",
    "original_title": "Quark/Gluon Discrimination and Top Tagging with Dual Attention Transformer",
    "source": "arxiv",
    "authors": [
      "Minxuan He",
      "Daohan Wang"
    ],
    "published": "2023-07-10T17:33:54+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2307.04723v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2411.14109v1_chunk_0",
    "chunk_text": "Global and Local Attention-Based Transformer for Hyperspectral Image Change Detection\n\nRecently Transformer-based hyperspectral image (HSI) change detection methods have shown remarkable performance. Nevertheless, existing attention mechanisms in Transformers have limitations in local feature representation. To address this issue, we propose Global and Local Attention-based Transformer (GLAFormer), which incorporates a global and local attention module (GLAM) to combine high-frequency and low-frequency signals. Furthermore, we introduce a cross-gating mechanism, called cross-gated feed-forward network (CGFN), to emphasize salient features and suppress noise interference. Specifically, the GLAM splits attention heads into global and local attention components to capture comprehensive spatial-spectral features.",
    "original_url": "http://arxiv.org/pdf/2411.14109v1",
    "original_title": "Global and Local Attention-Based Transformer for Hyperspectral Image Change Detection",
    "source": "arxiv",
    "authors": [
      "Ziyi Wang",
      "Feng Gao",
      "Junyu Dong",
      "Qian Du"
    ],
    "published": "2024-11-21T13:17:52+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2411.14109v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2411.14109v1_chunk_1",
    "chunk_text": "Specifically, the GLAM splits attention heads into global and local attention components to capture comprehensive spatial-spectral features. The global attention component employs global attention on downsampled feature maps to capture low-frequency information, while the local attention component focuses on high-frequency details using non-overlapping window-based local attention. The CGFN enhances the feature representation via convolutions and cross-gating mechanism in parallel paths. The proposed GLAFormer is evaluated on three HSI datasets. The results demonstrate its superiority over state-of-the-art HSI change detection methods.",
    "original_url": "http://arxiv.org/pdf/2411.14109v1",
    "original_title": "Global and Local Attention-Based Transformer for Hyperspectral Image Change Detection",
    "source": "arxiv",
    "authors": [
      "Ziyi Wang",
      "Feng Gao",
      "Junyu Dong",
      "Qian Du"
    ],
    "published": "2024-11-21T13:17:52+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2411.14109v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2411.14109v1_chunk_2",
    "chunk_text": "The results demonstrate its superiority over state-of-the-art HSI change detection methods. The source code of GLAFormer is available at \\url{https://github.com/summitgao/GLAFormer}.",
    "original_url": "http://arxiv.org/pdf/2411.14109v1",
    "original_title": "Global and Local Attention-Based Transformer for Hyperspectral Image Change Detection",
    "source": "arxiv",
    "authors": [
      "Ziyi Wang",
      "Feng Gao",
      "Junyu Dong",
      "Qian Du"
    ],
    "published": "2024-11-21T13:17:52+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2411.14109v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2305.14768v1_chunk_0",
    "chunk_text": "Dual Path Transformer with Partition Attention\n\nThis paper introduces a novel attention mechanism, called dual attention, which is both efficient and effective. The dual attention mechanism consists of two parallel components: local attention generated by Convolutional Neural Networks (CNNs) and long-range attention generated by Vision Transformers (ViTs). To address the high computational complexity and memory footprint of vanilla Multi-Head Self-Attention (MHSA), we introduce a novel Multi-Head Partition-wise Attention (MHPA) mechanism. The partition-wise attention approach models both intra-partition and inter-partition attention simultaneously. Building on the dual attention block and partition-wise attention mechanism, we present a hierarchical vision backbone called DualFormer.",
    "original_url": "http://arxiv.org/pdf/2305.14768v1",
    "original_title": "Dual Path Transformer with Partition Attention",
    "source": "arxiv",
    "authors": [
      "Zhengkai Jiang",
      "Liang Liu",
      "Jiangning Zhang",
      "Yabiao Wang",
      "Mingang Chen",
      "Chengjie Wang"
    ],
    "published": "2023-05-24T06:17:53+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2305.14768v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2305.14768v1_chunk_1",
    "chunk_text": "Building on the dual attention block and partition-wise attention mechanism, we present a hierarchical vision backbone called DualFormer. We evaluate the effectiveness of our model on several computer vision tasks, including image classification on ImageNet, object detection on COCO, and semantic segmentation on Cityscapes. Specifically, the proposed DualFormer-XS achieves 81.5\\% top-1 accuracy on ImageNet, outperforming the recent state-of-the-art MPViT-XS by 0.6\\% top-1 accuracy with much higher throughput.",
    "original_url": "http://arxiv.org/pdf/2305.14768v1",
    "original_title": "Dual Path Transformer with Partition Attention",
    "source": "arxiv",
    "authors": [
      "Zhengkai Jiang",
      "Liang Liu",
      "Jiangning Zhang",
      "Yabiao Wang",
      "Mingang Chen",
      "Chengjie Wang"
    ],
    "published": "2023-05-24T06:17:53+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2305.14768v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2106.05786v1_chunk_0",
    "chunk_text": "CAT: Cross Attention in Vision Transformer\n\nSince Transformer has found widespread use in NLP, the potential of Transformer in CV has been realized and has inspired many new approaches. However, the computation required for replacing word tokens with image patches for Transformer after the tokenization of the image is vast(e.g., ViT), which bottlenecks model training and inference. In this paper, we propose a new attention mechanism in Transformer termed Cross Attention, which alternates attention inner the image patch instead of the whole image to capture local information and apply attention between image patches which are divided from single-channel feature maps capture global information. Both operations have less computation than standard self-attention in Transformer. By alternately applying attention inner patch and between patches, we implement cross attention to maintain the performance with lower computational cost and build a hierarchical network called Cross Attention Transformer(CAT) for other vision tasks.",
    "original_url": "http://arxiv.org/pdf/2106.05786v1",
    "original_title": "CAT: Cross Attention in Vision Transformer",
    "source": "arxiv",
    "authors": [
      "Hezheng Lin",
      "Xing Cheng",
      "Xiangyu Wu",
      "Fan Yang",
      "Dong Shen",
      "Zhongyuan Wang",
      "Qing Song",
      "Wei Yuan"
    ],
    "published": "2021-06-10T14:38:32+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2106.05786v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2106.05786v1_chunk_1",
    "chunk_text": "By alternately applying attention inner patch and between patches, we implement cross attention to maintain the performance with lower computational cost and build a hierarchical network called Cross Attention Transformer(CAT) for other vision tasks. Our base model achieves state-of-the-arts on ImageNet-1K, and improves the performance of other methods on COCO and ADE20K, illustrating that our network has the potential to serve as general backbones. The code and models are available at \\url{https://github.com/linhezheng19/CAT}.",
    "original_url": "http://arxiv.org/pdf/2106.05786v1",
    "original_title": "CAT: Cross Attention in Vision Transformer",
    "source": "arxiv",
    "authors": [
      "Hezheng Lin",
      "Xing Cheng",
      "Xiangyu Wu",
      "Fan Yang",
      "Dong Shen",
      "Zhongyuan Wang",
      "Qing Song",
      "Wei Yuan"
    ],
    "published": "2021-06-10T14:38:32+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2106.05786v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2406.00427v1_chunk_0",
    "chunk_text": "You Only Need Less Attention at Each Stage in Vision Transformers\n\nThe advent of Vision Transformers (ViTs) marks a substantial paradigm shift in the realm of computer vision. ViTs capture the global information of images through self-attention modules, which perform dot product computations among patchified image tokens. While self-attention modules empower ViTs to capture long-range dependencies, the computational complexity grows quadratically with the number of tokens, which is a major hindrance to the practical application of ViTs. Moreover, the self-attention mechanism in deep ViTs is also susceptible to the attention saturation issue. Accordingly, we argue against the necessity of computing the attention scores in every layer, and we propose the Less-Attention Vision Transformer (LaViT), which computes only a few attention operations at each stage and calculates the subsequent feature alignments in other layers via attention transformations that leverage the previously calculated attention scores.",
    "original_url": "http://arxiv.org/pdf/2406.00427v1",
    "original_title": "You Only Need Less Attention at Each Stage in Vision Transformers",
    "source": "arxiv",
    "authors": [
      "Shuoxi Zhang",
      "Hanpeng Liu",
      "Stephen Lin",
      "Kun He"
    ],
    "published": "2024-06-01T12:49:16+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2406.00427v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2406.00427v1_chunk_1",
    "chunk_text": "Accordingly, we argue against the necessity of computing the attention scores in every layer, and we propose the Less-Attention Vision Transformer (LaViT), which computes only a few attention operations at each stage and calculates the subsequent feature alignments in other layers via attention transformations that leverage the previously calculated attention scores. This novel approach can mitigate two primary issues plaguing traditional self-attention modules: the heavy computational burden and attention saturation. Our proposed architecture offers superior efficiency and ease of implementation, merely requiring matrix multiplications that are highly optimized in contemporary deep learning frameworks. Moreover, our architecture demonstrates exceptional performance across various vision tasks including classification, detection and segmentation.",
    "original_url": "http://arxiv.org/pdf/2406.00427v1",
    "original_title": "You Only Need Less Attention at Each Stage in Vision Transformers",
    "source": "arxiv",
    "authors": [
      "Shuoxi Zhang",
      "Hanpeng Liu",
      "Stephen Lin",
      "Kun He"
    ],
    "published": "2024-06-01T12:49:16+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2406.00427v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2203.02636v1_chunk_0",
    "chunk_text": "Boosting Crowd Counting via Multifaceted Attention\n\nThis paper focuses on the challenging crowd counting task. As large-scale variations often exist within crowd images, neither fixed-size convolution kernel of CNN nor fixed-size attention of recent vision transformers can well handle this kind of variation. To address this problem, we propose a Multifaceted Attention Network (MAN) to improve transformer models in local spatial relation encoding. MAN incorporates global attention from a vanilla transformer, learnable local attention, and instance attention into a counting model. Firstly, the local Learnable Region Attention (LRA) is proposed to assign attention exclusively for each feature location dynamically.",
    "original_url": "http://arxiv.org/pdf/2203.02636v1",
    "original_title": "Boosting Crowd Counting via Multifaceted Attention",
    "source": "arxiv",
    "authors": [
      "Hui Lin",
      "Zhiheng Ma",
      "Rongrong Ji",
      "Yaowei Wang",
      "Xiaopeng Hong"
    ],
    "published": "2022-03-05T01:36:43+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2203.02636v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2203.02636v1_chunk_1",
    "chunk_text": "Firstly, the local Learnable Region Attention (LRA) is proposed to assign attention exclusively for each feature location dynamically. Secondly, we design the Local Attention Regularization to supervise the training of LRA by minimizing the deviation among the attention for different feature locations. Finally, we provide an Instance Attention mechanism to focus on the most important instances dynamically during training. Extensive experiments on four challenging crowd counting datasets namely ShanghaiTech, UCF-QNRF, JHU++, and NWPU have validated the proposed method. Codes: https://github.com/LoraLinH/Boosting-Crowd-Counting-via-Multifaceted-Attention.",
    "original_url": "http://arxiv.org/pdf/2203.02636v1",
    "original_title": "Boosting Crowd Counting via Multifaceted Attention",
    "source": "arxiv",
    "authors": [
      "Hui Lin",
      "Zhiheng Ma",
      "Rongrong Ji",
      "Yaowei Wang",
      "Xiaopeng Hong"
    ],
    "published": "2022-03-05T01:36:43+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2203.02636v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2203.02636v1_chunk_2",
    "chunk_text": "Codes: https://github.com/LoraLinH/Boosting-Crowd-Counting-via-Multifaceted-Attention.",
    "original_url": "http://arxiv.org/pdf/2203.02636v1",
    "original_title": "Boosting Crowd Counting via Multifaceted Attention",
    "source": "arxiv",
    "authors": [
      "Hui Lin",
      "Zhiheng Ma",
      "Rongrong Ji",
      "Yaowei Wang",
      "Xiaopeng Hong"
    ],
    "published": "2022-03-05T01:36:43+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2203.02636v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2502.19281v1_chunk_0",
    "chunk_text": "Integrating Biological and Machine Intelligence: Attention Mechanisms in Brain-Computer Interfaces\n\nWith the rapid advancement of deep learning, attention mechanisms have become indispensable in electroencephalography (EEG) signal analysis, significantly enhancing Brain-Computer Interface (BCI) applications. This paper presents a comprehensive review of traditional and Transformer-based attention mechanisms, their embedding strategies, and their applications in EEG-based BCI, with a particular emphasis on multimodal data fusion. By capturing EEG variations across time, frequency, and spatial channels, attention mechanisms improve feature extraction, representation learning, and model robustness. These methods can be broadly categorized into traditional attention mechanisms, which typically integrate with convolutional and recurrent networks, and Transformer-based multi-head self-attention, which excels in capturing long-range dependencies. Beyond single-modality analysis, attention mechanisms also enhance multimodal EEG applications, facilitating effective fusion between EEG and other physiological or sensory data.",
    "original_url": "http://arxiv.org/pdf/2502.19281v1",
    "original_title": "Integrating Biological and Machine Intelligence: Attention Mechanisms in Brain-Computer Interfaces",
    "source": "arxiv",
    "authors": [
      "Jiyuan Wang",
      "Weishan Ye",
      "Jialin He",
      "Li Zhang",
      "Gan Huang",
      "Zhuliang Yu",
      "Zhen Liang"
    ],
    "published": "2025-02-26T16:38:28+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2502.19281v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2502.19281v1_chunk_1",
    "chunk_text": "Beyond single-modality analysis, attention mechanisms also enhance multimodal EEG applications, facilitating effective fusion between EEG and other physiological or sensory data. Finally, we discuss existing challenges and emerging trends in attention-based EEG modeling, highlighting future directions for advancing BCI technology. This review aims to provide valuable insights for researchers seeking to leverage attention mechanisms for improved EEG interpretation and application.",
    "original_url": "http://arxiv.org/pdf/2502.19281v1",
    "original_title": "Integrating Biological and Machine Intelligence: Attention Mechanisms in Brain-Computer Interfaces",
    "source": "arxiv",
    "authors": [
      "Jiyuan Wang",
      "Weishan Ye",
      "Jialin He",
      "Li Zhang",
      "Gan Huang",
      "Zhuliang Yu",
      "Zhen Liang"
    ],
    "published": "2025-02-26T16:38:28+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2502.19281v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2210.00641v1_chunk_0",
    "chunk_text": "DARTFormer: Finding The Best Type Of Attention\n\nGiven the wide and ever growing range of different efficient Transformer attention mechanisms, it is important to identify which attention is most effective when given a task. In this work, we are also interested in combining different attention types to build heterogeneous Transformers. We first propose a DARTS-like Neural Architecture Search (NAS) method to find the best attention for a given task, in this setup, all heads use the same attention (homogeneous models). Our results suggest that NAS is highly effective on this task, and it identifies the best attention mechanisms for IMDb byte level text classification and Listops. We then extend our framework to search for and build Transformers with multiple different attention types, and call them heterogeneous Transformers.",
    "original_url": "http://arxiv.org/pdf/2210.00641v1",
    "original_title": "DARTFormer: Finding The Best Type Of Attention",
    "source": "arxiv",
    "authors": [
      "Jason Ross Brown",
      "Yiren Zhao",
      "Ilia Shumailov",
      "Robert D Mullins"
    ],
    "published": "2022-10-02T21:56:29+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2210.00641v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2210.00641v1_chunk_1",
    "chunk_text": "We then extend our framework to search for and build Transformers with multiple different attention types, and call them heterogeneous Transformers. We show that whilst these heterogeneous Transformers are better than the average homogeneous models, they cannot outperform the best. We explore the reasons why heterogeneous attention makes sense, and why it ultimately fails.",
    "original_url": "http://arxiv.org/pdf/2210.00641v1",
    "original_title": "DARTFormer: Finding The Best Type Of Attention",
    "source": "arxiv",
    "authors": [
      "Jason Ross Brown",
      "Yiren Zhao",
      "Ilia Shumailov",
      "Robert D Mullins"
    ],
    "published": "2022-10-02T21:56:29+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2210.00641v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2103.05103v1_chunk_0",
    "chunk_text": "Image Captioning using Multiple Transformers for Self-Attention Mechanism\n\nReal-time image captioning, along with adequate precision, is the main challenge of this research field. The present work, Multiple Transformers for Self-Attention Mechanism (MTSM), utilizes multiple transformers to address these problems. The proposed algorithm, MTSM, acquires region proposals using a transformer detector (DETR). Consequently, MTSM achieves the self-attention mechanism by transferring these region proposals and their visual and geometrical features through another transformer and learns the objects' local and global interconnections. The qualitative and quantitative results of the proposed algorithm, MTSM, are shown on the MSCOCO dataset.",
    "original_url": "http://arxiv.org/pdf/2103.05103v1",
    "original_title": "Image Captioning using Multiple Transformers for Self-Attention Mechanism",
    "source": "arxiv",
    "authors": [
      "Farrukh Olimov",
      "Shikha Dubey",
      "Labina Shrestha",
      "Tran Trung Tin",
      "Moongu Jeon"
    ],
    "published": "2021-02-14T05:35:54+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2103.05103v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2103.05103v1_chunk_1",
    "chunk_text": "The qualitative and quantitative results of the proposed algorithm, MTSM, are shown on the MSCOCO dataset.",
    "original_url": "http://arxiv.org/pdf/2103.05103v1",
    "original_title": "Image Captioning using Multiple Transformers for Self-Attention Mechanism",
    "source": "arxiv",
    "authors": [
      "Farrukh Olimov",
      "Shikha Dubey",
      "Labina Shrestha",
      "Tran Trung Tin",
      "Moongu Jeon"
    ],
    "published": "2021-02-14T05:35:54+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2103.05103v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2311.04653v2_chunk_0",
    "chunk_text": "Hybrid Focal and Full-Range Attention Based Graph Transformers\n\nThe paradigm of Transformers using the self-attention mechanism has manifested its advantage in learning graph-structured data. Yet, Graph Transformers are capable of modeling full range dependencies but are often deficient in extracting information from locality. A common practice is to utilize Message Passing Neural Networks (MPNNs) as an auxiliary to capture local information, which however are still inadequate for comprehending substructures. In this paper, we present a purely attention-based architecture, namely Focal and Full-Range Graph Transformer (FFGT), which can mitigate the loss of local information in learning global correlations. The core component of FFGT is a new mechanism of compound attention, which combines the conventional full-range attention with K-hop focal attention on ego-nets to aggregate both global and local information.",
    "original_url": "http://arxiv.org/pdf/2311.04653v2",
    "original_title": "Hybrid Focal and Full-Range Attention Based Graph Transformers",
    "source": "arxiv",
    "authors": [
      "Minhong Zhu",
      "Zhenhao Zhao",
      "Weiran Cai"
    ],
    "published": "2023-11-08T12:53:07+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2311.04653v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2311.04653v2_chunk_1",
    "chunk_text": "The core component of FFGT is a new mechanism of compound attention, which combines the conventional full-range attention with K-hop focal attention on ego-nets to aggregate both global and local information. Beyond the scope of canonical Transformers, the FFGT has the merit of being more substructure-aware. Our approach enhances the performance of existing Graph Transformers on various open datasets, while achieves compatible SOTA performance on several Long-Range Graph Benchmark (LRGB) datasets even with a vanilla transformer. We further examine influential factors on the optimal focal length of attention via introducing a novel synthetic dataset based on SBM-PATTERN.",
    "original_url": "http://arxiv.org/pdf/2311.04653v2",
    "original_title": "Hybrid Focal and Full-Range Attention Based Graph Transformers",
    "source": "arxiv",
    "authors": [
      "Minhong Zhu",
      "Zhenhao Zhao",
      "Weiran Cai"
    ],
    "published": "2023-11-08T12:53:07+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2311.04653v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1810.07595v1_chunk_0",
    "chunk_text": "An Analysis of Attention Mechanisms: The Case of Word Sense Disambiguation in Neural Machine Translation\n\nRecent work has shown that the encoder-decoder attention mechanisms in neural machine translation (NMT) are different from the word alignment in statistical machine translation. In this paper, we focus on analyzing encoder-decoder attention mechanisms, in the case of word sense disambiguation (WSD) in NMT models. We hypothesize that attention mechanisms pay more attention to context tokens when translating ambiguous words. We explore the attention distribution patterns when translating ambiguous nouns. Counter-intuitively, we find that attention mechanisms are likely to distribute more attention to the ambiguous noun itself rather than context tokens, in comparison to other nouns.",
    "original_url": "http://arxiv.org/pdf/1810.07595v1",
    "original_title": "An Analysis of Attention Mechanisms: The Case of Word Sense Disambiguation in Neural Machine Translation",
    "source": "arxiv",
    "authors": [
      "Gongbo Tang",
      "Rico Sennrich",
      "Joakim Nivre"
    ],
    "published": "2018-10-17T14:58:54+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1810.07595v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1810.07595v1_chunk_1",
    "chunk_text": "Counter-intuitively, we find that attention mechanisms are likely to distribute more attention to the ambiguous noun itself rather than context tokens, in comparison to other nouns. We conclude that attention mechanism is not the main mechanism used by NMT models to incorporate contextual information for WSD. The experimental results suggest that NMT models learn to encode contextual information necessary for WSD in the encoder hidden states. For the attention mechanism in Transformer models, we reveal that the first few layers gradually learn to \"align\" source and target tokens and the last few layers learn to extract features from the related but unaligned context tokens.",
    "original_url": "http://arxiv.org/pdf/1810.07595v1",
    "original_title": "An Analysis of Attention Mechanisms: The Case of Word Sense Disambiguation in Neural Machine Translation",
    "source": "arxiv",
    "authors": [
      "Gongbo Tang",
      "Rico Sennrich",
      "Joakim Nivre"
    ],
    "published": "2018-10-17T14:58:54+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1810.07595v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2108.09193v3_chunk_0",
    "chunk_text": "Smart Bird: Learnable Sparse Attention for Efficient and Effective Transformer\n\nTransformer has achieved great success in NLP. However, the quadratic complexity of the self-attention mechanism in Transformer makes it inefficient in handling long sequences. Many existing works explore to accelerate Transformers by computing sparse self-attention instead of a dense one, which usually attends to tokens at certain positions or randomly selected tokens. However, manually selected or random tokens may be uninformative for context modeling. In this paper, we propose Smart Bird, which is an efficient and effective Transformer with learnable sparse attention.",
    "original_url": "http://arxiv.org/pdf/2108.09193v3",
    "original_title": "Smart Bird: Learnable Sparse Attention for Efficient and Effective Transformer",
    "source": "arxiv",
    "authors": [
      "Chuhan Wu",
      "Fangzhao Wu",
      "Tao Qi",
      "Binxing Jiao",
      "Daxin Jiang",
      "Yongfeng Huang",
      "Xing Xie"
    ],
    "published": "2021-08-20T14:22:00+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2108.09193v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2108.09193v3_chunk_1",
    "chunk_text": "In this paper, we propose Smart Bird, which is an efficient and effective Transformer with learnable sparse attention. In Smart Bird, we first compute a sketched attention matrix with a single-head low-dimensional Transformer, which aims to find potential important interactions between tokens. We then sample token pairs based on their probability scores derived from the sketched attention matrix to generate different sparse attention index matrices for different attention heads. Finally, we select token embeddings according to the index matrices to form the input of sparse attention networks. Extensive experiments on six benchmark datasets for different tasks validate the efficiency and effectiveness of Smart Bird in text modeling.",
    "original_url": "http://arxiv.org/pdf/2108.09193v3",
    "original_title": "Smart Bird: Learnable Sparse Attention for Efficient and Effective Transformer",
    "source": "arxiv",
    "authors": [
      "Chuhan Wu",
      "Fangzhao Wu",
      "Tao Qi",
      "Binxing Jiao",
      "Daxin Jiang",
      "Yongfeng Huang",
      "Xing Xie"
    ],
    "published": "2021-08-20T14:22:00+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2108.09193v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2108.09193v3_chunk_2",
    "chunk_text": "Extensive experiments on six benchmark datasets for different tasks validate the efficiency and effectiveness of Smart Bird in text modeling.",
    "original_url": "http://arxiv.org/pdf/2108.09193v3",
    "original_title": "Smart Bird: Learnable Sparse Attention for Efficient and Effective Transformer",
    "source": "arxiv",
    "authors": [
      "Chuhan Wu",
      "Fangzhao Wu",
      "Tao Qi",
      "Binxing Jiao",
      "Daxin Jiang",
      "Yongfeng Huang",
      "Xing Xie"
    ],
    "published": "2021-08-20T14:22:00+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2108.09193v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2004.10102v2_chunk_0",
    "chunk_text": "Attention is Not Only a Weight: Analyzing Transformers with Vector Norms\n\nAttention is a key component of Transformers, which have recently achieved considerable success in natural language processing. Hence, attention is being extensively studied to investigate various linguistic capabilities of Transformers, focusing on analyzing the parallels between attention weights and specific linguistic phenomena. This paper shows that attention weights alone are only one of the two factors that determine the output of attention and proposes a norm-based analysis that incorporates the second factor, the norm of the transformed input vectors. The findings of our norm-based analyses of BERT and a Transformer-based neural machine translation system include the following: (i) contrary to previous studies, BERT pays poor attention to special tokens, and (ii) reasonable word alignment can be extracted from attention mechanisms of Transformer. These findings provide insights into the inner workings of Transformers.",
    "original_url": "http://arxiv.org/pdf/2004.10102v2",
    "original_title": "Attention is Not Only a Weight: Analyzing Transformers with Vector Norms",
    "source": "arxiv",
    "authors": [
      "Goro Kobayashi",
      "Tatsuki Kuribayashi",
      "Sho Yokoi",
      "Kentaro Inui"
    ],
    "published": "2020-04-21T15:22:27+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2004.10102v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2004.10102v2_chunk_1",
    "chunk_text": "These findings provide insights into the inner workings of Transformers.",
    "original_url": "http://arxiv.org/pdf/2004.10102v2",
    "original_title": "Attention is Not Only a Weight: Analyzing Transformers with Vector Norms",
    "source": "arxiv",
    "authors": [
      "Goro Kobayashi",
      "Tatsuki Kuribayashi",
      "Sho Yokoi",
      "Kentaro Inui"
    ],
    "published": "2020-04-21T15:22:27+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2004.10102v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2209.12816v2_chunk_0",
    "chunk_text": "Fast-FNet: Accelerating Transformer Encoder Models via Efficient Fourier Layers\n\nTransformer-based language models utilize the attention mechanism for substantial performance improvements in almost all natural language processing (NLP) tasks. Similar attention structures are also extensively studied in several other areas. Although the attention mechanism enhances the model performances significantly, its quadratic complexity prevents efficient processing of long sequences. Recent works focused on eliminating the disadvantages of computational inefficiency and showed that transformer-based models can still reach competitive results without the attention layer. A pioneering study proposed the FNet, which replaces the attention layer with the Fourier Transform (FT) in the transformer encoder architecture.",
    "original_url": "http://arxiv.org/pdf/2209.12816v2",
    "original_title": "Fast-FNet: Accelerating Transformer Encoder Models via Efficient Fourier Layers",
    "source": "arxiv",
    "authors": [
      "Nurullah Sevim",
      "Ege Ozan Özyedek",
      "Furkan Şahinuç",
      "Aykut Koç"
    ],
    "published": "2022-09-26T16:23:02+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2209.12816v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2209.12816v2_chunk_1",
    "chunk_text": "A pioneering study proposed the FNet, which replaces the attention layer with the Fourier Transform (FT) in the transformer encoder architecture. FNet achieves competitive performances concerning the original transformer encoder model while accelerating training process by removing the computational burden of the attention mechanism. However, the FNet model ignores essential properties of the FT from the classical signal processing that can be leveraged to increase model efficiency further. We propose different methods to deploy FT efficiently in transformer encoder models. Our proposed architectures have smaller number of model parameters, shorter training times, less memory usage, and some additional performance improvements.",
    "original_url": "http://arxiv.org/pdf/2209.12816v2",
    "original_title": "Fast-FNet: Accelerating Transformer Encoder Models via Efficient Fourier Layers",
    "source": "arxiv",
    "authors": [
      "Nurullah Sevim",
      "Ege Ozan Özyedek",
      "Furkan Şahinuç",
      "Aykut Koç"
    ],
    "published": "2022-09-26T16:23:02+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2209.12816v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2209.12816v2_chunk_2",
    "chunk_text": "Our proposed architectures have smaller number of model parameters, shorter training times, less memory usage, and some additional performance improvements. We demonstrate these improvements through extensive experiments on common benchmarks.",
    "original_url": "http://arxiv.org/pdf/2209.12816v2",
    "original_title": "Fast-FNet: Accelerating Transformer Encoder Models via Efficient Fourier Layers",
    "source": "arxiv",
    "authors": [
      "Nurullah Sevim",
      "Ege Ozan Özyedek",
      "Furkan Şahinuç",
      "Aykut Koç"
    ],
    "published": "2022-09-26T16:23:02+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2209.12816v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2412.18288v1_chunk_0",
    "chunk_text": "Towards understanding how attention mechanism works in deep learning\n\nAttention mechanism has been extensively integrated within mainstream neural network architectures, such as Transformers and graph attention networks. Yet, its underlying working principles remain somewhat elusive. What is its essence? Are there any connections between it and traditional machine learning algorithms? In this study, we inspect the process of computing similarity using classic metrics and vector space properties in manifold learning, clustering, and supervised learning.",
    "original_url": "http://arxiv.org/pdf/2412.18288v1",
    "original_title": "Towards understanding how attention mechanism works in deep learning",
    "source": "arxiv",
    "authors": [
      "Tianyu Ruan",
      "Shihua Zhang"
    ],
    "published": "2024-12-24T08:52:06+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2412.18288v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2412.18288v1_chunk_1",
    "chunk_text": "In this study, we inspect the process of computing similarity using classic metrics and vector space properties in manifold learning, clustering, and supervised learning. We identify the key characteristics of similarity computation and information propagation in these methods and demonstrate that the self-attention mechanism in deep learning adheres to the same principles but operates more flexibly and adaptively. We decompose the self-attention mechanism into a learnable pseudo-metric function and an information propagation process based on similarity computation. We prove that the self-attention mechanism converges to a drift-diffusion process through continuous modeling provided the pseudo-metric is a transformation of a metric and certain reasonable assumptions hold. This equation could be transformed into a heat equation under a new metric.",
    "original_url": "http://arxiv.org/pdf/2412.18288v1",
    "original_title": "Towards understanding how attention mechanism works in deep learning",
    "source": "arxiv",
    "authors": [
      "Tianyu Ruan",
      "Shihua Zhang"
    ],
    "published": "2024-12-24T08:52:06+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2412.18288v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2412.18288v1_chunk_2",
    "chunk_text": "This equation could be transformed into a heat equation under a new metric. In addition, we give a first-order analysis of attention mechanism with a general pseudo-metric function. This study aids in understanding the effects and principle of attention mechanism through physical intuition. Finally, we propose a modified attention mechanism called metric-attention by leveraging the concept of metric learning to facilitate the ability to learn desired metrics more effectively. Experimental results demonstrate that it outperforms self-attention regarding training efficiency, accuracy, and robustness.",
    "original_url": "http://arxiv.org/pdf/2412.18288v1",
    "original_title": "Towards understanding how attention mechanism works in deep learning",
    "source": "arxiv",
    "authors": [
      "Tianyu Ruan",
      "Shihua Zhang"
    ],
    "published": "2024-12-24T08:52:06+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2412.18288v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2412.18288v1_chunk_3",
    "chunk_text": "Experimental results demonstrate that it outperforms self-attention regarding training efficiency, accuracy, and robustness.",
    "original_url": "http://arxiv.org/pdf/2412.18288v1",
    "original_title": "Towards understanding how attention mechanism works in deep learning",
    "source": "arxiv",
    "authors": [
      "Tianyu Ruan",
      "Shihua Zhang"
    ],
    "published": "2024-12-24T08:52:06+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2412.18288v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2411.01537v1_chunk_0",
    "chunk_text": "LinRec: Linear Attention Mechanism for Long-term Sequential Recommender Systems\n\nTransformer models have achieved remarkable success in sequential recommender systems (SRSs). However, computing the attention matrix in traditional dot-product attention mechanisms results in a quadratic complexity with sequence lengths, leading to high computational costs for long-term sequential recommendation. Motivated by the above observation, we propose a novel L2-Normalized Linear Attention for the Transformer-based Sequential Recommender Systems (LinRec), which theoretically improves efficiency while preserving the learning capabilities of the traditional dot-product attention. Specifically, by thoroughly examining the equivalence conditions of efficient attention mechanisms, we show that LinRec possesses linear complexity while preserving the property of attention mechanisms. In addition, we reveal its latent efficiency properties by interpreting the proposed LinRec mechanism through a statistical lens.",
    "original_url": "http://arxiv.org/pdf/2411.01537v1",
    "original_title": "LinRec: Linear Attention Mechanism for Long-term Sequential Recommender Systems",
    "source": "arxiv",
    "authors": [
      "Langming Liu",
      "Xiangyu Zhao",
      "Chi Zhang",
      "Jingtong Gao",
      "Wanyu Wang",
      "Wenqi Fan",
      "Yiqi Wang",
      "Ming He",
      "Zitao Liu",
      "Qing Li"
    ],
    "published": "2024-11-03T11:56:00+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2411.01537v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2411.01537v1_chunk_1",
    "chunk_text": "In addition, we reveal its latent efficiency properties by interpreting the proposed LinRec mechanism through a statistical lens. Extensive experiments are conducted based on two public benchmark datasets, demonstrating that the combination of LinRec and Transformer models achieves comparable or even superior performance than state-of-the-art Transformer-based SRS models while significantly improving time and memory efficiency.",
    "original_url": "http://arxiv.org/pdf/2411.01537v1",
    "original_title": "LinRec: Linear Attention Mechanism for Long-term Sequential Recommender Systems",
    "source": "arxiv",
    "authors": [
      "Langming Liu",
      "Xiangyu Zhao",
      "Chi Zhang",
      "Jingtong Gao",
      "Wanyu Wang",
      "Wenqi Fan",
      "Yiqi Wang",
      "Ming He",
      "Zitao Liu",
      "Qing Li"
    ],
    "published": "2024-11-03T11:56:00+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2411.01537v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2312.08874v3_chunk_0",
    "chunk_text": "Agent Attention: On the Integration of Softmax and Linear Attention\n\nThe attention module is the key component in Transformers. While the global attention mechanism offers high expressiveness, its excessive computational cost restricts its applicability in various scenarios. In this paper, we propose a novel attention paradigm, Agent Attention, to strike a favorable balance between computational efficiency and representation power. Specifically, the Agent Attention, denoted as a quadruple $(Q, A, K, V)$, introduces an additional set of agent tokens $A$ into the conventional attention module. The agent tokens first act as the agent for the query tokens $Q$ to aggregate information from $K$ and $V$, and then broadcast the information back to $Q$.",
    "original_url": "http://arxiv.org/pdf/2312.08874v3",
    "original_title": "Agent Attention: On the Integration of Softmax and Linear Attention",
    "source": "arxiv",
    "authors": [
      "Dongchen Han",
      "Tianzhu Ye",
      "Yizeng Han",
      "Zhuofan Xia",
      "Siyuan Pan",
      "Pengfei Wan",
      "Shiji Song",
      "Gao Huang"
    ],
    "published": "2023-12-14T16:26:29+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2312.08874v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2312.08874v3_chunk_1",
    "chunk_text": "The agent tokens first act as the agent for the query tokens $Q$ to aggregate information from $K$ and $V$, and then broadcast the information back to $Q$. Given the number of agent tokens can be designed to be much smaller than the number of query tokens, the agent attention is significantly more efficient than the widely adopted Softmax attention, while preserving global context modelling capability. Interestingly, we show that the proposed agent attention is equivalent to a generalized form of linear attention. Therefore, agent attention seamlessly integrates the powerful Softmax attention and the highly efficient linear attention. Extensive experiments demonstrate the effectiveness of agent attention with various vision Transformers and across diverse vision tasks, including image classification, object detection, semantic segmentation and image generation.",
    "original_url": "http://arxiv.org/pdf/2312.08874v3",
    "original_title": "Agent Attention: On the Integration of Softmax and Linear Attention",
    "source": "arxiv",
    "authors": [
      "Dongchen Han",
      "Tianzhu Ye",
      "Yizeng Han",
      "Zhuofan Xia",
      "Siyuan Pan",
      "Pengfei Wan",
      "Shiji Song",
      "Gao Huang"
    ],
    "published": "2023-12-14T16:26:29+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2312.08874v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2312.08874v3_chunk_2",
    "chunk_text": "Extensive experiments demonstrate the effectiveness of agent attention with various vision Transformers and across diverse vision tasks, including image classification, object detection, semantic segmentation and image generation. Notably, agent attention has shown remarkable performance in high-resolution scenarios, owning to its linear attention nature. For instance, when applied to Stable Diffusion, our agent attention accelerates generation and substantially enhances image generation quality without any additional training. Code is available at https://github.com/LeapLabTHU/Agent-Attention.",
    "original_url": "http://arxiv.org/pdf/2312.08874v3",
    "original_title": "Agent Attention: On the Integration of Softmax and Linear Attention",
    "source": "arxiv",
    "authors": [
      "Dongchen Han",
      "Tianzhu Ye",
      "Yizeng Han",
      "Zhuofan Xia",
      "Siyuan Pan",
      "Pengfei Wan",
      "Shiji Song",
      "Gao Huang"
    ],
    "published": "2023-12-14T16:26:29+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2312.08874v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2406.13781v1_chunk_0",
    "chunk_text": "A Primal-Dual Framework for Transformers and Neural Networks\n\nSelf-attention is key to the remarkable success of transformers in sequence modeling tasks including many applications in natural language processing and computer vision. Like neural network layers, these attention mechanisms are often developed by heuristics and experience. To provide a principled framework for constructing attention layers in transformers, we show that the self-attention corresponds to the support vector expansion derived from a support vector regression problem, whose primal formulation has the form of a neural network layer. Using our framework, we derive popular attention layers used in practice and propose two new attentions: 1) the Batch Normalized Attention (Attention-BN) derived from the batch normalization layer and 2) the Attention with Scaled Head (Attention-SH) derived from using less training data to fit the SVR model. We empirically demonstrate the advantages of the Attention-BN and Attention-SH in reducing head redundancy, increasing the model's accuracy, and improving the model's efficiency in a variety of practical applications including image and time-series classification.",
    "original_url": "http://arxiv.org/pdf/2406.13781v1",
    "original_title": "A Primal-Dual Framework for Transformers and Neural Networks",
    "source": "arxiv",
    "authors": [
      "Tan M. Nguyen",
      "Tam Nguyen",
      "Nhat Ho",
      "Andrea L. Bertozzi",
      "Richard G. Baraniuk",
      "Stanley J. Osher"
    ],
    "published": "2024-06-19T19:11:22+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2406.13781v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2406.13781v1_chunk_1",
    "chunk_text": "We empirically demonstrate the advantages of the Attention-BN and Attention-SH in reducing head redundancy, increasing the model's accuracy, and improving the model's efficiency in a variety of practical applications including image and time-series classification.",
    "original_url": "http://arxiv.org/pdf/2406.13781v1",
    "original_title": "A Primal-Dual Framework for Transformers and Neural Networks",
    "source": "arxiv",
    "authors": [
      "Tan M. Nguyen",
      "Tam Nguyen",
      "Nhat Ho",
      "Andrea L. Bertozzi",
      "Richard G. Baraniuk",
      "Stanley J. Osher"
    ],
    "published": "2024-06-19T19:11:22+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2406.13781v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2501.19215v2_chunk_0",
    "chunk_text": "Strassen Attention: Unlocking Compositional Abilities in Transformers Based on a New Lower Bound Method\n\nWe propose a novel method to evaluate the theoretical limits of Transformers, allowing us to prove the first lower bounds against one-layer softmax Transformers with infinite precision. We establish those bounds for three tasks that require advanced reasoning. The first task, Match3 (Sanford et al., 2023), requires looking at all triples of positions. The second and third tasks address compositionality-based reasoning: one is composition of functions (Peng et al., 2024) and the other is composition of binary relations. We formally prove the inability of one-layer softmax Transformers to solve any of these tasks.",
    "original_url": "http://arxiv.org/pdf/2501.19215v2",
    "original_title": "Strassen Attention: Unlocking Compositional Abilities in Transformers Based on a New Lower Bound Method",
    "source": "arxiv",
    "authors": [
      "Alexander Kozachinskiy",
      "Felipe Urrutia",
      "Hector Jimenez",
      "Tomasz Steifer",
      "Germán Pizarro",
      "Matías Fuentes",
      "Francisco Meza",
      "Cristian B. Calderon",
      "Cristóbal Rojas"
    ],
    "published": "2025-01-31T15:21:54+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2501.19215v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2501.19215v2_chunk_1",
    "chunk_text": "We formally prove the inability of one-layer softmax Transformers to solve any of these tasks. In an attempt to overcome these limitations, we introduce Strassen attention and prove that with this mechanism a one-layer Transformer can in principle solve all these tasks. We also show that it enjoys sub-cubic running-time complexity, making it more scalable than similar previously proposed mechanisms, such as higher-order attention (Sanford et al., 2023). To complement our theoretical findings, we experimentally studied Strassen attention and compared it against standard (Vaswani et al, 2017), higher-order attention (Sanford et al., 2023) and triangular attention (Bergen et al. 2021).",
    "original_url": "http://arxiv.org/pdf/2501.19215v2",
    "original_title": "Strassen Attention: Unlocking Compositional Abilities in Transformers Based on a New Lower Bound Method",
    "source": "arxiv",
    "authors": [
      "Alexander Kozachinskiy",
      "Felipe Urrutia",
      "Hector Jimenez",
      "Tomasz Steifer",
      "Germán Pizarro",
      "Matías Fuentes",
      "Francisco Meza",
      "Cristian B. Calderon",
      "Cristóbal Rojas"
    ],
    "published": "2025-01-31T15:21:54+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2501.19215v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2501.19215v2_chunk_2",
    "chunk_text": "2021). Our results help to disentangle all these attention mechanisms, highlighting their strengths and limitations. In particular, Strassen attention outperforms standard attention significantly on all the tasks. Altogether, understanding the theoretical limitations can guide research towards scalable attention mechanisms that improve the reasoning abilities of Transformers.",
    "original_url": "http://arxiv.org/pdf/2501.19215v2",
    "original_title": "Strassen Attention: Unlocking Compositional Abilities in Transformers Based on a New Lower Bound Method",
    "source": "arxiv",
    "authors": [
      "Alexander Kozachinskiy",
      "Felipe Urrutia",
      "Hector Jimenez",
      "Tomasz Steifer",
      "Germán Pizarro",
      "Matías Fuentes",
      "Francisco Meza",
      "Cristian B. Calderon",
      "Cristóbal Rojas"
    ],
    "published": "2025-01-31T15:21:54+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2501.19215v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2412.02344v1_chunk_0",
    "chunk_text": "UniForm: A Reuse Attention Mechanism Optimized for Efficient Vision Transformers on Edge Devices\n\nTransformer-based architectures have demonstrated remarkable success across various domains, but their deployment on edge devices remains challenging due to high memory and computational demands. In this paper, we introduce a novel Reuse Attention mechanism, tailored for efficient memory access and computational optimization, enabling seamless operation on resource-constrained platforms without compromising performance. Unlike traditional multi-head attention (MHA), which redundantly computes separate attention matrices for each head, Reuse Attention consolidates these computations into a shared attention matrix, significantly reducing memory overhead and computational complexity. Comprehensive experiments on ImageNet-1K and downstream tasks show that the proposed UniForm models leveraging Reuse Attention achieve state-of-the-art imagenet classification accuracy while outperforming existing attention mechanisms, such as Linear Attention and Flash Attention, in inference speed and memory scalability. Notably, UniForm-l achieves a 76.7% Top-1 accuracy on ImageNet-1K with 21.8ms inference time on edge devices like the Jetson AGX Orin, representing up to a 5x speedup over competing benchmark methods.",
    "original_url": "http://arxiv.org/pdf/2412.02344v1",
    "original_title": "UniForm: A Reuse Attention Mechanism Optimized for Efficient Vision Transformers on Edge Devices",
    "source": "arxiv",
    "authors": [
      "Seul-Ki Yeom",
      "Tae-Ho Kim"
    ],
    "published": "2024-12-03T10:04:15+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2412.02344v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2412.02344v1_chunk_1",
    "chunk_text": "Notably, UniForm-l achieves a 76.7% Top-1 accuracy on ImageNet-1K with 21.8ms inference time on edge devices like the Jetson AGX Orin, representing up to a 5x speedup over competing benchmark methods. These results demonstrate the versatility of Reuse Attention across high-performance GPUs and edge platforms, paving the way for broader real-time applications",
    "original_url": "http://arxiv.org/pdf/2412.02344v1",
    "original_title": "UniForm: A Reuse Attention Mechanism Optimized for Efficient Vision Transformers on Edge Devices",
    "source": "arxiv",
    "authors": [
      "Seul-Ki Yeom",
      "Tae-Ho Kim"
    ],
    "published": "2024-12-03T10:04:15+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2412.02344v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2407.13806v1_chunk_0",
    "chunk_text": "Revisiting Attention for Multivariate Time Series Forecasting\n\nCurrent Transformer methods for Multivariate Time-Series Forecasting (MTSF) are all based on the conventional attention mechanism. They involve sequence embedding and performing a linear projection of Q, K, and V, and then computing attention within this latent space. We have never delved into the attention mechanism to explore whether such a mapping space is optimal for MTSF. To investigate this issue, this study first proposes Frequency Spectrum attention (FSatten), a novel attention mechanism based on the frequency domain space. It employs the Fourier transform for embedding and introduces Multi-head Spectrum Scaling (MSS) to replace the conventional linear mapping of Q and K. FSatten can accurately capture the periodic dependencies between sequences and outperform the conventional attention without changing mainstream architectures.",
    "original_url": "http://arxiv.org/pdf/2407.13806v1",
    "original_title": "Revisiting Attention for Multivariate Time Series Forecasting",
    "source": "arxiv",
    "authors": [
      "Haixiang Wu"
    ],
    "published": "2024-07-18T06:28:20+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2407.13806v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2407.13806v1_chunk_1",
    "chunk_text": "It employs the Fourier transform for embedding and introduces Multi-head Spectrum Scaling (MSS) to replace the conventional linear mapping of Q and K. FSatten can accurately capture the periodic dependencies between sequences and outperform the conventional attention without changing mainstream architectures. We further design a more general method dubbed Scaled Orthogonal attention (SOatten). We propose an orthogonal embedding and a Head-Coupling Convolution (HCC) based on the neighboring similarity bias to guide the model in learning comprehensive dependency patterns. Experiments show that FSatten and SOatten surpass the SOTA which uses conventional attention, making it a good alternative as a basic attention mechanism for MTSF. The codes and log files will be released at: https://github.com/Joeland4/FSatten-SOatten.",
    "original_url": "http://arxiv.org/pdf/2407.13806v1",
    "original_title": "Revisiting Attention for Multivariate Time Series Forecasting",
    "source": "arxiv",
    "authors": [
      "Haixiang Wu"
    ],
    "published": "2024-07-18T06:28:20+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2407.13806v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2407.13806v1_chunk_2",
    "chunk_text": "The codes and log files will be released at: https://github.com/Joeland4/FSatten-SOatten.",
    "original_url": "http://arxiv.org/pdf/2407.13806v1",
    "original_title": "Revisiting Attention for Multivariate Time Series Forecasting",
    "source": "arxiv",
    "authors": [
      "Haixiang Wu"
    ],
    "published": "2024-07-18T06:28:20+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2407.13806v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2210.17115v1_chunk_0",
    "chunk_text": "ViT-LSLA: Vision Transformer with Light Self-Limited-Attention\n\nTransformers have demonstrated a competitive performance across a wide range of vision tasks, while it is very expensive to compute the global self-attention. Many methods limit the range of attention within a local window to reduce computation complexity. However, their approaches cannot save the number of parameters; meanwhile, the self-attention and inner position bias (inside the softmax function) cause each query to focus on similar and close patches. Consequently, this paper presents a light self-limited-attention (LSLA) consisting of a light self-attention mechanism (LSA) to save the computation cost and the number of parameters, and a self-limited-attention mechanism (SLA) to improve the performance. Firstly, the LSA replaces the K (Key) and V (Value) of self-attention with the X(origin input).",
    "original_url": "http://arxiv.org/pdf/2210.17115v1",
    "original_title": "ViT-LSLA: Vision Transformer with Light Self-Limited-Attention",
    "source": "arxiv",
    "authors": [
      "Zhenzhe Hechen",
      "Wei Huang",
      "Yixin Zhao"
    ],
    "published": "2022-10-31T07:46:45+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2210.17115v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2210.17115v1_chunk_1",
    "chunk_text": "Firstly, the LSA replaces the K (Key) and V (Value) of self-attention with the X(origin input). Applying it in vision Transformers which have encoder architecture and self-attention mechanism, can simplify the computation. Secondly, the SLA has a positional information module and a limited-attention module. The former contains a dynamic scale and an inner position bias to adjust the distribution of the self-attention scores and enhance the positional information. The latter uses an outer position bias after the softmax function to limit some large values of attention weights.",
    "original_url": "http://arxiv.org/pdf/2210.17115v1",
    "original_title": "ViT-LSLA: Vision Transformer with Light Self-Limited-Attention",
    "source": "arxiv",
    "authors": [
      "Zhenzhe Hechen",
      "Wei Huang",
      "Yixin Zhao"
    ],
    "published": "2022-10-31T07:46:45+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2210.17115v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2210.17115v1_chunk_2",
    "chunk_text": "The latter uses an outer position bias after the softmax function to limit some large values of attention weights. Finally, a hierarchical Vision Transformer with Light self-Limited-attention (ViT-LSLA) is presented. The experiments show that ViT-LSLA achieves 71.6% top-1 accuracy on IP102 (2.4% absolute improvement of Swin-T); 87.2% top-1 accuracy on Mini-ImageNet (3.7% absolute improvement of Swin-T). Furthermore, it greatly reduces FLOPs (3.5GFLOPs vs. 4.5GFLOPs of Swin-T) and parameters (18.9M vs. 27.6M of Swin-T).",
    "original_url": "http://arxiv.org/pdf/2210.17115v1",
    "original_title": "ViT-LSLA: Vision Transformer with Light Self-Limited-Attention",
    "source": "arxiv",
    "authors": [
      "Zhenzhe Hechen",
      "Wei Huang",
      "Yixin Zhao"
    ],
    "published": "2022-10-31T07:46:45+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2210.17115v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1912.11959v2_chunk_0",
    "chunk_text": "Is Attention All What You Need? -- An Empirical Investigation on Convolution-Based Active Memory and Self-Attention\n\nThe key to a Transformer model is the self-attention mechanism, which allows the model to analyze an entire sequence in a computationally efficient manner. Recent work has suggested the possibility that general attention mechanisms used by RNNs could be replaced by active-memory mechanisms. In this work, we evaluate whether various active-memory mechanisms could replace self-attention in a Transformer. Our experiments suggest that active-memory alone achieves comparable results to the self-attention mechanism for language modelling, but optimal results are mostly achieved by using both active-memory and self-attention mechanisms together.",
    "original_url": "http://arxiv.org/pdf/1912.11959v2",
    "original_title": "Is Attention All What You Need? -- An Empirical Investigation on Convolution-Based Active Memory and Self-Attention",
    "source": "arxiv",
    "authors": [
      "Thomas Dowdell",
      "Hongyu Zhang"
    ],
    "published": "2019-12-27T02:01:13+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1912.11959v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1912.11959v2_chunk_1",
    "chunk_text": "Our experiments suggest that active-memory alone achieves comparable results to the self-attention mechanism for language modelling, but optimal results are mostly achieved by using both active-memory and self-attention mechanisms together. We also note that, for some specific algorithmic tasks, active-memory mechanisms alone outperform both self-attention and a combination of the two.",
    "original_url": "http://arxiv.org/pdf/1912.11959v2",
    "original_title": "Is Attention All What You Need? -- An Empirical Investigation on Convolution-Based Active Memory and Self-Attention",
    "source": "arxiv",
    "authors": [
      "Thomas Dowdell",
      "Hongyu Zhang"
    ],
    "published": "2019-12-27T02:01:13+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1912.11959v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2202.08371v1_chunk_0",
    "chunk_text": "The Quarks of Attention\n\nAttention plays a fundamental role in both natural and artificial intelligence systems. In deep learning, attention-based neural architectures, such as transformer architectures, are widely used to tackle problems in natural language processing and beyond. Here we investigate the fundamental building blocks of attention and their computational properties. Within the standard model of deep learning, we classify all possible fundamental building blocks of attention in terms of their source, target, and computational mechanism. We identify and study three most important mechanisms: additive activation attention, multiplicative output attention (output gating), and multiplicative synaptic attention (synaptic gating).",
    "original_url": "http://arxiv.org/pdf/2202.08371v1",
    "original_title": "The Quarks of Attention",
    "source": "arxiv",
    "authors": [
      "Pierre Baldi",
      "Roman Vershynin"
    ],
    "published": "2022-02-15T18:47:19+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2202.08371v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2202.08371v1_chunk_1",
    "chunk_text": "We identify and study three most important mechanisms: additive activation attention, multiplicative output attention (output gating), and multiplicative synaptic attention (synaptic gating). The gating mechanisms correspond to multiplicative extensions of the standard model and are used across all current attention-based deep learning architectures. We study their functional properties and estimate the capacity of several attentional building blocks in the case of linear and polynomial threshold gates. Surprisingly, additive activation attention plays a central role in the proofs of the lower bounds. Attention mechanisms reduce the depth of certain basic circuits and leverage the power of quadratic activations without incurring their full cost.",
    "original_url": "http://arxiv.org/pdf/2202.08371v1",
    "original_title": "The Quarks of Attention",
    "source": "arxiv",
    "authors": [
      "Pierre Baldi",
      "Roman Vershynin"
    ],
    "published": "2022-02-15T18:47:19+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2202.08371v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2202.08371v1_chunk_2",
    "chunk_text": "Attention mechanisms reduce the depth of certain basic circuits and leverage the power of quadratic activations without incurring their full cost.",
    "original_url": "http://arxiv.org/pdf/2202.08371v1",
    "original_title": "The Quarks of Attention",
    "source": "arxiv",
    "authors": [
      "Pierre Baldi",
      "Roman Vershynin"
    ],
    "published": "2022-02-15T18:47:19+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2202.08371v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2502.07244v1_chunk_0",
    "chunk_text": "Linear Transformers as VAR Models: Aligning Autoregressive Attention Mechanisms with Autoregressive Forecasting\n\nAutoregressive attention-based time series forecasting (TSF) has drawn increasing interest, with mechanisms like linear attention sometimes outperforming vanilla attention. However, deeper Transformer architectures frequently misalign with autoregressive objectives, obscuring the underlying VAR structure embedded within linear attention and hindering their ability to capture the data generative processes in TSF. In this work, we first show that a single linear attention layer can be interpreted as a dynamic vector autoregressive (VAR) structure. We then explain that existing multi-layer Transformers have structural mismatches with the autoregressive forecasting objective, which impair interpretability and generalization ability. To address this, we show that by rearranging the MLP, attention, and input-output flow, multi-layer linear attention can also be aligned as a VAR model.",
    "original_url": "http://arxiv.org/pdf/2502.07244v1",
    "original_title": "Linear Transformers as VAR Models: Aligning Autoregressive Attention Mechanisms with Autoregressive Forecasting",
    "source": "arxiv",
    "authors": [
      "Jiecheng Lu",
      "Shihao Yang"
    ],
    "published": "2025-02-11T04:24:43+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2502.07244v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2502.07244v1_chunk_1",
    "chunk_text": "To address this, we show that by rearranging the MLP, attention, and input-output flow, multi-layer linear attention can also be aligned as a VAR model. Then, we propose Structural Aligned Mixture of VAR (SAMoVAR), a linear Transformer variant that integrates interpretable dynamic VAR weights for multivariate TSF. By aligning the Transformer architecture with autoregressive objectives, SAMoVAR delivers improved performance, interpretability, and computational efficiency, comparing to SOTA TSF models.",
    "original_url": "http://arxiv.org/pdf/2502.07244v1",
    "original_title": "Linear Transformers as VAR Models: Aligning Autoregressive Attention Mechanisms with Autoregressive Forecasting",
    "source": "arxiv",
    "authors": [
      "Jiecheng Lu",
      "Shihao Yang"
    ],
    "published": "2025-02-11T04:24:43+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2502.07244v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2504.19414v1_chunk_0",
    "chunk_text": "GMAR: Gradient-Driven Multi-Head Attention Rollout for Vision Transformer Interpretability\n\nThe Vision Transformer (ViT) has made significant advancements in computer vision, utilizing self-attention mechanisms to achieve state-of-the-art performance across various tasks, including image classification, object detection, and segmentation. Its architectural flexibility and capabilities have made it a preferred choice among researchers and practitioners. However, the intricate multi-head attention mechanism of ViT presents significant challenges to interpretability, as the underlying prediction process remains opaque. A critical limitation arises from an observation commonly noted in transformer architectures: \"Not all attention heads are equally meaningful.\" Overlooking the relative importance of specific heads highlights the limitations of existing interpretability methods.",
    "original_url": "http://arxiv.org/pdf/2504.19414v1",
    "original_title": "GMAR: Gradient-Driven Multi-Head Attention Rollout for Vision Transformer Interpretability",
    "source": "arxiv",
    "authors": [
      "Sehyeong Jo",
      "Gangjae Jang",
      "Haesol Park"
    ],
    "published": "2025-04-28T01:58:39+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2504.19414v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2504.19414v1_chunk_1",
    "chunk_text": "Overlooking the relative importance of specific heads highlights the limitations of existing interpretability methods. To address these challenges, we introduce Gradient-Driven Multi-Head Attention Rollout (GMAR), a novel method that quantifies the importance of each attention head using gradient-based scores. These scores are normalized to derive a weighted aggregate attention score, effectively capturing the relative contributions of individual heads. GMAR clarifies the role of each head in the prediction process, enabling more precise interpretability at the head level. Experimental results demonstrate that GMAR consistently outperforms traditional attention rollout techniques.",
    "original_url": "http://arxiv.org/pdf/2504.19414v1",
    "original_title": "GMAR: Gradient-Driven Multi-Head Attention Rollout for Vision Transformer Interpretability",
    "source": "arxiv",
    "authors": [
      "Sehyeong Jo",
      "Gangjae Jang",
      "Haesol Park"
    ],
    "published": "2025-04-28T01:58:39+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2504.19414v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2504.19414v1_chunk_2",
    "chunk_text": "Experimental results demonstrate that GMAR consistently outperforms traditional attention rollout techniques. This work provides a practical contribution to transformer-based architectures, establishing a robust framework for enhancing the interpretability of Vision Transformer models.",
    "original_url": "http://arxiv.org/pdf/2504.19414v1",
    "original_title": "GMAR: Gradient-Driven Multi-Head Attention Rollout for Vision Transformer Interpretability",
    "source": "arxiv",
    "authors": [
      "Sehyeong Jo",
      "Gangjae Jang",
      "Haesol Park"
    ],
    "published": "2025-04-28T01:58:39+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2504.19414v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2210.15497v1_chunk_0",
    "chunk_text": "LSG Attention: Extrapolation of pretrained Transformers to long sequences\n\nTransformer models achieve state-of-the-art performance on a wide range of NLP tasks. They however suffer from a prohibitive limitation due to the self-attention mechanism, inducing $O(n^2)$ complexity with regard to sequence length. To answer this limitation we introduce the LSG architecture which relies on Local, Sparse and Global attention. We show that LSG attention is fast, efficient and competitive in classification and summarization tasks on long documents. Interestingly, it can also be used to adapt existing pretrained models to efficiently extrapolate to longer sequences with no additional training.",
    "original_url": "http://arxiv.org/pdf/2210.15497v1",
    "original_title": "LSG Attention: Extrapolation of pretrained Transformers to long sequences",
    "source": "arxiv",
    "authors": [
      "Charles Condevaux",
      "Sébastien Harispe"
    ],
    "published": "2022-10-13T13:10:41+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2210.15497v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2210.15497v1_chunk_1",
    "chunk_text": "Interestingly, it can also be used to adapt existing pretrained models to efficiently extrapolate to longer sequences with no additional training. Along with the introduction of the LSG attention mechanism, we propose tools to train new models and adapt existing ones based on this mechanism.",
    "original_url": "http://arxiv.org/pdf/2210.15497v1",
    "original_title": "LSG Attention: Extrapolation of pretrained Transformers to long sequences",
    "source": "arxiv",
    "authors": [
      "Charles Condevaux",
      "Sébastien Harispe"
    ],
    "published": "2022-10-13T13:10:41+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2210.15497v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2411.18003v3_chunk_0",
    "chunk_text": "HAAT: Hybrid Attention Aggregation Transformer for Image Super-Resolution\n\nIn the research area of image super-resolution, Swin-transformer-based models are favored for their global spatial modeling and shifting window attention mechanism. However, existing methods often limit self-attention to non overlapping windows to cut costs and ignore the useful information that exists across channels. To address this issue, this paper introduces a novel model, the Hybrid Attention Aggregation Transformer (HAAT), designed to better leverage feature information. HAAT is constructed by integrating Swin-Dense-Residual-Connected Blocks (SDRCB) with Hybrid Grid Attention Blocks (HGAB). SDRCB expands the receptive field while maintaining a streamlined architecture, resulting in enhanced performance.",
    "original_url": "http://arxiv.org/pdf/2411.18003v3",
    "original_title": "HAAT: Hybrid Attention Aggregation Transformer for Image Super-Resolution",
    "source": "arxiv",
    "authors": [
      "Song-Jiang Lai",
      "Tsun-Hin Cheung",
      "Ka-Chun Fung",
      "Kai-wen Xue",
      "Kin-Man Lam"
    ],
    "published": "2024-11-27T02:47:17+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2411.18003v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2411.18003v3_chunk_1",
    "chunk_text": "SDRCB expands the receptive field while maintaining a streamlined architecture, resulting in enhanced performance. HGAB incorporates channel attention, sparse attention, and window attention to improve nonlocal feature fusion and achieve more visually compelling results. Experimental evaluations demonstrate that HAAT surpasses state-of-the-art methods on benchmark datasets. Keywords: Image super-resolution, Computer vision, Attention mechanism, Transformer",
    "original_url": "http://arxiv.org/pdf/2411.18003v3",
    "original_title": "HAAT: Hybrid Attention Aggregation Transformer for Image Super-Resolution",
    "source": "arxiv",
    "authors": [
      "Song-Jiang Lai",
      "Tsun-Hin Cheung",
      "Ka-Chun Fung",
      "Kai-wen Xue",
      "Kin-Man Lam"
    ],
    "published": "2024-11-27T02:47:17+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2411.18003v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2405.16727v2_chunk_0",
    "chunk_text": "Disentangling and Integrating Relational and Sensory Information in Transformer Architectures\n\nRelational reasoning is a central component of generally intelligent systems, enabling robust and data-efficient inductive generalization. Recent empirical evidence shows that many existing neural architectures, including Transformers, struggle with tasks requiring relational reasoning. In this work, we distinguish between two types of information: sensory information about the properties of individual objects, and relational information about the relationships between objects. While neural attention provides a powerful mechanism for controlling the flow of sensory information between objects, the Transformer lacks an explicit computational mechanism for routing and processing relational information. To address this limitation, we propose an architectural extension of the Transformer framework that we call the Dual Attention Transformer (DAT), featuring two distinct attention mechanisms: sensory attention for directing the flow of sensory information, and a novel relational attention mechanism for directing the flow of relational information.",
    "original_url": "http://arxiv.org/pdf/2405.16727v2",
    "original_title": "Disentangling and Integrating Relational and Sensory Information in Transformer Architectures",
    "source": "arxiv",
    "authors": [
      "Awni Altabaa",
      "John Lafferty"
    ],
    "published": "2024-05-26T23:52:51+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2405.16727v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2405.16727v2_chunk_1",
    "chunk_text": "To address this limitation, we propose an architectural extension of the Transformer framework that we call the Dual Attention Transformer (DAT), featuring two distinct attention mechanisms: sensory attention for directing the flow of sensory information, and a novel relational attention mechanism for directing the flow of relational information. We empirically evaluate DAT on a diverse set of tasks ranging from synthetic relational benchmarks to complex real-world tasks such as language modeling and visual processing. Our results demonstrate that integrating explicit relational computational mechanisms into the Transformer architecture leads to significant performance gains in terms of data efficiency and parameter efficiency.",
    "original_url": "http://arxiv.org/pdf/2405.16727v2",
    "original_title": "Disentangling and Integrating Relational and Sensory Information in Transformer Architectures",
    "source": "arxiv",
    "authors": [
      "Awni Altabaa",
      "John Lafferty"
    ],
    "published": "2024-05-26T23:52:51+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2405.16727v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2411.03493v1_chunk_0",
    "chunk_text": "LASER: Attention with Exponential Transformation\n\nTransformers have had tremendous impact for several sequence related tasks, largely due to their ability to retrieve from any part of the sequence via softmax based dot-product attention. This mechanism plays a crucial role in Transformer's performance. We analyze the gradients backpropagated through the softmax operation in the attention mechanism and observe that these gradients can often be small. This poor gradient signal backpropagation can lead to inefficient learning of parameters preceeding the attention operations. To this end, we introduce a new attention mechanism called LASER, which we analytically show to admit a larger gradient signal.",
    "original_url": "http://arxiv.org/pdf/2411.03493v1",
    "original_title": "LASER: Attention with Exponential Transformation",
    "source": "arxiv",
    "authors": [
      "Sai Surya Duvvuri",
      "Inderjit S. Dhillon"
    ],
    "published": "2024-11-05T20:18:28+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2411.03493v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2411.03493v1_chunk_1",
    "chunk_text": "To this end, we introduce a new attention mechanism called LASER, which we analytically show to admit a larger gradient signal. We show that LASER Attention can be implemented by making small modifications to existing attention implementations. We conduct experiments on autoregressive large language models (LLMs) with upto 2.2 billion parameters where we show upto 3.38% and an average of ~1% improvement over standard attention on downstream evaluations. Using LASER gives the following relative improvements in generalization performance across a variety of tasks (vision, text and speech): 4.67% accuracy in Vision Transformer (ViT) on Imagenet, 2.25% error rate in Conformer on the Librispeech speech-to-text and 0.93% fraction of incorrect predictions in BERT with 2.2 billion parameters.",
    "original_url": "http://arxiv.org/pdf/2411.03493v1",
    "original_title": "LASER: Attention with Exponential Transformation",
    "source": "arxiv",
    "authors": [
      "Sai Surya Duvvuri",
      "Inderjit S. Dhillon"
    ],
    "published": "2024-11-05T20:18:28+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2411.03493v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2412.17019v1_chunk_0",
    "chunk_text": "Reversed Attention: On The Gradient Descent Of Attention Layers In GPT\n\nThe success of Transformer-based Language Models (LMs) stems from their attention mechanism. While this mechanism has been extensively studied in explainability research, particularly through the attention values obtained during the forward pass of LMs, the backward pass of attention has been largely overlooked. In this work, we study the mathematics of the backward pass of attention, revealing that it implicitly calculates an attention matrix we refer to as \"Reversed Attention\". We examine the properties of Reversed Attention and demonstrate its ability to elucidate the models' behavior and edit dynamics. In an experimental setup, we showcase the ability of Reversed Attention to directly alter the forward pass of attention, without modifying the model's weights, using a novel method called \"attention patching\".",
    "original_url": "http://arxiv.org/pdf/2412.17019v1",
    "original_title": "Reversed Attention: On The Gradient Descent Of Attention Layers In GPT",
    "source": "arxiv",
    "authors": [
      "Shahar Katz",
      "Lior Wolf"
    ],
    "published": "2024-12-22T13:48:04+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2412.17019v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2412.17019v1_chunk_1",
    "chunk_text": "In an experimental setup, we showcase the ability of Reversed Attention to directly alter the forward pass of attention, without modifying the model's weights, using a novel method called \"attention patching\". In addition to enhancing the comprehension of how LM configure attention layers during backpropagation, Reversed Attention maps contribute to a more interpretable backward pass.",
    "original_url": "http://arxiv.org/pdf/2412.17019v1",
    "original_title": "Reversed Attention: On The Gradient Descent Of Attention Layers In GPT",
    "source": "arxiv",
    "authors": [
      "Shahar Katz",
      "Lior Wolf"
    ],
    "published": "2024-12-22T13:48:04+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2412.17019v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2112.10809v1_chunk_0",
    "chunk_text": "Lite Vision Transformer with Enhanced Self-Attention\n\nDespite the impressive representation capacity of vision transformer models, current light-weight vision transformer models still suffer from inconsistent and incorrect dense predictions at local regions. We suspect that the power of their self-attention mechanism is limited in shallower and thinner networks. We propose Lite Vision Transformer (LVT), a novel light-weight transformer network with two enhanced self-attention mechanisms to improve the model performances for mobile deployment. For the low-level features, we introduce Convolutional Self-Attention (CSA). Unlike previous approaches of merging convolution and self-attention, CSA introduces local self-attention into the convolution within a kernel of size 3x3 to enrich low-level features in the first stage of LVT.",
    "original_url": "http://arxiv.org/pdf/2112.10809v1",
    "original_title": "Lite Vision Transformer with Enhanced Self-Attention",
    "source": "arxiv",
    "authors": [
      "Chenglin Yang",
      "Yilin Wang",
      "Jianming Zhang",
      "He Zhang",
      "Zijun Wei",
      "Zhe Lin",
      "Alan Yuille"
    ],
    "published": "2021-12-20T19:11:53+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2112.10809v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2112.10809v1_chunk_1",
    "chunk_text": "Unlike previous approaches of merging convolution and self-attention, CSA introduces local self-attention into the convolution within a kernel of size 3x3 to enrich low-level features in the first stage of LVT. For the high-level features, we propose Recursive Atrous Self-Attention (RASA), which utilizes the multi-scale context when calculating the similarity map and a recursive mechanism to increase the representation capability with marginal extra parameter cost. The superiority of LVT is demonstrated on ImageNet recognition, ADE20K semantic segmentation, and COCO panoptic segmentation. The code is made publicly available.",
    "original_url": "http://arxiv.org/pdf/2112.10809v1",
    "original_title": "Lite Vision Transformer with Enhanced Self-Attention",
    "source": "arxiv",
    "authors": [
      "Chenglin Yang",
      "Yilin Wang",
      "Jianming Zhang",
      "He Zhang",
      "Zijun Wei",
      "Zhe Lin",
      "Alan Yuille"
    ],
    "published": "2021-12-20T19:11:53+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2112.10809v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2207.08569v3_chunk_0",
    "chunk_text": "Multi-manifold Attention for Vision Transformers\n\nVision Transformers are very popular nowadays due to their state-of-the-art performance in several computer vision tasks, such as image classification and action recognition. Although their performance has been greatly enhanced through highly descriptive patch embeddings and hierarchical structures, there is still limited research on utilizing additional data representations so as to refine the selfattention map of a Transformer. To address this problem, a novel attention mechanism, called multi-manifold multihead attention, is proposed in this work to substitute the vanilla self-attention of a Transformer. The proposed mechanism models the input space in three distinct manifolds, namely Euclidean, Symmetric Positive Definite and Grassmann, thus leveraging different statistical and geometrical properties of the input for the computation of a highly descriptive attention map. In this way, the proposed attention mechanism can guide a Vision Transformer to become more attentive towards important appearance, color and texture features of an image, leading to improved classification and segmentation results, as shown by the experimental results on well-known datasets.",
    "original_url": "http://arxiv.org/pdf/2207.08569v3",
    "original_title": "Multi-manifold Attention for Vision Transformers",
    "source": "arxiv",
    "authors": [
      "Dimitrios Konstantinidis",
      "Ilias Papastratis",
      "Kosmas Dimitropoulos",
      "Petros Daras"
    ],
    "published": "2022-07-18T12:53:53+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2207.08569v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2207.08569v3_chunk_1",
    "chunk_text": "In this way, the proposed attention mechanism can guide a Vision Transformer to become more attentive towards important appearance, color and texture features of an image, leading to improved classification and segmentation results, as shown by the experimental results on well-known datasets.",
    "original_url": "http://arxiv.org/pdf/2207.08569v3",
    "original_title": "Multi-manifold Attention for Vision Transformers",
    "source": "arxiv",
    "authors": [
      "Dimitrios Konstantinidis",
      "Ilias Papastratis",
      "Kosmas Dimitropoulos",
      "Petros Daras"
    ],
    "published": "2022-07-18T12:53:53+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2207.08569v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1906.09777v3_chunk_0",
    "chunk_text": "A Tensorized Transformer for Language Modeling\n\nLatest development of neural models has connected the encoder and decoder through a self-attention mechanism. In particular, Transformer, which is solely based on self-attention, has led to breakthroughs in Natural Language Processing (NLP) tasks. However, the multi-head attention mechanism, as a key component of Transformer, limits the effective deployment of the model to a resource-limited setting. In this paper, based on the ideas of tensor decomposition and parameters sharing, we propose a novel self-attention model (namely Multi-linear attention) with Block-Term Tensor Decomposition (BTD). We test and verify the proposed attention method on three language modeling tasks (i.e., PTB, WikiText-103 and One-billion) and a neural machine translation task (i.e., WMT-2016 English-German).",
    "original_url": "http://arxiv.org/pdf/1906.09777v3",
    "original_title": "A Tensorized Transformer for Language Modeling",
    "source": "arxiv",
    "authors": [
      "Xindian Ma",
      "Peng Zhang",
      "Shuai Zhang",
      "Nan Duan",
      "Yuexian Hou",
      "Dawei Song",
      "Ming Zhou"
    ],
    "published": "2019-06-24T08:28:37+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1906.09777v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1906.09777v3_chunk_1",
    "chunk_text": "We test and verify the proposed attention method on three language modeling tasks (i.e., PTB, WikiText-103 and One-billion) and a neural machine translation task (i.e., WMT-2016 English-German). Multi-linear attention can not only largely compress the model parameters but also obtain performance improvements, compared with a number of language modeling approaches, such as Transformer, Transformer-XL, and Transformer with tensor train decomposition.",
    "original_url": "http://arxiv.org/pdf/1906.09777v3",
    "original_title": "A Tensorized Transformer for Language Modeling",
    "source": "arxiv",
    "authors": [
      "Xindian Ma",
      "Peng Zhang",
      "Shuai Zhang",
      "Nan Duan",
      "Yuexian Hou",
      "Dawei Song",
      "Ming Zhou"
    ],
    "published": "2019-06-24T08:28:37+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1906.09777v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2110.01655v1_chunk_0",
    "chunk_text": "VTAMIQ: Transformers for Attention Modulated Image Quality Assessment\n\nFollowing the major successes of self-attention and Transformers for image analysis, we investigate the use of such attention mechanisms in the context of Image Quality Assessment (IQA) and propose a novel full-reference IQA method, Vision Transformer for Attention Modulated Image Quality (VTAMIQ). Our method achieves competitive or state-of-the-art performance on the existing IQA datasets and significantly outperforms previous metrics in cross-database evaluations. Most patch-wise IQA methods treat each patch independently; this partially discards global information and limits the ability to model long-distance interactions. We avoid this problem altogether by employing a transformer to encode a sequence of patches as a single global representation, which by design considers interdependencies between patches. We rely on various attention mechanisms -- first with self-attention within the Transformer, and second with channel attention within our difference modulation network -- specifically to reveal and enhance the more salient features throughout our architecture.",
    "original_url": "http://arxiv.org/pdf/2110.01655v1",
    "original_title": "VTAMIQ: Transformers for Attention Modulated Image Quality Assessment",
    "source": "arxiv",
    "authors": [
      "Andrei Chubarau",
      "James Clark"
    ],
    "published": "2021-10-04T18:35:29+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2110.01655v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2110.01655v1_chunk_1",
    "chunk_text": "We rely on various attention mechanisms -- first with self-attention within the Transformer, and second with channel attention within our difference modulation network -- specifically to reveal and enhance the more salient features throughout our architecture. With large-scale pre-training for both classification and IQA tasks, VTAMIQ generalizes well to unseen sets of images and distortions, further demonstrating the strength of transformer-based networks for vision modelling.",
    "original_url": "http://arxiv.org/pdf/2110.01655v1",
    "original_title": "VTAMIQ: Transformers for Attention Modulated Image Quality Assessment",
    "source": "arxiv",
    "authors": [
      "Andrei Chubarau",
      "James Clark"
    ],
    "published": "2021-10-04T18:35:29+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2110.01655v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2503.11233v1_chunk_0",
    "chunk_text": "Addressing Information Loss and Interaction Collapse: A Dual Enhanced Attention Framework for Feature Interaction\n\nThe Transformer has proven to be a significant approach in feature interaction for CTR prediction, achieving considerable success in previous works. However, it also presents potential challenges in handling feature interactions. Firstly, Transformers may encounter information loss when capturing feature interactions. By relying on inner products to represent pairwise relationships, they compress raw interaction information, which can result in a degradation of fidelity. Secondly, due to the long-tail features distribution, feature fields with low information-abundance embeddings constrain the information abundance of other fields, leading to collapsed embedding matrices.",
    "original_url": "http://arxiv.org/pdf/2503.11233v1",
    "original_title": "Addressing Information Loss and Interaction Collapse: A Dual Enhanced Attention Framework for Feature Interaction",
    "source": "arxiv",
    "authors": [
      "Yi Xu",
      "Zhiyuan Lu",
      "Xiaochen Li",
      "Jinxin Hu",
      "Hong Wen",
      "Zulong Chen",
      "Yu Zhang",
      "Jing Zhang"
    ],
    "published": "2025-03-14T09:31:03+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2503.11233v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2503.11233v1_chunk_1",
    "chunk_text": "Secondly, due to the long-tail features distribution, feature fields with low information-abundance embeddings constrain the information abundance of other fields, leading to collapsed embedding matrices. To tackle these issues, we propose a Dual Attention Framework for Enhanced Feature Interaction, known as Dual Enhanced Attention. This framework integrates two attention mechanisms: the Combo-ID attention mechanism and the collapse-avoiding attention mechanism. The Combo-ID attention mechanism directly retains feature interaction pairs to mitigate information loss, while the collapse-avoiding attention mechanism adaptively filters out low information-abundance interaction pairs to prevent interaction collapse. Extensive experiments conducted on industrial datasets have shown the effectiveness of Dual Enhanced Attention.",
    "original_url": "http://arxiv.org/pdf/2503.11233v1",
    "original_title": "Addressing Information Loss and Interaction Collapse: A Dual Enhanced Attention Framework for Feature Interaction",
    "source": "arxiv",
    "authors": [
      "Yi Xu",
      "Zhiyuan Lu",
      "Xiaochen Li",
      "Jinxin Hu",
      "Hong Wen",
      "Zulong Chen",
      "Yu Zhang",
      "Jing Zhang"
    ],
    "published": "2025-03-14T09:31:03+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2503.11233v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2503.11233v1_chunk_2",
    "chunk_text": "Extensive experiments conducted on industrial datasets have shown the effectiveness of Dual Enhanced Attention.",
    "original_url": "http://arxiv.org/pdf/2503.11233v1",
    "original_title": "Addressing Information Loss and Interaction Collapse: A Dual Enhanced Attention Framework for Feature Interaction",
    "source": "arxiv",
    "authors": [
      "Yi Xu",
      "Zhiyuan Lu",
      "Xiaochen Li",
      "Jinxin Hu",
      "Hong Wen",
      "Zulong Chen",
      "Yu Zhang",
      "Jing Zhang"
    ],
    "published": "2025-03-14T09:31:03+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2503.11233v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2311.13508v1_chunk_0",
    "chunk_text": "Naturalness of Attention: Revisiting Attention in Code Language Models\n\nLanguage models for code such as CodeBERT offer the capability to learn advanced source code representation, but their opacity poses barriers to understanding of captured properties. Recent attention analysis studies provide initial interpretability insights by focusing solely on attention weights rather than considering the wider context modeling of Transformers. This study aims to shed some light on the previously ignored factors of the attention mechanism beyond the attention weights. We conduct an initial empirical study analyzing both attention distributions and transformed representations in CodeBERT. Across two programming languages, Java and Python, we find that the scaled transformation norms of the input better capture syntactic structure compared to attention weights alone.",
    "original_url": "http://arxiv.org/pdf/2311.13508v1",
    "original_title": "Naturalness of Attention: Revisiting Attention in Code Language Models",
    "source": "arxiv",
    "authors": [
      "Mootez Saad",
      "Tushar Sharma"
    ],
    "published": "2023-11-22T16:34:12+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2311.13508v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2311.13508v1_chunk_1",
    "chunk_text": "Across two programming languages, Java and Python, we find that the scaled transformation norms of the input better capture syntactic structure compared to attention weights alone. Our analysis reveals characterization of how CodeBERT embeds syntactic code properties. The findings demonstrate the importance of incorporating factors beyond just attention weights for rigorously understanding neural code models. This lays the groundwork for developing more interpretable models and effective uses of attention mechanisms in program analysis.",
    "original_url": "http://arxiv.org/pdf/2311.13508v1",
    "original_title": "Naturalness of Attention: Revisiting Attention in Code Language Models",
    "source": "arxiv",
    "authors": [
      "Mootez Saad",
      "Tushar Sharma"
    ],
    "published": "2023-11-22T16:34:12+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2311.13508v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2502.12352v2_chunk_0",
    "chunk_text": "Towards Mechanistic Interpretability of Graph Transformers via Attention Graphs\n\nWe introduce Attention Graphs, a new tool for mechanistic interpretability of Graph Neural Networks (GNNs) and Graph Transformers based on the mathematical equivalence between message passing in GNNs and the self-attention mechanism in Transformers. Attention Graphs aggregate attention matrices across Transformer layers and heads to describe how information flows among input nodes. Through experiments on homophilous and heterophilous node classification tasks, we analyze Attention Graphs from a network science perspective and find that: (1) When Graph Transformers are allowed to learn the optimal graph structure using all-to-all attention among input nodes, the Attention Graphs learned by the model do not tend to correlate with the input/original graph structure; and (2) For heterophilous graphs, different Graph Transformer variants can achieve similar performance while utilising distinct information flow patterns. Open source code: https://github.com/batu-el/understanding-inductive-biases-of-gnns",
    "original_url": "http://arxiv.org/pdf/2502.12352v2",
    "original_title": "Towards Mechanistic Interpretability of Graph Transformers via Attention Graphs",
    "source": "arxiv",
    "authors": [
      "Batu El",
      "Deepro Choudhury",
      "Pietro Liò",
      "Chaitanya K. Joshi"
    ],
    "published": "2025-02-17T22:35:16+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2502.12352v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2406.11274v1_chunk_0",
    "chunk_text": "Skip-Layer Attention: Bridging Abstract and Detailed Dependencies in Transformers\n\nThe Transformer architecture has significantly advanced deep learning, particularly in natural language processing, by effectively managing long-range dependencies. However, as the demand for understanding complex relationships grows, refining the Transformer's architecture becomes critical. This paper introduces Skip-Layer Attention (SLA) to enhance Transformer models by enabling direct attention between non-adjacent layers. This method improves the model's ability to capture dependencies between high-level abstract features and low-level details. By facilitating direct attention between these diverse feature levels, our approach overcomes the limitations of current Transformers, which often rely on suboptimal intra-layer attention.",
    "original_url": "http://arxiv.org/pdf/2406.11274v1",
    "original_title": "Skip-Layer Attention: Bridging Abstract and Detailed Dependencies in Transformers",
    "source": "arxiv",
    "authors": [
      "Qian Chen",
      "Wen Wang",
      "Qinglin Zhang",
      "Siqi Zheng",
      "Shiliang Zhang",
      "Chong Deng",
      "Hai Yu",
      "Jiaqing Liu",
      "Yukun Ma",
      "Chong Zhang"
    ],
    "published": "2024-06-17T07:24:38+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2406.11274v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2406.11274v1_chunk_1",
    "chunk_text": "By facilitating direct attention between these diverse feature levels, our approach overcomes the limitations of current Transformers, which often rely on suboptimal intra-layer attention. Our implementation extends the Transformer's functionality by enabling queries in a given layer to interact with keys and values from both the current layer and one preceding layer, thus enhancing the diversity of multi-head attention without additional computational burden. Extensive experiments demonstrate that our enhanced Transformer model achieves superior performance in language modeling tasks, highlighting the effectiveness of our skip-layer attention mechanism.",
    "original_url": "http://arxiv.org/pdf/2406.11274v1",
    "original_title": "Skip-Layer Attention: Bridging Abstract and Detailed Dependencies in Transformers",
    "source": "arxiv",
    "authors": [
      "Qian Chen",
      "Wen Wang",
      "Qinglin Zhang",
      "Siqi Zheng",
      "Shiliang Zhang",
      "Chong Deng",
      "Hai Yu",
      "Jiaqing Liu",
      "Yukun Ma",
      "Chong Zhang"
    ],
    "published": "2024-06-17T07:24:38+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2406.11274v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2102.12871v3_chunk_0",
    "chunk_text": "SparseBERT: Rethinking the Importance Analysis in Self-attention\n\nTransformer-based models are popularly used in natural language processing (NLP). Its core component, self-attention, has aroused widespread interest. To understand the self-attention mechanism, a direct method is to visualize the attention map of a pre-trained model. Based on the patterns observed, a series of efficient Transformers with different sparse attention masks have been proposed. From a theoretical perspective, universal approximability of Transformer-based models is also recently proved.",
    "original_url": "http://arxiv.org/pdf/2102.12871v3",
    "original_title": "SparseBERT: Rethinking the Importance Analysis in Self-attention",
    "source": "arxiv",
    "authors": [
      "Han Shi",
      "Jiahui Gao",
      "Xiaozhe Ren",
      "Hang Xu",
      "Xiaodan Liang",
      "Zhenguo Li",
      "James T. Kwok"
    ],
    "published": "2021-02-25T14:13:44+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2102.12871v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2102.12871v3_chunk_1",
    "chunk_text": "From a theoretical perspective, universal approximability of Transformer-based models is also recently proved. However, the above understanding and analysis of self-attention is based on a pre-trained model. To rethink the importance analysis in self-attention, we study the significance of different positions in attention matrix during pre-training. A surprising result is that diagonal elements in the attention map are the least important compared with other attention positions. We provide a proof showing that these diagonal elements can indeed be removed without deteriorating model performance.",
    "original_url": "http://arxiv.org/pdf/2102.12871v3",
    "original_title": "SparseBERT: Rethinking the Importance Analysis in Self-attention",
    "source": "arxiv",
    "authors": [
      "Han Shi",
      "Jiahui Gao",
      "Xiaozhe Ren",
      "Hang Xu",
      "Xiaodan Liang",
      "Zhenguo Li",
      "James T. Kwok"
    ],
    "published": "2021-02-25T14:13:44+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2102.12871v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2102.12871v3_chunk_2",
    "chunk_text": "We provide a proof showing that these diagonal elements can indeed be removed without deteriorating model performance. Furthermore, we propose a Differentiable Attention Mask (DAM) algorithm, which further guides the design of the SparseBERT. Extensive experiments verify our interesting findings and illustrate the effect of the proposed algorithm.",
    "original_url": "http://arxiv.org/pdf/2102.12871v3",
    "original_title": "SparseBERT: Rethinking the Importance Analysis in Self-attention",
    "source": "arxiv",
    "authors": [
      "Han Shi",
      "Jiahui Gao",
      "Xiaozhe Ren",
      "Hang Xu",
      "Xiaodan Liang",
      "Zhenguo Li",
      "James T. Kwok"
    ],
    "published": "2021-02-25T14:13:44+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2102.12871v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2202.06258v2_chunk_0",
    "chunk_text": "Flowformer: Linearizing Transformers with Conservation Flows\n\nTransformers based on the attention mechanism have achieved impressive success in various areas. However, the attention mechanism has a quadratic complexity, significantly impeding Transformers from dealing with numerous tokens and scaling up to bigger models. Previous methods mainly utilize the similarity decomposition and the associativity of matrix multiplication to devise linear-time attention mechanisms. They avoid degeneration of attention to a trivial distribution by reintroducing inductive biases such as the locality, thereby at the expense of model generality and expressiveness. In this paper, we linearize Transformers free from specific inductive biases based on the flow network theory.",
    "original_url": "http://arxiv.org/pdf/2202.06258v2",
    "original_title": "Flowformer: Linearizing Transformers with Conservation Flows",
    "source": "arxiv",
    "authors": [
      "Haixu Wu",
      "Jialong Wu",
      "Jiehui Xu",
      "Jianmin Wang",
      "Mingsheng Long"
    ],
    "published": "2022-02-13T08:44:10+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2202.06258v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2202.06258v2_chunk_1",
    "chunk_text": "In this paper, we linearize Transformers free from specific inductive biases based on the flow network theory. We cast attention as the information flow aggregated from the sources (values) to the sinks (results) through the learned flow capacities (attentions). Within this framework, we apply the property of flow conservation into attention and propose the Flow-Attention mechanism of linear complexity. By respectively conserving the incoming flow of sinks for source competition and the outgoing flow of sources for sink allocation, Flow-Attention inherently generates informative attentions without using specific inductive biases. Empowered by the Flow-Attention, Flowformer yields strong performance in linear time for wide areas, including long sequence, time series, vision, natural language, and reinforcement learning.",
    "original_url": "http://arxiv.org/pdf/2202.06258v2",
    "original_title": "Flowformer: Linearizing Transformers with Conservation Flows",
    "source": "arxiv",
    "authors": [
      "Haixu Wu",
      "Jialong Wu",
      "Jiehui Xu",
      "Jianmin Wang",
      "Mingsheng Long"
    ],
    "published": "2022-02-13T08:44:10+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2202.06258v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2202.06258v2_chunk_2",
    "chunk_text": "Empowered by the Flow-Attention, Flowformer yields strong performance in linear time for wide areas, including long sequence, time series, vision, natural language, and reinforcement learning. The code and settings are available at this repository: https://github.com/thuml/Flowformer.",
    "original_url": "http://arxiv.org/pdf/2202.06258v2",
    "original_title": "Flowformer: Linearizing Transformers with Conservation Flows",
    "source": "arxiv",
    "authors": [
      "Haixu Wu",
      "Jialong Wu",
      "Jiehui Xu",
      "Jianmin Wang",
      "Mingsheng Long"
    ],
    "published": "2022-02-13T08:44:10+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2202.06258v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2104.06399v2_chunk_0",
    "chunk_text": "Co-Scale Conv-Attentional Image Transformers\n\nIn this paper, we present Co-scale conv-attentional image Transformers (CoaT), a Transformer-based image classifier equipped with co-scale and conv-attentional mechanisms. First, the co-scale mechanism maintains the integrity of Transformers' encoder branches at individual scales, while allowing representations learned at different scales to effectively communicate with each other; we design a series of serial and parallel blocks to realize the co-scale mechanism. Second, we devise a conv-attentional mechanism by realizing a relative position embedding formulation in the factorized attention module with an efficient convolution-like implementation. CoaT empowers image Transformers with enriched multi-scale and contextual modeling capabilities. On ImageNet, relatively small CoaT models attain superior classification results compared with similar-sized convolutional neural networks and image/vision Transformers.",
    "original_url": "http://arxiv.org/pdf/2104.06399v2",
    "original_title": "Co-Scale Conv-Attentional Image Transformers",
    "source": "arxiv",
    "authors": [
      "Weijian Xu",
      "Yifan Xu",
      "Tyler Chang",
      "Zhuowen Tu"
    ],
    "published": "2021-04-13T17:58:29+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2104.06399v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2104.06399v2_chunk_1",
    "chunk_text": "On ImageNet, relatively small CoaT models attain superior classification results compared with similar-sized convolutional neural networks and image/vision Transformers. The effectiveness of CoaT's backbone is also illustrated on object detection and instance segmentation, demonstrating its applicability to downstream computer vision tasks.",
    "original_url": "http://arxiv.org/pdf/2104.06399v2",
    "original_title": "Co-Scale Conv-Attentional Image Transformers",
    "source": "arxiv",
    "authors": [
      "Weijian Xu",
      "Yifan Xu",
      "Tyler Chang",
      "Zhuowen Tu"
    ],
    "published": "2021-04-13T17:58:29+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2104.06399v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2211.11052v1_chunk_0",
    "chunk_text": "Convexifying Transformers: Improving optimization and understanding of transformer networks\n\nUnderstanding the fundamental mechanism behind the success of transformer networks is still an open problem in the deep learning literature. Although their remarkable performance has been mostly attributed to the self-attention mechanism, the literature still lacks a solid analysis of these networks and interpretation of the functions learned by them. To this end, we study the training problem of attention/transformer networks and introduce a novel convex analytic approach to improve the understanding and optimization of these networks. Particularly, we first introduce a convex alternative to the self-attention mechanism and reformulate the regularized training problem of transformer networks with our alternative convex attention. Then, we cast the reformulation as a convex optimization problem that is interpretable and easier to optimize.",
    "original_url": "http://arxiv.org/pdf/2211.11052v1",
    "original_title": "Convexifying Transformers: Improving optimization and understanding of transformer networks",
    "source": "arxiv",
    "authors": [
      "Tolga Ergen",
      "Behnam Neyshabur",
      "Harsh Mehta"
    ],
    "published": "2022-11-20T18:17:47+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2211.11052v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2211.11052v1_chunk_1",
    "chunk_text": "Then, we cast the reformulation as a convex optimization problem that is interpretable and easier to optimize. Moreover, as a byproduct of our convex analysis, we reveal an implicit regularization mechanism, which promotes sparsity across tokens. Therefore, we not only improve the optimization of attention/transformer networks but also provide a solid theoretical understanding of the functions learned by them. We also demonstrate the effectiveness of our theory through several numerical experiments.",
    "original_url": "http://arxiv.org/pdf/2211.11052v1",
    "original_title": "Convexifying Transformers: Improving optimization and understanding of transformer networks",
    "source": "arxiv",
    "authors": [
      "Tolga Ergen",
      "Behnam Neyshabur",
      "Harsh Mehta"
    ],
    "published": "2022-11-20T18:17:47+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2211.11052v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2310.05296v1_chunk_0",
    "chunk_text": "Tailoring Self-Attention for Graph via Rooted Subtrees\n\nAttention mechanisms have made significant strides in graph learning, yet they still exhibit notable limitations: local attention faces challenges in capturing long-range information due to the inherent problems of the message-passing scheme, while global attention cannot reflect the hierarchical neighborhood structure and fails to capture fine-grained local information. In this paper, we propose a novel multi-hop graph attention mechanism, named Subtree Attention (STA), to address the aforementioned issues. STA seamlessly bridges the fully-attentional structure and the rooted subtree, with theoretical proof that STA approximates the global attention under extreme settings. By allowing direct computation of attention weights among multi-hop neighbors, STA mitigates the inherent problems in existing graph attention mechanisms. Further we devise an efficient form for STA by employing kernelized softmax, which yields a linear time complexity.",
    "original_url": "http://arxiv.org/pdf/2310.05296v1",
    "original_title": "Tailoring Self-Attention for Graph via Rooted Subtrees",
    "source": "arxiv",
    "authors": [
      "Siyuan Huang",
      "Yunchong Song",
      "Jiayue Zhou",
      "Zhouhan Lin"
    ],
    "published": "2023-10-08T21:47:18+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2310.05296v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2310.05296v1_chunk_1",
    "chunk_text": "Further we devise an efficient form for STA by employing kernelized softmax, which yields a linear time complexity. Our resulting GNN architecture, the STAGNN, presents a simple yet performant STA-based graph neural network leveraging a hop-aware attention strategy. Comprehensive evaluations on ten node classification datasets demonstrate that STA-based models outperform existing graph transformers and mainstream GNNs. The code is available at https://github.com/LUMIA-Group/SubTree-Attention.",
    "original_url": "http://arxiv.org/pdf/2310.05296v1",
    "original_title": "Tailoring Self-Attention for Graph via Rooted Subtrees",
    "source": "arxiv",
    "authors": [
      "Siyuan Huang",
      "Yunchong Song",
      "Jiayue Zhou",
      "Zhouhan Lin"
    ],
    "published": "2023-10-08T21:47:18+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2310.05296v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2410.05258v2_chunk_0",
    "chunk_text": "Differential Transformer\n\nTransformer tends to overallocate attention to irrelevant context. In this work, we introduce Diff Transformer, which amplifies attention to the relevant context while canceling noise. Specifically, the differential attention mechanism calculates attention scores as the difference between two separate softmax attention maps. The subtraction cancels noise, promoting the emergence of sparse attention patterns. Experimental results on language modeling show that Diff Transformer outperforms Transformer in various settings of scaling up model size and training tokens.",
    "original_url": "http://arxiv.org/pdf/2410.05258v2",
    "original_title": "Differential Transformer",
    "source": "arxiv",
    "authors": [
      "Tianzhu Ye",
      "Li Dong",
      "Yuqing Xia",
      "Yutao Sun",
      "Yi Zhu",
      "Gao Huang",
      "Furu Wei"
    ],
    "published": "2024-10-07T17:57:38+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2410.05258v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2410.05258v2_chunk_1",
    "chunk_text": "Experimental results on language modeling show that Diff Transformer outperforms Transformer in various settings of scaling up model size and training tokens. More intriguingly, it offers notable advantages in practical applications, such as long-context modeling, key information retrieval, hallucination mitigation, in-context learning, and reduction of activation outliers. By being less distracted by irrelevant context, Diff Transformer can mitigate hallucination in question answering and text summarization. For in-context learning, Diff Transformer not only enhances accuracy but is also more robust to order permutation, which was considered as a chronic robustness issue. The results position Diff Transformer as a highly effective and promising architecture to advance large language models.",
    "original_url": "http://arxiv.org/pdf/2410.05258v2",
    "original_title": "Differential Transformer",
    "source": "arxiv",
    "authors": [
      "Tianzhu Ye",
      "Li Dong",
      "Yuqing Xia",
      "Yutao Sun",
      "Yi Zhu",
      "Gao Huang",
      "Furu Wei"
    ],
    "published": "2024-10-07T17:57:38+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2410.05258v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2410.05258v2_chunk_2",
    "chunk_text": "The results position Diff Transformer as a highly effective and promising architecture to advance large language models.",
    "original_url": "http://arxiv.org/pdf/2410.05258v2",
    "original_title": "Differential Transformer",
    "source": "arxiv",
    "authors": [
      "Tianzhu Ye",
      "Li Dong",
      "Yuqing Xia",
      "Yutao Sun",
      "Yi Zhu",
      "Gao Huang",
      "Furu Wei"
    ],
    "published": "2024-10-07T17:57:38+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2410.05258v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2305.17937v1_chunk_0",
    "chunk_text": "Attention Mechanisms in Medical Image Segmentation: A Survey\n\nMedical image segmentation plays an important role in computer-aided diagnosis. Attention mechanisms that distinguish important parts from irrelevant parts have been widely used in medical image segmentation tasks. This paper systematically reviews the basic principles of attention mechanisms and their applications in medical image segmentation. First, we review the basic concepts of attention mechanism and formulation. Second, we surveyed over 300 articles related to medical image segmentation, and divided them into two groups based on their attention mechanisms, non-Transformer attention and Transformer attention.",
    "original_url": "http://arxiv.org/pdf/2305.17937v1",
    "original_title": "Attention Mechanisms in Medical Image Segmentation: A Survey",
    "source": "arxiv",
    "authors": [
      "Yutong Xie",
      "Bing Yang",
      "Qingbiao Guan",
      "Jianpeng Zhang",
      "Qi Wu",
      "Yong Xia"
    ],
    "published": "2023-05-29T08:00:54+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2305.17937v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2305.17937v1_chunk_1",
    "chunk_text": "Second, we surveyed over 300 articles related to medical image segmentation, and divided them into two groups based on their attention mechanisms, non-Transformer attention and Transformer attention. In each group, we deeply analyze the attention mechanisms from three aspects based on the current literature work, i.e., the principle of the mechanism (what to use), implementation methods (how to use), and application tasks (where to use). We also thoroughly analyzed the advantages and limitations of their applications to different tasks. Finally, we summarize the current state of research and shortcomings in the field, and discuss the potential challenges in the future, including task specificity, robustness, standard evaluation, etc. We hope that this review can showcase the overall research context of traditional and Transformer attention methods, provide a clear reference for subsequent research, and inspire more advanced attention research, not only in medical image segmentation, but also in other image analysis scenarios.",
    "original_url": "http://arxiv.org/pdf/2305.17937v1",
    "original_title": "Attention Mechanisms in Medical Image Segmentation: A Survey",
    "source": "arxiv",
    "authors": [
      "Yutong Xie",
      "Bing Yang",
      "Qingbiao Guan",
      "Jianpeng Zhang",
      "Qi Wu",
      "Yong Xia"
    ],
    "published": "2023-05-29T08:00:54+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2305.17937v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2305.17937v1_chunk_2",
    "chunk_text": "We hope that this review can showcase the overall research context of traditional and Transformer attention methods, provide a clear reference for subsequent research, and inspire more advanced attention research, not only in medical image segmentation, but also in other image analysis scenarios.",
    "original_url": "http://arxiv.org/pdf/2305.17937v1",
    "original_title": "Attention Mechanisms in Medical Image Segmentation: A Survey",
    "source": "arxiv",
    "authors": [
      "Yutong Xie",
      "Bing Yang",
      "Qingbiao Guan",
      "Jianpeng Zhang",
      "Qi Wu",
      "Yong Xia"
    ],
    "published": "2023-05-29T08:00:54+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2305.17937v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2304.13276v1_chunk_0",
    "chunk_text": "The Closeness of In-Context Learning and Weight Shifting for Softmax Regression\n\nLarge language models (LLMs) are known for their exceptional performance in natural language processing, making them highly effective in many human life-related or even job-related tasks. The attention mechanism in the Transformer architecture is a critical component of LLMs, as it allows the model to selectively focus on specific input parts. The softmax unit, which is a key part of the attention mechanism, normalizes the attention scores. Hence, the performance of LLMs in various NLP tasks depends significantly on the crucial role played by the attention mechanism with the softmax unit. In-context learning, as one of the celebrated abilities of recent LLMs, is an important concept in querying LLMs such as ChatGPT.",
    "original_url": "http://arxiv.org/pdf/2304.13276v1",
    "original_title": "The Closeness of In-Context Learning and Weight Shifting for Softmax Regression",
    "source": "arxiv",
    "authors": [
      "Shuai Li",
      "Zhao Song",
      "Yu Xia",
      "Tong Yu",
      "Tianyi Zhou"
    ],
    "published": "2023-04-26T04:33:41+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2304.13276v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2304.13276v1_chunk_1",
    "chunk_text": "In-context learning, as one of the celebrated abilities of recent LLMs, is an important concept in querying LLMs such as ChatGPT. Without further parameter updates, Transformers can learn to predict based on few in-context examples. However, the reason why Transformers becomes in-context learners is not well understood. Recently, several works [ASA+22,GTLV22,ONR+22] have studied the in-context learning from a mathematical perspective based on a linear regression formulation $\\min_x\\| Ax - b \\|_2$, which show Transformers' capability of learning linear functions in context. In this work, we study the in-context learning based on a softmax regression formulation $\\min_{x} \\| \\langle \\exp(Ax), {\\bf 1}_n \\rangle^{-1} \\exp(Ax) - b \\|_2$ of Transformer's attention mechanism.",
    "original_url": "http://arxiv.org/pdf/2304.13276v1",
    "original_title": "The Closeness of In-Context Learning and Weight Shifting for Softmax Regression",
    "source": "arxiv",
    "authors": [
      "Shuai Li",
      "Zhao Song",
      "Yu Xia",
      "Tong Yu",
      "Tianyi Zhou"
    ],
    "published": "2023-04-26T04:33:41+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2304.13276v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2304.13276v1_chunk_2",
    "chunk_text": "In this work, we study the in-context learning based on a softmax regression formulation $\\min_{x} \\| \\langle \\exp(Ax), {\\bf 1}_n \\rangle^{-1} \\exp(Ax) - b \\|_2$ of Transformer's attention mechanism. We show the upper bounds of the data transformations induced by a single self-attention layer and by gradient-descent on a $\\ell_2$ regression loss for softmax prediction function, which imply that when training self-attention-only Transformers for fundamental regression tasks, the models learned by gradient-descent and Transformers show great similarity.",
    "original_url": "http://arxiv.org/pdf/2304.13276v1",
    "original_title": "The Closeness of In-Context Learning and Weight Shifting for Softmax Regression",
    "source": "arxiv",
    "authors": [
      "Shuai Li",
      "Zhao Song",
      "Yu Xia",
      "Tong Yu",
      "Tianyi Zhou"
    ],
    "published": "2023-04-26T04:33:41+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2304.13276v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2309.16354v2_chunk_0",
    "chunk_text": "Transformer-VQ: Linear-Time Transformers via Vector Quantization\n\nWe introduce Transformer-VQ, a decoder-only transformer computing softmax-based dense self-attention in linear time. Transformer-VQ's efficient attention is enabled by vector-quantized keys and a novel caching mechanism. In our large-scale experiments, Transformer-VQ is shown highly competitive in quality, obtaining 0.99 bpb on Enwik8, 26.6 ppl on PG-19, and 3.16 bpb on ImageNet64. In addition, the optimized implementation of Transformer-VQ is over 3x faster than a comparable quadratic-time transformer at sequence length 8k, is over 12x faster at 32k, and can scale to 131k with similar throughput. Code available: \\url{https://github.com/transformer-vq/transformer_vq}",
    "original_url": "http://arxiv.org/pdf/2309.16354v2",
    "original_title": "Transformer-VQ: Linear-Time Transformers via Vector Quantization",
    "source": "arxiv",
    "authors": [
      "Lucas D. Lingle"
    ],
    "published": "2023-09-28T11:26:52+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2309.16354v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2309.16354v2_chunk_1",
    "chunk_text": "Code available: \\url{https://github.com/transformer-vq/transformer_vq}",
    "original_url": "http://arxiv.org/pdf/2309.16354v2",
    "original_title": "Transformer-VQ: Linear-Time Transformers via Vector Quantization",
    "source": "arxiv",
    "authors": [
      "Lucas D. Lingle"
    ],
    "published": "2023-09-28T11:26:52+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2309.16354v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2110.05036v2_chunk_0",
    "chunk_text": "Multi-View Self-Attention Based Transformer for Speaker Recognition\n\nInitially developed for natural language processing (NLP), Transformer model is now widely used for speech processing tasks such as speaker recognition, due to its powerful sequence modeling capabilities. However, conventional self-attention mechanisms are originally designed for modeling textual sequence without considering the characteristics of speech and speaker modeling. Besides, different Transformer variants for speaker recognition have not been well studied. In this work, we propose a novel multi-view self-attention mechanism and present an empirical study of different Transformer variants with or without the proposed attention mechanism for speaker recognition. Specifically, to balance the capabilities of capturing global dependencies and modeling the locality, we propose a multi-view self-attention mechanism for speaker Transformer, in which different attention heads can attend to different ranges of the receptive field.",
    "original_url": "http://arxiv.org/pdf/2110.05036v2",
    "original_title": "Multi-View Self-Attention Based Transformer for Speaker Recognition",
    "source": "arxiv",
    "authors": [
      "Rui Wang",
      "Junyi Ao",
      "Long Zhou",
      "Shujie Liu",
      "Zhihua Wei",
      "Tom Ko",
      "Qing Li",
      "Yu Zhang"
    ],
    "published": "2021-10-11T07:03:23+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2110.05036v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2110.05036v2_chunk_1",
    "chunk_text": "Specifically, to balance the capabilities of capturing global dependencies and modeling the locality, we propose a multi-view self-attention mechanism for speaker Transformer, in which different attention heads can attend to different ranges of the receptive field. Furthermore, we introduce and compare five Transformer variants with different network architectures, embedding locations, and pooling methods to learn speaker embeddings. Experimental results on the VoxCeleb1 and VoxCeleb2 datasets show that the proposed multi-view self-attention mechanism achieves improvement in the performance of speaker recognition, and the proposed speaker Transformer network attains excellent results compared with state-of-the-art models.",
    "original_url": "http://arxiv.org/pdf/2110.05036v2",
    "original_title": "Multi-View Self-Attention Based Transformer for Speaker Recognition",
    "source": "arxiv",
    "authors": [
      "Rui Wang",
      "Junyi Ao",
      "Long Zhou",
      "Shujie Liu",
      "Zhihua Wei",
      "Tom Ko",
      "Qing Li",
      "Yu Zhang"
    ],
    "published": "2021-10-11T07:03:23+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2110.05036v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2012.02144v1_chunk_0",
    "chunk_text": "Do We Really Need That Many Parameters In Transformer For Extractive Summarization? Discourse Can Help ! The multi-head self-attention of popular transformer models is widely used within Natural Language Processing (NLP), including for the task of extractive summarization. With the goal of analyzing and pruning the parameter-heavy self-attention mechanism, there are multiple approaches proposing more parameter-light self-attention alternatives. In this paper, we present a novel parameter-lean self-attention mechanism using discourse priors.",
    "original_url": "http://arxiv.org/pdf/2012.02144v1",
    "original_title": "Do We Really Need That Many Parameters In Transformer For Extractive Summarization? Discourse Can Help !",
    "source": "arxiv",
    "authors": [
      "Wen Xiao",
      "Patrick Huber",
      "Giuseppe Carenini"
    ],
    "published": "2020-12-03T18:23:21+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2012.02144v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2012.02144v1_chunk_1",
    "chunk_text": "In this paper, we present a novel parameter-lean self-attention mechanism using discourse priors. Our new tree self-attention is based on document-level discourse information, extending the recently proposed \"Synthesizer\" framework with another lightweight alternative. We show empirical results that our tree self-attention approach achieves competitive ROUGE-scores on the task of extractive summarization. When compared to the original single-head transformer model, the tree attention approach reaches similar performance on both, EDU and sentence level, despite the significant reduction of parameters in the attention component. We further significantly outperform the 8-head transformer model on sentence level when applying a more balanced hyper-parameter setting, requiring an order of magnitude less parameters.",
    "original_url": "http://arxiv.org/pdf/2012.02144v1",
    "original_title": "Do We Really Need That Many Parameters In Transformer For Extractive Summarization? Discourse Can Help !",
    "source": "arxiv",
    "authors": [
      "Wen Xiao",
      "Patrick Huber",
      "Giuseppe Carenini"
    ],
    "published": "2020-12-03T18:23:21+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2012.02144v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2012.02144v1_chunk_2",
    "chunk_text": "We further significantly outperform the 8-head transformer model on sentence level when applying a more balanced hyper-parameter setting, requiring an order of magnitude less parameters.",
    "original_url": "http://arxiv.org/pdf/2012.02144v1",
    "original_title": "Do We Really Need That Many Parameters In Transformer For Extractive Summarization? Discourse Can Help !",
    "source": "arxiv",
    "authors": [
      "Wen Xiao",
      "Patrick Huber",
      "Giuseppe Carenini"
    ],
    "published": "2020-12-03T18:23:21+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2012.02144v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2110.06553v2_chunk_0",
    "chunk_text": "Spatial-temporal Transformers for EEG Emotion Recognition\n\nElectroencephalography (EEG) is a popular and effective tool for emotion recognition. However, the propagation mechanisms of EEG in the human brain and its intrinsic correlation with emotions are still obscure to researchers. This work proposes four variant transformer frameworks~(spatial attention, temporal attention, sequential spatial-temporal attention and simultaneous spatial-temporal attention) for EEG emotion recognition to explore the relationship between emotion and spatial-temporal EEG features. Specifically, spatial attention and temporal attention are to learn the topological structure information and time-varying EEG characteristics for emotion recognition respectively. Sequential spatial-temporal attention does the spatial attention within a one-second segment and temporal attention within one sample sequentially to explore the influence degree of emotional stimulation on EEG signals of diverse EEG electrodes in the same temporal segment.",
    "original_url": "http://arxiv.org/pdf/2110.06553v2",
    "original_title": "Spatial-temporal Transformers for EEG Emotion Recognition",
    "source": "arxiv",
    "authors": [
      "Jiyao Liu",
      "Hao Wu",
      "Li Zhang",
      "Yanxi Zhao"
    ],
    "published": "2021-10-13T08:08:57+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2110.06553v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2110.06553v2_chunk_1",
    "chunk_text": "Sequential spatial-temporal attention does the spatial attention within a one-second segment and temporal attention within one sample sequentially to explore the influence degree of emotional stimulation on EEG signals of diverse EEG electrodes in the same temporal segment. The simultaneous spatial-temporal attention, whose spatial and temporal attention are performed simultaneously, is used to model the relationship between different spatial features in different time segments. The experimental results demonstrate that simultaneous spatial-temporal attention leads to the best emotion recognition accuracy among the design choices, indicating modeling the correlation of spatial and temporal features of EEG signals is significant to emotion recognition.",
    "original_url": "http://arxiv.org/pdf/2110.06553v2",
    "original_title": "Spatial-temporal Transformers for EEG Emotion Recognition",
    "source": "arxiv",
    "authors": [
      "Jiyao Liu",
      "Hao Wu",
      "Li Zhang",
      "Yanxi Zhao"
    ],
    "published": "2021-10-13T08:08:57+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2110.06553v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2501.06480v2_chunk_0",
    "chunk_text": "Flash Window Attention: speedup the attention computation for Swin Transformer\n\nTo address the high resolution of image pixels, the Swin Transformer introduces window attention. This mechanism divides an image into non-overlapping windows and restricts attention computation to within each window, significantly enhancing computational efficiency. To further optimize this process, one might consider replacing standard attention with flash attention, which has proven to be more efficient in language models. However, a direct substitution is ineffective. Flash attention is designed for long sequences, whereas window attention deals with shorter sequences but must handle numerous of them in parallel.",
    "original_url": "http://arxiv.org/pdf/2501.06480v2",
    "original_title": "Flash Window Attention: speedup the attention computation for Swin Transformer",
    "source": "arxiv",
    "authors": [
      "Zhendong Zhang"
    ],
    "published": "2025-01-11T08:13:13+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2501.06480v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2501.06480v2_chunk_1",
    "chunk_text": "Flash attention is designed for long sequences, whereas window attention deals with shorter sequences but must handle numerous of them in parallel. In this report, we present an optimized solution called Flash Window Attention, tailored specifically for window attention. Flash Window Attention improves attention computation efficiency by up to 300% and enhances end-to-end runtime efficiency by up to 30%. Our code is available online.",
    "original_url": "http://arxiv.org/pdf/2501.06480v2",
    "original_title": "Flash Window Attention: speedup the attention computation for Swin Transformer",
    "source": "arxiv",
    "authors": [
      "Zhendong Zhang"
    ],
    "published": "2025-01-11T08:13:13+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2501.06480v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2312.08268v1_chunk_0",
    "chunk_text": "Efficient Multi-Object Pose Estimation using Multi-Resolution Deformable Attention and Query Aggregation\n\nObject pose estimation is a long-standing problem in computer vision. Recently, attention-based vision transformer models have achieved state-of-the-art results in many computer vision applications. Exploiting the permutation-invariant nature of the attention mechanism, a family of vision transformer models formulate multi-object pose estimation as a set prediction problem. However, existing vision transformer models for multi-object pose estimation rely exclusively on the attention mechanism. Convolutional neural networks, on the other hand, hard-wire various inductive biases into their architecture.",
    "original_url": "http://arxiv.org/pdf/2312.08268v1",
    "original_title": "Efficient Multi-Object Pose Estimation using Multi-Resolution Deformable Attention and Query Aggregation",
    "source": "arxiv",
    "authors": [
      "Arul Selvam Periyasamy",
      "Vladimir Tsaturyan",
      "Sven Behnke"
    ],
    "published": "2023-12-13T16:30:00+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2312.08268v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2312.08268v1_chunk_1",
    "chunk_text": "Convolutional neural networks, on the other hand, hard-wire various inductive biases into their architecture. In this paper, we investigate incorporating inductive biases in vision transformer models for multi-object pose estimation, which facilitates learning long-range dependencies while circumventing the costly global attention. In particular, we use multi-resolution deformable attention, where the attention operation is performed only between a few deformed reference points. Furthermore, we propose a query aggregation mechanism that enables increasing the number of object queries without increasing the computational complexity. We evaluate the proposed model on the challenging YCB-Video dataset and report state-of-the-art results.",
    "original_url": "http://arxiv.org/pdf/2312.08268v1",
    "original_title": "Efficient Multi-Object Pose Estimation using Multi-Resolution Deformable Attention and Query Aggregation",
    "source": "arxiv",
    "authors": [
      "Arul Selvam Periyasamy",
      "Vladimir Tsaturyan",
      "Sven Behnke"
    ],
    "published": "2023-12-13T16:30:00+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2312.08268v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2312.08268v1_chunk_2",
    "chunk_text": "We evaluate the proposed model on the challenging YCB-Video dataset and report state-of-the-art results.",
    "original_url": "http://arxiv.org/pdf/2312.08268v1",
    "original_title": "Efficient Multi-Object Pose Estimation using Multi-Resolution Deformable Attention and Query Aggregation",
    "source": "arxiv",
    "authors": [
      "Arul Selvam Periyasamy",
      "Vladimir Tsaturyan",
      "Sven Behnke"
    ],
    "published": "2023-12-13T16:30:00+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2312.08268v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2405.09285v1_chunk_0",
    "chunk_text": "Positional Knowledge is All You Need: Position-induced Transformer (PiT) for Operator Learning\n\nOperator learning for Partial Differential Equations (PDEs) is rapidly emerging as a promising approach for surrogate modeling of intricate systems. Transformers with the self-attention mechanism$\\unicode{x2013}$a powerful tool originally designed for natural language processing$\\unicode{x2013}$have recently been adapted for operator learning. However, they confront challenges, including high computational demands and limited interpretability. This raises a critical question: Is there a more efficient attention mechanism for Transformer-based operator learning? This paper proposes the Position-induced Transformer (PiT), built on an innovative position-attention mechanism, which demonstrates significant advantages over the classical self-attention in operator learning.",
    "original_url": "http://arxiv.org/pdf/2405.09285v1",
    "original_title": "Positional Knowledge is All You Need: Position-induced Transformer (PiT) for Operator Learning",
    "source": "arxiv",
    "authors": [
      "Junfeng Chen",
      "Kailiang Wu"
    ],
    "published": "2024-05-15T12:09:24+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2405.09285v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2405.09285v1_chunk_1",
    "chunk_text": "This paper proposes the Position-induced Transformer (PiT), built on an innovative position-attention mechanism, which demonstrates significant advantages over the classical self-attention in operator learning. Position-attention draws inspiration from numerical methods for PDEs. Different from self-attention, position-attention is induced by only the spatial interrelations of sampling positions for input functions of the operators, and does not rely on the input function values themselves, thereby greatly boosting efficiency. PiT exhibits superior performance over current state-of-the-art neural operators in a variety of complex operator learning tasks across diverse PDE benchmarks. Additionally, PiT possesses an enhanced discretization convergence feature, compared to the widely-used Fourier neural operator.",
    "original_url": "http://arxiv.org/pdf/2405.09285v1",
    "original_title": "Positional Knowledge is All You Need: Position-induced Transformer (PiT) for Operator Learning",
    "source": "arxiv",
    "authors": [
      "Junfeng Chen",
      "Kailiang Wu"
    ],
    "published": "2024-05-15T12:09:24+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2405.09285v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2405.09285v1_chunk_2",
    "chunk_text": "Additionally, PiT possesses an enhanced discretization convergence feature, compared to the widely-used Fourier neural operator.",
    "original_url": "http://arxiv.org/pdf/2405.09285v1",
    "original_title": "Positional Knowledge is All You Need: Position-induced Transformer (PiT) for Operator Learning",
    "source": "arxiv",
    "authors": [
      "Junfeng Chen",
      "Kailiang Wu"
    ],
    "published": "2024-05-15T12:09:24+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2405.09285v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2503.15983v1_chunk_0",
    "chunk_text": "InhibiDistilbert: Knowledge Distillation for a ReLU and Addition-based Transformer\n\nThis work explores optimizing transformer-based language models by integrating model compression techniques with inhibitor attention, a novel alternative attention mechanism. Inhibitor attention employs Manhattan distances and ReLU activations instead of the matrix multiplications and softmax activation of the conventional scaled dot-product attention. This shift offers potential computational and energy savings while maintaining model effectiveness. We propose further adjustments to improve the inhibitor mechanism's training efficiency and evaluate its performance on the DistilBERT architecture. Our knowledge distillation experiments indicate that the modified inhibitor transformer model can achieve competitive performance on standard NLP benchmarks, including General Language Understanding Evaluation (GLUE) and sentiment analysis tasks.",
    "original_url": "http://arxiv.org/pdf/2503.15983v1",
    "original_title": "InhibiDistilbert: Knowledge Distillation for a ReLU and Addition-based Transformer",
    "source": "arxiv",
    "authors": [
      "Tony Zhang",
      "Rickard Brännvall"
    ],
    "published": "2025-03-20T09:30:35+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2503.15983v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2503.15983v1_chunk_1",
    "chunk_text": "Our knowledge distillation experiments indicate that the modified inhibitor transformer model can achieve competitive performance on standard NLP benchmarks, including General Language Understanding Evaluation (GLUE) and sentiment analysis tasks.",
    "original_url": "http://arxiv.org/pdf/2503.15983v1",
    "original_title": "InhibiDistilbert: Knowledge Distillation for a ReLU and Addition-based Transformer",
    "source": "arxiv",
    "authors": [
      "Tony Zhang",
      "Rickard Brännvall"
    ],
    "published": "2025-03-20T09:30:35+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2503.15983v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2006.03265v2_chunk_0",
    "chunk_text": "Understanding Self-Attention of Self-Supervised Audio Transformers\n\nSelf-supervised Audio Transformers (SAT) enable great success in many downstream speech applications like ASR, but how they work has not been widely explored yet. In this work, we present multiple strategies for the analysis of attention mechanisms in SAT. We categorize attentions into explainable categories, where we discover each category possesses its own unique functionality. We provide a visualization tool for understanding multi-head self-attention, importance ranking strategies for identifying critical attention, and attention refinement techniques to improve model performance.",
    "original_url": "http://arxiv.org/pdf/2006.03265v2",
    "original_title": "Understanding Self-Attention of Self-Supervised Audio Transformers",
    "source": "arxiv",
    "authors": [
      "Shu-wen Yang",
      "Andy T. Liu",
      "Hung-yi Lee"
    ],
    "published": "2020-06-05T07:23:03+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2006.03265v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2102.12895v1_chunk_0",
    "chunk_text": "Evolving Attention with Residual Convolutions\n\nTransformer is a ubiquitous model for natural language processing and has attracted wide attentions in computer vision. The attention maps are indispensable for a transformer model to encode the dependencies among input tokens. However, they are learned independently in each layer and sometimes fail to capture precise patterns. In this paper, we propose a novel and generic mechanism based on evolving attention to improve the performance of transformers. On one hand, the attention maps in different layers share common knowledge, thus the ones in preceding layers can instruct the attention in succeeding layers through residual connections.",
    "original_url": "http://arxiv.org/pdf/2102.12895v1",
    "original_title": "Evolving Attention with Residual Convolutions",
    "source": "arxiv",
    "authors": [
      "Yujing Wang",
      "Yaming Yang",
      "Jiangang Bai",
      "Mingliang Zhang",
      "Jing Bai",
      "Jing Yu",
      "Ce Zhang",
      "Gao Huang",
      "Yunhai Tong"
    ],
    "published": "2021-02-20T15:24:06+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2102.12895v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2102.12895v1_chunk_1",
    "chunk_text": "On one hand, the attention maps in different layers share common knowledge, thus the ones in preceding layers can instruct the attention in succeeding layers through residual connections. On the other hand, low-level and high-level attentions vary in the level of abstraction, so we adopt convolutional layers to model the evolutionary process of attention maps. The proposed evolving attention mechanism achieves significant performance improvement over various state-of-the-art models for multiple tasks, including image classification, natural language understanding and machine translation.",
    "original_url": "http://arxiv.org/pdf/2102.12895v1",
    "original_title": "Evolving Attention with Residual Convolutions",
    "source": "arxiv",
    "authors": [
      "Yujing Wang",
      "Yaming Yang",
      "Jiangang Bai",
      "Mingliang Zhang",
      "Jing Bai",
      "Jing Yu",
      "Ce Zhang",
      "Gao Huang",
      "Yunhai Tong"
    ],
    "published": "2021-02-20T15:24:06+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2102.12895v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2108.04962v1_chunk_0",
    "chunk_text": "Adaptive Multi-Resolution Attention with Linear Complexity\n\nTransformers have improved the state-of-the-art across numerous tasks in sequence modeling. Besides the quadratic computational and memory complexity w.r.t the sequence length, the self-attention mechanism only processes information at the same scale, i.e., all attention heads are in the same resolution, resulting in the limited power of the Transformer. To remedy this, we propose a novel and efficient structure named Adaptive Multi-Resolution Attention (AdaMRA for short), which scales linearly to sequence length in terms of time and space. Specifically, we leverage a multi-resolution multi-head attention mechanism, enabling attention heads to capture long-range contextual information in a coarse-to-fine fashion. Moreover, to capture the potential relations between query representation and clues of different attention granularities, we leave the decision of which resolution of attention to use to query, which further improves the model's capacity compared to vanilla Transformer.",
    "original_url": "http://arxiv.org/pdf/2108.04962v1",
    "original_title": "Adaptive Multi-Resolution Attention with Linear Complexity",
    "source": "arxiv",
    "authors": [
      "Yao Zhang",
      "Yunpu Ma",
      "Thomas Seidl",
      "Volker Tresp"
    ],
    "published": "2021-08-10T23:17:16+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2108.04962v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2108.04962v1_chunk_1",
    "chunk_text": "Moreover, to capture the potential relations between query representation and clues of different attention granularities, we leave the decision of which resolution of attention to use to query, which further improves the model's capacity compared to vanilla Transformer. In an effort to reduce complexity, we adopt kernel attention without degrading the performance. Extensive experiments on several benchmarks demonstrate the effectiveness and efficiency of our model by achieving a state-of-the-art performance-efficiency-memory trade-off. To facilitate AdaMRA utilization by the scientific community, the code implementation will be made publicly available.",
    "original_url": "http://arxiv.org/pdf/2108.04962v1",
    "original_title": "Adaptive Multi-Resolution Attention with Linear Complexity",
    "source": "arxiv",
    "authors": [
      "Yao Zhang",
      "Yunpu Ma",
      "Thomas Seidl",
      "Volker Tresp"
    ],
    "published": "2021-08-10T23:17:16+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2108.04962v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2207.09603v2_chunk_0",
    "chunk_text": "AiATrack: Attention in Attention for Transformer Visual Tracking\n\nTransformer trackers have achieved impressive advancements recently, where the attention mechanism plays an important role. However, the independent correlation computation in the attention mechanism could result in noisy and ambiguous attention weights, which inhibits further performance improvement. To address this issue, we propose an attention in attention (AiA) module, which enhances appropriate correlations and suppresses erroneous ones by seeking consensus among all correlation vectors. Our AiA module can be readily applied to both self-attention blocks and cross-attention blocks to facilitate feature aggregation and information propagation for visual tracking. Moreover, we propose a streamlined Transformer tracking framework, dubbed AiATrack, by introducing efficient feature reuse and target-background embeddings to make full use of temporal references.",
    "original_url": "http://arxiv.org/pdf/2207.09603v2",
    "original_title": "AiATrack: Attention in Attention for Transformer Visual Tracking",
    "source": "arxiv",
    "authors": [
      "Shenyuan Gao",
      "Chunluan Zhou",
      "Chao Ma",
      "Xinggang Wang",
      "Junsong Yuan"
    ],
    "published": "2022-07-20T00:44:03+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2207.09603v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2207.09603v2_chunk_1",
    "chunk_text": "Moreover, we propose a streamlined Transformer tracking framework, dubbed AiATrack, by introducing efficient feature reuse and target-background embeddings to make full use of temporal references. Experiments show that our tracker achieves state-of-the-art performance on six tracking benchmarks while running at a real-time speed.",
    "original_url": "http://arxiv.org/pdf/2207.09603v2",
    "original_title": "AiATrack: Attention in Attention for Transformer Visual Tracking",
    "source": "arxiv",
    "authors": [
      "Shenyuan Gao",
      "Chunluan Zhou",
      "Chao Ma",
      "Xinggang Wang",
      "Junsong Yuan"
    ],
    "published": "2022-07-20T00:44:03+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2207.09603v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2112.12748v1_chunk_0",
    "chunk_text": "Assessing the Impact of Attention and Self-Attention Mechanisms on the Classification of Skin Lesions\n\nAttention mechanisms have raised significant interest in the research community, since they promise significant improvements in the performance of neural network architectures. However, in any specific problem, we still lack a principled way to choose specific mechanisms and hyper-parameters that lead to guaranteed improvements. More recently, self-attention has been proposed and widely used in transformer-like architectures, leading to significant breakthroughs in some applications. In this work we focus on two forms of attention mechanisms: attention modules and self-attention. Attention modules are used to reweight the features of each layer input tensor.",
    "original_url": "http://arxiv.org/pdf/2112.12748v1",
    "original_title": "Assessing the Impact of Attention and Self-Attention Mechanisms on the Classification of Skin Lesions",
    "source": "arxiv",
    "authors": [
      "Rafael Pedro",
      "Arlindo L. Oliveira"
    ],
    "published": "2021-12-23T18:02:48+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2112.12748v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2112.12748v1_chunk_1",
    "chunk_text": "Attention modules are used to reweight the features of each layer input tensor. Different modules have different ways to perform this reweighting in fully connected or convolutional layers. The attention models studied are completely modular and in this work they will be used with the popular ResNet architecture. Self-Attention, originally proposed in the area of Natural Language Processing makes it possible to relate all the items in an input sequence. Self-Attention is becoming increasingly popular in Computer Vision, where it is sometimes combined with convolutional layers, although some recent architectures do away entirely with convolutions.",
    "original_url": "http://arxiv.org/pdf/2112.12748v1",
    "original_title": "Assessing the Impact of Attention and Self-Attention Mechanisms on the Classification of Skin Lesions",
    "source": "arxiv",
    "authors": [
      "Rafael Pedro",
      "Arlindo L. Oliveira"
    ],
    "published": "2021-12-23T18:02:48+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2112.12748v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2112.12748v1_chunk_2",
    "chunk_text": "Self-Attention is becoming increasingly popular in Computer Vision, where it is sometimes combined with convolutional layers, although some recent architectures do away entirely with convolutions. In this work, we study and perform an objective comparison of a number of different attention mechanisms in a specific computer vision task, the classification of samples in the widely used Skin Cancer MNIST dataset. The results show that attention modules do sometimes improve the performance of convolutional neural network architectures, but also that this improvement, although noticeable and statistically significant, is not consistent in different settings. The results obtained with self-attention mechanisms, on the other hand, show consistent and significant improvements, leading to the best results even in architectures with a reduced number of parameters.",
    "original_url": "http://arxiv.org/pdf/2112.12748v1",
    "original_title": "Assessing the Impact of Attention and Self-Attention Mechanisms on the Classification of Skin Lesions",
    "source": "arxiv",
    "authors": [
      "Rafael Pedro",
      "Arlindo L. Oliveira"
    ],
    "published": "2021-12-23T18:02:48+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2112.12748v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2207.04399v1_chunk_0",
    "chunk_text": "Horizontal and Vertical Attention in Transformers\n\nTransformers are built upon multi-head scaled dot-product attention and positional encoding, which aim to learn the feature representations and token dependencies. In this work, we focus on enhancing the distinctive representation by learning to augment the feature maps with the self-attention mechanism in Transformers. Specifically, we propose the horizontal attention to re-weight the multi-head output of the scaled dot-product attention before dimensionality reduction, and propose the vertical attention to adaptively re-calibrate channel-wise feature responses by explicitly modelling inter-dependencies among different channels. We demonstrate the Transformer models equipped with the two attentions have a high generalization capability across different supervised learning tasks, with a very minor additional computational cost overhead. The proposed horizontal and vertical attentions are highly modular, which can be inserted into various Transformer models to further improve the performance.",
    "original_url": "http://arxiv.org/pdf/2207.04399v1",
    "original_title": "Horizontal and Vertical Attention in Transformers",
    "source": "arxiv",
    "authors": [
      "Litao Yu",
      "Jian Zhang"
    ],
    "published": "2022-07-10T07:08:18+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2207.04399v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2207.04399v1_chunk_1",
    "chunk_text": "The proposed horizontal and vertical attentions are highly modular, which can be inserted into various Transformer models to further improve the performance. Our code is available in the supplementary material.",
    "original_url": "http://arxiv.org/pdf/2207.04399v1",
    "original_title": "Horizontal and Vertical Attention in Transformers",
    "source": "arxiv",
    "authors": [
      "Litao Yu",
      "Jian Zhang"
    ],
    "published": "2022-07-10T07:08:18+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2207.04399v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2302.14346v1_chunk_0",
    "chunk_text": "Sampled Transformer for Point Sets\n\nThe sparse transformer can reduce the computational complexity of the self-attention layers to $O(n)$, whilst still being a universal approximator of continuous sequence-to-sequence functions. However, this permutation variant operation is not appropriate for direct application to sets. In this paper, we proposed an $O(n)$ complexity sampled transformer that can process point set elements directly without any additional inductive bias. Our sampled transformer introduces random element sampling, which randomly splits point sets into subsets, followed by applying a shared Hamiltonian self-attention mechanism to each subset. The overall attention mechanism can be viewed as a Hamiltonian cycle in the complete attention graph, and the permutation of point set elements is equivalent to randomly sampling Hamiltonian cycles.",
    "original_url": "http://arxiv.org/pdf/2302.14346v1",
    "original_title": "Sampled Transformer for Point Sets",
    "source": "arxiv",
    "authors": [
      "Shidi Li",
      "Christian Walder",
      "Alexander Soen",
      "Lexing Xie",
      "Miaomiao Liu"
    ],
    "published": "2023-02-28T06:38:05+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2302.14346v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2302.14346v1_chunk_1",
    "chunk_text": "The overall attention mechanism can be viewed as a Hamiltonian cycle in the complete attention graph, and the permutation of point set elements is equivalent to randomly sampling Hamiltonian cycles. This mechanism implements a Monte Carlo simulation of the $O(n^2)$ dense attention connections. We show that it is a universal approximator for continuous set-to-set functions. Experimental results on point-clouds show comparable or better accuracy with significantly reduced computational complexity compared to the dense transformer or alternative sparse attention schemes.",
    "original_url": "http://arxiv.org/pdf/2302.14346v1",
    "original_title": "Sampled Transformer for Point Sets",
    "source": "arxiv",
    "authors": [
      "Shidi Li",
      "Christian Walder",
      "Alexander Soen",
      "Lexing Xie",
      "Miaomiao Liu"
    ],
    "published": "2023-02-28T06:38:05+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2302.14346v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2109.03888v1_chunk_0",
    "chunk_text": "Sparsity and Sentence Structure in Encoder-Decoder Attention of Summarization Systems\n\nTransformer models have achieved state-of-the-art results in a wide range of NLP tasks including summarization. Training and inference using large transformer models can be computationally expensive. Previous work has focused on one important bottleneck, the quadratic self-attention mechanism in the encoder. Modified encoder architectures such as LED or LoBART use local attention patterns to address this problem for summarization. In contrast, this work focuses on the transformer's encoder-decoder attention mechanism.",
    "original_url": "http://arxiv.org/pdf/2109.03888v1",
    "original_title": "Sparsity and Sentence Structure in Encoder-Decoder Attention of Summarization Systems",
    "source": "arxiv",
    "authors": [
      "Potsawee Manakul",
      "Mark J. F. Gales"
    ],
    "published": "2021-09-08T19:32:42+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2109.03888v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2109.03888v1_chunk_1",
    "chunk_text": "In contrast, this work focuses on the transformer's encoder-decoder attention mechanism. The cost of this attention becomes more significant in inference or training approaches that require model-generated histories. First, we examine the complexity of the encoder-decoder attention. We demonstrate empirically that there is a sparse sentence structure in document summarization that can be exploited by constraining the attention mechanism to a subset of input sentences, whilst maintaining system performance. Second, we propose a modified architecture that selects the subset of sentences to constrain the encoder-decoder attention.",
    "original_url": "http://arxiv.org/pdf/2109.03888v1",
    "original_title": "Sparsity and Sentence Structure in Encoder-Decoder Attention of Summarization Systems",
    "source": "arxiv",
    "authors": [
      "Potsawee Manakul",
      "Mark J. F. Gales"
    ],
    "published": "2021-09-08T19:32:42+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2109.03888v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2109.03888v1_chunk_2",
    "chunk_text": "Second, we propose a modified architecture that selects the subset of sentences to constrain the encoder-decoder attention. Experiments are carried out on abstractive summarization tasks, including CNN/DailyMail, XSum, Spotify Podcast, and arXiv.",
    "original_url": "http://arxiv.org/pdf/2109.03888v1",
    "original_title": "Sparsity and Sentence Structure in Encoder-Decoder Attention of Summarization Systems",
    "source": "arxiv",
    "authors": [
      "Potsawee Manakul",
      "Mark J. F. Gales"
    ],
    "published": "2021-09-08T19:32:42+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2109.03888v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2402.07901v1_chunk_0",
    "chunk_text": "FAST: Factorizable Attention for Speeding up Transformers\n\nMotivated by the factorization inherent in the original fast multipole method and the improved fast Gauss transform we introduce a factorable form of attention that operates efficiently in high dimensions. This approach reduces the computational and memory complexity of the attention mechanism in transformers from $O(N^2)$ to $O(N)$. In comparison to previous attempts, our work presents a linearly scaled attention mechanism that maintains the full representation of the attention matrix without compromising on sparsification and incorporates the all-to-all relationship between tokens. We explore the properties of our new attention metric and conduct tests in various standard settings. Results indicate that our attention mechanism has a robust performance and holds significant promise for diverse applications where self-attention is used.",
    "original_url": "http://arxiv.org/pdf/2402.07901v1",
    "original_title": "FAST: Factorizable Attention for Speeding up Transformers",
    "source": "arxiv",
    "authors": [
      "Armin Gerami",
      "Monte Hoover",
      "Pranav S. Dulepet",
      "Ramani Duraiswami"
    ],
    "published": "2024-02-12T18:59:39+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2402.07901v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2402.07901v1_chunk_1",
    "chunk_text": "Results indicate that our attention mechanism has a robust performance and holds significant promise for diverse applications where self-attention is used.",
    "original_url": "http://arxiv.org/pdf/2402.07901v1",
    "original_title": "FAST: Factorizable Attention for Speeding up Transformers",
    "source": "arxiv",
    "authors": [
      "Armin Gerami",
      "Monte Hoover",
      "Pranav S. Dulepet",
      "Ramani Duraiswami"
    ],
    "published": "2024-02-12T18:59:39+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2402.07901v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2310.01749v2_chunk_0",
    "chunk_text": "Stack Attention: Improving the Ability of Transformers to Model Hierarchical Patterns\n\nAttention, specifically scaled dot-product attention, has proven effective for natural language, but it does not have a mechanism for handling hierarchical patterns of arbitrary nesting depth, which limits its ability to recognize certain syntactic structures. To address this shortcoming, we propose stack attention: an attention operator that incorporates stacks, inspired by their theoretical connections to context-free languages (CFLs). We show that stack attention is analogous to standard attention, but with a latent model of syntax that requires no syntactic supervision. We propose two variants: one related to deterministic pushdown automata (PDAs) and one based on nondeterministic PDAs, which allows transformers to recognize arbitrary CFLs. We show that transformers with stack attention are very effective at learning CFLs that standard transformers struggle on, achieving strong results on a CFL with theoretically maximal parsing difficulty.",
    "original_url": "http://arxiv.org/pdf/2310.01749v2",
    "original_title": "Stack Attention: Improving the Ability of Transformers to Model Hierarchical Patterns",
    "source": "arxiv",
    "authors": [
      "Brian DuSell",
      "David Chiang"
    ],
    "published": "2023-10-03T02:18:06+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2310.01749v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2310.01749v2_chunk_1",
    "chunk_text": "We show that transformers with stack attention are very effective at learning CFLs that standard transformers struggle on, achieving strong results on a CFL with theoretically maximal parsing difficulty. We also show that stack attention is more effective at natural language modeling under a constrained parameter budget, and we include results on machine translation.",
    "original_url": "http://arxiv.org/pdf/2310.01749v2",
    "original_title": "Stack Attention: Improving the Ability of Transformers to Model Hierarchical Patterns",
    "source": "arxiv",
    "authors": [
      "Brian DuSell",
      "David Chiang"
    ],
    "published": "2023-10-03T02:18:06+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2310.01749v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2502.17206v1_chunk_0",
    "chunk_text": "Neural Attention: A Novel Mechanism for Enhanced Expressive Power in Transformer Models\n\nTransformer models typically calculate attention matrices using dot products, which have limitations when capturing nonlinear relationships between embedding vectors. We propose Neural Attention, a technique that replaces dot products with feed-forward networks, enabling a more expressive representation of relationships between tokens. This approach modifies only the attention matrix calculation while preserving the matrix dimensions, making it easily adaptable to existing transformer-based architectures. We provide a detailed mathematical justification for why Neural Attention increases representational capacity and conduct controlled experiments to validate this claim. When comparing Neural Attention and Dot-Product Attention, NLP experiments on WikiText-103 show a reduction in perplexity of over 5 percent.",
    "original_url": "http://arxiv.org/pdf/2502.17206v1",
    "original_title": "Neural Attention: A Novel Mechanism for Enhanced Expressive Power in Transformer Models",
    "source": "arxiv",
    "authors": [
      "Andrew DiGiugno",
      "Ausif Mahmood"
    ],
    "published": "2025-02-24T14:39:40+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2502.17206v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2502.17206v1_chunk_1",
    "chunk_text": "When comparing Neural Attention and Dot-Product Attention, NLP experiments on WikiText-103 show a reduction in perplexity of over 5 percent. Similarly, experiments on CIFAR-10 and CIFAR-100 show comparable improvements for image classification tasks. While Neural Attention introduces higher computational demands, we develop techniques to mitigate these challenges, ensuring practical usability without sacrificing the increased expressivity it provides. This work establishes Neural Attention as an effective means of enhancing the predictive capabilities of transformer models across a variety of applications.",
    "original_url": "http://arxiv.org/pdf/2502.17206v1",
    "original_title": "Neural Attention: A Novel Mechanism for Enhanced Expressive Power in Transformer Models",
    "source": "arxiv",
    "authors": [
      "Andrew DiGiugno",
      "Ausif Mahmood"
    ],
    "published": "2025-02-24T14:39:40+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2502.17206v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2501.02393v3_chunk_0",
    "chunk_text": "Graph-Aware Isomorphic Attention for Adaptive Dynamics in Transformers\n\nWe present an approach to modifying Transformer architectures by integrating graph-aware relational reasoning into the attention mechanism, merging concepts from graph neural networks and language modeling. Building on the inherent connection between attention and graph theory, we reformulate the Transformer's attention mechanism as a graph operation and propose Graph-Aware Isomorphic Attention. This method leverages advanced graph modeling strategies, including Graph Isomorphism Networks (GIN) and Principal Neighborhood Aggregation (PNA), to enrich the representation of relational structures. Our approach captures complex dependencies and generalizes across tasks, as evidenced by a reduced generalization gap and improved learning performance. Additionally, we expand the concept of graph-aware attention to introduce Sparse GIN-Attention, a fine-tuning approach that employs sparse GINs.",
    "original_url": "http://arxiv.org/pdf/2501.02393v3",
    "original_title": "Graph-Aware Isomorphic Attention for Adaptive Dynamics in Transformers",
    "source": "arxiv",
    "authors": [
      "Markus J. Buehler"
    ],
    "published": "2025-01-04T22:30:21+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2501.02393v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2501.02393v3_chunk_1",
    "chunk_text": "Additionally, we expand the concept of graph-aware attention to introduce Sparse GIN-Attention, a fine-tuning approach that employs sparse GINs. By interpreting attention matrices as sparse adjacency graphs, this technique enhances the adaptability of pre-trained foundational models with minimal computational overhead, endowing them with graph-aware capabilities. Sparse GIN-Attention fine-tuning achieves improved training dynamics and better generalization compared to alternative methods like low-rank adaption (LoRA). We discuss latent graph-like structures within traditional attention mechanisms, offering a new lens through which Transformers can be understood. By evolving Transformers as hierarchical GIN models for relational reasoning.",
    "original_url": "http://arxiv.org/pdf/2501.02393v3",
    "original_title": "Graph-Aware Isomorphic Attention for Adaptive Dynamics in Transformers",
    "source": "arxiv",
    "authors": [
      "Markus J. Buehler"
    ],
    "published": "2025-01-04T22:30:21+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2501.02393v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2501.02393v3_chunk_2",
    "chunk_text": "By evolving Transformers as hierarchical GIN models for relational reasoning. This perspective suggests profound implications for foundational model development, enabling the design of architectures that dynamically adapt to both local and global dependencies. Applications in bioinformatics, materials science, language modeling, and beyond could benefit from this synthesis of relational and sequential data modeling, setting the stage for interpretable and generalizable modeling strategies.",
    "original_url": "http://arxiv.org/pdf/2501.02393v3",
    "original_title": "Graph-Aware Isomorphic Attention for Adaptive Dynamics in Transformers",
    "source": "arxiv",
    "authors": [
      "Markus J. Buehler"
    ],
    "published": "2025-01-04T22:30:21+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2501.02393v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2503.02130v2_chunk_0",
    "chunk_text": "Forgetting Transformer: Softmax Attention with a Forget Gate\n\nAn essential component of modern recurrent sequence models is the forget gate. While Transformers do not have an explicit recurrent form, we show that a forget gate can be naturally incorporated into Transformers by down-weighting the unnormalized attention scores in a data-dependent way. We name this attention mechanism Forgetting Attention and the resulting model the Forgetting Transformer (FoX). We show that FoX outperforms the Transformer on long-context language modeling, length extrapolation, and short-context downstream tasks, while performing on par with the Transformer on long-context downstream tasks. Moreover, it is compatible with the FlashAttention algorithm and does not require any positional embeddings.",
    "original_url": "http://arxiv.org/pdf/2503.02130v2",
    "original_title": "Forgetting Transformer: Softmax Attention with a Forget Gate",
    "source": "arxiv",
    "authors": [
      "Zhixuan Lin",
      "Evgenii Nikishin",
      "Xu Owen He",
      "Aaron Courville"
    ],
    "published": "2025-03-03T23:35:23+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2503.02130v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2503.02130v2_chunk_1",
    "chunk_text": "Moreover, it is compatible with the FlashAttention algorithm and does not require any positional embeddings. Several analyses, including the needle-in-the-haystack test, show that FoX also retains the Transformer's superior long-context capabilities over recurrent sequence models such as Mamba-2, HGRN2, and DeltaNet. We also introduce a \"Pro\" block design that incorporates some common architectural components in recurrent sequence models and find it significantly improves the performance of both FoX and the Transformer. Our code is available at https://github.com/zhixuan-lin/forgetting-transformer.",
    "original_url": "http://arxiv.org/pdf/2503.02130v2",
    "original_title": "Forgetting Transformer: Softmax Attention with a Forget Gate",
    "source": "arxiv",
    "authors": [
      "Zhixuan Lin",
      "Evgenii Nikishin",
      "Xu Owen He",
      "Aaron Courville"
    ],
    "published": "2025-03-03T23:35:23+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2503.02130v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2207.13955v1_chunk_0",
    "chunk_text": "Neural Architecture Search on Efficient Transformers and Beyond\n\nRecently, numerous efficient Transformers have been proposed to reduce the quadratic computational complexity of standard Transformers caused by the Softmax attention. However, most of them simply swap Softmax with an efficient attention mechanism without considering the customized architectures specially for the efficient attention. In this paper, we argue that the handcrafted vanilla Transformer architectures for Softmax attention may not be suitable for efficient Transformers. To address this issue, we propose a new framework to find optimal architectures for efficient Transformers with the neural architecture search (NAS) technique. The proposed method is validated on popular machine translation and image classification tasks.",
    "original_url": "http://arxiv.org/pdf/2207.13955v1",
    "original_title": "Neural Architecture Search on Efficient Transformers and Beyond",
    "source": "arxiv",
    "authors": [
      "Zexiang Liu",
      "Dong Li",
      "Kaiyue Lu",
      "Zhen Qin",
      "Weixuan Sun",
      "Jiacheng Xu",
      "Yiran Zhong"
    ],
    "published": "2022-07-28T08:41:41+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2207.13955v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2207.13955v1_chunk_1",
    "chunk_text": "The proposed method is validated on popular machine translation and image classification tasks. We observe that the optimal architecture of the efficient Transformer has the reduced computation compared with that of the standard Transformer, but the general accuracy is less comparable. It indicates that the Softmax attention and efficient attention have their own distinctions but neither of them can simultaneously balance the accuracy and efficiency well. This motivates us to mix the two types of attention to reduce the performance imbalance. Besides the search spaces that commonly used in existing NAS Transformer approaches, we propose a new search space that allows the NAS algorithm to automatically search the attention variants along with architectures.",
    "original_url": "http://arxiv.org/pdf/2207.13955v1",
    "original_title": "Neural Architecture Search on Efficient Transformers and Beyond",
    "source": "arxiv",
    "authors": [
      "Zexiang Liu",
      "Dong Li",
      "Kaiyue Lu",
      "Zhen Qin",
      "Weixuan Sun",
      "Jiacheng Xu",
      "Yiran Zhong"
    ],
    "published": "2022-07-28T08:41:41+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2207.13955v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2207.13955v1_chunk_2",
    "chunk_text": "Besides the search spaces that commonly used in existing NAS Transformer approaches, we propose a new search space that allows the NAS algorithm to automatically search the attention variants along with architectures. Extensive experiments on WMT' 14 En-De and CIFAR-10 demonstrate that our searched architecture maintains comparable accuracy to the standard Transformer with notably improved computational efficiency.",
    "original_url": "http://arxiv.org/pdf/2207.13955v1",
    "original_title": "Neural Architecture Search on Efficient Transformers and Beyond",
    "source": "arxiv",
    "authors": [
      "Zexiang Liu",
      "Dong Li",
      "Kaiyue Lu",
      "Zhen Qin",
      "Weixuan Sun",
      "Jiacheng Xu",
      "Yiran Zhong"
    ],
    "published": "2022-07-28T08:41:41+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2207.13955v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2410.03805v2_chunk_0",
    "chunk_text": "Local Attention Mechanism: Boosting the Transformer Architecture for Long-Sequence Time Series Forecasting\n\nTransformers have become the leading choice in natural language processing over other deep learning architectures. This trend has also permeated the field of time series analysis, especially for long-horizon forecasting, showcasing promising results both in performance and running time. In this paper, we introduce Local Attention Mechanism (LAM), an efficient attention mechanism tailored for time series analysis. This mechanism exploits the continuity properties of time series to reduce the number of attention scores computed. We present an algorithm for implementing LAM in tensor algebra that runs in time and memory O(nlogn), significantly improving upon the O(n^2) time and memory complexity of traditional attention mechanisms.",
    "original_url": "http://arxiv.org/pdf/2410.03805v2",
    "original_title": "Local Attention Mechanism: Boosting the Transformer Architecture for Long-Sequence Time Series Forecasting",
    "source": "arxiv",
    "authors": [
      "Ignacio Aguilera-Martos",
      "Andrés Herrera-Poyatos",
      "Julián Luengo",
      "Francisco Herrera"
    ],
    "published": "2024-10-04T11:32:02+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2410.03805v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2410.03805v2_chunk_1",
    "chunk_text": "We present an algorithm for implementing LAM in tensor algebra that runs in time and memory O(nlogn), significantly improving upon the O(n^2) time and memory complexity of traditional attention mechanisms. We also note the lack of proper datasets to evaluate long-horizon forecast models. Thus, we propose a novel set of datasets to improve the evaluation of models addressing long-horizon forecasting challenges. Our experimental analysis demonstrates that the vanilla transformer architecture magnified with LAM surpasses state-of-the-art models, including the vanilla attention mechanism. These results confirm the effectiveness of our approach and highlight a range of future challenges in long-sequence time series forecasting.",
    "original_url": "http://arxiv.org/pdf/2410.03805v2",
    "original_title": "Local Attention Mechanism: Boosting the Transformer Architecture for Long-Sequence Time Series Forecasting",
    "source": "arxiv",
    "authors": [
      "Ignacio Aguilera-Martos",
      "Andrés Herrera-Poyatos",
      "Julián Luengo",
      "Francisco Herrera"
    ],
    "published": "2024-10-04T11:32:02+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2410.03805v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2410.03805v2_chunk_2",
    "chunk_text": "These results confirm the effectiveness of our approach and highlight a range of future challenges in long-sequence time series forecasting.",
    "original_url": "http://arxiv.org/pdf/2410.03805v2",
    "original_title": "Local Attention Mechanism: Boosting the Transformer Architecture for Long-Sequence Time Series Forecasting",
    "source": "arxiv",
    "authors": [
      "Ignacio Aguilera-Martos",
      "Andrés Herrera-Poyatos",
      "Julián Luengo",
      "Francisco Herrera"
    ],
    "published": "2024-10-04T11:32:02+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2410.03805v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2406.14100v1_chunk_0",
    "chunk_text": "Self-Attention in Transformer Networks Explains Monkeys' Gaze Pattern in Pac-Man Game\n\nWe proactively direct our eyes and attention to collect information during problem solving and decision making. Understanding gaze patterns is crucial for gaining insights into the computation underlying the problem-solving process. However, there is a lack of interpretable models that can account for how the brain directs the eyes to collect information and utilize it, especially in the context of complex problem solving. In the current study, we analyzed the gaze patterns of two monkeys playing the Pac-Man game. We trained a transformer network to mimic the monkeys' gameplay and found its attention pattern captures the monkeys' eye movements.",
    "original_url": "http://arxiv.org/pdf/2406.14100v1",
    "original_title": "Self-Attention in Transformer Networks Explains Monkeys' Gaze Pattern in Pac-Man Game",
    "source": "arxiv",
    "authors": [
      "Zhongqiao Lin",
      "Yunwei Li",
      "Tianming Yang"
    ],
    "published": "2024-06-20T08:32:35+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2406.14100v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2406.14100v1_chunk_1",
    "chunk_text": "We trained a transformer network to mimic the monkeys' gameplay and found its attention pattern captures the monkeys' eye movements. In addition, the prediction based on the transformer network's attention outperforms the human subjects' predictions. Importantly, we dissected the computation underlying the attention mechanism of the transformer network, revealing its layered structures reflecting a value-based attention component and a component that captures the interactions between Pac-Man and other game objects. Based on these findings, we built a condensed attention model that is not only as accurate as the transformer network but also fully interpretable. Our results highlight the potential of using transformer neural networks to model and understand the cognitive processes underlying complex problem solving in the brain, opening new avenues for investigating the neural basis of cognition.",
    "original_url": "http://arxiv.org/pdf/2406.14100v1",
    "original_title": "Self-Attention in Transformer Networks Explains Monkeys' Gaze Pattern in Pac-Man Game",
    "source": "arxiv",
    "authors": [
      "Zhongqiao Lin",
      "Yunwei Li",
      "Tianming Yang"
    ],
    "published": "2024-06-20T08:32:35+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2406.14100v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2406.14100v1_chunk_2",
    "chunk_text": "Our results highlight the potential of using transformer neural networks to model and understand the cognitive processes underlying complex problem solving in the brain, opening new avenues for investigating the neural basis of cognition.",
    "original_url": "http://arxiv.org/pdf/2406.14100v1",
    "original_title": "Self-Attention in Transformer Networks Explains Monkeys' Gaze Pattern in Pac-Man Game",
    "source": "arxiv",
    "authors": [
      "Zhongqiao Lin",
      "Yunwei Li",
      "Tianming Yang"
    ],
    "published": "2024-06-20T08:32:35+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2406.14100v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2210.07661v4_chunk_0",
    "chunk_text": "CAB: Comprehensive Attention Benchmarking on Long Sequence Modeling\n\nTransformer has achieved remarkable success in language, image, and speech processing. Recently, various efficient attention architectures have been proposed to improve transformer's efficiency while largely preserving its efficacy, especially in modeling long sequences. A widely-used benchmark to test these efficient methods' capability on long-range modeling is Long Range Arena (LRA). However, LRA only focuses on the standard bidirectional (or noncausal) self attention, and completely ignores cross attentions and unidirectional (or causal) attentions, which are equally important to downstream applications. In this paper, we propose Comprehensive Attention Benchmark (CAB) under a fine-grained attention taxonomy with four distinguishable attention patterns, namely, noncausal self, causal self, noncausal cross, and causal cross attentions.",
    "original_url": "http://arxiv.org/pdf/2210.07661v4",
    "original_title": "CAB: Comprehensive Attention Benchmarking on Long Sequence Modeling",
    "source": "arxiv",
    "authors": [
      "Jun Zhang",
      "Shuyang Jiang",
      "Jiangtao Feng",
      "Lin Zheng",
      "Lingpeng Kong"
    ],
    "published": "2022-10-14T09:25:47+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2210.07661v4"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2210.07661v4_chunk_1",
    "chunk_text": "In this paper, we propose Comprehensive Attention Benchmark (CAB) under a fine-grained attention taxonomy with four distinguishable attention patterns, namely, noncausal self, causal self, noncausal cross, and causal cross attentions. CAB collects seven real-world tasks from different research areas to evaluate efficient attentions under the four attention patterns. Among these tasks, CAB validates efficient attentions in eight backbone networks to show their generalization across neural architectures. We conduct exhaustive experiments to benchmark the performances of nine widely-used efficient attention architectures designed with different philosophies on CAB. Extensive experimental results also shed light on the fundamental problems of efficient attentions, such as efficiency length against vanilla attention, performance consistency across attention patterns, the benefit of attention mechanisms, and interpolation/extrapolation on long-context language modeling.",
    "original_url": "http://arxiv.org/pdf/2210.07661v4",
    "original_title": "CAB: Comprehensive Attention Benchmarking on Long Sequence Modeling",
    "source": "arxiv",
    "authors": [
      "Jun Zhang",
      "Shuyang Jiang",
      "Jiangtao Feng",
      "Lin Zheng",
      "Lingpeng Kong"
    ],
    "published": "2022-10-14T09:25:47+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2210.07661v4"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2210.07661v4_chunk_2",
    "chunk_text": "Extensive experimental results also shed light on the fundamental problems of efficient attentions, such as efficiency length against vanilla attention, performance consistency across attention patterns, the benefit of attention mechanisms, and interpolation/extrapolation on long-context language modeling.",
    "original_url": "http://arxiv.org/pdf/2210.07661v4",
    "original_title": "CAB: Comprehensive Attention Benchmarking on Long Sequence Modeling",
    "source": "arxiv",
    "authors": [
      "Jun Zhang",
      "Shuyang Jiang",
      "Jiangtao Feng",
      "Lin Zheng",
      "Lingpeng Kong"
    ],
    "published": "2022-10-14T09:25:47+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2210.07661v4"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2211.03495v1_chunk_0",
    "chunk_text": "How Much Does Attention Actually Attend? Questioning the Importance of Attention in Pretrained Transformers\n\nThe attention mechanism is considered the backbone of the widely-used Transformer architecture. It contextualizes the input by computing input-specific attention matrices. We find that this mechanism, while powerful and elegant, is not as important as typically thought for pretrained language models. We introduce PAPA, a new probing method that replaces the input-dependent attention matrices with constant ones -- the average attention weights over multiple inputs.",
    "original_url": "http://arxiv.org/pdf/2211.03495v1",
    "original_title": "How Much Does Attention Actually Attend? Questioning the Importance of Attention in Pretrained Transformers",
    "source": "arxiv",
    "authors": [
      "Michael Hassid",
      "Hao Peng",
      "Daniel Rotem",
      "Jungo Kasai",
      "Ivan Montero",
      "Noah A. Smith",
      "Roy Schwartz"
    ],
    "published": "2022-11-07T12:37:54+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2211.03495v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2211.03495v1_chunk_1",
    "chunk_text": "We introduce PAPA, a new probing method that replaces the input-dependent attention matrices with constant ones -- the average attention weights over multiple inputs. We use PAPA to analyze several established pretrained Transformers on six downstream tasks. We find that without any input-dependent attention, all models achieve competitive performance -- an average relative drop of only 8% from the probing baseline. Further, little or no performance drop is observed when replacing half of the input-dependent attention matrices with constant (input-independent) ones. Interestingly, we show that better-performing models lose more from applying our method than weaker models, suggesting that the utilization of the input-dependent attention mechanism might be a factor in their success.",
    "original_url": "http://arxiv.org/pdf/2211.03495v1",
    "original_title": "How Much Does Attention Actually Attend? Questioning the Importance of Attention in Pretrained Transformers",
    "source": "arxiv",
    "authors": [
      "Michael Hassid",
      "Hao Peng",
      "Daniel Rotem",
      "Jungo Kasai",
      "Ivan Montero",
      "Noah A. Smith",
      "Roy Schwartz"
    ],
    "published": "2022-11-07T12:37:54+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2211.03495v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2211.03495v1_chunk_2",
    "chunk_text": "Interestingly, we show that better-performing models lose more from applying our method than weaker models, suggesting that the utilization of the input-dependent attention mechanism might be a factor in their success. Our results motivate research on simpler alternatives to input-dependent attention, as well as on methods for better utilization of this mechanism in the Transformer architecture.",
    "original_url": "http://arxiv.org/pdf/2211.03495v1",
    "original_title": "How Much Does Attention Actually Attend? Questioning the Importance of Attention in Pretrained Transformers",
    "source": "arxiv",
    "authors": [
      "Michael Hassid",
      "Hao Peng",
      "Daniel Rotem",
      "Jungo Kasai",
      "Ivan Montero",
      "Noah A. Smith",
      "Roy Schwartz"
    ],
    "published": "2022-11-07T12:37:54+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2211.03495v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2303.07585v1_chunk_0",
    "chunk_text": "Input-length-shortening and text generation via attention values\n\nIdentifying words that impact a task's performance more than others is a challenge in natural language processing. Transformers models have recently addressed this issue by incorporating an attention mechanism that assigns greater attention (i.e., relevance) scores to some words than others. Because of the attention mechanism's high computational cost, transformer models usually have an input-length limitation caused by hardware constraints. This limitation applies to many transformers, including the well-known bidirectional encoder representations of the transformer (BERT) model. In this paper, we examined BERT's attention assignment mechanism, focusing on two questions: (1) How can attention be employed to reduce input length?",
    "original_url": "http://arxiv.org/pdf/2303.07585v1",
    "original_title": "Input-length-shortening and text generation via attention values",
    "source": "arxiv",
    "authors": [
      "Neşet Özkan Tan",
      "Alex Yuxuan Peng",
      "Joshua Bensemann",
      "Qiming Bao",
      "Tim Hartill",
      "Mark Gahegan",
      "Michael Witbrock"
    ],
    "published": "2023-03-14T02:11:24+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2303.07585v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2303.07585v1_chunk_1",
    "chunk_text": "In this paper, we examined BERT's attention assignment mechanism, focusing on two questions: (1) How can attention be employed to reduce input length? (2) How can attention be used as a control mechanism for conditional text generation? We investigated these questions in the context of a text classification task. We discovered that BERT's early layers assign more critical attention scores for text classification tasks compared to later layers. We demonstrated that the first layer's attention sums could be used to filter tokens in a given sequence, considerably decreasing the input length while maintaining good test accuracy.",
    "original_url": "http://arxiv.org/pdf/2303.07585v1",
    "original_title": "Input-length-shortening and text generation via attention values",
    "source": "arxiv",
    "authors": [
      "Neşet Özkan Tan",
      "Alex Yuxuan Peng",
      "Joshua Bensemann",
      "Qiming Bao",
      "Tim Hartill",
      "Mark Gahegan",
      "Michael Witbrock"
    ],
    "published": "2023-03-14T02:11:24+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2303.07585v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2303.07585v1_chunk_2",
    "chunk_text": "We demonstrated that the first layer's attention sums could be used to filter tokens in a given sequence, considerably decreasing the input length while maintaining good test accuracy. We also applied filtering, which uses a compute-efficient semantic similarities algorithm, and discovered that retaining approximately 6\\% of the original sequence is sufficient to obtain 86.5\\% accuracy. Finally, we showed that we could generate data in a stable manner and indistinguishable from the original one by only using a small percentage (10\\%) of the tokens with high attention scores according to BERT's first layer.",
    "original_url": "http://arxiv.org/pdf/2303.07585v1",
    "original_title": "Input-length-shortening and text generation via attention values",
    "source": "arxiv",
    "authors": [
      "Neşet Özkan Tan",
      "Alex Yuxuan Peng",
      "Joshua Bensemann",
      "Qiming Bao",
      "Tim Hartill",
      "Mark Gahegan",
      "Michael Witbrock"
    ],
    "published": "2023-03-14T02:11:24+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2303.07585v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2504.00068v1_chunk_0",
    "chunk_text": "Integrating Quantum-Classical Attention in Patch Transformers for Enhanced Time Series Forecasting\n\nQCAAPatchTF is a quantum attention network integrated with an advanced patch-based transformer, designed for multivariate time series forecasting, classification, and anomaly detection. Leveraging quantum superpositions, entanglement, and variational quantum eigensolver principles, the model introduces a quantum-classical hybrid self-attention mechanism to capture multivariate correlations across time points. For multivariate long-term time series, the quantum self-attention mechanism can reduce computational complexity while maintaining temporal relationships. It then applies the quantum-classical hybrid self-attention mechanism alongside a feed-forward network in the encoder stage of the advanced patch-based transformer. While the feed-forward network learns nonlinear representations for each variable frame, the quantum self-attention mechanism processes individual series to enhance multivariate relationships.",
    "original_url": "http://arxiv.org/pdf/2504.00068v1",
    "original_title": "Integrating Quantum-Classical Attention in Patch Transformers for Enhanced Time Series Forecasting",
    "source": "arxiv",
    "authors": [
      "Sanjay Chakraborty",
      "Fredrik Heintz"
    ],
    "published": "2025-03-31T17:23:36+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2504.00068v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2504.00068v1_chunk_1",
    "chunk_text": "While the feed-forward network learns nonlinear representations for each variable frame, the quantum self-attention mechanism processes individual series to enhance multivariate relationships. The advanced patch-based transformer computes the optimized patch length by dividing the sequence length into a fixed number of patches instead of using an arbitrary set of values. The stride is then set to half of the patch length to ensure efficient overlapping representations while maintaining temporal continuity. QCAAPatchTF achieves state-of-the-art performance in both long-term and short-term forecasting, classification, and anomaly detection tasks, demonstrating state-of-the-art accuracy and efficiency on complex real-world datasets.",
    "original_url": "http://arxiv.org/pdf/2504.00068v1",
    "original_title": "Integrating Quantum-Classical Attention in Patch Transformers for Enhanced Time Series Forecasting",
    "source": "arxiv",
    "authors": [
      "Sanjay Chakraborty",
      "Fredrik Heintz"
    ],
    "published": "2025-03-31T17:23:36+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2504.00068v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2112.05425v1_chunk_0",
    "chunk_text": "Couplformer:Rethinking Vision Transformer with Coupling Attention Map\n\nWith the development of the self-attention mechanism, the Transformer model has demonstrated its outstanding performance in the computer vision domain. However, the massive computation brought from the full attention mechanism became a heavy burden for memory consumption. Sequentially, the limitation of memory reduces the possibility of improving the Transformer model. To remedy this problem, we propose a novel memory economy attention mechanism named Couplformer, which decouples the attention map into two sub-matrices and generates the alignment scores from spatial information. A series of different scale image classification tasks are applied to evaluate the effectiveness of our model.",
    "original_url": "http://arxiv.org/pdf/2112.05425v1",
    "original_title": "Couplformer:Rethinking Vision Transformer with Coupling Attention Map",
    "source": "arxiv",
    "authors": [
      "Hai Lan",
      "Xihao Wang",
      "Xian Wei"
    ],
    "published": "2021-12-10T10:05:35+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2112.05425v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2112.05425v1_chunk_1",
    "chunk_text": "A series of different scale image classification tasks are applied to evaluate the effectiveness of our model. The result of experiments shows that on the ImageNet-1k classification task, the Couplformer can significantly decrease 28% memory consumption compared with regular Transformer while accessing sufficient accuracy requirements and outperforming 0.92% on Top-1 accuracy while occupying the same memory footprint. As a result, the Couplformer can serve as an efficient backbone in visual tasks, and provide a novel perspective on the attention mechanism for researchers.",
    "original_url": "http://arxiv.org/pdf/2112.05425v1",
    "original_title": "Couplformer:Rethinking Vision Transformer with Coupling Attention Map",
    "source": "arxiv",
    "authors": [
      "Hai Lan",
      "Xihao Wang",
      "Xian Wei"
    ],
    "published": "2021-12-10T10:05:35+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2112.05425v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2205.12231v1_chunk_0",
    "chunk_text": "ASSET: Autoregressive Semantic Scene Editing with Transformers at High Resolutions\n\nWe present ASSET, a neural architecture for automatically modifying an input high-resolution image according to a user's edits on its semantic segmentation map. Our architecture is based on a transformer with a novel attention mechanism. Our key idea is to sparsify the transformer's attention matrix at high resolutions, guided by dense attention extracted at lower image resolutions. While previous attention mechanisms are computationally too expensive for handling high-resolution images or are overly constrained within specific image regions hampering long-range interactions, our novel attention mechanism is both computationally efficient and effective. Our sparsified attention mechanism is able to capture long-range interactions and context, leading to synthesizing interesting phenomena in scenes, such as reflections of landscapes onto water or flora consistent with the rest of the landscape, that were not possible to generate reliably with previous convnets and transformer approaches.",
    "original_url": "http://arxiv.org/pdf/2205.12231v1",
    "original_title": "ASSET: Autoregressive Semantic Scene Editing with Transformers at High Resolutions",
    "source": "arxiv",
    "authors": [
      "Difan Liu",
      "Sandesh Shetty",
      "Tobias Hinz",
      "Matthew Fisher",
      "Richard Zhang",
      "Taesung Park",
      "Evangelos Kalogerakis"
    ],
    "published": "2022-05-24T17:39:53+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2205.12231v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2205.12231v1_chunk_1",
    "chunk_text": "Our sparsified attention mechanism is able to capture long-range interactions and context, leading to synthesizing interesting phenomena in scenes, such as reflections of landscapes onto water or flora consistent with the rest of the landscape, that were not possible to generate reliably with previous convnets and transformer approaches. We present qualitative and quantitative results, along with user studies, demonstrating the effectiveness of our method.",
    "original_url": "http://arxiv.org/pdf/2205.12231v1",
    "original_title": "ASSET: Autoregressive Semantic Scene Editing with Transformers at High Resolutions",
    "source": "arxiv",
    "authors": [
      "Difan Liu",
      "Sandesh Shetty",
      "Tobias Hinz",
      "Matthew Fisher",
      "Richard Zhang",
      "Taesung Park",
      "Evangelos Kalogerakis"
    ],
    "published": "2022-05-24T17:39:53+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2205.12231v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2309.06282v1_chunk_0",
    "chunk_text": "IBAFormer: Intra-batch Attention Transformer for Domain Generalized Semantic Segmentation\n\nDomain generalized semantic segmentation (DGSS) is a critical yet challenging task, where the model is trained only on source data without access to any target data. Despite the proposal of numerous DGSS strategies, the generalization capability remains limited in CNN architectures. Though some Transformer-based segmentation models show promising performance, they primarily focus on capturing intra-sample attentive relationships, disregarding inter-sample correlations which can potentially benefit DGSS. To this end, we enhance the attention modules in Transformer networks for improving DGSS by incorporating information from other independent samples in the same batch, enriching contextual information, and diversifying the training data for each attention block. Specifically, we propose two alternative intra-batch attention mechanisms, namely mean-based intra-batch attention (MIBA) and element-wise intra-batch attention (EIBA), to capture correlations between different samples, enhancing feature representation and generalization capabilities.",
    "original_url": "http://arxiv.org/pdf/2309.06282v1",
    "original_title": "IBAFormer: Intra-batch Attention Transformer for Domain Generalized Semantic Segmentation",
    "source": "arxiv",
    "authors": [
      "Qiyu Sun",
      "Huilin Chen",
      "Meng Zheng",
      "Ziyan Wu",
      "Michael Felsberg",
      "Yang Tang"
    ],
    "published": "2023-09-12T14:42:22+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2309.06282v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2309.06282v1_chunk_1",
    "chunk_text": "Specifically, we propose two alternative intra-batch attention mechanisms, namely mean-based intra-batch attention (MIBA) and element-wise intra-batch attention (EIBA), to capture correlations between different samples, enhancing feature representation and generalization capabilities. Building upon intra-batch attention, we introduce IBAFormer, which integrates self-attention modules with the proposed intra-batch attention for DGSS. Extensive experiments demonstrate that IBAFormer achieves SOTA performance in DGSS, and ablation studies further confirm the effectiveness of each introduced component.",
    "original_url": "http://arxiv.org/pdf/2309.06282v1",
    "original_title": "IBAFormer: Intra-batch Attention Transformer for Domain Generalized Semantic Segmentation",
    "source": "arxiv",
    "authors": [
      "Qiyu Sun",
      "Huilin Chen",
      "Meng Zheng",
      "Ziyan Wu",
      "Michael Felsberg",
      "Yang Tang"
    ],
    "published": "2023-09-12T14:42:22+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2309.06282v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2212.13504v3_chunk_0",
    "chunk_text": "DAE-Former: Dual Attention-guided Efficient Transformer for Medical Image Segmentation\n\nTransformers have recently gained attention in the computer vision domain due to their ability to model long-range dependencies. However, the self-attention mechanism, which is the core part of the Transformer model, usually suffers from quadratic computational complexity with respect to the number of tokens. Many architectures attempt to reduce model complexity by limiting the self-attention mechanism to local regions or by redesigning the tokenization process. In this paper, we propose DAE-Former, a novel method that seeks to provide an alternative perspective by efficiently designing the self-attention mechanism. More specifically, we reformulate the self-attention mechanism to capture both spatial and channel relations across the whole feature dimension while staying computationally efficient.",
    "original_url": "http://arxiv.org/pdf/2212.13504v3",
    "original_title": "DAE-Former: Dual Attention-guided Efficient Transformer for Medical Image Segmentation",
    "source": "arxiv",
    "authors": [
      "Reza Azad",
      "René Arimond",
      "Ehsan Khodapanah Aghdam",
      "Amirhossein Kazerouni",
      "Dorit Merhof"
    ],
    "published": "2022-12-27T14:39:39+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2212.13504v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2212.13504v3_chunk_1",
    "chunk_text": "More specifically, we reformulate the self-attention mechanism to capture both spatial and channel relations across the whole feature dimension while staying computationally efficient. Furthermore, we redesign the skip connection path by including the cross-attention module to ensure the feature reusability and enhance the localization power. Our method outperforms state-of-the-art methods on multi-organ cardiac and skin lesion segmentation datasets without requiring pre-training weights. The code is publicly available at https://github.com/mindflow-institue/DAEFormer.",
    "original_url": "http://arxiv.org/pdf/2212.13504v3",
    "original_title": "DAE-Former: Dual Attention-guided Efficient Transformer for Medical Image Segmentation",
    "source": "arxiv",
    "authors": [
      "Reza Azad",
      "René Arimond",
      "Ehsan Khodapanah Aghdam",
      "Amirhossein Kazerouni",
      "Dorit Merhof"
    ],
    "published": "2022-12-27T14:39:39+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2212.13504v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2112.13310v1_chunk_0",
    "chunk_text": "Miti-DETR: Object Detection based on Transformers with Mitigatory Self-Attention Convergence\n\nObject Detection with Transformers (DETR) and related works reach or even surpass the highly-optimized Faster-RCNN baseline with self-attention network architectures. Inspired by the evidence that pure self-attention possesses a strong inductive bias that leads to the transformer losing the expressive power with respect to network depth, we propose a transformer architecture with a mitigatory self-attention mechanism by applying possible direct mapping connections in the transformer architecture to mitigate the rank collapse so as to counteract feature expression loss and enhance the model performance. We apply this proposal in object detection tasks and develop a model named Miti-DETR. Miti-DETR reserves the inputs of each single attention layer to the outputs of that layer so that the \"non-attention\" information has participated in any attention propagation. The formed residual self-attention network addresses two critical issues: (1) stop the self-attention networks from degenerating to rank-1 to the maximized degree; and (2) further diversify the path distribution of parameter update so that easier attention learning is expected.",
    "original_url": "http://arxiv.org/pdf/2112.13310v1",
    "original_title": "Miti-DETR: Object Detection based on Transformers with Mitigatory Self-Attention Convergence",
    "source": "arxiv",
    "authors": [
      "Wenchi Ma",
      "Tianxiao Zhang",
      "Guanghui Wang"
    ],
    "published": "2021-12-26T03:23:59+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2112.13310v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2112.13310v1_chunk_1",
    "chunk_text": "The formed residual self-attention network addresses two critical issues: (1) stop the self-attention networks from degenerating to rank-1 to the maximized degree; and (2) further diversify the path distribution of parameter update so that easier attention learning is expected. Miti-DETR significantly enhances the average detection precision and convergence speed towards existing DETR-based models on the challenging COCO object detection dataset. Moreover, the proposed transformer with the residual self-attention network can be easily generalized or plugged in other related task models without specific customization.",
    "original_url": "http://arxiv.org/pdf/2112.13310v1",
    "original_title": "Miti-DETR: Object Detection based on Transformers with Mitigatory Self-Attention Convergence",
    "source": "arxiv",
    "authors": [
      "Wenchi Ma",
      "Tianxiao Zhang",
      "Guanghui Wang"
    ],
    "published": "2021-12-26T03:23:59+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2112.13310v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2404.04617v2_chunk_0",
    "chunk_text": "Empowering Image Recovery_ A Multi-Attention Approach\n\nWe propose Diverse Restormer (DART), a novel image restoration method that effectively integrates information from various sources (long sequences, local and global regions, feature dimensions, and positional dimensions) to address restoration challenges. While Transformer models have demonstrated excellent performance in image restoration due to their self-attention mechanism, they face limitations in complex scenarios. Leveraging recent advancements in Transformers and various attention mechanisms, our method utilizes customized attention mechanisms to enhance overall performance. DART, our novel network architecture, employs windowed attention to mimic the selective focusing mechanism of human eyes. By dynamically adjusting receptive fields, it optimally captures the fundamental features crucial for image resolution reconstruction.",
    "original_url": "http://arxiv.org/pdf/2404.04617v2",
    "original_title": "Empowering Image Recovery_ A Multi-Attention Approach",
    "source": "arxiv",
    "authors": [
      "Juan Wen",
      "Yawei Li",
      "Chao Zhang",
      "Weiyan Hou",
      "Radu Timofte",
      "Luc Van Gool"
    ],
    "published": "2024-04-06T12:50:08+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2404.04617v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2404.04617v2_chunk_1",
    "chunk_text": "By dynamically adjusting receptive fields, it optimally captures the fundamental features crucial for image resolution reconstruction. Efficiency and performance balance are achieved through the LongIR attention mechanism for long sequence image restoration. Integration of attention mechanisms across feature and positional dimensions further enhances the recovery of fine details. Evaluation across five restoration tasks consistently positions DART at the forefront. Upon acceptance, we commit to providing publicly accessible code and models to ensure reproducibility and facilitate further research.",
    "original_url": "http://arxiv.org/pdf/2404.04617v2",
    "original_title": "Empowering Image Recovery_ A Multi-Attention Approach",
    "source": "arxiv",
    "authors": [
      "Juan Wen",
      "Yawei Li",
      "Chao Zhang",
      "Weiyan Hou",
      "Radu Timofte",
      "Luc Van Gool"
    ],
    "published": "2024-04-06T12:50:08+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2404.04617v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2404.04617v2_chunk_2",
    "chunk_text": "Upon acceptance, we commit to providing publicly accessible code and models to ensure reproducibility and facilitate further research.",
    "original_url": "http://arxiv.org/pdf/2404.04617v2",
    "original_title": "Empowering Image Recovery_ A Multi-Attention Approach",
    "source": "arxiv",
    "authors": [
      "Juan Wen",
      "Yawei Li",
      "Chao Zhang",
      "Weiyan Hou",
      "Radu Timofte",
      "Luc Van Gool"
    ],
    "published": "2024-04-06T12:50:08+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2404.04617v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2206.11990v2_chunk_0",
    "chunk_text": "Equiformer: Equivariant Graph Attention Transformer for 3D Atomistic Graphs\n\nDespite their widespread success in various domains, Transformer networks have yet to perform well across datasets in the domain of 3D atomistic graphs such as molecules even when 3D-related inductive biases like translational invariance and rotational equivariance are considered. In this paper, we demonstrate that Transformers can generalize well to 3D atomistic graphs and present Equiformer, a graph neural network leveraging the strength of Transformer architectures and incorporating SE(3)/E(3)-equivariant features based on irreducible representations (irreps). First, we propose a simple and effective architecture by only replacing original operations in Transformers with their equivariant counterparts and including tensor products. Using equivariant operations enables encoding equivariant information in channels of irreps features without complicating graph structures. With minimal modifications to Transformers, this architecture has already achieved strong empirical results.",
    "original_url": "http://arxiv.org/pdf/2206.11990v2",
    "original_title": "Equiformer: Equivariant Graph Attention Transformer for 3D Atomistic Graphs",
    "source": "arxiv",
    "authors": [
      "Yi-Lun Liao",
      "Tess Smidt"
    ],
    "published": "2022-06-23T21:40:37+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2206.11990v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2206.11990v2_chunk_1",
    "chunk_text": "With minimal modifications to Transformers, this architecture has already achieved strong empirical results. Second, we propose a novel attention mechanism called equivariant graph attention, which improves upon typical attention in Transformers through replacing dot product attention with multi-layer perceptron attention and including non-linear message passing. With these two innovations, Equiformer achieves competitive results to previous models on QM9, MD17 and OC20 datasets.",
    "original_url": "http://arxiv.org/pdf/2206.11990v2",
    "original_title": "Equiformer: Equivariant Graph Attention Transformer for 3D Atomistic Graphs",
    "source": "arxiv",
    "authors": [
      "Yi-Lun Liao",
      "Tess Smidt"
    ],
    "published": "2022-06-23T21:40:37+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2206.11990v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2501.17486v1_chunk_0",
    "chunk_text": "DINT Transformer\n\nDIFF Transformer addresses the issue of irrelevant context interference by introducing a differential attention mechanism that enhances the robustness of local attention. However, it has two critical limitations: the lack of global context modeling, which is essential for identifying globally significant tokens, and numerical instability due to the absence of strict row normalization in the attention matrix. To overcome these challenges, we propose DINT Transformer, which extends DIFF Transformer by incorporating a differential-integral mechanism. By computing global importance scores and integrating them into the attention matrix, DINT Transformer improves its ability to capture global dependencies. Moreover, the unified parameter design enforces row-normalized attention matrices, improving numerical stability.",
    "original_url": "http://arxiv.org/pdf/2501.17486v1",
    "original_title": "DINT Transformer",
    "source": "arxiv",
    "authors": [
      "Yueyang Cang",
      "Yuhang Liu",
      "Xiaoteng Zhang",
      "Erlu Zhao",
      "Li Shi"
    ],
    "published": "2025-01-29T08:53:29+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2501.17486v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2501.17486v1_chunk_1",
    "chunk_text": "Moreover, the unified parameter design enforces row-normalized attention matrices, improving numerical stability. Experimental results demonstrate that DINT Transformer excels in accuracy and robustness across various practical applications, such as long-context language modeling and key information retrieval. These results position DINT Transformer as a highly effective and promising architecture.",
    "original_url": "http://arxiv.org/pdf/2501.17486v1",
    "original_title": "DINT Transformer",
    "source": "arxiv",
    "authors": [
      "Yueyang Cang",
      "Yuhang Liu",
      "Xiaoteng Zhang",
      "Erlu Zhao",
      "Li Shi"
    ],
    "published": "2025-01-29T08:53:29+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2501.17486v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2311.13541v4_chunk_0",
    "chunk_text": "Linear Log-Normal Attention with Unbiased Concentration\n\nTransformer models have achieved remarkable results in a wide range of applications. However, their scalability is hampered by the quadratic time and memory complexity of the self-attention mechanism concerning the sequence length. This limitation poses a substantial obstacle when dealing with long documents or high-resolution images. In this work, we study the self-attention mechanism by analyzing the distribution of the attention matrix and its concentration ability. Furthermore, we propose instruments to measure these quantities and introduce a novel self-attention mechanism, Linear Log-Normal Attention, designed to emulate the distribution and concentration behavior of the original self-attention.",
    "original_url": "http://arxiv.org/pdf/2311.13541v4",
    "original_title": "Linear Log-Normal Attention with Unbiased Concentration",
    "source": "arxiv",
    "authors": [
      "Yury Nahshan",
      "Joseph Kampeas",
      "Emir Haleva"
    ],
    "published": "2023-11-22T17:30:41+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2311.13541v4"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2311.13541v4_chunk_1",
    "chunk_text": "Furthermore, we propose instruments to measure these quantities and introduce a novel self-attention mechanism, Linear Log-Normal Attention, designed to emulate the distribution and concentration behavior of the original self-attention. Our experimental results on popular natural language benchmarks reveal that our proposed Linear Log-Normal Attention outperforms other linearized attention alternatives, offering a promising avenue for enhancing the scalability of transformer models.",
    "original_url": "http://arxiv.org/pdf/2311.13541v4",
    "original_title": "Linear Log-Normal Attention with Unbiased Concentration",
    "source": "arxiv",
    "authors": [
      "Yury Nahshan",
      "Joseph Kampeas",
      "Emir Haleva"
    ],
    "published": "2023-11-22T17:30:41+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2311.13541v4"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2009.14658v2_chunk_0",
    "chunk_text": "Learning Hard Retrieval Decoder Attention for Transformers\n\nThe Transformer translation model is based on the multi-head attention mechanism, which can be parallelized easily. The multi-head attention network performs the scaled dot-product attention function in parallel, empowering the model by jointly attending to information from different representation subspaces at different positions. In this paper, we present an approach to learning a hard retrieval attention where an attention head only attends to one token in the sentence rather than all tokens. The matrix multiplication between attention probabilities and the value sequence in the standard scaled dot-product attention can thus be replaced by a simple and efficient retrieval operation. We show that our hard retrieval attention mechanism is 1.43 times faster in decoding, while preserving translation quality on a wide range of machine translation tasks when used in the decoder self- and cross-attention networks.",
    "original_url": "http://arxiv.org/pdf/2009.14658v2",
    "original_title": "Learning Hard Retrieval Decoder Attention for Transformers",
    "source": "arxiv",
    "authors": [
      "Hongfei Xu",
      "Qiuhui Liu",
      "Josef van Genabith",
      "Deyi Xiong"
    ],
    "published": "2020-09-30T13:18:57+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2009.14658v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2009.14658v2_chunk_1",
    "chunk_text": "We show that our hard retrieval attention mechanism is 1.43 times faster in decoding, while preserving translation quality on a wide range of machine translation tasks when used in the decoder self- and cross-attention networks.",
    "original_url": "http://arxiv.org/pdf/2009.14658v2",
    "original_title": "Learning Hard Retrieval Decoder Attention for Transformers",
    "source": "arxiv",
    "authors": [
      "Hongfei Xu",
      "Qiuhui Liu",
      "Josef van Genabith",
      "Deyi Xiong"
    ],
    "published": "2020-09-30T13:18:57+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2009.14658v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2009.07053v1_chunk_0",
    "chunk_text": "Attention Flows: Analyzing and Comparing Attention Mechanisms in Language Models\n\nAdvances in language modeling have led to the development of deep attention-based models that are performant across a wide variety of natural language processing (NLP) problems. These language models are typified by a pre-training process on large unlabeled text corpora and subsequently fine-tuned for specific tasks. Although considerable work has been devoted to understanding the attention mechanisms of pre-trained models, it is less understood how a model's attention mechanisms change when trained for a target NLP task. In this paper, we propose a visual analytics approach to understanding fine-tuning in attention-based language models. Our visualization, Attention Flows, is designed to support users in querying, tracing, and comparing attention within layers, across layers, and amongst attention heads in Transformer-based language models.",
    "original_url": "http://arxiv.org/pdf/2009.07053v1",
    "original_title": "Attention Flows: Analyzing and Comparing Attention Mechanisms in Language Models",
    "source": "arxiv",
    "authors": [
      "Joseph F DeRose",
      "Jiayao Wang",
      "Matthew Berger"
    ],
    "published": "2020-09-03T19:56:30+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2009.07053v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2009.07053v1_chunk_1",
    "chunk_text": "Our visualization, Attention Flows, is designed to support users in querying, tracing, and comparing attention within layers, across layers, and amongst attention heads in Transformer-based language models. To help users gain insight on how a classification decision is made, our design is centered on depicting classification-based attention at the deepest layer and how attention from prior layers flows throughout words in the input. Attention Flows supports the analysis of a single model, as well as the visual comparison between pre-trained and fine-tuned models via their similarities and differences. We use Attention Flows to study attention mechanisms in various sentence understanding tasks and highlight how attention evolves to address the nuances of solving these tasks.",
    "original_url": "http://arxiv.org/pdf/2009.07053v1",
    "original_title": "Attention Flows: Analyzing and Comparing Attention Mechanisms in Language Models",
    "source": "arxiv",
    "authors": [
      "Joseph F DeRose",
      "Jiayao Wang",
      "Matthew Berger"
    ],
    "published": "2020-09-03T19:56:30+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2009.07053v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2204.07756v3_chunk_0",
    "chunk_text": "Visual Attention Methods in Deep Learning: An In-Depth Survey\n\nInspired by the human cognitive system, attention is a mechanism that imitates the human cognitive awareness about specific information, amplifying critical details to focus more on the essential aspects of data. Deep learning has employed attention to boost performance for many applications. Interestingly, the same attention design can suit processing different data modalities and can easily be incorporated into large networks. Furthermore, multiple complementary attention mechanisms can be incorporated into one network. Hence, attention techniques have become extremely attractive.",
    "original_url": "http://arxiv.org/pdf/2204.07756v3",
    "original_title": "Visual Attention Methods in Deep Learning: An In-Depth Survey",
    "source": "arxiv",
    "authors": [
      "Mohammed Hassanin",
      "Saeed Anwar",
      "Ibrahim Radwan",
      "Fahad S Khan",
      "Ajmal Mian"
    ],
    "published": "2022-04-16T08:57:00+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2204.07756v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2204.07756v3_chunk_1",
    "chunk_text": "Hence, attention techniques have become extremely attractive. However, the literature lacks a comprehensive survey on attention techniques to guide researchers in employing attention in their deep models. Note that, besides being demanding in terms of training data and computational resources, transformers only cover a single category in self-attention out of the many categories available. We fill this gap and provide an in-depth survey of 50 attention techniques, categorizing them by their most prominent features. We initiate our discussion by introducing the fundamental concepts behind the success of the attention mechanism.",
    "original_url": "http://arxiv.org/pdf/2204.07756v3",
    "original_title": "Visual Attention Methods in Deep Learning: An In-Depth Survey",
    "source": "arxiv",
    "authors": [
      "Mohammed Hassanin",
      "Saeed Anwar",
      "Ibrahim Radwan",
      "Fahad S Khan",
      "Ajmal Mian"
    ],
    "published": "2022-04-16T08:57:00+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2204.07756v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2204.07756v3_chunk_2",
    "chunk_text": "We initiate our discussion by introducing the fundamental concepts behind the success of the attention mechanism. Next, we furnish some essentials such as the strengths and limitations of each attention category, describe their fundamental building blocks, basic formulations with primary usage, and applications specifically for computer vision. We also discuss the challenges and general open questions related to attention mechanisms. Finally, we recommend possible future research directions for deep attention. All the information about visual attention methods in deep learning is provided at \\href{https://github.com/saeed-anwar/VisualAttention}{https://github.com/saeed-anwar/VisualAttention}",
    "original_url": "http://arxiv.org/pdf/2204.07756v3",
    "original_title": "Visual Attention Methods in Deep Learning: An In-Depth Survey",
    "source": "arxiv",
    "authors": [
      "Mohammed Hassanin",
      "Saeed Anwar",
      "Ibrahim Radwan",
      "Fahad S Khan",
      "Ajmal Mian"
    ],
    "published": "2022-04-16T08:57:00+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2204.07756v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2204.07756v3_chunk_3",
    "chunk_text": "All the information about visual attention methods in deep learning is provided at \\href{https://github.com/saeed-anwar/VisualAttention}{https://github.com/saeed-anwar/VisualAttention}",
    "original_url": "http://arxiv.org/pdf/2204.07756v3",
    "original_title": "Visual Attention Methods in Deep Learning: An In-Depth Survey",
    "source": "arxiv",
    "authors": [
      "Mohammed Hassanin",
      "Saeed Anwar",
      "Ibrahim Radwan",
      "Fahad S Khan",
      "Ajmal Mian"
    ],
    "published": "2022-04-16T08:57:00+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2204.07756v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2412.17810v1_chunk_0",
    "chunk_text": "Token Statistics Transformer: Linear-Time Attention via Variational Rate Reduction\n\nThe attention operator is arguably the key distinguishing factor of transformer architectures, which have demonstrated state-of-the-art performance on a variety of tasks. However, transformer attention operators often impose a significant computational burden, with the computational complexity scaling quadratically with the number of tokens. In this work, we propose a novel transformer attention operator whose computational complexity scales linearly with the number of tokens. We derive our network architecture by extending prior work which has shown that a transformer style architecture naturally arises by \"white-box\" architecture design, where each layer of the network is designed to implement an incremental optimization step of a maximal coding rate reduction objective (MCR$^2$). Specifically, we derive a novel variational form of the MCR$^2$ objective and show that the architecture that results from unrolled gradient descent of this variational objective leads to a new attention module called Token Statistics Self-Attention (TSSA).",
    "original_url": "http://arxiv.org/pdf/2412.17810v1",
    "original_title": "Token Statistics Transformer: Linear-Time Attention via Variational Rate Reduction",
    "source": "arxiv",
    "authors": [
      "Ziyang Wu",
      "Tianjiao Ding",
      "Yifu Lu",
      "Druv Pai",
      "Jingyuan Zhang",
      "Weida Wang",
      "Yaodong Yu",
      "Yi Ma",
      "Benjamin D. Haeffele"
    ],
    "published": "2024-12-23T18:59:21+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2412.17810v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2412.17810v1_chunk_1",
    "chunk_text": "Specifically, we derive a novel variational form of the MCR$^2$ objective and show that the architecture that results from unrolled gradient descent of this variational objective leads to a new attention module called Token Statistics Self-Attention (TSSA). TSSA has linear computational and memory complexity and radically departs from the typical attention architecture that computes pairwise similarities between tokens. Experiments on vision, language, and long sequence tasks show that simply swapping TSSA for standard self-attention, which we refer to as the Token Statistics Transformer (ToST), achieves competitive performance with conventional transformers while being significantly more computationally efficient and interpretable. Our results also somewhat call into question the conventional wisdom that pairwise similarity style attention mechanisms are critical to the success of transformer architectures. Code will be available at https://github.com/RobinWu218/ToST.",
    "original_url": "http://arxiv.org/pdf/2412.17810v1",
    "original_title": "Token Statistics Transformer: Linear-Time Attention via Variational Rate Reduction",
    "source": "arxiv",
    "authors": [
      "Ziyang Wu",
      "Tianjiao Ding",
      "Yifu Lu",
      "Druv Pai",
      "Jingyuan Zhang",
      "Weida Wang",
      "Yaodong Yu",
      "Yi Ma",
      "Benjamin D. Haeffele"
    ],
    "published": "2024-12-23T18:59:21+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2412.17810v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2412.17810v1_chunk_2",
    "chunk_text": "Code will be available at https://github.com/RobinWu218/ToST.",
    "original_url": "http://arxiv.org/pdf/2412.17810v1",
    "original_title": "Token Statistics Transformer: Linear-Time Attention via Variational Rate Reduction",
    "source": "arxiv",
    "authors": [
      "Ziyang Wu",
      "Tianjiao Ding",
      "Yifu Lu",
      "Druv Pai",
      "Jingyuan Zhang",
      "Weida Wang",
      "Yaodong Yu",
      "Yi Ma",
      "Benjamin D. Haeffele"
    ],
    "published": "2024-12-23T18:59:21+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2412.17810v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2410.11842v1_chunk_0",
    "chunk_text": "MoH: Multi-Head Attention as Mixture-of-Head Attention\n\nIn this work, we upgrade the multi-head attention mechanism, the core of the Transformer model, to improve efficiency while maintaining or surpassing the previous accuracy level. We show that multi-head attention can be expressed in the summation form. Drawing on the insight that not all attention heads hold equal significance, we propose Mixture-of-Head attention (MoH), a new architecture that treats attention heads as experts in the Mixture-of-Experts (MoE) mechanism. MoH has two significant advantages: First, MoH enables each token to select the appropriate attention heads, enhancing inference efficiency without compromising accuracy or increasing the number of parameters. Second, MoH replaces the standard summation in multi-head attention with a weighted summation, introducing flexibility to the attention mechanism and unlocking extra performance potential.",
    "original_url": "http://arxiv.org/pdf/2410.11842v1",
    "original_title": "MoH: Multi-Head Attention as Mixture-of-Head Attention",
    "source": "arxiv",
    "authors": [
      "Peng Jin",
      "Bo Zhu",
      "Li Yuan",
      "Shuicheng Yan"
    ],
    "published": "2024-10-15T17:59:44+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2410.11842v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2410.11842v1_chunk_1",
    "chunk_text": "Second, MoH replaces the standard summation in multi-head attention with a weighted summation, introducing flexibility to the attention mechanism and unlocking extra performance potential. Extensive experiments on ViT, DiT, and LLMs demonstrate that MoH outperforms multi-head attention by using only 50%-90% of the attention heads. Moreover, we demonstrate that pre-trained multi-head attention models, such as LLaMA3-8B, can be further continue-tuned into our MoH models. Notably, MoH-LLaMA3-8B achieves an average accuracy of 64.0% across 14 benchmarks, outperforming LLaMA3-8B by 2.4% by utilizing only 75% of the attention heads. We believe the proposed MoH is a promising alternative to multi-head attention and provides a strong foundation for developing advanced and efficient attention-based models.",
    "original_url": "http://arxiv.org/pdf/2410.11842v1",
    "original_title": "MoH: Multi-Head Attention as Mixture-of-Head Attention",
    "source": "arxiv",
    "authors": [
      "Peng Jin",
      "Bo Zhu",
      "Li Yuan",
      "Shuicheng Yan"
    ],
    "published": "2024-10-15T17:59:44+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2410.11842v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2410.11842v1_chunk_2",
    "chunk_text": "We believe the proposed MoH is a promising alternative to multi-head attention and provides a strong foundation for developing advanced and efficient attention-based models.",
    "original_url": "http://arxiv.org/pdf/2410.11842v1",
    "original_title": "MoH: Multi-Head Attention as Mixture-of-Head Attention",
    "source": "arxiv",
    "authors": [
      "Peng Jin",
      "Bo Zhu",
      "Li Yuan",
      "Shuicheng Yan"
    ],
    "published": "2024-10-15T17:59:44+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2410.11842v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2412.07049v1_chunk_0",
    "chunk_text": "Static Key Attention in Vision\n\nThe success of vision transformers is widely attributed to the expressive power of their dynamically parameterized multi-head self-attention mechanism. We examine the impact of substituting the dynamic parameterized key with a static key within the standard attention mechanism in Vision Transformers. Our findings reveal that static key attention mechanisms can match or even exceed the performance of standard self-attention. Integrating static key attention modules into a Metaformer backbone, we find that it serves as a better intermediate stage in hierarchical hybrid architectures, balancing the strengths of depth-wise convolution and self-attention. Experiments on several vision tasks underscore the effectiveness of the static key mechanism, indicating that the typical two-step dynamic parameterization in attention can be streamlined to a single step without impacting performance under certain circumstances.",
    "original_url": "http://arxiv.org/pdf/2412.07049v1",
    "original_title": "Static Key Attention in Vision",
    "source": "arxiv",
    "authors": [
      "Zizhao Hu",
      "Xiaolin Zhou",
      "Mohammad Rostami"
    ],
    "published": "2024-12-09T23:18:09+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2412.07049v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2412.07049v1_chunk_1",
    "chunk_text": "Experiments on several vision tasks underscore the effectiveness of the static key mechanism, indicating that the typical two-step dynamic parameterization in attention can be streamlined to a single step without impacting performance under certain circumstances.",
    "original_url": "http://arxiv.org/pdf/2412.07049v1",
    "original_title": "Static Key Attention in Vision",
    "source": "arxiv",
    "authors": [
      "Zizhao Hu",
      "Xiaolin Zhou",
      "Mohammad Rostami"
    ],
    "published": "2024-12-09T23:18:09+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2412.07049v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2404.14996v1_chunk_0",
    "chunk_text": "CA-Stream: Attention-based pooling for interpretable image recognition\n\nExplanations obtained from transformer-based architectures in the form of raw attention, can be seen as a class-agnostic saliency map. Additionally, attention-based pooling serves as a form of masking the in feature space. Motivated by this observation, we design an attention-based pooling mechanism intended to replace Global Average Pooling (GAP) at inference. This mechanism, called Cross-Attention Stream (CA-Stream), comprises a stream of cross attention blocks interacting with features at different network depths. CA-Stream enhances interpretability in models, while preserving recognition performance.",
    "original_url": "http://arxiv.org/pdf/2404.14996v1",
    "original_title": "CA-Stream: Attention-based pooling for interpretable image recognition",
    "source": "arxiv",
    "authors": [
      "Felipe Torres",
      "Hanwei Zhang",
      "Ronan Sicre",
      "Stéphane Ayache",
      "Yannis Avrithis"
    ],
    "published": "2024-04-23T12:57:35+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2404.14996v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2404.14996v1_chunk_1",
    "chunk_text": "CA-Stream enhances interpretability in models, while preserving recognition performance.",
    "original_url": "http://arxiv.org/pdf/2404.14996v1",
    "original_title": "CA-Stream: Attention-based pooling for interpretable image recognition",
    "source": "arxiv",
    "authors": [
      "Felipe Torres",
      "Hanwei Zhang",
      "Ronan Sicre",
      "Stéphane Ayache",
      "Yannis Avrithis"
    ],
    "published": "2024-04-23T12:57:35+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2404.14996v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2410.18613v1_chunk_0",
    "chunk_text": "Rethinking Softmax: Self-Attention with Polynomial Activations\n\nThis paper challenges the conventional belief that softmax attention in transformers is effective primarily because it generates a probability distribution for attention allocation. Instead, we theoretically show that its success lies in its ability to implicitly regularize the Frobenius norm of the attention matrix during training. We then explore alternative activations that regularize the Frobenius norm of the attention matrix, demonstrating that certain polynomial activations can achieve this effect, making them suitable for attention-based architectures. Empirical results indicate these activations perform comparably or better than softmax across various computer vision and language tasks, suggesting new possibilities for attention mechanisms beyond softmax.",
    "original_url": "http://arxiv.org/pdf/2410.18613v1",
    "original_title": "Rethinking Softmax: Self-Attention with Polynomial Activations",
    "source": "arxiv",
    "authors": [
      "Hemanth Saratchandran",
      "Jianqiao Zheng",
      "Yiping Ji",
      "Wenbo Zhang",
      "Simon Lucey"
    ],
    "published": "2024-10-24T10:08:25+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2410.18613v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2307.04128v1_chunk_0",
    "chunk_text": "Marine Debris Detection in Satellite Surveillance using Attention Mechanisms\n\nMarine debris is an important issue for environmental protection, but current methods for locating marine debris are yet limited. In order to achieve higher efficiency and wider applicability in the localization of Marine debris, this study tries to combine the instance segmentation of YOLOv7 with different attention mechanisms and explores the best model. By utilizing a labelled dataset consisting of satellite images containing ocean debris, we examined three attentional models including lightweight coordinate attention, CBAM (combining spatial and channel focus), and bottleneck transformer (based on self-attention). Box detection assessment revealed that CBAM achieved the best outcome (F1 score of 77%) compared to coordinate attention (F1 score of 71%) and YOLOv7/bottleneck transformer (both F1 scores around 66%). Mask evaluation showed CBAM again leading with an F1 score of 73%, whereas coordinate attention and YOLOv7 had comparable performances (around F1 score of 68%/69%) and bottleneck transformer lagged behind at F1 score of 56%.",
    "original_url": "http://arxiv.org/pdf/2307.04128v1",
    "original_title": "Marine Debris Detection in Satellite Surveillance using Attention Mechanisms",
    "source": "arxiv",
    "authors": [
      "Ao Shen",
      "Yijie Zhu",
      "Richard Jiang"
    ],
    "published": "2023-07-09T08:53:45+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2307.04128v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2307.04128v1_chunk_1",
    "chunk_text": "Mask evaluation showed CBAM again leading with an F1 score of 73%, whereas coordinate attention and YOLOv7 had comparable performances (around F1 score of 68%/69%) and bottleneck transformer lagged behind at F1 score of 56%. These findings suggest that CBAM offers optimal suitability for detecting marine debris. However, it should be noted that the bottleneck transformer detected some areas missed by manual annotation and displayed better mask precision for larger debris pieces, signifying potentially superior practical performance.",
    "original_url": "http://arxiv.org/pdf/2307.04128v1",
    "original_title": "Marine Debris Detection in Satellite Surveillance using Attention Mechanisms",
    "source": "arxiv",
    "authors": [
      "Ao Shen",
      "Yijie Zhu",
      "Richard Jiang"
    ],
    "published": "2023-07-09T08:53:45+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2307.04128v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2403.10336v1_chunk_0",
    "chunk_text": "How Powerful Potential of Attention on Image Restoration? Transformers have demonstrated their effectiveness in image restoration tasks. Existing Transformer architectures typically comprise two essential components: multi-head self-attention and feed-forward network (FFN). The former captures long-range pixel dependencies, while the latter enables the model to learn complex patterns and relationships in the data. Previous studies have demonstrated that FFNs are key-value memories \\cite{geva2020transformer}, which are vital in modern Transformer architectures.",
    "original_url": "http://arxiv.org/pdf/2403.10336v1",
    "original_title": "How Powerful Potential of Attention on Image Restoration?",
    "source": "arxiv",
    "authors": [
      "Cong Wang",
      "Jinshan Pan",
      "Yeying Jin",
      "Liyan Wang",
      "Wei Wang",
      "Gang Fu",
      "Wenqi Ren",
      "Xiaochun Cao"
    ],
    "published": "2024-03-15T14:23:12+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2403.10336v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2403.10336v1_chunk_1",
    "chunk_text": "Previous studies have demonstrated that FFNs are key-value memories \\cite{geva2020transformer}, which are vital in modern Transformer architectures. In this paper, we conduct an empirical study to explore the potential of attention mechanisms without using FFN and provide novel structures to demonstrate that removing FFN is flexible for image restoration. Specifically, we propose Continuous Scaling Attention (\\textbf{CSAttn}), a method that computes attention continuously in three stages without using FFN. To achieve competitive performance, we propose a series of key components within the attention. Our designs provide a closer look at the attention mechanism and reveal that some simple operations can significantly affect the model performance.",
    "original_url": "http://arxiv.org/pdf/2403.10336v1",
    "original_title": "How Powerful Potential of Attention on Image Restoration?",
    "source": "arxiv",
    "authors": [
      "Cong Wang",
      "Jinshan Pan",
      "Yeying Jin",
      "Liyan Wang",
      "Wei Wang",
      "Gang Fu",
      "Wenqi Ren",
      "Xiaochun Cao"
    ],
    "published": "2024-03-15T14:23:12+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2403.10336v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2403.10336v1_chunk_2",
    "chunk_text": "Our designs provide a closer look at the attention mechanism and reveal that some simple operations can significantly affect the model performance. We apply our \\textbf{CSAttn} to several image restoration tasks and show that our model can outperform CNN-based and Transformer-based image restoration approaches.",
    "original_url": "http://arxiv.org/pdf/2403.10336v1",
    "original_title": "How Powerful Potential of Attention on Image Restoration?",
    "source": "arxiv",
    "authors": [
      "Cong Wang",
      "Jinshan Pan",
      "Yeying Jin",
      "Liyan Wang",
      "Wei Wang",
      "Gang Fu",
      "Wenqi Ren",
      "Xiaochun Cao"
    ],
    "published": "2024-03-15T14:23:12+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2403.10336v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2411.11214v1_chunk_0",
    "chunk_text": "DeforHMR: Vision Transformer with Deformable Cross-Attention for 3D Human Mesh Recovery\n\nHuman Mesh Recovery (HMR) is an important yet challenging problem with applications across various domains including motion capture, augmented reality, and biomechanics. Accurately predicting human pose parameters from a single image remains a challenging 3D computer vision task. In this work, we introduce DeforHMR, a novel regression-based monocular HMR framework designed to enhance the prediction of human pose parameters using deformable attention transformers. DeforHMR leverages a novel query-agnostic deformable cross-attention mechanism within the transformer decoder to effectively regress the visual features extracted from a frozen pretrained vision transformer (ViT) encoder. The proposed deformable cross-attention mechanism allows the model to attend to relevant spatial features more flexibly and in a data-dependent manner.",
    "original_url": "http://arxiv.org/pdf/2411.11214v1",
    "original_title": "DeforHMR: Vision Transformer with Deformable Cross-Attention for 3D Human Mesh Recovery",
    "source": "arxiv",
    "authors": [
      "Jaewoo Heo",
      "George Hu",
      "Zeyu Wang",
      "Serena Yeung-Levy"
    ],
    "published": "2024-11-18T00:46:59+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2411.11214v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2411.11214v1_chunk_1",
    "chunk_text": "The proposed deformable cross-attention mechanism allows the model to attend to relevant spatial features more flexibly and in a data-dependent manner. Equipped with a transformer decoder capable of spatially-nuanced attention, DeforHMR achieves state-of-the-art performance for single-frame regression-based methods on the widely used 3D HMR benchmarks 3DPW and RICH. By pushing the boundary on the field of 3D human mesh recovery through deformable attention, we introduce an new, effective paradigm for decoding local spatial information from large pretrained vision encoders in computer vision.",
    "original_url": "http://arxiv.org/pdf/2411.11214v1",
    "original_title": "DeforHMR: Vision Transformer with Deformable Cross-Attention for 3D Human Mesh Recovery",
    "source": "arxiv",
    "authors": [
      "Jaewoo Heo",
      "George Hu",
      "Zeyu Wang",
      "Serena Yeung-Levy"
    ],
    "published": "2024-11-18T00:46:59+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2411.11214v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2503.02841v2_chunk_0",
    "chunk_text": "Boltzmann Attention Sampling for Image Analysis with Small Objects\n\nDetecting and segmenting small objects, such as lung nodules and tumor lesions, remains a critical challenge in image analysis. These objects often occupy less than 0.1% of an image, making traditional transformer architectures inefficient and prone to performance degradation due to redundant attention computations on irrelevant regions. Existing sparse attention mechanisms rely on rigid hierarchical structures, which are poorly suited for detecting small, variable, and uncertain object locations. In this paper, we propose BoltzFormer, a novel transformer-based architecture designed to address these challenges through dynamic sparse attention. BoltzFormer identifies and focuses attention on relevant areas by modeling uncertainty using a Boltzmann distribution with an annealing schedule.",
    "original_url": "http://arxiv.org/pdf/2503.02841v2",
    "original_title": "Boltzmann Attention Sampling for Image Analysis with Small Objects",
    "source": "arxiv",
    "authors": [
      "Theodore Zhao",
      "Sid Kiblawi",
      "Naoto Usuyama",
      "Ho Hin Lee",
      "Sam Preston",
      "Hoifung Poon",
      "Mu Wei"
    ],
    "published": "2025-03-04T18:12:58+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2503.02841v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2503.02841v2_chunk_1",
    "chunk_text": "BoltzFormer identifies and focuses attention on relevant areas by modeling uncertainty using a Boltzmann distribution with an annealing schedule. Initially, a higher temperature allows broader area sampling in early layers, when object location uncertainty is greatest. As the temperature decreases in later layers, attention becomes more focused, enhancing efficiency and accuracy. BoltzFormer seamlessly integrates into existing transformer architectures via a modular Boltzmann attention sampling mechanism. Comprehensive evaluations on benchmark datasets demonstrate that BoltzFormer significantly improves segmentation performance for small objects while reducing attention computation by an order of magnitude compared to previous state-of-the-art methods.",
    "original_url": "http://arxiv.org/pdf/2503.02841v2",
    "original_title": "Boltzmann Attention Sampling for Image Analysis with Small Objects",
    "source": "arxiv",
    "authors": [
      "Theodore Zhao",
      "Sid Kiblawi",
      "Naoto Usuyama",
      "Ho Hin Lee",
      "Sam Preston",
      "Hoifung Poon",
      "Mu Wei"
    ],
    "published": "2025-03-04T18:12:58+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2503.02841v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2503.02841v2_chunk_2",
    "chunk_text": "Comprehensive evaluations on benchmark datasets demonstrate that BoltzFormer significantly improves segmentation performance for small objects while reducing attention computation by an order of magnitude compared to previous state-of-the-art methods.",
    "original_url": "http://arxiv.org/pdf/2503.02841v2",
    "original_title": "Boltzmann Attention Sampling for Image Analysis with Small Objects",
    "source": "arxiv",
    "authors": [
      "Theodore Zhao",
      "Sid Kiblawi",
      "Naoto Usuyama",
      "Ho Hin Lee",
      "Sam Preston",
      "Hoifung Poon",
      "Mu Wei"
    ],
    "published": "2025-03-04T18:12:58+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2503.02841v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2403.14552v1_chunk_0",
    "chunk_text": "Token Transformation Matters: Towards Faithful Post-hoc Explanation for Vision Transformer\n\nWhile Transformers have rapidly gained popularity in various computer vision applications, post-hoc explanations of their internal mechanisms remain largely unexplored. Vision Transformers extract visual information by representing image regions as transformed tokens and integrating them via attention weights. However, existing post-hoc explanation methods merely consider these attention weights, neglecting crucial information from the transformed tokens, which fails to accurately illustrate the rationales behind the models' predictions. To incorporate the influence of token transformation into interpretation, we propose TokenTM, a novel post-hoc explanation method that utilizes our introduced measurement of token transformation effects. Specifically, we quantify token transformation effects by measuring changes in token lengths and correlations in their directions pre- and post-transformation.",
    "original_url": "http://arxiv.org/pdf/2403.14552v1",
    "original_title": "Token Transformation Matters: Towards Faithful Post-hoc Explanation for Vision Transformer",
    "source": "arxiv",
    "authors": [
      "Junyi Wu",
      "Bin Duan",
      "Weitai Kang",
      "Hao Tang",
      "Yan Yan"
    ],
    "published": "2024-03-21T16:52:27+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2403.14552v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2403.14552v1_chunk_1",
    "chunk_text": "Specifically, we quantify token transformation effects by measuring changes in token lengths and correlations in their directions pre- and post-transformation. Moreover, we develop initialization and aggregation rules to integrate both attention weights and token transformation effects across all layers, capturing holistic token contributions throughout the model. Experimental results on segmentation and perturbation tests demonstrate the superiority of our proposed TokenTM compared to state-of-the-art Vision Transformer explanation methods.",
    "original_url": "http://arxiv.org/pdf/2403.14552v1",
    "original_title": "Token Transformation Matters: Towards Faithful Post-hoc Explanation for Vision Transformer",
    "source": "arxiv",
    "authors": [
      "Junyi Wu",
      "Bin Duan",
      "Weitai Kang",
      "Hao Tang",
      "Yan Yan"
    ],
    "published": "2024-03-21T16:52:27+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2403.14552v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2407.02251v1_chunk_0",
    "chunk_text": "White-Box 3D-OMP-Transformer for ISAC\n\nTransformers have found broad applications for their great ability to capture long-range dependency among the inputs using attention mechanisms. The recent success of transformers increases the need for mathematical interpretation of their underlying working mechanisms, leading to the development of a family of white-box transformer-like deep network architectures. However, designing white-box transformers with efficient three-dimensional (3D) attention is still an open challenge. In this work, we revisit the 3D-orthogonal matching pursuit (OMP) algorithm and demonstrate that the operation of 3D-OMP is analogous to a specific kind of transformer with 3D attention. Therefore, we build a white-box 3D-OMP-transformer by introducing additional learnable parameters to 3D-OMP.",
    "original_url": "http://arxiv.org/pdf/2407.02251v1",
    "original_title": "White-Box 3D-OMP-Transformer for ISAC",
    "source": "arxiv",
    "authors": [
      "Bowen Zhang",
      "Geoffrey Ye Li"
    ],
    "published": "2024-07-02T13:17:00+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2407.02251v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2407.02251v1_chunk_1",
    "chunk_text": "Therefore, we build a white-box 3D-OMP-transformer by introducing additional learnable parameters to 3D-OMP. As a transformer, its 3D-attention can be mathematically interpreted from 3D-OMP; while as a variant of OMP, it can learn to improve the matching pursuit process from data. Besides, a transformer's performance can be improved by stacking more transformer blocks. To simulate this process, we design a cascaded 3D-OMP-Transformer with dynamic small-scale dictionaries, which can improve the performance of the 3D-OMP-Transformer with low costs. We evaluate the designed 3D-OMP-transformer in the multi-target detection task of integrated sensing and communications (ISAC).",
    "original_url": "http://arxiv.org/pdf/2407.02251v1",
    "original_title": "White-Box 3D-OMP-Transformer for ISAC",
    "source": "arxiv",
    "authors": [
      "Bowen Zhang",
      "Geoffrey Ye Li"
    ],
    "published": "2024-07-02T13:17:00+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2407.02251v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2407.02251v1_chunk_2",
    "chunk_text": "We evaluate the designed 3D-OMP-transformer in the multi-target detection task of integrated sensing and communications (ISAC). Experimental results show that the designed 3D-OMP-Transformer can outperform current baselines.",
    "original_url": "http://arxiv.org/pdf/2407.02251v1",
    "original_title": "White-Box 3D-OMP-Transformer for ISAC",
    "source": "arxiv",
    "authors": [
      "Bowen Zhang",
      "Geoffrey Ye Li"
    ],
    "published": "2024-07-02T13:17:00+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2407.02251v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2109.06446v1_chunk_0",
    "chunk_text": "Multi-modal Motion Prediction with Transformer-based Neural Network for Autonomous Driving\n\nPredicting the behaviors of other agents on the road is critical for autonomous driving to ensure safety and efficiency. However, the challenging part is how to represent the social interactions between agents and output different possible trajectories with interpretability. In this paper, we introduce a neural prediction framework based on the Transformer structure to model the relationship among the interacting agents and extract the attention of the target agent on the map waypoints. Specifically, we organize the interacting agents into a graph and utilize the multi-head attention Transformer encoder to extract the relations between them. To address the multi-modality of motion prediction, we propose a multi-modal attention Transformer encoder, which modifies the multi-head attention mechanism to multi-modal attention, and each predicted trajectory is conditioned on an independent attention mode.",
    "original_url": "http://arxiv.org/pdf/2109.06446v1",
    "original_title": "Multi-modal Motion Prediction with Transformer-based Neural Network for Autonomous Driving",
    "source": "arxiv",
    "authors": [
      "Zhiyu Huang",
      "Xiaoyu Mo",
      "Chen Lv"
    ],
    "published": "2021-09-14T05:16:47+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2109.06446v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2109.06446v1_chunk_1",
    "chunk_text": "To address the multi-modality of motion prediction, we propose a multi-modal attention Transformer encoder, which modifies the multi-head attention mechanism to multi-modal attention, and each predicted trajectory is conditioned on an independent attention mode. The proposed model is validated on the Argoverse motion forecasting dataset and shows state-of-the-art prediction accuracy while maintaining a small model size and a simple training process. We also demonstrate that the multi-modal attention module can automatically identify different modes of the target agent's attention on the map, which improves the interpretability of the model.",
    "original_url": "http://arxiv.org/pdf/2109.06446v1",
    "original_title": "Multi-modal Motion Prediction with Transformer-based Neural Network for Autonomous Driving",
    "source": "arxiv",
    "authors": [
      "Zhiyu Huang",
      "Xiaoyu Mo",
      "Chen Lv"
    ],
    "published": "2021-09-14T05:16:47+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2109.06446v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2106.04279v1_chunk_0",
    "chunk_text": "Staircase Attention for Recurrent Processing of Sequences\n\nAttention mechanisms have become a standard tool for sequence modeling tasks, in particular by stacking self-attention layers over the entire input sequence as in the Transformer architecture. In this work we introduce a novel attention procedure called staircase attention that, unlike self-attention, operates across the sequence (in time) recurrently processing the input by adding another step of processing. A step in the staircase comprises of backward tokens (encoding the sequence so far seen) and forward tokens (ingesting a new part of the sequence), or an extreme Ladder version with a forward step of zero that simply repeats the Transformer on each step of the ladder, sharing the weights. We thus describe a family of such models that can trade off performance and compute, by either increasing the amount of recurrence through time, the amount of sequential processing via recurrence in depth, or both. Staircase attention is shown to be able to solve tasks that involve tracking that conventional Transformers cannot, due to this recurrence.",
    "original_url": "http://arxiv.org/pdf/2106.04279v1",
    "original_title": "Staircase Attention for Recurrent Processing of Sequences",
    "source": "arxiv",
    "authors": [
      "Da Ju",
      "Stephen Roller",
      "Sainbayar Sukhbaatar",
      "Jason Weston"
    ],
    "published": "2021-06-08T12:19:31+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2106.04279v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2106.04279v1_chunk_1",
    "chunk_text": "Staircase attention is shown to be able to solve tasks that involve tracking that conventional Transformers cannot, due to this recurrence. Further, it is shown to provide improved modeling power for the same size model (number of parameters) compared to self-attentive Transformers on large language modeling and dialogue tasks, yielding significant perplexity gains.",
    "original_url": "http://arxiv.org/pdf/2106.04279v1",
    "original_title": "Staircase Attention for Recurrent Processing of Sequences",
    "source": "arxiv",
    "authors": [
      "Da Ju",
      "Stephen Roller",
      "Sainbayar Sukhbaatar",
      "Jason Weston"
    ],
    "published": "2021-06-08T12:19:31+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2106.04279v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2412.18040v1_chunk_0",
    "chunk_text": "Theoretical Constraints on the Expressive Power of $\\mathsf{RoPE}$-based Tensor Attention Transformers\n\nTensor Attention extends traditional attention mechanisms by capturing high-order correlations across multiple modalities, addressing the limitations of classical matrix-based attention. Meanwhile, Rotary Position Embedding ($\\mathsf{RoPE}$) has shown superior performance in encoding positional information in long-context scenarios, significantly enhancing transformer models' expressiveness. Despite these empirical successes, the theoretical limitations of these technologies remain underexplored. In this study, we analyze the circuit complexity of Tensor Attention and $\\mathsf{RoPE}$-based Tensor Attention, showing that with polynomial precision, constant-depth layers, and linear or sublinear hidden dimension, they cannot solve fixed membership problems or $(A_{F,r})^*$ closure problems, under the assumption that $\\mathsf{TC}^0 \\neq \\mathsf{NC}^1$. These findings highlight a gap between the empirical performance and theoretical constraints of Tensor Attention and $\\mathsf{RoPE}$-based Tensor Attention Transformers, offering insights that could guide the development of more theoretically grounded approaches to Transformer model design and scaling.",
    "original_url": "http://arxiv.org/pdf/2412.18040v1",
    "original_title": "Theoretical Constraints on the Expressive Power of $\\mathsf{RoPE}$-based Tensor Attention Transformers",
    "source": "arxiv",
    "authors": [
      "Xiaoyu Li",
      "Yingyu Liang",
      "Zhenmei Shi",
      "Zhao Song",
      "Mingda Wan"
    ],
    "published": "2024-12-23T23:26:07+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2412.18040v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2412.18040v1_chunk_1",
    "chunk_text": "These findings highlight a gap between the empirical performance and theoretical constraints of Tensor Attention and $\\mathsf{RoPE}$-based Tensor Attention Transformers, offering insights that could guide the development of more theoretically grounded approaches to Transformer model design and scaling.",
    "original_url": "http://arxiv.org/pdf/2412.18040v1",
    "original_title": "Theoretical Constraints on the Expressive Power of $\\mathsf{RoPE}$-based Tensor Attention Transformers",
    "source": "arxiv",
    "authors": [
      "Xiaoyu Li",
      "Yingyu Liang",
      "Zhenmei Shi",
      "Zhao Song",
      "Mingda Wan"
    ],
    "published": "2024-12-23T23:26:07+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2412.18040v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2405.15932v2_chunk_0",
    "chunk_text": "Steerable Transformers\n\nIn this work we introduce Steerable Transformers, an extension of the Vision Transformer mechanism that maintains equivariance to the special Euclidean group $\\mathrm{SE}(d)$. We propose an equivariant attention mechanism that operates on features extracted by steerable convolutions. Operating in Fourier space, our network utilizes Fourier space non-linearities. Our experiments in both two and three dimensions show that adding steerable transformer layers to steerable convolutional networks enhances performance.",
    "original_url": "http://arxiv.org/pdf/2405.15932v2",
    "original_title": "Steerable Transformers",
    "source": "arxiv",
    "authors": [
      "Soumyabrata Kundu",
      "Risi Kondor"
    ],
    "published": "2024-05-24T20:43:19+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2405.15932v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2011.04004v2_chunk_0",
    "chunk_text": "Stochastic Attention Head Removal: A simple and effective method for improving Transformer Based ASR Models\n\nRecently, Transformer based models have shown competitive automatic speech recognition (ASR) performance. One key factor in the success of these models is the multi-head attention mechanism. However, for trained models, we have previously observed that many attention matrices are close to diagonal, indicating the redundancy of the corresponding attention heads. We have also found that some architectures with reduced numbers of attention heads have better performance. Since the search for the best structure is time prohibitive, we propose to randomly remove attention heads during training and keep all attention heads at test time, thus the final model is an ensemble of models with different architectures.",
    "original_url": "http://arxiv.org/pdf/2011.04004v2",
    "original_title": "Stochastic Attention Head Removal: A simple and effective method for improving Transformer Based ASR Models",
    "source": "arxiv",
    "authors": [
      "Shucong Zhang",
      "Erfan Loweimi",
      "Peter Bell",
      "Steve Renals"
    ],
    "published": "2020-11-08T15:41:03+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2011.04004v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2011.04004v2_chunk_1",
    "chunk_text": "Since the search for the best structure is time prohibitive, we propose to randomly remove attention heads during training and keep all attention heads at test time, thus the final model is an ensemble of models with different architectures. The proposed method also forces each head independently learn the most useful patterns. We apply the proposed method to train Transformer based and Convolution-augmented Transformer (Conformer) based ASR models. Our method gives consistent performance gains over strong baselines on the Wall Street Journal, AISHELL, Switchboard and AMI datasets. To the best of our knowledge, we have achieved state-of-the-art end-to-end Transformer based model performance on Switchboard and AMI.",
    "original_url": "http://arxiv.org/pdf/2011.04004v2",
    "original_title": "Stochastic Attention Head Removal: A simple and effective method for improving Transformer Based ASR Models",
    "source": "arxiv",
    "authors": [
      "Shucong Zhang",
      "Erfan Loweimi",
      "Peter Bell",
      "Steve Renals"
    ],
    "published": "2020-11-08T15:41:03+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2011.04004v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2011.04004v2_chunk_2",
    "chunk_text": "To the best of our knowledge, we have achieved state-of-the-art end-to-end Transformer based model performance on Switchboard and AMI.",
    "original_url": "http://arxiv.org/pdf/2011.04004v2",
    "original_title": "Stochastic Attention Head Removal: A simple and effective method for improving Transformer Based ASR Models",
    "source": "arxiv",
    "authors": [
      "Shucong Zhang",
      "Erfan Loweimi",
      "Peter Bell",
      "Steve Renals"
    ],
    "published": "2020-11-08T15:41:03+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2011.04004v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2006.14264v1_chunk_0",
    "chunk_text": "Self-Segregating and Coordinated-Segregating Transformer for Focused Deep Multi-Modular Network for Visual Question Answering\n\nAttention mechanism has gained huge popularity due to its effectiveness in achieving high accuracy in different domains. But attention is opportunistic and is not justified by the content or usability of the content. Transformer like structure creates all/any possible attention(s). We define segregating strategies that can prioritize the contents for the applications for enhancement of performance. We defined two strategies: Self-Segregating Transformer (SST) and Coordinated-Segregating Transformer (CST) and used it to solve visual question answering application.",
    "original_url": "http://arxiv.org/pdf/2006.14264v1",
    "original_title": "Self-Segregating and Coordinated-Segregating Transformer for Focused Deep Multi-Modular Network for Visual Question Answering",
    "source": "arxiv",
    "authors": [
      "Chiranjib Sur"
    ],
    "published": "2020-06-25T09:17:03+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2006.14264v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2006.14264v1_chunk_1",
    "chunk_text": "We defined two strategies: Self-Segregating Transformer (SST) and Coordinated-Segregating Transformer (CST) and used it to solve visual question answering application. Self-segregation strategy for attention contributes in better understanding and filtering the information that can be most helpful for answering the question and create diversity of visual-reasoning for attention. This work can easily be used in many other applications that involve repetition and multiple frames of features and would reduce the commonality of the attentions to a great extent. Visual Question Answering (VQA) requires understanding and coordination of both images and textual interpretations. Experiments demonstrate that segregation strategies for cascaded multi-head transformer attention outperforms many previous works and achieved considerable improvement for VQA-v2 dataset benchmark.",
    "original_url": "http://arxiv.org/pdf/2006.14264v1",
    "original_title": "Self-Segregating and Coordinated-Segregating Transformer for Focused Deep Multi-Modular Network for Visual Question Answering",
    "source": "arxiv",
    "authors": [
      "Chiranjib Sur"
    ],
    "published": "2020-06-25T09:17:03+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2006.14264v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2006.14264v1_chunk_2",
    "chunk_text": "Experiments demonstrate that segregation strategies for cascaded multi-head transformer attention outperforms many previous works and achieved considerable improvement for VQA-v2 dataset benchmark.",
    "original_url": "http://arxiv.org/pdf/2006.14264v1",
    "original_title": "Self-Segregating and Coordinated-Segregating Transformer for Focused Deep Multi-Modular Network for Visual Question Answering",
    "source": "arxiv",
    "authors": [
      "Chiranjib Sur"
    ],
    "published": "2020-06-25T09:17:03+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2006.14264v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2406.06366v2_chunk_0",
    "chunk_text": "Symmetric Dot-Product Attention for Efficient Training of BERT Language Models\n\nInitially introduced as a machine translation model, the Transformer architecture has now become the foundation for modern deep learning architecture, with applications in a wide range of fields, from computer vision to natural language processing. Nowadays, to tackle increasingly more complex tasks, Transformer-based models are stretched to enormous sizes, requiring increasingly larger training datasets, and unsustainable amount of compute resources. The ubiquitous nature of the Transformer and its core component, the attention mechanism, are thus prime targets for efficiency research. In this work, we propose an alternative compatibility function for the self-attention mechanism introduced by the Transformer architecture. This compatibility function exploits an overlap in the learned representation of the traditional scaled dot-product attention, leading to a symmetric with pairwise coefficient dot-product attention.",
    "original_url": "http://arxiv.org/pdf/2406.06366v2",
    "original_title": "Symmetric Dot-Product Attention for Efficient Training of BERT Language Models",
    "source": "arxiv",
    "authors": [
      "Martin Courtois",
      "Malte Ostendorff",
      "Leonhard Hennig",
      "Georg Rehm"
    ],
    "published": "2024-06-10T15:24:15+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2406.06366v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2406.06366v2_chunk_1",
    "chunk_text": "This compatibility function exploits an overlap in the learned representation of the traditional scaled dot-product attention, leading to a symmetric with pairwise coefficient dot-product attention. When applied to the pre-training of BERT-like models, this new symmetric attention mechanism reaches a score of 79.36 on the GLUE benchmark against 78.74 for the traditional implementation, leads to a reduction of 6% in the number of trainable parameters, and reduces the number of training steps required before convergence by half.",
    "original_url": "http://arxiv.org/pdf/2406.06366v2",
    "original_title": "Symmetric Dot-Product Attention for Efficient Training of BERT Language Models",
    "source": "arxiv",
    "authors": [
      "Martin Courtois",
      "Malte Ostendorff",
      "Leonhard Hennig",
      "Georg Rehm"
    ],
    "published": "2024-06-10T15:24:15+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2406.06366v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2408.01739v1_chunk_0",
    "chunk_text": "LAM3D: Leveraging Attention for Monocular 3D Object Detection\n\nSince the introduction of the self-attention mechanism and the adoption of the Transformer architecture for Computer Vision tasks, the Vision Transformer-based architectures gained a lot of popularity in the field, being used for tasks such as image classification, object detection and image segmentation. However, efficiently leveraging the attention mechanism in vision transformers for the Monocular 3D Object Detection task remains an open question. In this paper, we present LAM3D, a framework that Leverages self-Attention mechanism for Monocular 3D object Detection. To do so, the proposed method is built upon a Pyramid Vision Transformer v2 (PVTv2) as feature extraction backbone and 2D/3D detection machinery. We evaluate the proposed method on the KITTI 3D Object Detection Benchmark, proving the applicability of the proposed solution in the autonomous driving domain and outperforming reference methods.",
    "original_url": "http://arxiv.org/pdf/2408.01739v1",
    "original_title": "LAM3D: Leveraging Attention for Monocular 3D Object Detection",
    "source": "arxiv",
    "authors": [
      "Diana-Alexandra Sas",
      "Leandro Di Bella",
      "Yangxintong Lyu",
      "Florin Oniga",
      "Adrian Munteanu"
    ],
    "published": "2024-08-03T10:50:07+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2408.01739v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2408.01739v1_chunk_1",
    "chunk_text": "We evaluate the proposed method on the KITTI 3D Object Detection Benchmark, proving the applicability of the proposed solution in the autonomous driving domain and outperforming reference methods. Moreover, due to the usage of self-attention, LAM3D is able to systematically outperform the equivalent architecture that does not employ self-attention.",
    "original_url": "http://arxiv.org/pdf/2408.01739v1",
    "original_title": "LAM3D: Leveraging Attention for Monocular 3D Object Detection",
    "source": "arxiv",
    "authors": [
      "Diana-Alexandra Sas",
      "Leandro Di Bella",
      "Yangxintong Lyu",
      "Florin Oniga",
      "Adrian Munteanu"
    ],
    "published": "2024-08-03T10:50:07+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2408.01739v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2006.01713v1_chunk_0",
    "chunk_text": "SAN-M: Memory Equipped Self-Attention for End-to-End Speech Recognition\n\nEnd-to-end speech recognition has become popular in recent years, since it can integrate the acoustic, pronunciation and language models into a single neural network. Among end-to-end approaches, attention-based methods have emerged as being superior. For example, Transformer, which adopts an encoder-decoder architecture. The key improvement introduced by Transformer is the utilization of self-attention instead of recurrent mechanisms, enabling both encoder and decoder to capture long-range dependencies with lower computational complexity.In this work, we propose boosting the self-attention ability with a DFSMN memory block, forming the proposed memory equipped self-attention (SAN-M) mechanism. Theoretical and empirical comparisons have been made to demonstrate the relevancy and complementarity between self-attention and the DFSMN memory block.",
    "original_url": "http://arxiv.org/pdf/2006.01713v1",
    "original_title": "SAN-M: Memory Equipped Self-Attention for End-to-End Speech Recognition",
    "source": "arxiv",
    "authors": [
      "Zhifu Gao",
      "Shiliang Zhang",
      "Ming Lei",
      "Ian McLoughlin"
    ],
    "published": "2020-05-21T03:33:09+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2006.01713v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2006.01713v1_chunk_1",
    "chunk_text": "Theoretical and empirical comparisons have been made to demonstrate the relevancy and complementarity between self-attention and the DFSMN memory block. Furthermore, the proposed SAN-M provides an efficient mechanism to integrate these two modules. We have evaluated our approach on the public AISHELL-1 benchmark and an industrial-level 20,000-hour Mandarin speech recognition task. On both tasks, SAN-M systems achieved much better performance than the self-attention based Transformer baseline system. Specially, it can achieve a CER of 6.46% on the AISHELL-1 task even without using any external LM, comfortably outperforming other state-of-the-art systems.",
    "original_url": "http://arxiv.org/pdf/2006.01713v1",
    "original_title": "SAN-M: Memory Equipped Self-Attention for End-to-End Speech Recognition",
    "source": "arxiv",
    "authors": [
      "Zhifu Gao",
      "Shiliang Zhang",
      "Ming Lei",
      "Ian McLoughlin"
    ],
    "published": "2020-05-21T03:33:09+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2006.01713v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2006.01713v1_chunk_2",
    "chunk_text": "Specially, it can achieve a CER of 6.46% on the AISHELL-1 task even without using any external LM, comfortably outperforming other state-of-the-art systems.",
    "original_url": "http://arxiv.org/pdf/2006.01713v1",
    "original_title": "SAN-M: Memory Equipped Self-Attention for End-to-End Speech Recognition",
    "source": "arxiv",
    "authors": [
      "Zhifu Gao",
      "Shiliang Zhang",
      "Ming Lei",
      "Ian McLoughlin"
    ],
    "published": "2020-05-21T03:33:09+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2006.01713v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2309.12010v1_chunk_0",
    "chunk_text": "Convolution and Attention Mixer for Synthetic Aperture Radar Image Change Detection\n\nSynthetic aperture radar (SAR) image change detection is a critical task and has received increasing attentions in the remote sensing community. However, existing SAR change detection methods are mainly based on convolutional neural networks (CNNs), with limited consideration of global attention mechanism. In this letter, we explore Transformer-like architecture for SAR change detection to incorporate global attention. To this end, we propose a convolution and attention mixer (CAMixer). First, to compensate the inductive bias for Transformer, we combine self-attention with shift convolution in a parallel way.",
    "original_url": "http://arxiv.org/pdf/2309.12010v1",
    "original_title": "Convolution and Attention Mixer for Synthetic Aperture Radar Image Change Detection",
    "source": "arxiv",
    "authors": [
      "Haopeng Zhang",
      "Zijing Lin",
      "Feng Gao",
      "Junyu Dong",
      "Qian Du",
      "Heng-Chao Li"
    ],
    "published": "2023-09-21T12:28:23+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2309.12010v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2309.12010v1_chunk_1",
    "chunk_text": "First, to compensate the inductive bias for Transformer, we combine self-attention with shift convolution in a parallel way. The parallel design effectively captures the global semantic information via the self-attention and performs local feature extraction through shift convolution simultaneously. Second, we adopt a gating mechanism in the feed-forward network to enhance the non-linear feature transformation. The gating mechanism is formulated as the element-wise multiplication of two parallel linear layers. Important features can be highlighted, leading to high-quality representations against speckle noise.",
    "original_url": "http://arxiv.org/pdf/2309.12010v1",
    "original_title": "Convolution and Attention Mixer for Synthetic Aperture Radar Image Change Detection",
    "source": "arxiv",
    "authors": [
      "Haopeng Zhang",
      "Zijing Lin",
      "Feng Gao",
      "Junyu Dong",
      "Qian Du",
      "Heng-Chao Li"
    ],
    "published": "2023-09-21T12:28:23+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2309.12010v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2309.12010v1_chunk_2",
    "chunk_text": "Important features can be highlighted, leading to high-quality representations against speckle noise. Extensive experiments conducted on three SAR datasets verify the superior performance of the proposed CAMixer. The source codes will be publicly available at https://github.com/summitgao/CAMixer .",
    "original_url": "http://arxiv.org/pdf/2309.12010v1",
    "original_title": "Convolution and Attention Mixer for Synthetic Aperture Radar Image Change Detection",
    "source": "arxiv",
    "authors": [
      "Haopeng Zhang",
      "Zijing Lin",
      "Feng Gao",
      "Junyu Dong",
      "Qian Du",
      "Heng-Chao Li"
    ],
    "published": "2023-09-21T12:28:23+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2309.12010v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2412.02919v1_chunk_0",
    "chunk_text": "Higher Order Transformers: Efficient Attention Mechanism for Tensor Structured Data\n\nTransformers are now ubiquitous for sequence modeling tasks, but their extension to multi-dimensional data remains a challenge due to the quadratic cost of the attention mechanism. In this paper, we propose Higher-Order Transformers (HOT), a novel architecture designed to efficiently process data with more than two axes, i.e. higher-order tensors. To address the computational challenges associated with high-order tensor attention, we introduce a novel Kronecker factorized attention mechanism that reduces the attention cost to quadratic in each axis' dimension, rather than quadratic in the total size of the input tensor. To further enhance efficiency, HOT leverages kernelized attention, reducing the complexity to linear.",
    "original_url": "http://arxiv.org/pdf/2412.02919v1",
    "original_title": "Higher Order Transformers: Efficient Attention Mechanism for Tensor Structured Data",
    "source": "arxiv",
    "authors": [
      "Soroush Omranpour",
      "Guillaume Rabusseau",
      "Reihaneh Rabbany"
    ],
    "published": "2024-12-04T00:10:47+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2412.02919v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2412.02919v1_chunk_1",
    "chunk_text": "To further enhance efficiency, HOT leverages kernelized attention, reducing the complexity to linear. This strategy maintains the model's expressiveness while enabling scalable attention computation. We validate the effectiveness of HOT on two high-dimensional tasks, including multivariate time series forecasting, and 3D medical image classification. Experimental results demonstrate that HOT achieves competitive performance while significantly improving computational efficiency, showcasing its potential for tackling a wide range of complex, multi-dimensional data.",
    "original_url": "http://arxiv.org/pdf/2412.02919v1",
    "original_title": "Higher Order Transformers: Efficient Attention Mechanism for Tensor Structured Data",
    "source": "arxiv",
    "authors": [
      "Soroush Omranpour",
      "Guillaume Rabusseau",
      "Reihaneh Rabbany"
    ],
    "published": "2024-12-04T00:10:47+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2412.02919v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2203.00091v1_chunk_0",
    "chunk_text": "Dynamic N:M Fine-grained Structured Sparse Attention Mechanism\n\nTransformers are becoming the mainstream solutions for various tasks like NLP and Computer vision. Despite their success, the high complexity of the attention mechanism hinders them from being applied to latency-sensitive tasks. Tremendous efforts have been made to alleviate this problem, and many of them successfully reduce the asymptotic complexity to linear. Nevertheless, most of them fail to achieve practical speedup over the original full attention under moderate sequence lengths and are unfriendly to finetuning. In this paper, we present DFSS, an attention mechanism that dynamically prunes the full attention weight matrix to N:M fine-grained structured sparse pattern.",
    "original_url": "http://arxiv.org/pdf/2203.00091v1",
    "original_title": "Dynamic N:M Fine-grained Structured Sparse Attention Mechanism",
    "source": "arxiv",
    "authors": [
      "Zhaodong Chen",
      "Yuying Quan",
      "Zheng Qu",
      "Liu Liu",
      "Yufei Ding",
      "Yuan Xie"
    ],
    "published": "2022-02-28T20:52:24+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2203.00091v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2203.00091v1_chunk_1",
    "chunk_text": "In this paper, we present DFSS, an attention mechanism that dynamically prunes the full attention weight matrix to N:M fine-grained structured sparse pattern. We provide both theoretical and empirical evidence that demonstrates DFSS is a good approximation of the full attention mechanism. We propose a dedicated CUDA kernel design that completely eliminates the dynamic pruning overhead and achieves speedups under arbitrary sequence length. We evaluate the 1:2 and 2:4 sparsity under different configurations and achieve 1.27~ 1.89x speedups over the full-attention mechanism. It only takes a couple of finetuning epochs from the pretrained model to achieve on par accuracy with full attention mechanism on tasks from various domains under different sequence lengths from 384 to 4096.",
    "original_url": "http://arxiv.org/pdf/2203.00091v1",
    "original_title": "Dynamic N:M Fine-grained Structured Sparse Attention Mechanism",
    "source": "arxiv",
    "authors": [
      "Zhaodong Chen",
      "Yuying Quan",
      "Zheng Qu",
      "Liu Liu",
      "Yufei Ding",
      "Yuan Xie"
    ],
    "published": "2022-02-28T20:52:24+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2203.00091v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2203.00091v1_chunk_2",
    "chunk_text": "It only takes a couple of finetuning epochs from the pretrained model to achieve on par accuracy with full attention mechanism on tasks from various domains under different sequence lengths from 384 to 4096.",
    "original_url": "http://arxiv.org/pdf/2203.00091v1",
    "original_title": "Dynamic N:M Fine-grained Structured Sparse Attention Mechanism",
    "source": "arxiv",
    "authors": [
      "Zhaodong Chen",
      "Yuying Quan",
      "Zheng Qu",
      "Liu Liu",
      "Yufei Ding",
      "Yuan Xie"
    ],
    "published": "2022-02-28T20:52:24+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2203.00091v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2209.07738v3_chunk_0",
    "chunk_text": "DMFormer: Closing the Gap Between CNN and Vision Transformers\n\nVision transformers have shown excellent performance in computer vision tasks. As the computation cost of their self-attention mechanism is expensive, recent works tried to replace the self-attention mechanism in vision transformers with convolutional operations, which is more efficient with built-in inductive bias. However, these efforts either ignore multi-level features or lack dynamic prosperity, leading to sub-optimal performance. In this paper, we propose a Dynamic Multi-level Attention mechanism (DMA), which captures different patterns of input images by multiple kernel sizes and enables input-adaptive weights with a gating mechanism. Based on DMA, we present an efficient backbone network named DMFormer.",
    "original_url": "http://arxiv.org/pdf/2209.07738v3",
    "original_title": "DMFormer: Closing the Gap Between CNN and Vision Transformers",
    "source": "arxiv",
    "authors": [
      "Zimian Wei",
      "Hengyue Pan",
      "Lujun Li",
      "Menglong Lu",
      "Xin Niu",
      "Peijie Dong",
      "Dongsheng Li"
    ],
    "published": "2022-09-16T06:45:01+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2209.07738v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2209.07738v3_chunk_1",
    "chunk_text": "Based on DMA, we present an efficient backbone network named DMFormer. DMFormer adopts the overall architecture of vision transformers, while replacing the self-attention mechanism with our proposed DMA. Extensive experimental results on ImageNet-1K and ADE20K datasets demonstrated that DMFormer achieves state-of-the-art performance, which outperforms similar-sized vision transformers(ViTs) and convolutional neural networks (CNNs).",
    "original_url": "http://arxiv.org/pdf/2209.07738v3",
    "original_title": "DMFormer: Closing the Gap Between CNN and Vision Transformers",
    "source": "arxiv",
    "authors": [
      "Zimian Wei",
      "Hengyue Pan",
      "Lujun Li",
      "Menglong Lu",
      "Xin Niu",
      "Peijie Dong",
      "Dongsheng Li"
    ],
    "published": "2022-09-16T06:45:01+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2209.07738v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2107.05768v2_chunk_0",
    "chunk_text": "Combiner: Full Attention Transformer with Sparse Computation Cost\n\nTransformers provide a class of expressive architectures that are extremely effective for sequence modeling. However, the key limitation of transformers is their quadratic memory and time complexity $\\mathcal{O}(L^2)$ with respect to the sequence length in attention layers, which restricts application in extremely long sequences. Most existing approaches leverage sparsity or low-rank assumptions in the attention matrix to reduce cost, but sacrifice expressiveness. Instead, we propose Combiner, which provides full attention capability in each attention head while maintaining low computation and memory complexity. The key idea is to treat the self-attention mechanism as a conditional expectation over embeddings at each location, and approximate the conditional distribution with a structured factorization.",
    "original_url": "http://arxiv.org/pdf/2107.05768v2",
    "original_title": "Combiner: Full Attention Transformer with Sparse Computation Cost",
    "source": "arxiv",
    "authors": [
      "Hongyu Ren",
      "Hanjun Dai",
      "Zihang Dai",
      "Mengjiao Yang",
      "Jure Leskovec",
      "Dale Schuurmans",
      "Bo Dai"
    ],
    "published": "2021-07-12T22:43:11+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2107.05768v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2107.05768v2_chunk_1",
    "chunk_text": "The key idea is to treat the self-attention mechanism as a conditional expectation over embeddings at each location, and approximate the conditional distribution with a structured factorization. Each location can attend to all other locations, either via direct attention, or through indirect attention to abstractions, which are again conditional expectations of embeddings from corresponding local regions. We show that most sparse attention patterns used in existing sparse transformers are able to inspire the design of such factorization for full attention, resulting in the same sub-quadratic cost ($\\mathcal{O}(L\\log(L))$ or $\\mathcal{O}(L\\sqrt{L})$). Combiner is a drop-in replacement for attention layers in existing transformers and can be easily implemented in common frameworks. An experimental evaluation on both autoregressive and bidirectional sequence tasks demonstrates the effectiveness of this approach, yielding state-of-the-art results on several image and text modeling tasks.",
    "original_url": "http://arxiv.org/pdf/2107.05768v2",
    "original_title": "Combiner: Full Attention Transformer with Sparse Computation Cost",
    "source": "arxiv",
    "authors": [
      "Hongyu Ren",
      "Hanjun Dai",
      "Zihang Dai",
      "Mengjiao Yang",
      "Jure Leskovec",
      "Dale Schuurmans",
      "Bo Dai"
    ],
    "published": "2021-07-12T22:43:11+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2107.05768v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2107.05768v2_chunk_2",
    "chunk_text": "An experimental evaluation on both autoregressive and bidirectional sequence tasks demonstrates the effectiveness of this approach, yielding state-of-the-art results on several image and text modeling tasks.",
    "original_url": "http://arxiv.org/pdf/2107.05768v2",
    "original_title": "Combiner: Full Attention Transformer with Sparse Computation Cost",
    "source": "arxiv",
    "authors": [
      "Hongyu Ren",
      "Hanjun Dai",
      "Zihang Dai",
      "Mengjiao Yang",
      "Jure Leskovec",
      "Dale Schuurmans",
      "Bo Dai"
    ],
    "published": "2021-07-12T22:43:11+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2107.05768v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2403.19882v1_chunk_0",
    "chunk_text": "Enhancing Efficiency in Vision Transformer Networks: Design Techniques and Insights\n\nIntrigued by the inherent ability of the human visual system to identify salient regions in complex scenes, attention mechanisms have been seamlessly integrated into various Computer Vision (CV) tasks. Building upon this paradigm, Vision Transformer (ViT) networks exploit attention mechanisms for improved efficiency. This review navigates the landscape of redesigned attention mechanisms within ViTs, aiming to enhance their performance. This paper provides a comprehensive exploration of techniques and insights for designing attention mechanisms, systematically reviewing recent literature in the field of CV. This survey begins with an introduction to the theoretical foundations and fundamental concepts underlying attention mechanisms.",
    "original_url": "http://arxiv.org/pdf/2403.19882v1",
    "original_title": "Enhancing Efficiency in Vision Transformer Networks: Design Techniques and Insights",
    "source": "arxiv",
    "authors": [
      "Moein Heidari",
      "Reza Azad",
      "Sina Ghorbani Kolahi",
      "René Arimond",
      "Leon Niggemeier",
      "Alaa Sulaiman",
      "Afshin Bozorgpour",
      "Ehsan Khodapanah Aghdam",
      "Amirhossein Kazerouni",
      "Ilker Hacihaliloglu",
      "Dorit Merhof"
    ],
    "published": "2024-03-28T23:31:59+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2403.19882v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2403.19882v1_chunk_1",
    "chunk_text": "This survey begins with an introduction to the theoretical foundations and fundamental concepts underlying attention mechanisms. We then present a systematic taxonomy of various attention mechanisms within ViTs, employing redesigned approaches. A multi-perspective categorization is proposed based on their application, objectives, and the type of attention applied. The analysis includes an exploration of the novelty, strengths, weaknesses, and an in-depth evaluation of the different proposed strategies. This culminates in the development of taxonomies that highlight key properties and contributions.",
    "original_url": "http://arxiv.org/pdf/2403.19882v1",
    "original_title": "Enhancing Efficiency in Vision Transformer Networks: Design Techniques and Insights",
    "source": "arxiv",
    "authors": [
      "Moein Heidari",
      "Reza Azad",
      "Sina Ghorbani Kolahi",
      "René Arimond",
      "Leon Niggemeier",
      "Alaa Sulaiman",
      "Afshin Bozorgpour",
      "Ehsan Khodapanah Aghdam",
      "Amirhossein Kazerouni",
      "Ilker Hacihaliloglu",
      "Dorit Merhof"
    ],
    "published": "2024-03-28T23:31:59+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2403.19882v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2403.19882v1_chunk_2",
    "chunk_text": "This culminates in the development of taxonomies that highlight key properties and contributions. Finally, we gather the reviewed studies along with their available open-source implementations at our \\href{https://github.com/mindflow-institue/Awesome-Attention-Mechanism-in-Medical-Imaging}{GitHub}\\footnote{\\url{https://github.com/xmindflow/Awesome-Attention-Mechanism-in-Medical-Imaging}}. We aim to regularly update it with the most recent relevant papers.",
    "original_url": "http://arxiv.org/pdf/2403.19882v1",
    "original_title": "Enhancing Efficiency in Vision Transformer Networks: Design Techniques and Insights",
    "source": "arxiv",
    "authors": [
      "Moein Heidari",
      "Reza Azad",
      "Sina Ghorbani Kolahi",
      "René Arimond",
      "Leon Niggemeier",
      "Alaa Sulaiman",
      "Afshin Bozorgpour",
      "Ehsan Khodapanah Aghdam",
      "Amirhossein Kazerouni",
      "Ilker Hacihaliloglu",
      "Dorit Merhof"
    ],
    "published": "2024-03-28T23:31:59+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2403.19882v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2211.07970v1_chunk_0",
    "chunk_text": "Adaptive Multi-Neighborhood Attention based Transformer for Graph Representation Learning\n\nBy incorporating the graph structural information into Transformers, graph Transformers have exhibited promising performance for graph representation learning in recent years. Existing graph Transformers leverage specific strategies, such as Laplacian eigenvectors and shortest paths of the node pairs, to preserve the structural features of nodes and feed them into the vanilla Transformer to learn the representations of nodes. It is hard for such predefined rules to extract informative graph structural features for arbitrary graphs whose topology structure varies greatly, limiting the learning capacity of the models. To this end, we propose an adaptive graph Transformer, termed Multi-Neighborhood Attention based Graph Transformer (MNA-GT), which captures the graph structural information for each node from the multi-neighborhood attention mechanism adaptively. By defining the input to perform scaled-dot product as an attention kernel, MNA-GT constructs multiple attention kernels based on different hops of neighborhoods such that each attention kernel can capture specific graph structural information of the corresponding neighborhood for each node pair.",
    "original_url": "http://arxiv.org/pdf/2211.07970v1",
    "original_title": "Adaptive Multi-Neighborhood Attention based Transformer for Graph Representation Learning",
    "source": "arxiv",
    "authors": [
      "Gaichao Li",
      "Jinsong Chen",
      "Kun He"
    ],
    "published": "2022-11-15T08:12:44+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2211.07970v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2211.07970v1_chunk_1",
    "chunk_text": "By defining the input to perform scaled-dot product as an attention kernel, MNA-GT constructs multiple attention kernels based on different hops of neighborhoods such that each attention kernel can capture specific graph structural information of the corresponding neighborhood for each node pair. In this way, MNA-GT can preserve the graph structural information efficiently by incorporating node representations learned by different attention kernels. MNA-GT further employs an attention layer to learn the importance of different attention kernels to enable the model to adaptively capture the graph structural information for different nodes. Extensive experiments are conducted on a variety of graph benchmarks, and the empirical results show that MNA-GT outperforms many strong baselines.",
    "original_url": "http://arxiv.org/pdf/2211.07970v1",
    "original_title": "Adaptive Multi-Neighborhood Attention based Transformer for Graph Representation Learning",
    "source": "arxiv",
    "authors": [
      "Gaichao Li",
      "Jinsong Chen",
      "Kun He"
    ],
    "published": "2022-11-15T08:12:44+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2211.07970v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1910.06611v2_chunk_0",
    "chunk_text": "Enhancing the Transformer with Explicit Relational Encoding for Math Problem Solving\n\nWe incorporate Tensor-Product Representations within the Transformer in order to better support the explicit representation of relation structure. Our Tensor-Product Transformer (TP-Transformer) sets a new state of the art on the recently-introduced Mathematics Dataset containing 56 categories of free-form math word-problems. The essential component of the model is a novel attention mechanism, called TP-Attention, which explicitly encodes the relations between each Transformer cell and the other cells from which values have been retrieved by attention. TP-Attention goes beyond linear combination of retrieved values, strengthening representation-building and resolving ambiguities introduced by multiple layers of standard attention. The TP-Transformer's attention maps give better insights into how it is capable of solving the Mathematics Dataset's challenging problems.",
    "original_url": "http://arxiv.org/pdf/1910.06611v2",
    "original_title": "Enhancing the Transformer with Explicit Relational Encoding for Math Problem Solving",
    "source": "arxiv",
    "authors": [
      "Imanol Schlag",
      "Paul Smolensky",
      "Roland Fernandez",
      "Nebojsa Jojic",
      "Jürgen Schmidhuber",
      "Jianfeng Gao"
    ],
    "published": "2019-10-15T09:19:55+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1910.06611v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1910.06611v2_chunk_1",
    "chunk_text": "The TP-Transformer's attention maps give better insights into how it is capable of solving the Mathematics Dataset's challenging problems. Pretrained models and code will be made available after publication.",
    "original_url": "http://arxiv.org/pdf/1910.06611v2",
    "original_title": "Enhancing the Transformer with Explicit Relational Encoding for Math Problem Solving",
    "source": "arxiv",
    "authors": [
      "Imanol Schlag",
      "Paul Smolensky",
      "Roland Fernandez",
      "Nebojsa Jojic",
      "Jürgen Schmidhuber",
      "Jianfeng Gao"
    ],
    "published": "2019-10-15T09:19:55+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1910.06611v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2504.00070v1_chunk_0",
    "chunk_text": "Enhancing Time Series Forecasting with Fuzzy Attention-Integrated Transformers\n\nThis paper introduces FANTF (Fuzzy Attention Network-Based Transformers), a novel approach that integrates fuzzy logic with existing transformer architectures to advance time series forecasting, classification, and anomaly detection tasks. FANTF leverages a proposed fuzzy attention mechanism incorporating fuzzy membership functions to handle uncertainty and imprecision in noisy and ambiguous time series data. The FANTF approach enhances its ability to capture complex temporal dependencies and multivariate relationships by embedding fuzzy logic principles into the self-attention module of the existing transformer's architecture. The framework combines fuzzy-enhanced attention with a set of benchmark existing transformer-based architectures to provide efficient predictions, classification and anomaly detection. Specifically, FANTF generates learnable fuzziness attention scores that highlight the relative importance of temporal features and data points, offering insights into its decision-making process.",
    "original_url": "http://arxiv.org/pdf/2504.00070v1",
    "original_title": "Enhancing Time Series Forecasting with Fuzzy Attention-Integrated Transformers",
    "source": "arxiv",
    "authors": [
      "Sanjay Chakraborty",
      "Fredrik Heintz"
    ],
    "published": "2025-03-31T17:33:50+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2504.00070v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2504.00070v1_chunk_1",
    "chunk_text": "Specifically, FANTF generates learnable fuzziness attention scores that highlight the relative importance of temporal features and data points, offering insights into its decision-making process. Experimental evaluatios on some real-world datasets reveal that FANTF significantly enhances the performance of forecasting, classification, and anomaly detection tasks over traditional transformer-based models.",
    "original_url": "http://arxiv.org/pdf/2504.00070v1",
    "original_title": "Enhancing Time Series Forecasting with Fuzzy Attention-Integrated Transformers",
    "source": "arxiv",
    "authors": [
      "Sanjay Chakraborty",
      "Fredrik Heintz"
    ],
    "published": "2025-03-31T17:33:50+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2504.00070v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2402.04239v1_chunk_0",
    "chunk_text": "CAST: Clustering Self-Attention using Surrogate Tokens for Efficient Transformers\n\nThe Transformer architecture has shown to be a powerful tool for a wide range of tasks. It is based on the self-attention mechanism, which is an inherently computationally expensive operation with quadratic computational complexity: memory usage and compute time increase quadratically with the length of the input sequences, thus limiting the application of Transformers. In this work, we propose a novel Clustering self-Attention mechanism using Surrogate Tokens (CAST), to optimize the attention computation and achieve efficient transformers. CAST utilizes learnable surrogate tokens to construct a cluster affinity matrix, used to cluster the input sequence and generate novel cluster summaries. The self-attention from within each cluster is then combined with the cluster summaries of other clusters, enabling information flow across the entire input sequence.",
    "original_url": "http://arxiv.org/pdf/2402.04239v1",
    "original_title": "CAST: Clustering Self-Attention using Surrogate Tokens for Efficient Transformers",
    "source": "arxiv",
    "authors": [
      "Adjorn van Engelenhoven",
      "Nicola Strisciuglio",
      "Estefanía Talavera"
    ],
    "published": "2024-02-06T18:47:52+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2402.04239v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2402.04239v1_chunk_1",
    "chunk_text": "The self-attention from within each cluster is then combined with the cluster summaries of other clusters, enabling information flow across the entire input sequence. CAST improves efficiency by reducing the complexity from $O(N^2)$ to $O(\\alpha N)$ where N is the sequence length, and {\\alpha} is constant according to the number of clusters and samples per cluster. We show that CAST performs better than or comparable to the baseline Transformers on long-range sequence modeling tasks, while also achieving higher results on time and memory efficiency than other efficient transformers.",
    "original_url": "http://arxiv.org/pdf/2402.04239v1",
    "original_title": "CAST: Clustering Self-Attention using Surrogate Tokens for Efficient Transformers",
    "source": "arxiv",
    "authors": [
      "Adjorn van Engelenhoven",
      "Nicola Strisciuglio",
      "Estefanía Talavera"
    ],
    "published": "2024-02-06T18:47:52+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2402.04239v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2501.17900v1_chunk_0",
    "chunk_text": "Shared DIFF Transformer\n\nDIFF Transformer improves attention allocation by enhancing focus on relevant context while suppressing noise. It introduces a differential attention mechanism that calculates the difference between two independently generated attention distributions, effectively reducing noise and promoting sparse attention patterns. However, the independent signal generation in DIFF Transformer results in parameter redundancy and suboptimal utilization of information. In this work, we propose Shared DIFF Transformer, which draws on the idea of a differential amplifier by introducing a shared base matrix to model global patterns and incorporating low-rank updates to enhance task-specific flexibility. This design significantly reduces parameter redundancy, improves efficiency, and retains strong noise suppression capabilities.",
    "original_url": "http://arxiv.org/pdf/2501.17900v1",
    "original_title": "Shared DIFF Transformer",
    "source": "arxiv",
    "authors": [
      "Yueyang Cang",
      "Yuhang Liu",
      "Xiaoteng Zhang",
      "Xiangju Wang"
    ],
    "published": "2025-01-29T09:29:07+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2501.17900v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2501.17900v1_chunk_1",
    "chunk_text": "This design significantly reduces parameter redundancy, improves efficiency, and retains strong noise suppression capabilities. Experimental results show that, compared to DIFF Transformer, our method achieves better performance in tasks such as long-sequence modeling, key information retrieval, and in-context learning. Our work provides a novel and efficient approach to optimizing differential attention mechanisms and advancing robust Transformer architectures.",
    "original_url": "http://arxiv.org/pdf/2501.17900v1",
    "original_title": "Shared DIFF Transformer",
    "source": "arxiv",
    "authors": [
      "Yueyang Cang",
      "Yuhang Liu",
      "Xiaoteng Zhang",
      "Xiangju Wang"
    ],
    "published": "2025-01-29T09:29:07+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2501.17900v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2304.04237v1_chunk_0",
    "chunk_text": "Slide-Transformer: Hierarchical Vision Transformer with Local Self-Attention\n\nSelf-attention mechanism has been a key factor in the recent progress of Vision Transformer (ViT), which enables adaptive feature extraction from global contexts. However, existing self-attention methods either adopt sparse global attention or window attention to reduce the computation complexity, which may compromise the local feature learning or subject to some handcrafted designs. In contrast, local attention, which restricts the receptive field of each query to its own neighboring pixels, enjoys the benefits of both convolution and self-attention, namely local inductive bias and dynamic feature selection. Nevertheless, current local attention modules either use inefficient Im2Col function or rely on specific CUDA kernels that are hard to generalize to devices without CUDA support. In this paper, we propose a novel local attention module, Slide Attention, which leverages common convolution operations to achieve high efficiency, flexibility and generalizability.",
    "original_url": "http://arxiv.org/pdf/2304.04237v1",
    "original_title": "Slide-Transformer: Hierarchical Vision Transformer with Local Self-Attention",
    "source": "arxiv",
    "authors": [
      "Xuran Pan",
      "Tianzhu Ye",
      "Zhuofan Xia",
      "Shiji Song",
      "Gao Huang"
    ],
    "published": "2023-04-09T13:37:59+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2304.04237v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2304.04237v1_chunk_1",
    "chunk_text": "In this paper, we propose a novel local attention module, Slide Attention, which leverages common convolution operations to achieve high efficiency, flexibility and generalizability. Specifically, we first re-interpret the column-based Im2Col function from a new row-based perspective and use Depthwise Convolution as an efficient substitution. On this basis, we propose a deformed shifting module based on the re-parameterization technique, which further relaxes the fixed key/value positions to deformed features in the local region. In this way, our module realizes the local attention paradigm in both efficient and flexible manner. Extensive experiments show that our slide attention module is applicable to a variety of advanced Vision Transformer models and compatible with various hardware devices, and achieves consistently improved performances on comprehensive benchmarks.",
    "original_url": "http://arxiv.org/pdf/2304.04237v1",
    "original_title": "Slide-Transformer: Hierarchical Vision Transformer with Local Self-Attention",
    "source": "arxiv",
    "authors": [
      "Xuran Pan",
      "Tianzhu Ye",
      "Zhuofan Xia",
      "Shiji Song",
      "Gao Huang"
    ],
    "published": "2023-04-09T13:37:59+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2304.04237v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2304.04237v1_chunk_2",
    "chunk_text": "Extensive experiments show that our slide attention module is applicable to a variety of advanced Vision Transformer models and compatible with various hardware devices, and achieves consistently improved performances on comprehensive benchmarks. Code is available at https://github.com/LeapLabTHU/Slide-Transformer.",
    "original_url": "http://arxiv.org/pdf/2304.04237v1",
    "original_title": "Slide-Transformer: Hierarchical Vision Transformer with Local Self-Attention",
    "source": "arxiv",
    "authors": [
      "Xuran Pan",
      "Tianzhu Ye",
      "Zhuofan Xia",
      "Shiji Song",
      "Gao Huang"
    ],
    "published": "2023-04-09T13:37:59+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2304.04237v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2409.18747v1_chunk_0",
    "chunk_text": "Cottention: Linear Transformers With Cosine Attention\n\nAttention mechanisms, particularly softmax attention, have been instrumental in the success of transformer-based models such as GPT. However, the quadratic memory complexity of softmax attention with respect to sequence length poses significant challenges for processing longer sequences. We introduce Cottention, a novel attention mechanism that replaces the softmax operation with cosine similarity. By leveraging the properties of cosine similarity and rearranging the attention equation, Cottention achieves native linear memory complexity with respect to sequence length, making it inherently more memory-efficient than softmax attention. We demonstrate that Cottention can be reformulated as a recurrent neural network (RNN) with a finite hidden state, allowing for constant memory usage during inference.",
    "original_url": "http://arxiv.org/pdf/2409.18747v1",
    "original_title": "Cottention: Linear Transformers With Cosine Attention",
    "source": "arxiv",
    "authors": [
      "Gabriel Mongaras",
      "Trevor Dohm",
      "Eric C. Larson"
    ],
    "published": "2024-09-27T13:38:36+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2409.18747v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2409.18747v1_chunk_1",
    "chunk_text": "We demonstrate that Cottention can be reformulated as a recurrent neural network (RNN) with a finite hidden state, allowing for constant memory usage during inference. We evaluate Cottention on both the bidirectional BERT and causal GPT tasks, demonstrating comparable performance to softmax attention while significantly reducing memory requirements. To ensure efficient computation, we develop a custom CUDA kernel for Cottention. Our results show that Cottention is a promising alternative to softmax attention, enabling the processing of longer sequences without sacrificing performance, due to its native linear memory complexity and ability to maintain a constant memory footprint during inference.",
    "original_url": "http://arxiv.org/pdf/2409.18747v1",
    "original_title": "Cottention: Linear Transformers With Cosine Attention",
    "source": "arxiv",
    "authors": [
      "Gabriel Mongaras",
      "Trevor Dohm",
      "Eric C. Larson"
    ],
    "published": "2024-09-27T13:38:36+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2409.18747v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2410.22768v1_chunk_0",
    "chunk_text": "Emergence of Human-Like Attention in Self-Supervised Vision Transformers: an eye-tracking study\n\nMany models of visual attention have been proposed so far. Traditional bottom-up models, like saliency models, fail to replicate human gaze patterns, and deep gaze prediction models lack biological plausibility due to their reliance on supervised learning. Vision Transformers (ViTs), with their self-attention mechanisms, offer a new approach but often produce dispersed attention patterns if trained with supervised learning. This study explores whether self-supervised DINO (self-DIstillation with NO labels) training enables ViTs to develop attention mechanisms resembling human visual attention. Using video stimuli to capture human gaze dynamics, we found that DINO-trained ViTs closely mimic human attention patterns, while those trained with supervised learning deviate significantly.",
    "original_url": "http://arxiv.org/pdf/2410.22768v1",
    "original_title": "Emergence of Human-Like Attention in Self-Supervised Vision Transformers: an eye-tracking study",
    "source": "arxiv",
    "authors": [
      "Takuto Yamamoto",
      "Hirosato Akahoshi",
      "Shigeru Kitazawa"
    ],
    "published": "2024-10-30T07:37:19+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2410.22768v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2410.22768v1_chunk_1",
    "chunk_text": "Using video stimuli to capture human gaze dynamics, we found that DINO-trained ViTs closely mimic human attention patterns, while those trained with supervised learning deviate significantly. An analysis of self-attention heads revealed three distinct clusters: one focusing on foreground objects, one on entire objects, and one on the background. DINO-trained ViTs offer insight into how human overt attention and figure-ground separation develop in visual perception.",
    "original_url": "http://arxiv.org/pdf/2410.22768v1",
    "original_title": "Emergence of Human-Like Attention in Self-Supervised Vision Transformers: an eye-tracking study",
    "source": "arxiv",
    "authors": [
      "Takuto Yamamoto",
      "Hirosato Akahoshi",
      "Shigeru Kitazawa"
    ],
    "published": "2024-10-30T07:37:19+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2410.22768v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2503.23814v1_chunk_0",
    "chunk_text": "An extension of linear self-attention for in-context learning\n\nIn-context learning is a remarkable property of transformers and has been the focus of recent research. An attention mechanism is a key component in transformers, in which an attention matrix encodes relationships between words in a sentence and is used as weights for words in a sentence. This mechanism is effective for capturing language representations. However, it is questionable whether naive self-attention is suitable for in-context learning in general tasks, since the computation implemented by self-attention is somewhat restrictive in terms of matrix multiplication. In fact, we may need appropriate input form designs when considering heuristic implementations of computational algorithms.",
    "original_url": "http://arxiv.org/pdf/2503.23814v1",
    "original_title": "An extension of linear self-attention for in-context learning",
    "source": "arxiv",
    "authors": [
      "Katsuyuki Hagiwara"
    ],
    "published": "2025-03-31T07:49:05+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2503.23814v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2503.23814v1_chunk_1",
    "chunk_text": "In fact, we may need appropriate input form designs when considering heuristic implementations of computational algorithms. In this paper, in case of linear self-attention, we extend it by introducing a bias matrix in addition to a weight matrix for an input. Despite the simple extension, the extended linear self-attention can output any constant matrix, input matrix and multiplications of two or three matrices in the input. Note that the second property implies that it can be a skip connection. Therefore, flexible matrix manipulations can be implemented by connecting the extended linear self-attention components.",
    "original_url": "http://arxiv.org/pdf/2503.23814v1",
    "original_title": "An extension of linear self-attention for in-context learning",
    "source": "arxiv",
    "authors": [
      "Katsuyuki Hagiwara"
    ],
    "published": "2025-03-31T07:49:05+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2503.23814v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2503.23814v1_chunk_2",
    "chunk_text": "Therefore, flexible matrix manipulations can be implemented by connecting the extended linear self-attention components. As an example of implementation using the extended linear self-attention, we show a heuristic construction of a batch-type gradient descent of ridge regression under a reasonable input form.",
    "original_url": "http://arxiv.org/pdf/2503.23814v1",
    "original_title": "An extension of linear self-attention for in-context learning",
    "source": "arxiv",
    "authors": [
      "Katsuyuki Hagiwara"
    ],
    "published": "2025-03-31T07:49:05+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2503.23814v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2412.13553v1_chunk_0",
    "chunk_text": "Combining Aggregated Attention and Transformer Architecture for Accurate and Efficient Performance of Spiking Neural Networks\n\nSpiking Neural Networks have attracted significant attention in recent years due to their distinctive low-power characteristics. Meanwhile, Transformer models, known for their powerful self-attention mechanisms and parallel processing capabilities, have demonstrated exceptional performance across various domains, including natural language processing and computer vision. Despite the significant advantages of both SNNs and Transformers, directly combining the low-power benefits of SNNs with the high performance of Transformers remains challenging. Specifically, while the sparse computing mode of SNNs contributes to reduced energy consumption, traditional attention mechanisms depend on dense matrix computations and complex softmax operations. This reliance poses significant challenges for effective execution in low-power scenarios.",
    "original_url": "http://arxiv.org/pdf/2412.13553v1",
    "original_title": "Combining Aggregated Attention and Transformer Architecture for Accurate and Efficient Performance of Spiking Neural Networks",
    "source": "arxiv",
    "authors": [
      "Hangming Zhang",
      "Alexander Sboev",
      "Roman Rybka",
      "Qiang Yu"
    ],
    "published": "2024-12-18T07:07:38+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2412.13553v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2412.13553v1_chunk_1",
    "chunk_text": "This reliance poses significant challenges for effective execution in low-power scenarios. Given the tremendous success of Transformers in deep learning, it is a necessary step to explore the integration of SNNs and Transformers to harness the strengths of both. In this paper, we propose a novel model architecture, Spike Aggregation Transformer (SAFormer), that integrates the low-power characteristics of SNNs with the high-performance advantages of Transformer models. The core contribution of SAFormer lies in the design of the Spike Aggregated Self-Attention (SASA) mechanism, which significantly simplifies the computation process by calculating attention weights using only the spike matrices query and key, thereby effectively reducing energy consumption. Additionally, we introduce a Depthwise Convolution Module (DWC) to enhance the feature extraction capabilities, further improving overall accuracy.",
    "original_url": "http://arxiv.org/pdf/2412.13553v1",
    "original_title": "Combining Aggregated Attention and Transformer Architecture for Accurate and Efficient Performance of Spiking Neural Networks",
    "source": "arxiv",
    "authors": [
      "Hangming Zhang",
      "Alexander Sboev",
      "Roman Rybka",
      "Qiang Yu"
    ],
    "published": "2024-12-18T07:07:38+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2412.13553v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2412.13553v1_chunk_2",
    "chunk_text": "Additionally, we introduce a Depthwise Convolution Module (DWC) to enhance the feature extraction capabilities, further improving overall accuracy. We evaluated and demonstrated that SAFormer outperforms state-of-the-art SNNs in both accuracy and energy consumption, highlighting its significant advantages in low-power and high-performance computing.",
    "original_url": "http://arxiv.org/pdf/2412.13553v1",
    "original_title": "Combining Aggregated Attention and Transformer Architecture for Accurate and Efficient Performance of Spiking Neural Networks",
    "source": "arxiv",
    "authors": [
      "Hangming Zhang",
      "Alexander Sboev",
      "Roman Rybka",
      "Qiang Yu"
    ],
    "published": "2024-12-18T07:07:38+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2412.13553v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2209.04881v1_chunk_0",
    "chunk_text": "On The Computational Complexity of Self-Attention\n\nTransformer architectures have led to remarkable progress in many state-of-art applications. However, despite their successes, modern transformers rely on the self-attention mechanism, whose time- and space-complexity is quadratic in the length of the input. Several approaches have been proposed to speed up self-attention mechanisms to achieve sub-quadratic running time; however, the large majority of these works are not accompanied by rigorous error guarantees. In this work, we establish lower bounds on the computational complexity of self-attention in a number of scenarios. We prove that the time complexity of self-attention is necessarily quadratic in the input length, unless the Strong Exponential Time Hypothesis (SETH) is false.",
    "original_url": "http://arxiv.org/pdf/2209.04881v1",
    "original_title": "On The Computational Complexity of Self-Attention",
    "source": "arxiv",
    "authors": [
      "Feyza Duman Keles",
      "Pruthuvi Mahesakya Wijewardena",
      "Chinmay Hegde"
    ],
    "published": "2022-09-11T14:38:10+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2209.04881v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2209.04881v1_chunk_1",
    "chunk_text": "We prove that the time complexity of self-attention is necessarily quadratic in the input length, unless the Strong Exponential Time Hypothesis (SETH) is false. This argument holds even if the attention computation is performed only approximately, and for a variety of attention mechanisms. As a complement to our lower bounds, we show that it is indeed possible to approximate dot-product self-attention using finite Taylor series in linear-time, at the cost of having an exponential dependence on the polynomial order.",
    "original_url": "http://arxiv.org/pdf/2209.04881v1",
    "original_title": "On The Computational Complexity of Self-Attention",
    "source": "arxiv",
    "authors": [
      "Feyza Duman Keles",
      "Pruthuvi Mahesakya Wijewardena",
      "Chinmay Hegde"
    ],
    "published": "2022-09-11T14:38:10+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2209.04881v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2310.11685v1_chunk_0",
    "chunk_text": "Superiority of Softmax: Unveiling the Performance Edge Over Linear Attention\n\nLarge transformer models have achieved state-of-the-art results in numerous natural language processing tasks. Among the pivotal components of the transformer architecture, the attention mechanism plays a crucial role in capturing token interactions within sequences through the utilization of softmax function. Conversely, linear attention presents a more computationally efficient alternative by approximating the softmax operation with linear complexity. However, it exhibits substantial performance degradation when compared to the traditional softmax attention mechanism. In this paper, we bridge the gap in our theoretical understanding of the reasons behind the practical performance gap between softmax and linear attention.",
    "original_url": "http://arxiv.org/pdf/2310.11685v1",
    "original_title": "Superiority of Softmax: Unveiling the Performance Edge Over Linear Attention",
    "source": "arxiv",
    "authors": [
      "Yichuan Deng",
      "Zhao Song",
      "Tianyi Zhou"
    ],
    "published": "2023-10-18T03:17:57+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2310.11685v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2310.11685v1_chunk_1",
    "chunk_text": "In this paper, we bridge the gap in our theoretical understanding of the reasons behind the practical performance gap between softmax and linear attention. By conducting a comprehensive comparative analysis of these two attention mechanisms, we shed light on the underlying reasons for why softmax attention outperforms linear attention in most scenarios.",
    "original_url": "http://arxiv.org/pdf/2310.11685v1",
    "original_title": "Superiority of Softmax: Unveiling the Performance Edge Over Linear Attention",
    "source": "arxiv",
    "authors": [
      "Yichuan Deng",
      "Zhao Song",
      "Tianyi Zhou"
    ],
    "published": "2023-10-18T03:17:57+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2310.11685v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2411.16618v1_chunk_0",
    "chunk_text": "StructFormer: Document Structure-based Masked Attention and its Impact on Language Model Pre-Training\n\nMost state-of-the-art techniques for Language Models (LMs) today rely on transformer-based architectures and their ubiquitous attention mechanism. However, the exponential growth in computational requirements with longer input sequences confines Transformers to handling short passages. Recent efforts have aimed to address this limitation by introducing selective attention mechanisms, notably local and global attention. While sparse attention mechanisms, akin to full attention in being Turing-complete, have been theoretically established, their practical impact on pre-training remains unexplored. This study focuses on empirically assessing the influence of global attention on BERT pre-training.",
    "original_url": "http://arxiv.org/pdf/2411.16618v1",
    "original_title": "StructFormer: Document Structure-based Masked Attention and its Impact on Language Model Pre-Training",
    "source": "arxiv",
    "authors": [
      "Kaustubh Ponkshe",
      "Venkatapathy Subramanian",
      "Natwar Modani",
      "Ganesh Ramakrishnan"
    ],
    "published": "2024-11-25T17:57:52+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2411.16618v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2411.16618v1_chunk_1",
    "chunk_text": "This study focuses on empirically assessing the influence of global attention on BERT pre-training. The primary steps involve creating an extensive corpus of structure-aware text through arXiv data, alongside a text-only counterpart. We carry out pre-training on these two datasets, investigate shifts in attention patterns, and assess their implications for downstream tasks. Our analysis underscores the significance of incorporating document structure into LM models, demonstrating their capacity to excel in more abstract tasks, such as document understanding.",
    "original_url": "http://arxiv.org/pdf/2411.16618v1",
    "original_title": "StructFormer: Document Structure-based Masked Attention and its Impact on Language Model Pre-Training",
    "source": "arxiv",
    "authors": [
      "Kaustubh Ponkshe",
      "Venkatapathy Subramanian",
      "Natwar Modani",
      "Ganesh Ramakrishnan"
    ],
    "published": "2024-11-25T17:57:52+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2411.16618v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2411.00140v2_chunk_0",
    "chunk_text": "ViT-LCA: A Neuromorphic Approach for Vision Transformers\n\nThe recent success of Vision Transformers has generated significant interest in attention mechanisms and transformer architectures. Although existing methods have proposed spiking self-attention mechanisms compatible with spiking neural networks, they often face challenges in effective deployment on current neuromorphic platforms. This paper introduces a novel model that combines vision transformers with the Locally Competitive Algorithm (LCA) to facilitate efficient neuromorphic deployment. Our experiments show that ViT-LCA achieves higher accuracy on ImageNet-1K dataset while consuming significantly less energy than other spiking vision transformer counterparts. Furthermore, ViT-LCA's neuromorphic-friendly design allows for more direct mapping onto current neuromorphic architectures.",
    "original_url": "http://arxiv.org/pdf/2411.00140v2",
    "original_title": "ViT-LCA: A Neuromorphic Approach for Vision Transformers",
    "source": "arxiv",
    "authors": [
      "Sanaz Mahmoodi Takaghaj"
    ],
    "published": "2024-10-31T18:41:30+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2411.00140v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2411.00140v2_chunk_1",
    "chunk_text": "Furthermore, ViT-LCA's neuromorphic-friendly design allows for more direct mapping onto current neuromorphic architectures.",
    "original_url": "http://arxiv.org/pdf/2411.00140v2",
    "original_title": "ViT-LCA: A Neuromorphic Approach for Vision Transformers",
    "source": "arxiv",
    "authors": [
      "Sanaz Mahmoodi Takaghaj"
    ],
    "published": "2024-10-31T18:41:30+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2411.00140v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2111.05498v2_chunk_0",
    "chunk_text": "Attention Approximates Sparse Distributed Memory\n\nWhile Attention has come to be an important mechanism in deep learning, there remains limited intuition for why it works so well. Here, we show that Transformer Attention can be closely related under certain data conditions to Kanerva's Sparse Distributed Memory (SDM), a biologically plausible associative memory model. We confirm that these conditions are satisfied in pre-trained GPT2 Transformer models. We discuss the implications of the Attention-SDM map and provide new computational and biological interpretations of Attention.",
    "original_url": "http://arxiv.org/pdf/2111.05498v2",
    "original_title": "Attention Approximates Sparse Distributed Memory",
    "source": "arxiv",
    "authors": [
      "Trenton Bricken",
      "Cengiz Pehlevan"
    ],
    "published": "2021-11-10T02:36:32+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2111.05498v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2410.12861v2_chunk_0",
    "chunk_text": "Scaled and Inter-token Relation Enhanced Transformer for Sample-restricted Residential NILM\n\nTransformers have demonstrated exceptional performance across various domains due to their self-attention mechanism, which captures complex relationships in data. However, training on smaller datasets poses challenges, as standard attention mechanisms can over-smooth attention scores and overly prioritize intra-token relationships, reducing the capture of meaningful inter-token dependencies critical for tasks like Non-Intrusive Load Monitoring (NILM). To address this, we propose a novel transformer architecture with two key innovations: inter-token relation enhancement and dynamic temperature tuning. The inter-token relation enhancement mechanism removes diagonal entries in the similarity matrix to improve attention focus on inter-token relations. The dynamic temperature tuning mechanism, a learnable parameter, adapts attention sharpness during training, preventing over-smoothing and enhancing sensitivity to token relationships.",
    "original_url": "http://arxiv.org/pdf/2410.12861v2",
    "original_title": "Scaled and Inter-token Relation Enhanced Transformer for Sample-restricted Residential NILM",
    "source": "arxiv",
    "authors": [
      "Minhajur Rahman",
      "Yasir Arafat"
    ],
    "published": "2024-10-12T18:58:45+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2410.12861v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2410.12861v2_chunk_1",
    "chunk_text": "The dynamic temperature tuning mechanism, a learnable parameter, adapts attention sharpness during training, preventing over-smoothing and enhancing sensitivity to token relationships. We validate our method on the REDD dataset and show that it outperforms the original transformer and state-of-the-art models by 10-15\\% in F1 score across various appliance types, demonstrating its efficacy for training on smaller datasets.",
    "original_url": "http://arxiv.org/pdf/2410.12861v2",
    "original_title": "Scaled and Inter-token Relation Enhanced Transformer for Sample-restricted Residential NILM",
    "source": "arxiv",
    "authors": [
      "Minhajur Rahman",
      "Yasir Arafat"
    ],
    "published": "2024-10-12T18:58:45+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2410.12861v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2308.07661v2_chunk_0",
    "chunk_text": "Attention Is Not All You Need Anymore\n\nIn recent years, the popular Transformer architecture has achieved great success in many application areas, including natural language processing and computer vision. Many existing works aim to reduce the computational and memory complexity of the self-attention mechanism in the Transformer by trading off performance. However, performance is key for the continuing success of the Transformer. In this paper, a family of drop-in replacements for the self-attention mechanism in the Transformer, called the Extractors, is proposed. Four types of the Extractors, namely the super high-performance Extractor (SHE), the higher-performance Extractor (HE), the worthwhile Extractor (WE), and the minimalist Extractor (ME), are proposed as examples.",
    "original_url": "http://arxiv.org/pdf/2308.07661v2",
    "original_title": "Attention Is Not All You Need Anymore",
    "source": "arxiv",
    "authors": [
      "Zhe Chen"
    ],
    "published": "2023-08-15T09:24:38+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2308.07661v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2308.07661v2_chunk_1",
    "chunk_text": "Four types of the Extractors, namely the super high-performance Extractor (SHE), the higher-performance Extractor (HE), the worthwhile Extractor (WE), and the minimalist Extractor (ME), are proposed as examples. Experimental results show that replacing the self-attention mechanism with the SHE evidently improves the performance of the Transformer, whereas the simplified versions of the SHE, i.e., the HE, the WE, and the ME, perform close to or better than the self-attention mechanism with less computational and memory complexity. Furthermore, the proposed Extractors have the potential or are able to run faster than the self-attention mechanism since their critical paths of computation are much shorter. Additionally, the sequence prediction problem in the context of text generation is formulated using variable-length discrete-time Markov chains, and the Transformer is reviewed based on our understanding.",
    "original_url": "http://arxiv.org/pdf/2308.07661v2",
    "original_title": "Attention Is Not All You Need Anymore",
    "source": "arxiv",
    "authors": [
      "Zhe Chen"
    ],
    "published": "2023-08-15T09:24:38+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2308.07661v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2403.04175v1_chunk_0",
    "chunk_text": "Understanding the PULSAR Effect in Combined Radiotherapy and Immunotherapy through Attention Mechanisms with a Transformer Model\n\nPULSAR (personalized, ultra-fractionated stereotactic adaptive radiotherapy) is the adaptation of stereotactic ablative radiotherapy towards personalized cancer management. For the first time, we applied a transformer-based attention mechanism to investigate the underlying interactions between combined PULSAR and PD-L1 blockade immunotherapy based on a murine cancer model (Lewis Lung Carcinoma, LLC). The proposed approach is able to predict the trend of tumor volume change semi-quantitatively, and excels in identifying the potential causal relationships through both self-attention and cross-attention scores.",
    "original_url": "http://arxiv.org/pdf/2403.04175v1",
    "original_title": "Understanding the PULSAR Effect in Combined Radiotherapy and Immunotherapy through Attention Mechanisms with a Transformer Model",
    "source": "arxiv",
    "authors": [
      "Hao Peng",
      "Casey Moore",
      "Debabrata Saha",
      "Steve Jiang",
      "Robert Timmerman"
    ],
    "published": "2024-03-07T03:12:31+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2403.04175v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2112.05080v2_chunk_0",
    "chunk_text": "Locally Shifted Attention With Early Global Integration\n\nRecent work has shown the potential of transformers for computer vision applications. An image is first partitioned into patches, which are then used as input tokens for the attention mechanism. Due to the expensive quadratic cost of the attention mechanism, either a large patch size is used, resulting in coarse-grained global interactions, or alternatively, attention is applied only on a local region of the image, at the expense of long-range interactions. In this work, we propose an approach that allows for both coarse global interactions and fine-grained local interactions already at early layers of a vision transformer. At the core of our method is the application of local and global attention layers.",
    "original_url": "http://arxiv.org/pdf/2112.05080v2",
    "original_title": "Locally Shifted Attention With Early Global Integration",
    "source": "arxiv",
    "authors": [
      "Shelly Sheynin",
      "Sagie Benaim",
      "Adam Polyak",
      "Lior Wolf"
    ],
    "published": "2021-12-09T18:12:24+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2112.05080v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2112.05080v2_chunk_1",
    "chunk_text": "At the core of our method is the application of local and global attention layers. In the local attention layer, we apply attention to each patch and its local shifts, resulting in virtually located local patches, which are not bound to a single, specific location. These virtually located patches are then used in a global attention layer. The separation of the attention layer into local and global counterparts allows for a low computational cost in the number of patches, while still supporting data-dependent localization already at the first layer, as opposed to the static positioning in other visual transformers. Our method is shown to be superior to both convolutional and transformer-based methods for image classification on CIFAR10, CIFAR100, and ImageNet.",
    "original_url": "http://arxiv.org/pdf/2112.05080v2",
    "original_title": "Locally Shifted Attention With Early Global Integration",
    "source": "arxiv",
    "authors": [
      "Shelly Sheynin",
      "Sagie Benaim",
      "Adam Polyak",
      "Lior Wolf"
    ],
    "published": "2021-12-09T18:12:24+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2112.05080v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2112.05080v2_chunk_2",
    "chunk_text": "Our method is shown to be superior to both convolutional and transformer-based methods for image classification on CIFAR10, CIFAR100, and ImageNet. Code is available at: https://github.com/shellysheynin/Locally-SAG-Transformer.",
    "original_url": "http://arxiv.org/pdf/2112.05080v2",
    "original_title": "Locally Shifted Attention With Early Global Integration",
    "source": "arxiv",
    "authors": [
      "Shelly Sheynin",
      "Sagie Benaim",
      "Adam Polyak",
      "Lior Wolf"
    ],
    "published": "2021-12-09T18:12:24+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2112.05080v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2504.15897v1_chunk_0",
    "chunk_text": "SUPRA: Subspace Parameterized Attention for Neural Operator on General Domains\n\nNeural operators are efficient surrogate models for solving partial differential equations (PDEs), but their key components face challenges: (1) in order to improve accuracy, attention mechanisms suffer from computational inefficiency on large-scale meshes, and (2) spectral convolutions rely on the Fast Fourier Transform (FFT) on regular grids and assume a flat geometry, which causes accuracy degradation on irregular domains. To tackle these problems, we regard the matrix-vector operations in the standard attention mechanism on vectors in Euclidean space as bilinear forms and linear operators in vector spaces and generalize the attention mechanism to function spaces. This new attention mechanism is fully equivalent to the standard attention but impossible to compute due to the infinite dimensionality of function spaces. To address this, inspired by model reduction techniques, we propose a Subspace Parameterized Attention (SUPRA) neural operator, which approximates the attention mechanism within a finite-dimensional subspace. To construct a subspace on irregular domains for SUPRA, we propose using the Laplacian eigenfunctions, which naturally adapt to domains' geometry and guarantee the optimal approximation for smooth functions.",
    "original_url": "http://arxiv.org/pdf/2504.15897v1",
    "original_title": "SUPRA: Subspace Parameterized Attention for Neural Operator on General Domains",
    "source": "arxiv",
    "authors": [
      "Zherui Yang",
      "Zhengyang Xue",
      "Ligang Liu"
    ],
    "published": "2025-04-22T13:40:04+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2504.15897v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2504.15897v1_chunk_1",
    "chunk_text": "To construct a subspace on irregular domains for SUPRA, we propose using the Laplacian eigenfunctions, which naturally adapt to domains' geometry and guarantee the optimal approximation for smooth functions. Experiments show that the SUPRA neural operator reduces error rates by up to 33% on various PDE datasets while maintaining state-of-the-art computational efficiency.",
    "original_url": "http://arxiv.org/pdf/2504.15897v1",
    "original_title": "SUPRA: Subspace Parameterized Attention for Neural Operator on General Domains",
    "source": "arxiv",
    "authors": [
      "Zherui Yang",
      "Zhengyang Xue",
      "Ligang Liu"
    ],
    "published": "2025-04-22T13:40:04+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2504.15897v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2308.12874v3_chunk_0",
    "chunk_text": "Easy attention: A simple attention mechanism for temporal predictions with transformers\n\nTo improve the robustness of transformer neural networks used for temporal-dynamics prediction of chaotic systems, we propose a novel attention mechanism called easy attention which we demonstrate in time-series reconstruction and prediction. While the standard self attention only makes use of the inner product of queries and keys, it is demonstrated that the keys, queries and softmax are not necessary for obtaining the attention score required to capture long-term dependencies in temporal sequences. Through the singular-value decomposition (SVD) on the softmax attention score, we further observe that self attention compresses the contributions from both queries and keys in the space spanned by the attention score. Therefore, our proposed easy-attention method directly treats the attention scores as learnable parameters. This approach produces excellent results when reconstructing and predicting the temporal dynamics of chaotic systems exhibiting more robustness and less complexity than self attention or the widely-used long short-term memory (LSTM) network.",
    "original_url": "http://arxiv.org/pdf/2308.12874v3",
    "original_title": "Easy attention: A simple attention mechanism for temporal predictions with transformers",
    "source": "arxiv",
    "authors": [
      "Marcial Sanchis-Agudo",
      "Yuning Wang",
      "Roger Arnau",
      "Luca Guastoni",
      "Jasmin Lim",
      "Karthik Duraisamy",
      "Ricardo Vinuesa"
    ],
    "published": "2023-08-24T15:54:32+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2308.12874v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2308.12874v3_chunk_1",
    "chunk_text": "This approach produces excellent results when reconstructing and predicting the temporal dynamics of chaotic systems exhibiting more robustness and less complexity than self attention or the widely-used long short-term memory (LSTM) network. We show the improved performance of the easy-attention method in the Lorenz system, a turbulence shear flow and a model of a nuclear reactor.",
    "original_url": "http://arxiv.org/pdf/2308.12874v3",
    "original_title": "Easy attention: A simple attention mechanism for temporal predictions with transformers",
    "source": "arxiv",
    "authors": [
      "Marcial Sanchis-Agudo",
      "Yuning Wang",
      "Roger Arnau",
      "Luca Guastoni",
      "Jasmin Lim",
      "Karthik Duraisamy",
      "Ricardo Vinuesa"
    ],
    "published": "2023-08-24T15:54:32+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2308.12874v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2208.10247v1_chunk_0",
    "chunk_text": "Generalized Attention Mechanism and Relative Position for Transformer\n\nIn this paper, we propose generalized attention mechanism (GAM) by first suggesting a new interpretation for self-attention mechanism of Vaswani et al. . Following the interpretation, we provide description for different variants of attention mechanism which together form GAM. Further, we propose a new relative position representation within the framework of GAM. This representation can be easily utilized for cases in which elements next to each other in input sequence can be at random locations in actual dataset/corpus.",
    "original_url": "http://arxiv.org/pdf/2208.10247v1",
    "original_title": "Generalized Attention Mechanism and Relative Position for Transformer",
    "source": "arxiv",
    "authors": [
      "R. V. R. Pandya"
    ],
    "published": "2022-07-24T00:57:06+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2208.10247v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2208.10247v1_chunk_1",
    "chunk_text": "This representation can be easily utilized for cases in which elements next to each other in input sequence can be at random locations in actual dataset/corpus.",
    "original_url": "http://arxiv.org/pdf/2208.10247v1",
    "original_title": "Generalized Attention Mechanism and Relative Position for Transformer",
    "source": "arxiv",
    "authors": [
      "R. V. R. Pandya"
    ],
    "published": "2022-07-24T00:57:06+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2208.10247v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2409.00346v3_chunk_0",
    "chunk_text": "SMAFormer: Synergistic Multi-Attention Transformer for Medical Image Segmentation\n\nIn medical image segmentation, specialized computer vision techniques, notably transformers grounded in attention mechanisms and residual networks employing skip connections, have been instrumental in advancing performance. Nonetheless, previous models often falter when segmenting small, irregularly shaped tumors. To this end, we introduce SMAFormer, an efficient, Transformer-based architecture that fuses multiple attention mechanisms for enhanced segmentation of small tumors and organs. SMAFormer can capture both local and global features for medical image segmentation. The architecture comprises two pivotal components.",
    "original_url": "http://arxiv.org/pdf/2409.00346v3",
    "original_title": "SMAFormer: Synergistic Multi-Attention Transformer for Medical Image Segmentation",
    "source": "arxiv",
    "authors": [
      "Fuchen Zheng",
      "Xuhang Chen",
      "Weihuang Liu",
      "Haolun Li",
      "Yingtie Lei",
      "Jiahui He",
      "Chi-Man Pun",
      "Shounjun Zhou"
    ],
    "published": "2024-08-31T04:23:33+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2409.00346v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2409.00346v3_chunk_1",
    "chunk_text": "The architecture comprises two pivotal components. First, a Synergistic Multi-Attention (SMA) Transformer block is proposed, which has the benefits of Pixel Attention, Channel Attention, and Spatial Attention for feature enrichment. Second, addressing the challenge of information loss incurred during attention mechanism transitions and feature fusion, we design a Feature Fusion Modulator. This module bolsters the integration between the channel and spatial attention by mitigating reshaping-induced information attrition. To evaluate our method, we conduct extensive experiments on various medical image segmentation tasks, including multi-organ, liver tumor, and bladder tumor segmentation, achieving state-of-the-art results.",
    "original_url": "http://arxiv.org/pdf/2409.00346v3",
    "original_title": "SMAFormer: Synergistic Multi-Attention Transformer for Medical Image Segmentation",
    "source": "arxiv",
    "authors": [
      "Fuchen Zheng",
      "Xuhang Chen",
      "Weihuang Liu",
      "Haolun Li",
      "Yingtie Lei",
      "Jiahui He",
      "Chi-Man Pun",
      "Shounjun Zhou"
    ],
    "published": "2024-08-31T04:23:33+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2409.00346v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2409.00346v3_chunk_2",
    "chunk_text": "To evaluate our method, we conduct extensive experiments on various medical image segmentation tasks, including multi-organ, liver tumor, and bladder tumor segmentation, achieving state-of-the-art results. Code and models are available at: https://github.com/CXH-Research/SMAFormer.",
    "original_url": "http://arxiv.org/pdf/2409.00346v3",
    "original_title": "SMAFormer: Synergistic Multi-Attention Transformer for Medical Image Segmentation",
    "source": "arxiv",
    "authors": [
      "Fuchen Zheng",
      "Xuhang Chen",
      "Weihuang Liu",
      "Haolun Li",
      "Yingtie Lei",
      "Jiahui He",
      "Chi-Man Pun",
      "Shounjun Zhou"
    ],
    "published": "2024-08-31T04:23:33+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2409.00346v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2107.10932v1_chunk_0",
    "chunk_text": "FNetAR: Mixing Tokens with Autoregressive Fourier Transforms\n\nIn this note we examine the autoregressive generalization of the FNet algorithm, in which self-attention layers from the standard Transformer architecture are substituted with a trivial sparse-uniformsampling procedure based on Fourier transforms. Using the Wikitext-103 benchmark, we demonstratethat FNetAR retains state-of-the-art performance (25.8 ppl) on the task of causal language modelingcompared to a Transformer-XL baseline (24.2 ppl) with only half the number self-attention layers,thus providing further evidence for the superfluity of deep neural networks with heavily compoundedattention mechanisms. The autoregressive Fourier transform could likely be used for parameterreduction on most Transformer-based time-series prediction models.",
    "original_url": "http://arxiv.org/pdf/2107.10932v1",
    "original_title": "FNetAR: Mixing Tokens with Autoregressive Fourier Transforms",
    "source": "arxiv",
    "authors": [
      "Tim Lou",
      "Michael Park",
      "Mohammad Ramezanali",
      "Vincent Tang"
    ],
    "published": "2021-07-22T21:24:02+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2107.10932v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2410.22710v1_chunk_0",
    "chunk_text": "LoFLAT: Local Feature Matching using Focused Linear Attention Transformer\n\nLocal feature matching is an essential technique in image matching and plays a critical role in a wide range of vision-based applications. However, existing Transformer-based detector-free local feature matching methods encounter challenges due to the quadratic computational complexity of attention mechanisms, especially at high resolutions. However, while existing Transformer-based detector-free local feature matching methods have reduced computational costs using linear attention mechanisms, they still struggle to capture detailed local interactions, which affects the accuracy and robustness of precise local correspondences. In order to enhance representations of attention mechanisms while preserving low computational complexity, we propose the LoFLAT, a novel Local Feature matching using Focused Linear Attention Transformer in this paper. Our LoFLAT consists of three main modules: the Feature Extraction Module, the Feature Transformer Module, and the Matching Module.",
    "original_url": "http://arxiv.org/pdf/2410.22710v1",
    "original_title": "LoFLAT: Local Feature Matching using Focused Linear Attention Transformer",
    "source": "arxiv",
    "authors": [
      "Naijian Cao",
      "Renjie He",
      "Yuchao Dai",
      "Mingyi He"
    ],
    "published": "2024-10-30T05:38:07+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2410.22710v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2410.22710v1_chunk_1",
    "chunk_text": "Our LoFLAT consists of three main modules: the Feature Extraction Module, the Feature Transformer Module, and the Matching Module. Specifically, the Feature Extraction Module firstly uses ResNet and a Feature Pyramid Network to extract hierarchical features. The Feature Transformer Module further employs the Focused Linear Attention to refine attention distribution with a focused mapping function and to enhance feature diversity with a depth-wise convolution. Finally, the Matching Module predicts accurate and robust matches through a coarse-to-fine strategy. Extensive experimental evaluations demonstrate that the proposed LoFLAT outperforms the LoFTR method in terms of both efficiency and accuracy.",
    "original_url": "http://arxiv.org/pdf/2410.22710v1",
    "original_title": "LoFLAT: Local Feature Matching using Focused Linear Attention Transformer",
    "source": "arxiv",
    "authors": [
      "Naijian Cao",
      "Renjie He",
      "Yuchao Dai",
      "Mingyi He"
    ],
    "published": "2024-10-30T05:38:07+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2410.22710v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2410.22710v1_chunk_2",
    "chunk_text": "Extensive experimental evaluations demonstrate that the proposed LoFLAT outperforms the LoFTR method in terms of both efficiency and accuracy.",
    "original_url": "http://arxiv.org/pdf/2410.22710v1",
    "original_title": "LoFLAT: Local Feature Matching using Focused Linear Attention Transformer",
    "source": "arxiv",
    "authors": [
      "Naijian Cao",
      "Renjie He",
      "Yuchao Dai",
      "Mingyi He"
    ],
    "published": "2024-10-30T05:38:07+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2410.22710v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2412.02682v1_chunk_0",
    "chunk_text": "The Asymptotic Behavior of Attention in Transformers\n\nA key component of transformers is the attention mechanism orchestrating how each token influences the propagation of every other token through a transformer. In this paper we provide a rigorous, mathematical analysis of the asymptotic properties of attention in transformers. Although we present several results based on different assumptions, all of them point to the same conclusion, all tokens asymptotically converge to each other, a phenomenon that has been empirically reported in the literature. Our findings are carefully compared with existing theoretical results and illustrated by simulations and experimental studies using the GPT-2 model.",
    "original_url": "http://arxiv.org/pdf/2412.02682v1",
    "original_title": "The Asymptotic Behavior of Attention in Transformers",
    "source": "arxiv",
    "authors": [
      "Álvaro Rodríguez Abella",
      "João Pedro Silvestre",
      "Paulo Tabuada"
    ],
    "published": "2024-12-03T18:54:49+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2412.02682v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2501.01242v1_chunk_0",
    "chunk_text": "An Efficient Attention Mechanism for Sequential Recommendation Tasks: HydraRec\n\nTransformer based models are increasingly being used in various domains including recommender systems (RS). Pretrained transformer models such as BERT have shown good performance at language modelling. With the greater ability to model sequential tasks, variants of Encoder-only models (like BERT4Rec, SASRec etc.) have found success in sequential RS problems. Computing dot-product attention in traditional transformer models has quadratic complexity in sequence length.",
    "original_url": "http://arxiv.org/pdf/2501.01242v1",
    "original_title": "An Efficient Attention Mechanism for Sequential Recommendation Tasks: HydraRec",
    "source": "arxiv",
    "authors": [
      "Uzma Mushtaque"
    ],
    "published": "2025-01-02T13:03:06+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2501.01242v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2501.01242v1_chunk_1",
    "chunk_text": "Computing dot-product attention in traditional transformer models has quadratic complexity in sequence length. This is a bigger problem with RS because unlike language models, new items are added to the catalogue every day. User buying history is a dynamic sequence which depends on multiple factors. Recently, various linear attention models have tried to solve this problem by making the model linear in sequence length (token dimensions). Hydra attention is one such linear complexity model proposed for vision transformers which reduces the complexity of attention for both the number of tokens as well as model embedding dimensions.",
    "original_url": "http://arxiv.org/pdf/2501.01242v1",
    "original_title": "An Efficient Attention Mechanism for Sequential Recommendation Tasks: HydraRec",
    "source": "arxiv",
    "authors": [
      "Uzma Mushtaque"
    ],
    "published": "2025-01-02T13:03:06+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2501.01242v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2501.01242v1_chunk_2",
    "chunk_text": "Hydra attention is one such linear complexity model proposed for vision transformers which reduces the complexity of attention for both the number of tokens as well as model embedding dimensions. Building on the idea of Hydra attention, we introduce an efficient Transformer based Sequential RS (HydraRec) which significantly improves theoretical complexity of computing attention for longer sequences and bigger datasets while preserving the temporal context. Extensive experiments are conducted to evaluate other linear transformer-based RS models and compared with HydraRec across various evaluation metrics. HydraRec outperforms other linear attention-based models as well as dot-product based attention models when used with causal masking for sequential recommendation next item prediction tasks. For bi-directional models its performance is comparable to the BERT4Rec model with an improvement in running time.",
    "original_url": "http://arxiv.org/pdf/2501.01242v1",
    "original_title": "An Efficient Attention Mechanism for Sequential Recommendation Tasks: HydraRec",
    "source": "arxiv",
    "authors": [
      "Uzma Mushtaque"
    ],
    "published": "2025-01-02T13:03:06+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2501.01242v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2501.01242v1_chunk_3",
    "chunk_text": "For bi-directional models its performance is comparable to the BERT4Rec model with an improvement in running time.",
    "original_url": "http://arxiv.org/pdf/2501.01242v1",
    "original_title": "An Efficient Attention Mechanism for Sequential Recommendation Tasks: HydraRec",
    "source": "arxiv",
    "authors": [
      "Uzma Mushtaque"
    ],
    "published": "2025-01-02T13:03:06+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2501.01242v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2209.15001v3_chunk_0",
    "chunk_text": "Dilated Neighborhood Attention Transformer\n\nTransformers are quickly becoming one of the most heavily applied deep learning architectures across modalities, domains, and tasks. In vision, on top of ongoing efforts into plain transformers, hierarchical transformers have also gained significant attention, thanks to their performance and easy integration into existing frameworks. These models typically employ localized attention mechanisms, such as the sliding-window Neighborhood Attention (NA) or Swin Transformer's Shifted Window Self Attention. While effective at reducing self attention's quadratic complexity, local attention weakens two of the most desirable properties of self attention: long range inter-dependency modeling, and global receptive field. In this paper, we introduce Dilated Neighborhood Attention (DiNA), a natural, flexible and efficient extension to NA that can capture more global context and expand receptive fields exponentially at no additional cost.",
    "original_url": "http://arxiv.org/pdf/2209.15001v3",
    "original_title": "Dilated Neighborhood Attention Transformer",
    "source": "arxiv",
    "authors": [
      "Ali Hassani",
      "Humphrey Shi"
    ],
    "published": "2022-09-29T17:57:08+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2209.15001v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2209.15001v3_chunk_1",
    "chunk_text": "In this paper, we introduce Dilated Neighborhood Attention (DiNA), a natural, flexible and efficient extension to NA that can capture more global context and expand receptive fields exponentially at no additional cost. NA's local attention and DiNA's sparse global attention complement each other, and therefore we introduce Dilated Neighborhood Attention Transformer (DiNAT), a new hierarchical vision transformer built upon both. DiNAT variants enjoy significant improvements over strong baselines such as NAT, Swin, and ConvNeXt. Our large model is faster and ahead of its Swin counterpart by 1.6% box AP in COCO object detection, 1.4% mask AP in COCO instance segmentation, and 1.4% mIoU in ADE20K semantic segmentation. Paired with new frameworks, our large variant is the new state of the art panoptic segmentation model on COCO (58.5 PQ) and ADE20K (49.4 PQ), and instance segmentation model on Cityscapes (45.1 AP) and ADE20K (35.4 AP) (no extra data).",
    "original_url": "http://arxiv.org/pdf/2209.15001v3",
    "original_title": "Dilated Neighborhood Attention Transformer",
    "source": "arxiv",
    "authors": [
      "Ali Hassani",
      "Humphrey Shi"
    ],
    "published": "2022-09-29T17:57:08+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2209.15001v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2209.15001v3_chunk_2",
    "chunk_text": "Paired with new frameworks, our large variant is the new state of the art panoptic segmentation model on COCO (58.5 PQ) and ADE20K (49.4 PQ), and instance segmentation model on Cityscapes (45.1 AP) and ADE20K (35.4 AP) (no extra data). It also matches the state of the art specialized semantic segmentation models on ADE20K (58.1 mIoU), and ranks second on Cityscapes (84.5 mIoU) (no extra data).",
    "original_url": "http://arxiv.org/pdf/2209.15001v3",
    "original_title": "Dilated Neighborhood Attention Transformer",
    "source": "arxiv",
    "authors": [
      "Ali Hassani",
      "Humphrey Shi"
    ],
    "published": "2022-09-29T17:57:08+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2209.15001v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1911.06478v1_chunk_0",
    "chunk_text": "Sequential Recommendation with Relation-Aware Kernelized Self-Attention\n\nRecent studies identified that sequential Recommendation is improved by the attention mechanism. By following this development, we propose Relation-Aware Kernelized Self-Attention (RKSA) adopting a self-attention mechanism of the Transformer with augmentation of a probabilistic model. The original self-attention of Transformer is a deterministic measure without relation-awareness. Therefore, we introduce a latent space to the self-attention, and the latent space models the recommendation context from relation as a multivariate skew-normal distribution with a kernelized covariance matrix from co-occurrences, item characteristics, and user information. This work merges the self-attention of the Transformer and the sequential recommendation by adding a probabilistic model of the recommendation task specifics.",
    "original_url": "http://arxiv.org/pdf/1911.06478v1",
    "original_title": "Sequential Recommendation with Relation-Aware Kernelized Self-Attention",
    "source": "arxiv",
    "authors": [
      "Mingi Ji",
      "Weonyoung Joo",
      "Kyungwoo Song",
      "Yoon-Yeong Kim",
      "Il-Chul Moon"
    ],
    "published": "2019-11-15T04:54:54+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1911.06478v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1911.06478v1_chunk_1",
    "chunk_text": "This work merges the self-attention of the Transformer and the sequential recommendation by adding a probabilistic model of the recommendation task specifics. We experimented RKSA over the benchmark datasets, and RKSA shows significant improvements compared to the recent baseline models. Also, RKSA were able to produce a latent space model that answers the reasons for recommendation.",
    "original_url": "http://arxiv.org/pdf/1911.06478v1",
    "original_title": "Sequential Recommendation with Relation-Aware Kernelized Self-Attention",
    "source": "arxiv",
    "authors": [
      "Mingi Ji",
      "Weonyoung Joo",
      "Kyungwoo Song",
      "Yoon-Yeong Kim",
      "Il-Chul Moon"
    ],
    "published": "2019-11-15T04:54:54+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1911.06478v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1906.05714v1_chunk_0",
    "chunk_text": "A Multiscale Visualization of Attention in the Transformer Model\n\nThe Transformer is a sequence model that forgoes traditional recurrent architectures in favor of a fully attention-based approach. Besides improving performance, an advantage of using attention is that it can also help to interpret a model by showing how the model assigns weight to different input elements. However, the multi-layer, multi-head attention mechanism in the Transformer model can be difficult to decipher. To make the model more accessible, we introduce an open-source tool that visualizes attention at multiple scales, each of which provides a unique perspective on the attention mechanism. We demonstrate the tool on BERT and OpenAI GPT-2 and present three example use cases: detecting model bias, locating relevant attention heads, and linking neurons to model behavior.",
    "original_url": "http://arxiv.org/pdf/1906.05714v1",
    "original_title": "A Multiscale Visualization of Attention in the Transformer Model",
    "source": "arxiv",
    "authors": [
      "Jesse Vig"
    ],
    "published": "2019-06-12T15:45:26+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1906.05714v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1906.05714v1_chunk_1",
    "chunk_text": "We demonstrate the tool on BERT and OpenAI GPT-2 and present three example use cases: detecting model bias, locating relevant attention heads, and linking neurons to model behavior.",
    "original_url": "http://arxiv.org/pdf/1906.05714v1",
    "original_title": "A Multiscale Visualization of Attention in the Transformer Model",
    "source": "arxiv",
    "authors": [
      "Jesse Vig"
    ],
    "published": "2019-06-12T15:45:26+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1906.05714v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2208.04946v1_chunk_0",
    "chunk_text": "Attention Hijacking in Trojan Transformers\n\nTrojan attacks pose a severe threat to AI systems. Recent works on Transformer models received explosive popularity and the self-attentions are now indisputable. This raises a central question: Can we reveal the Trojans through attention mechanisms in BERTs and ViTs? In this paper, we investigate the attention hijacking pattern in Trojan AIs, \\ie, the trigger token ``kidnaps'' the attention weights when a specific trigger is present. We observe the consistent attention hijacking pattern in Trojan Transformers from both Natural Language Processing (NLP) and Computer Vision (CV) domains.",
    "original_url": "http://arxiv.org/pdf/2208.04946v1",
    "original_title": "Attention Hijacking in Trojan Transformers",
    "source": "arxiv",
    "authors": [
      "Weimin Lyu",
      "Songzhu Zheng",
      "Tengfei Ma",
      "Haibin Ling",
      "Chao Chen"
    ],
    "published": "2022-08-09T04:05:04+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2208.04946v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2208.04946v1_chunk_1",
    "chunk_text": "We observe the consistent attention hijacking pattern in Trojan Transformers from both Natural Language Processing (NLP) and Computer Vision (CV) domains. This intriguing property helps us to understand the Trojan mechanism in BERTs and ViTs. We also propose an Attention-Hijacking Trojan Detector (AHTD) to discriminate the Trojan AIs from the clean ones.",
    "original_url": "http://arxiv.org/pdf/2208.04946v1",
    "original_title": "Attention Hijacking in Trojan Transformers",
    "source": "arxiv",
    "authors": [
      "Weimin Lyu",
      "Songzhu Zheng",
      "Tengfei Ma",
      "Haibin Ling",
      "Chao Chen"
    ],
    "published": "2022-08-09T04:05:04+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2208.04946v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2402.17512v4_chunk_0",
    "chunk_text": "Latte: Latent Attention for Linear Time Transformers\n\nThe time complexity of the standard attention mechanism in transformers scales quadratically with sequence length. We propose a probabilistic framework for attention, enabling us to derive a novel low-rank linear re-parameterisation of both bidirectional and causal cases, based on defining a latent variable model. Our method can be seamlessly integrated as a drop-in replacement for the standard attention mechanism. Additionally, this framework provides a natural extension for combining local standard attention with our global linear attention. This approach allows us to extend the context length of existing large pre-trained models with only a few additional training steps.",
    "original_url": "http://arxiv.org/pdf/2402.17512v4",
    "original_title": "Latte: Latent Attention for Linear Time Transformers",
    "source": "arxiv",
    "authors": [
      "Rares Dolga",
      "Lucas Maystre",
      "Marius Cobzarenco",
      "David Barber"
    ],
    "published": "2024-02-27T13:54:48+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2402.17512v4"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2402.17512v4_chunk_1",
    "chunk_text": "This approach allows us to extend the context length of existing large pre-trained models with only a few additional training steps. The resulting ``Latte Transformer'' achieves performance comparable to standard attention and other state-of-the-art models, while maintaining linear time and memory complexity, along with constant-time next-token prediction during inference.",
    "original_url": "http://arxiv.org/pdf/2402.17512v4",
    "original_title": "Latte: Latent Attention for Linear Time Transformers",
    "source": "arxiv",
    "authors": [
      "Rares Dolga",
      "Lucas Maystre",
      "Marius Cobzarenco",
      "David Barber"
    ],
    "published": "2024-02-27T13:54:48+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2402.17512v4"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2404.07143v2_chunk_0",
    "chunk_text": "Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention\n\nThis work introduces an efficient method to scale Transformer-based Large Language Models (LLMs) to infinitely long inputs with bounded memory and computation. A key component in our proposed approach is a new attention technique dubbed Infini-attention. The Infini-attention incorporates a compressive memory into the vanilla attention mechanism and builds in both masked local attention and long-term linear attention mechanisms in a single Transformer block. We demonstrate the effectiveness of our approach on long-context language modeling benchmarks, 1M sequence length passkey context block retrieval and 500K length book summarization tasks with 1B and 8B LLMs. Our approach introduces minimal bounded memory parameters and enables fast streaming inference for LLMs.",
    "original_url": "http://arxiv.org/pdf/2404.07143v2",
    "original_title": "Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention",
    "source": "arxiv",
    "authors": [
      "Tsendsuren Munkhdalai",
      "Manaal Faruqui",
      "Siddharth Gopal"
    ],
    "published": "2024-04-10T16:18:42+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2404.07143v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2404.07143v2_chunk_1",
    "chunk_text": "Our approach introduces minimal bounded memory parameters and enables fast streaming inference for LLMs.",
    "original_url": "http://arxiv.org/pdf/2404.07143v2",
    "original_title": "Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention",
    "source": "arxiv",
    "authors": [
      "Tsendsuren Munkhdalai",
      "Manaal Faruqui",
      "Siddharth Gopal"
    ],
    "published": "2024-04-10T16:18:42+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2404.07143v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2501.11765v1_chunk_0",
    "chunk_text": "Is logical analysis performed by transformers taking place in self-attention or in the fully connected part? Transformers architecture apply self-attention to tokens represented as vectors, before a fully connected (neuronal network) layer. These two parts can be layered many times. Traditionally, self-attention is seen as a mechanism for aggregating information before logical operations are performed by the fully connected layer. In this paper, we show, that quite counter-intuitively, the logical analysis can also be performed within the self-attention.",
    "original_url": "http://arxiv.org/pdf/2501.11765v1",
    "original_title": "Is logical analysis performed by transformers taking place in self-attention or in the fully connected part?",
    "source": "arxiv",
    "authors": [
      "Evgeniy Shin",
      "Heinrich Matzinger"
    ],
    "published": "2025-01-20T21:58:35+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2501.11765v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2501.11765v1_chunk_1",
    "chunk_text": "In this paper, we show, that quite counter-intuitively, the logical analysis can also be performed within the self-attention. For this we implement a handcrafted single-level encoder layer which performs the logical analysis within self-attention. We then study the scenario in which a one-level transformer model undergoes self-learning using gradient descent. We investigate whether the model utilizes fully connected layers or self-attention mechanisms for logical analysis when it has the choice. Given that gradient descent can become stuck at undesired zeros, we explicitly calculate these unwanted zeros and find ways to avoid them.",
    "original_url": "http://arxiv.org/pdf/2501.11765v1",
    "original_title": "Is logical analysis performed by transformers taking place in self-attention or in the fully connected part?",
    "source": "arxiv",
    "authors": [
      "Evgeniy Shin",
      "Heinrich Matzinger"
    ],
    "published": "2025-01-20T21:58:35+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2501.11765v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2501.11765v1_chunk_2",
    "chunk_text": "Given that gradient descent can become stuck at undesired zeros, we explicitly calculate these unwanted zeros and find ways to avoid them. We do all this in the context of predicting grammatical category pairs of adjacent tokens in a text. We believe that our findings have broader implications for understanding the potential logical operations performed by self-attention.",
    "original_url": "http://arxiv.org/pdf/2501.11765v1",
    "original_title": "Is logical analysis performed by transformers taking place in self-attention or in the fully connected part?",
    "source": "arxiv",
    "authors": [
      "Evgeniy Shin",
      "Heinrich Matzinger"
    ],
    "published": "2025-01-20T21:58:35+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2501.11765v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2205.03806v1_chunk_0",
    "chunk_text": "Transformer Tracking with Cyclic Shifting Window Attention\n\nTransformer architecture has been showing its great strength in visual object tracking, for its effective attention mechanism. Existing transformer-based approaches adopt the pixel-to-pixel attention strategy on flattened image features and unavoidably ignore the integrity of objects. In this paper, we propose a new transformer architecture with multi-scale cyclic shifting window attention for visual object tracking, elevating the attention from pixel to window level. The cross-window multi-scale attention has the advantage of aggregating attention at different scales and generates the best fine-scale match for the target object. Furthermore, the cyclic shifting strategy brings greater accuracy by expanding the window samples with positional information, and at the same time saves huge amounts of computational power by removing redundant calculations.",
    "original_url": "http://arxiv.org/pdf/2205.03806v1",
    "original_title": "Transformer Tracking with Cyclic Shifting Window Attention",
    "source": "arxiv",
    "authors": [
      "Zikai Song",
      "Junqing Yu",
      "Yi-Ping Phoebe Chen",
      "Wei Yang"
    ],
    "published": "2022-05-08T07:46:34+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2205.03806v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2205.03806v1_chunk_1",
    "chunk_text": "Furthermore, the cyclic shifting strategy brings greater accuracy by expanding the window samples with positional information, and at the same time saves huge amounts of computational power by removing redundant calculations. Extensive experiments demonstrate the superior performance of our method, which also sets the new state-of-the-art records on five challenging datasets, along with the VOT2020, UAV123, LaSOT, TrackingNet, and GOT-10k benchmarks.",
    "original_url": "http://arxiv.org/pdf/2205.03806v1",
    "original_title": "Transformer Tracking with Cyclic Shifting Window Attention",
    "source": "arxiv",
    "authors": [
      "Zikai Song",
      "Junqing Yu",
      "Yi-Ping Phoebe Chen",
      "Wei Yang"
    ],
    "published": "2022-05-08T07:46:34+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2205.03806v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2211.02695v2_chunk_0",
    "chunk_text": "WaveNets: Wavelet Channel Attention Networks\n\nChannel Attention reigns supreme as an effective technique in the field of computer vision. However, the proposed channel attention by SENet suffers from information loss in feature learning caused by the use of Global Average Pooling (GAP) to represent channels as scalars. Thus, designing effective channel attention mechanisms requires finding a solution to enhance features preservation in modeling channel inter-dependencies. In this work, we utilize Wavelet transform compression as a solution to the channel representation problem. We first test wavelet transform as an Auto-Encoder model equipped with conventional channel attention module.",
    "original_url": "http://arxiv.org/pdf/2211.02695v2",
    "original_title": "WaveNets: Wavelet Channel Attention Networks",
    "source": "arxiv",
    "authors": [
      "Hadi Salman",
      "Caleb Parks",
      "Shi Yin Hong",
      "Justin Zhan"
    ],
    "published": "2022-11-04T18:26:47+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2211.02695v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2211.02695v2_chunk_1",
    "chunk_text": "We first test wavelet transform as an Auto-Encoder model equipped with conventional channel attention module. Next, we test wavelet transform as a standalone channel compression method. We prove that global average pooling is equivalent to the recursive approximate Haar wavelet transform. With this proof, we generalize channel attention using Wavelet compression and name it WaveNet. Implementation of our method can be embedded within existing channel attention methods with a couple of lines of code.",
    "original_url": "http://arxiv.org/pdf/2211.02695v2",
    "original_title": "WaveNets: Wavelet Channel Attention Networks",
    "source": "arxiv",
    "authors": [
      "Hadi Salman",
      "Caleb Parks",
      "Shi Yin Hong",
      "Justin Zhan"
    ],
    "published": "2022-11-04T18:26:47+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2211.02695v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2211.02695v2_chunk_2",
    "chunk_text": "Implementation of our method can be embedded within existing channel attention methods with a couple of lines of code. We test our proposed method using ImageNet dataset for image classification task. Our method outperforms the baseline SENet, and achieves the state-of-the-art results. Our code implementation is publicly available at https://github.com/hady1011/WaveNet-C.",
    "original_url": "http://arxiv.org/pdf/2211.02695v2",
    "original_title": "WaveNets: Wavelet Channel Attention Networks",
    "source": "arxiv",
    "authors": [
      "Hadi Salman",
      "Caleb Parks",
      "Shi Yin Hong",
      "Justin Zhan"
    ],
    "published": "2022-11-04T18:26:47+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2211.02695v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2402.16790v1_chunk_0",
    "chunk_text": "Beyond Self-learned Attention: Mitigating Attention Bias in Transformer-based Models Using Attention Guidance\n\nTransformer-based models have demonstrated considerable potential for source code modeling tasks in software engineering. However, they are limited by their dependence solely on automatic self-attention weight learning mechanisms. Previous studies have shown that these models overemphasize delimiters added by tokenizers (e.g., [CLS], [SEP]), which may lead to overlooking essential information in the original input source code. To address this challenge, we introduce SyntaGuid, a novel approach that utilizes the observation that attention weights tend to be biased towards specific source code syntax tokens and abstract syntax tree (AST) elements in fine-tuned language models when they make correct predictions. SyntaGuid facilitates the guidance of attention-weight learning, leading to improved model performance on various software engineering tasks.",
    "original_url": "http://arxiv.org/pdf/2402.16790v1",
    "original_title": "Beyond Self-learned Attention: Mitigating Attention Bias in Transformer-based Models Using Attention Guidance",
    "source": "arxiv",
    "authors": [
      "Jiri Gesi",
      "Iftekhar Ahmed"
    ],
    "published": "2024-02-26T18:03:50+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2402.16790v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2402.16790v1_chunk_1",
    "chunk_text": "SyntaGuid facilitates the guidance of attention-weight learning, leading to improved model performance on various software engineering tasks. We evaluate the effectiveness of SyntaGuid on multiple tasks and demonstrate that it outperforms existing state-of-the-art models in overall performance without requiring additional data. Experimental result shows that SyntaGuid can improve overall performance up to 3.25% and fix up to 28.3% wrong predictions. Our work represents the first attempt to guide the attention of Transformer-based models towards critical source code tokens during fine-tuning, highlighting the potential for enhancing Transformer-based models in software engineering.",
    "original_url": "http://arxiv.org/pdf/2402.16790v1",
    "original_title": "Beyond Self-learned Attention: Mitigating Attention Bias in Transformer-based Models Using Attention Guidance",
    "source": "arxiv",
    "authors": [
      "Jiri Gesi",
      "Iftekhar Ahmed"
    ],
    "published": "2024-02-26T18:03:50+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2402.16790v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2503.05840v1_chunk_0",
    "chunk_text": "Slim attention: cut your context memory in half without loss of accuracy -- K-cache is all you need for MHA\n\nSlim attention shrinks the context memory size by 2x for transformer models with MHA (multi-head attention), which can speed up inference by up to 2x for large context windows. Slim attention is an exact, mathematically identical implementation of the standard attention mechanism and therefore does not compromise model accuracy. In other words, slim attention losslessly compresses the context memory by a factor of 2. For encoder-decoder transformers, the context memory size can be reduced even further: For the Whisper models for example, slim attention reduces the context memory by 8x, which can speed up token generation by 5x for batch size 64 for example. And for rare cases where the MHA projection dimension is larger than the embedding dimension, the memory can be reduced by a factor of 32 for the T5-11B model for example.",
    "original_url": "http://arxiv.org/pdf/2503.05840v1",
    "original_title": "Slim attention: cut your context memory in half without loss of accuracy -- K-cache is all you need for MHA",
    "source": "arxiv",
    "authors": [
      "Nils Graef",
      "Andrew Wasielewski"
    ],
    "published": "2025-03-07T01:44:52+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2503.05840v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2503.05840v1_chunk_1",
    "chunk_text": "And for rare cases where the MHA projection dimension is larger than the embedding dimension, the memory can be reduced by a factor of 32 for the T5-11B model for example. See https://github.com/OpenMachine-ai/transformer-tricks for code and more transformer tricks, and https://www.youtube.com/watch?v=uVtk3B6YO4Y for a video about this paper.",
    "original_url": "http://arxiv.org/pdf/2503.05840v1",
    "original_title": "Slim attention: cut your context memory in half without loss of accuracy -- K-cache is all you need for MHA",
    "source": "arxiv",
    "authors": [
      "Nils Graef",
      "Andrew Wasielewski"
    ],
    "published": "2025-03-07T01:44:52+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2503.05840v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2305.16300v2_chunk_0",
    "chunk_text": "Landmark Attention: Random-Access Infinite Context Length for Transformers\n\nWhile Transformers have shown remarkable success in natural language processing, their attention mechanism's large memory requirements have limited their ability to handle longer contexts. Prior approaches, such as recurrent memory or retrieval-based augmentation, have either compromised the random-access flexibility of attention (i.e., the capability to select any token in the entire context) or relied on separate mechanisms for relevant context retrieval, which may not be compatible with the model's attention. In this paper, we present a novel approach that allows access to the complete context while retaining random-access flexibility, closely resembling running attention on the entire context. Our method uses a landmark token to represent each block of the input and trains the attention to use it for selecting relevant blocks, enabling retrieval of blocks directly through the attention mechanism instead of by relying on a separate mechanism. Our approach seamlessly integrates with specialized data structures and the system's memory hierarchy, enabling processing of arbitrarily long context lengths.",
    "original_url": "http://arxiv.org/pdf/2305.16300v2",
    "original_title": "Landmark Attention: Random-Access Infinite Context Length for Transformers",
    "source": "arxiv",
    "authors": [
      "Amirkeivan Mohtashami",
      "Martin Jaggi"
    ],
    "published": "2023-05-25T17:53:42+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2305.16300v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2305.16300v2_chunk_1",
    "chunk_text": "Our approach seamlessly integrates with specialized data structures and the system's memory hierarchy, enabling processing of arbitrarily long context lengths. We demonstrate that our method can obtain comparable performance with Transformer-XL while significantly reducing the number of retrieved tokens in each step. Finally, we show that fine-tuning LLaMA 7B with our method successfully extends its context length capacity to over 32k tokens, allowing for inference at the context lengths of GPT-4. We release the implementation of landmark attention and the code to reproduce our experiments at https://github.com/epfml/landmark-attention/.",
    "original_url": "http://arxiv.org/pdf/2305.16300v2",
    "original_title": "Landmark Attention: Random-Access Infinite Context Length for Transformers",
    "source": "arxiv",
    "authors": [
      "Amirkeivan Mohtashami",
      "Martin Jaggi"
    ],
    "published": "2023-05-25T17:53:42+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2305.16300v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2406.17670v2_chunk_0",
    "chunk_text": "Brain Tumor Classification using Vision Transformer with Selective Cross-Attention Mechanism and Feature Calibration\n\nBrain tumor classification is a challenging task in medical image analysis. In this paper, we propose a novel approach to brain tumor classification using a vision transformer with a novel cross-attention mechanism. Our approach leverages the strengths of transformers in modeling long-range dependencies and multi-scale feature fusion. We introduce two new mechanisms to improve the performance of the cross-attention fusion module: Feature Calibration Mechanism (FCM) and Selective Cross-Attention (SCA). FCM calibrates the features from different branches to make them more compatible, while SCA selectively attends to the most informative features.",
    "original_url": "http://arxiv.org/pdf/2406.17670v2",
    "original_title": "Brain Tumor Classification using Vision Transformer with Selective Cross-Attention Mechanism and Feature Calibration",
    "source": "arxiv",
    "authors": [
      "Mohammad Ali Labbaf Khaniki",
      "Marzieh Mirzaeibonehkhater",
      "Mohammad Manthouri",
      "Elham Hasani"
    ],
    "published": "2024-06-25T15:58:56+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2406.17670v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2406.17670v2_chunk_1",
    "chunk_text": "FCM calibrates the features from different branches to make them more compatible, while SCA selectively attends to the most informative features. Our experiments demonstrate that the proposed approach outperforms other state-of-the-art methods in brain tumor classification, achieving improved accuracy and efficiency. The proposed FCM and SCA mechanisms can be easily integrated into other vision transformer architectures, making them a promising direction for future research in medical image analysis. Experimental results confirm that our approach surpasses existing methods, achieving state-of-the-art performance in brain tumor classification tasks.",
    "original_url": "http://arxiv.org/pdf/2406.17670v2",
    "original_title": "Brain Tumor Classification using Vision Transformer with Selective Cross-Attention Mechanism and Feature Calibration",
    "source": "arxiv",
    "authors": [
      "Mohammad Ali Labbaf Khaniki",
      "Marzieh Mirzaeibonehkhater",
      "Mohammad Manthouri",
      "Elham Hasani"
    ],
    "published": "2024-06-25T15:58:56+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2406.17670v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2203.15473v1_chunk_0",
    "chunk_text": "Frequency-Directional Attention Model for Multilingual Automatic Speech Recognition\n\nThis paper proposes a model for transforming speech features using the frequency-directional attention model for End-to-End (E2E) automatic speech recognition. The idea is based on the hypothesis that in the phoneme system of each language, the characteristics of the frequency bands of speech when uttering them are different. By transforming the input Mel filter bank features with an attention model that characterizes the frequency direction, a feature transformation suitable for ASR in each language can be expected. This paper introduces a Transformer-encoder as a frequency-directional attention model. We evaluated the proposed method on a multilingual E2E ASR system for six different languages and found that the proposed method could achieve, on average, 5.3 points higher accuracy than the ASR model for each language by introducing the frequency-directional attention mechanism.",
    "original_url": "http://arxiv.org/pdf/2203.15473v1",
    "original_title": "Frequency-Directional Attention Model for Multilingual Automatic Speech Recognition",
    "source": "arxiv",
    "authors": [
      "Akihiro Dobashi",
      "Chee Siang Leow",
      "Hiromitsu Nishizaki"
    ],
    "published": "2022-03-29T12:21:20+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2203.15473v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2203.15473v1_chunk_1",
    "chunk_text": "We evaluated the proposed method on a multilingual E2E ASR system for six different languages and found that the proposed method could achieve, on average, 5.3 points higher accuracy than the ASR model for each language by introducing the frequency-directional attention mechanism. Furthermore, visualization of the attention weights based on the proposed method suggested that it is possible to transform acoustic features considering the frequency characteristics of each language.",
    "original_url": "http://arxiv.org/pdf/2203.15473v1",
    "original_title": "Frequency-Directional Attention Model for Multilingual Automatic Speech Recognition",
    "source": "arxiv",
    "authors": [
      "Akihiro Dobashi",
      "Chee Siang Leow",
      "Hiromitsu Nishizaki"
    ],
    "published": "2022-03-29T12:21:20+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2203.15473v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2211.13654v2_chunk_0",
    "chunk_text": "Cross Aggregation Transformer for Image Restoration\n\nRecently, Transformer architecture has been introduced into image restoration to replace convolution neural network (CNN) with surprising results. Considering the high computational complexity of Transformer with global attention, some methods use the local square window to limit the scope of self-attention. However, these methods lack direct interaction among different windows, which limits the establishment of long-range dependencies. To address the above issue, we propose a new image restoration model, Cross Aggregation Transformer (CAT). The core of our CAT is the Rectangle-Window Self-Attention (Rwin-SA), which utilizes horizontal and vertical rectangle window attention in different heads parallelly to expand the attention area and aggregate the features cross different windows.",
    "original_url": "http://arxiv.org/pdf/2211.13654v2",
    "original_title": "Cross Aggregation Transformer for Image Restoration",
    "source": "arxiv",
    "authors": [
      "Zheng Chen",
      "Yulun Zhang",
      "Jinjin Gu",
      "Yongbing Zhang",
      "Linghe Kong",
      "Xin Yuan"
    ],
    "published": "2022-11-24T15:09:33+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2211.13654v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2211.13654v2_chunk_1",
    "chunk_text": "The core of our CAT is the Rectangle-Window Self-Attention (Rwin-SA), which utilizes horizontal and vertical rectangle window attention in different heads parallelly to expand the attention area and aggregate the features cross different windows. We also introduce the Axial-Shift operation for different window interactions. Furthermore, we propose the Locality Complementary Module to complement the self-attention mechanism, which incorporates the inductive bias of CNN (e.g., translation invariance and locality) into Transformer, enabling global-local coupling. Extensive experiments demonstrate that our CAT outperforms recent state-of-the-art methods on several image restoration applications. The code and models are available at https://github.com/zhengchen1999/CAT.",
    "original_url": "http://arxiv.org/pdf/2211.13654v2",
    "original_title": "Cross Aggregation Transformer for Image Restoration",
    "source": "arxiv",
    "authors": [
      "Zheng Chen",
      "Yulun Zhang",
      "Jinjin Gu",
      "Yongbing Zhang",
      "Linghe Kong",
      "Xin Yuan"
    ],
    "published": "2022-11-24T15:09:33+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2211.13654v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2211.13654v2_chunk_2",
    "chunk_text": "The code and models are available at https://github.com/zhengchen1999/CAT.",
    "original_url": "http://arxiv.org/pdf/2211.13654v2",
    "original_title": "Cross Aggregation Transformer for Image Restoration",
    "source": "arxiv",
    "authors": [
      "Zheng Chen",
      "Yulun Zhang",
      "Jinjin Gu",
      "Yongbing Zhang",
      "Linghe Kong",
      "Xin Yuan"
    ],
    "published": "2022-11-24T15:09:33+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2211.13654v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2211.11014v1_chunk_0",
    "chunk_text": "Understanding and Improving Knowledge Distillation for Quantization-Aware Training of Large Transformer Encoders\n\nKnowledge distillation (KD) has been a ubiquitous method for model compression to strengthen the capability of a lightweight model with the transferred knowledge from the teacher. In particular, KD has been employed in quantization-aware training (QAT) of Transformer encoders like BERT to improve the accuracy of the student model with the reduced-precision weight parameters. However, little is understood about which of the various KD approaches best fits the QAT of Transformers. In this work, we provide an in-depth analysis of the mechanism of KD on attention recovery of quantized large Transformers. In particular, we reveal that the previously adopted MSE loss on the attention score is insufficient for recovering the self-attention information.",
    "original_url": "http://arxiv.org/pdf/2211.11014v1",
    "original_title": "Understanding and Improving Knowledge Distillation for Quantization-Aware Training of Large Transformer Encoders",
    "source": "arxiv",
    "authors": [
      "Minsoo Kim",
      "Sihwa Lee",
      "Sukjin Hong",
      "Du-Seong Chang",
      "Jungwook Choi"
    ],
    "published": "2022-11-20T16:23:23+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2211.11014v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2211.11014v1_chunk_1",
    "chunk_text": "In particular, we reveal that the previously adopted MSE loss on the attention score is insufficient for recovering the self-attention information. Therefore, we propose two KD methods; attention-map and attention-output losses. Furthermore, we explore the unification of both losses to address task-dependent preference between attention-map and output losses. The experimental results on various Transformer encoder models demonstrate that the proposed KD methods achieve state-of-the-art accuracy for QAT with sub-2-bit weight quantization.",
    "original_url": "http://arxiv.org/pdf/2211.11014v1",
    "original_title": "Understanding and Improving Knowledge Distillation for Quantization-Aware Training of Large Transformer Encoders",
    "source": "arxiv",
    "authors": [
      "Minsoo Kim",
      "Sihwa Lee",
      "Sukjin Hong",
      "Du-Seong Chang",
      "Jungwook Choi"
    ],
    "published": "2022-11-20T16:23:23+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2211.11014v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2409.15375v1_chunk_0",
    "chunk_text": "DS2TA: Denoising Spiking Transformer with Attenuated Spatiotemporal Attention\n\nVision Transformers (ViT) are current high-performance models of choice for various vision applications. Recent developments have given rise to biologically inspired spiking transformers that thrive in ultra-low power operations on neuromorphic hardware, however, without fully unlocking the potential of spiking neural networks. We introduce DS2TA, a Denoising Spiking transformer with attenuated SpatioTemporal Attention, designed specifically for vision applications. DS2TA introduces a new spiking attenuated spatiotemporal attention mechanism that considers input firing correlations occurring in both time and space, thereby fully harnessing the computational power of spiking neurons at the core of the transformer architecture. Importantly, DS2TA facilitates parameter-efficient spatiotemporal attention computation without introducing extra weights.",
    "original_url": "http://arxiv.org/pdf/2409.15375v1",
    "original_title": "DS2TA: Denoising Spiking Transformer with Attenuated Spatiotemporal Attention",
    "source": "arxiv",
    "authors": [
      "Boxun Xu",
      "Hejia Geng",
      "Yuxuan Yin",
      "Peng Li"
    ],
    "published": "2024-09-20T02:26:04+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2409.15375v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2409.15375v1_chunk_1",
    "chunk_text": "Importantly, DS2TA facilitates parameter-efficient spatiotemporal attention computation without introducing extra weights. DS2TA employs efficient hashmap-based nonlinear spiking attention denoisers to enhance the robustness and expressive power of spiking attention maps. DS2TA demonstrates state-of-the-art performances on several widely adopted static image and dynamic neuromorphic datasets. Operated over 4 time steps, DS2TA achieves 94.92% top-1 accuracy on CIFAR10 and 77.47% top-1 accuracy on CIFAR100, as well as 79.1% and 94.44% on CIFAR10-DVS and DVS-Gesture using 10 time steps.",
    "original_url": "http://arxiv.org/pdf/2409.15375v1",
    "original_title": "DS2TA: Denoising Spiking Transformer with Attenuated Spatiotemporal Attention",
    "source": "arxiv",
    "authors": [
      "Boxun Xu",
      "Hejia Geng",
      "Yuxuan Yin",
      "Peng Li"
    ],
    "published": "2024-09-20T02:26:04+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2409.15375v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1612.01033v2_chunk_0",
    "chunk_text": "Areas of Attention for Image Captioning\n\nWe propose \"Areas of Attention\", a novel attention-based model for automatic image captioning. Our approach models the dependencies between image regions, caption words, and the state of an RNN language model, using three pairwise interactions. In contrast to previous attention-based approaches that associate image regions only to the RNN state, our method allows a direct association between caption words and image regions. During training these associations are inferred from image-level captions, akin to weakly-supervised object detector training. These associations help to improve captioning by localizing the corresponding regions during testing.",
    "original_url": "http://arxiv.org/pdf/1612.01033v2",
    "original_title": "Areas of Attention for Image Captioning",
    "source": "arxiv",
    "authors": [
      "Marco Pedersoli",
      "Thomas Lucas",
      "Cordelia Schmid",
      "Jakob Verbeek"
    ],
    "published": "2016-12-03T23:01:36+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1612.01033v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1612.01033v2_chunk_1",
    "chunk_text": "These associations help to improve captioning by localizing the corresponding regions during testing. We also propose and compare different ways of generating attention areas: CNN activation grids, object proposals, and spatial transformers nets applied in a convolutional fashion. Spatial transformers give the best results. They allow for image specific attention areas, and can be trained jointly with the rest of the network. Our attention mechanism and spatial transformer attention areas together yield state-of-the-art results on the MSCOCO dataset.o meaningful latent semantic structure in the generated captions.",
    "original_url": "http://arxiv.org/pdf/1612.01033v2",
    "original_title": "Areas of Attention for Image Captioning",
    "source": "arxiv",
    "authors": [
      "Marco Pedersoli",
      "Thomas Lucas",
      "Cordelia Schmid",
      "Jakob Verbeek"
    ],
    "published": "2016-12-03T23:01:36+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1612.01033v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1612.01033v2_chunk_2",
    "chunk_text": "Our attention mechanism and spatial transformer attention areas together yield state-of-the-art results on the MSCOCO dataset.o meaningful latent semantic structure in the generated captions.",
    "original_url": "http://arxiv.org/pdf/1612.01033v2",
    "original_title": "Areas of Attention for Image Captioning",
    "source": "arxiv",
    "authors": [
      "Marco Pedersoli",
      "Thomas Lucas",
      "Cordelia Schmid",
      "Jakob Verbeek"
    ],
    "published": "2016-12-03T23:01:36+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1612.01033v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2209.08726v2_chunk_0",
    "chunk_text": "Axially Expanded Windows for Local-Global Interaction in Vision Transformers\n\nRecently, Transformers have shown promising performance in various vision tasks. A challenging issue in Transformer design is that global self-attention is very expensive to compute, especially for the high-resolution vision tasks. Local self-attention performs attention computation within a local region to improve its efficiency, which leads to their receptive fields in a single attention layer are not large enough, resulting in insufficient context modeling. When observing a scene, humans usually focus on a local region while attending to non-attentional regions at coarse granularity. Based on this observation, we develop the axially expanded window self-attention mechanism that performs fine-grained self-attention within the local window and coarse-grained self-attention in the horizontal and vertical axes, and thus can effectively capturing both short- and long-range visual dependencies.",
    "original_url": "http://arxiv.org/pdf/2209.08726v2",
    "original_title": "Axially Expanded Windows for Local-Global Interaction in Vision Transformers",
    "source": "arxiv",
    "authors": [
      "Zhemin Zhang",
      "Xun Gong"
    ],
    "published": "2022-09-19T02:53:07+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2209.08726v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2209.08726v2_chunk_1",
    "chunk_text": "Based on this observation, we develop the axially expanded window self-attention mechanism that performs fine-grained self-attention within the local window and coarse-grained self-attention in the horizontal and vertical axes, and thus can effectively capturing both short- and long-range visual dependencies.",
    "original_url": "http://arxiv.org/pdf/2209.08726v2",
    "original_title": "Axially Expanded Windows for Local-Global Interaction in Vision Transformers",
    "source": "arxiv",
    "authors": [
      "Zhemin Zhang",
      "Xun Gong"
    ],
    "published": "2022-09-19T02:53:07+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2209.08726v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2102.08606v2_chunk_0",
    "chunk_text": "Centroid Transformers: Learning to Abstract with Attention\n\nSelf-attention, as the key block of transformers, is a powerful mechanism for extracting features from the inputs. In essence, what self-attention does is to infer the pairwise relations between the elements of the inputs, and modify the inputs by propagating information between input pairs. As a result, it maps inputs to N outputs and casts a quadratic $O(N^2)$ memory and time complexity. We propose centroid attention, a generalization of self-attention that maps N inputs to M outputs $(M\\leq N)$, such that the key information in the inputs are summarized in the smaller number of outputs (called centroids). We design centroid attention by amortizing the gradient descent update rule of a clustering objective function on the inputs, which reveals an underlying connection between attention and clustering.",
    "original_url": "http://arxiv.org/pdf/2102.08606v2",
    "original_title": "Centroid Transformers: Learning to Abstract with Attention",
    "source": "arxiv",
    "authors": [
      "Lemeng Wu",
      "Xingchao Liu",
      "Qiang Liu"
    ],
    "published": "2021-02-17T07:04:19+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2102.08606v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2102.08606v2_chunk_1",
    "chunk_text": "We design centroid attention by amortizing the gradient descent update rule of a clustering objective function on the inputs, which reveals an underlying connection between attention and clustering. By compressing the inputs to the centroids, we extract the key information useful for prediction and also reduce the computation of the attention module and the subsequent layers. We apply our method to various applications, including abstractive text summarization, 3D vision, and image processing. Empirical results demonstrate the effectiveness of our method over the standard transformers.",
    "original_url": "http://arxiv.org/pdf/2102.08606v2",
    "original_title": "Centroid Transformers: Learning to Abstract with Attention",
    "source": "arxiv",
    "authors": [
      "Lemeng Wu",
      "Xingchao Liu",
      "Qiang Liu"
    ],
    "published": "2021-02-17T07:04:19+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2102.08606v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2205.08078v2_chunk_0",
    "chunk_text": "Unraveling Attention via Convex Duality: Analysis and Interpretations of Vision Transformers\n\nVision transformers using self-attention or its proposed alternatives have demonstrated promising results in many image related tasks. However, the underpinning inductive bias of attention is not well understood. To address this issue, this paper analyzes attention through the lens of convex duality. For the non-linear dot-product self-attention, and alternative mechanisms such as MLP-mixer and Fourier Neural Operator (FNO), we derive equivalent finite-dimensional convex problems that are interpretable and solvable to global optimality. The convex programs lead to {\\it block nuclear-norm regularization} that promotes low rank in the latent feature and token dimensions.",
    "original_url": "http://arxiv.org/pdf/2205.08078v2",
    "original_title": "Unraveling Attention via Convex Duality: Analysis and Interpretations of Vision Transformers",
    "source": "arxiv",
    "authors": [
      "Arda Sahiner",
      "Tolga Ergen",
      "Batu Ozturkler",
      "John Pauly",
      "Morteza Mardani",
      "Mert Pilanci"
    ],
    "published": "2022-05-17T04:01:15+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2205.08078v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2205.08078v2_chunk_1",
    "chunk_text": "The convex programs lead to {\\it block nuclear-norm regularization} that promotes low rank in the latent feature and token dimensions. In particular, we show how self-attention networks implicitly clusters the tokens, based on their latent similarity. We conduct experiments for transferring a pre-trained transformer backbone for CIFAR-100 classification by fine-tuning a variety of convex attention heads. The results indicate the merits of the bias induced by attention compared with the existing MLP or linear heads.",
    "original_url": "http://arxiv.org/pdf/2205.08078v2",
    "original_title": "Unraveling Attention via Convex Duality: Analysis and Interpretations of Vision Transformers",
    "source": "arxiv",
    "authors": [
      "Arda Sahiner",
      "Tolga Ergen",
      "Batu Ozturkler",
      "John Pauly",
      "Morteza Mardani",
      "Mert Pilanci"
    ],
    "published": "2022-05-17T04:01:15+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2205.08078v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2412.09023v1_chunk_0",
    "chunk_text": "STEAM: Squeeze and Transform Enhanced Attention Module\n\nChannel and spatial attention mechanisms introduced by earlier works enhance the representation abilities of deep convolutional neural networks (CNNs) but often lead to increased parameter and computation costs. While recent approaches focus solely on efficient feature context modeling for channel attention, we aim to model both channel and spatial attention comprehensively with minimal parameters and reduced computation. Leveraging the principles of relational modeling in graphs, we introduce a constant-parameter module, STEAM: Squeeze and Transform Enhanced Attention Module, which integrates channel and spatial attention to enhance the representation power of CNNs. To our knowledge, we are the first to propose a graph-based approach for modeling both channel and spatial attention, utilizing concepts from multi-head graph transformers. Additionally, we introduce Output Guided Pooling (OGP), which efficiently captures spatial context to further enhance spatial attention.",
    "original_url": "http://arxiv.org/pdf/2412.09023v1",
    "original_title": "STEAM: Squeeze and Transform Enhanced Attention Module",
    "source": "arxiv",
    "authors": [
      "Rishabh Sabharwal",
      "Ram Samarth B B",
      "Parikshit Singh Rathore",
      "Punit Rathore"
    ],
    "published": "2024-12-12T07:38:10+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2412.09023v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2412.09023v1_chunk_1",
    "chunk_text": "Additionally, we introduce Output Guided Pooling (OGP), which efficiently captures spatial context to further enhance spatial attention. We extensively evaluate STEAM for large-scale image classification, object detection and instance segmentation on standard benchmark datasets. STEAM achieves a 2% increase in accuracy over the standard ResNet-50 model with only a meager increase in GFLOPs. Furthermore, STEAM outperforms leading modules ECA and GCT in terms of accuracy while achieving a three-fold reduction in GFLOPs.",
    "original_url": "http://arxiv.org/pdf/2412.09023v1",
    "original_title": "STEAM: Squeeze and Transform Enhanced Attention Module",
    "source": "arxiv",
    "authors": [
      "Rishabh Sabharwal",
      "Ram Samarth B B",
      "Parikshit Singh Rathore",
      "Punit Rathore"
    ],
    "published": "2024-12-12T07:38:10+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2412.09023v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2202.11402v1_chunk_0",
    "chunk_text": "A Differential Attention Fusion Model Based on Transformer for Time Series Forecasting\n\nTime series forecasting is widely used in the fields of equipment life cycle forecasting, weather forecasting, traffic flow forecasting, and other fields. Recently, some scholars have tried to apply Transformer to time series forecasting because of its powerful parallel training ability. However, the existing Transformer methods do not pay enough attention to the small time segments that play a decisive role in prediction, making it insensitive to small changes that affect the trend of time series, and it is difficult to effectively learn continuous time-dependent features. To solve this problem, we propose a differential attention fusion model based on Transformer, which designs the differential layer, neighbor attention, sliding fusion mechanism, and residual layer on the basis of classical Transformer architecture. Specifically, the differences of adjacent time points are extracted and focused by difference and neighbor attention.",
    "original_url": "http://arxiv.org/pdf/2202.11402v1",
    "original_title": "A Differential Attention Fusion Model Based on Transformer for Time Series Forecasting",
    "source": "arxiv",
    "authors": [
      "Benhan Li",
      "Shengdong Du",
      "Tianrui Li"
    ],
    "published": "2022-02-23T10:33:12+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2202.11402v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2202.11402v1_chunk_1",
    "chunk_text": "Specifically, the differences of adjacent time points are extracted and focused by difference and neighbor attention. The sliding fusion mechanism fuses various features of each time point so that the data can participate in encoding and decoding without losing important information. The residual layer including convolution and LSTM further learns the dependence between time points and enables our model to carry out deeper training. A large number of experiments on three datasets show that the prediction results produced by our method are favorably comparable to the state-of-the-art.",
    "original_url": "http://arxiv.org/pdf/2202.11402v1",
    "original_title": "A Differential Attention Fusion Model Based on Transformer for Time Series Forecasting",
    "source": "arxiv",
    "authors": [
      "Benhan Li",
      "Shengdong Du",
      "Tianrui Li"
    ],
    "published": "2022-02-23T10:33:12+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2202.11402v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2412.07220v1_chunk_0",
    "chunk_text": "Comateformer: Combined Attention Transformer for Semantic Sentence Matching\n\nThe Transformer-based model have made significant strides in semantic matching tasks by capturing connections between phrase pairs. However, to assess the relevance of sentence pairs, it is insufficient to just examine the general similarity between the sentences. It is crucial to also consider the tiny subtleties that differentiate them from each other. Regrettably, attention softmax operations in transformers tend to miss these subtle differences. To this end, in this work, we propose a novel semantic sentence matching model named Combined Attention Network based on Transformer model (Comateformer).",
    "original_url": "http://arxiv.org/pdf/2412.07220v1",
    "original_title": "Comateformer: Combined Attention Transformer for Semantic Sentence Matching",
    "source": "arxiv",
    "authors": [
      "Bo Li",
      "Di Liang",
      "Zixin Zhang"
    ],
    "published": "2024-12-10T06:18:07+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2412.07220v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2412.07220v1_chunk_1",
    "chunk_text": "To this end, in this work, we propose a novel semantic sentence matching model named Combined Attention Network based on Transformer model (Comateformer). In Comateformer model, we design a novel transformer-based quasi-attention mechanism with compositional properties. Unlike traditional attention mechanisms that merely adjust the weights of input tokens, our proposed method learns how to combine, subtract, or resize specific vectors when building a representation. Moreover, our proposed approach builds on the intuition of similarity and dissimilarity (negative affinity) when calculating dual affinity scores. This allows for a more meaningful representation of relationships between sentences.",
    "original_url": "http://arxiv.org/pdf/2412.07220v1",
    "original_title": "Comateformer: Combined Attention Transformer for Semantic Sentence Matching",
    "source": "arxiv",
    "authors": [
      "Bo Li",
      "Di Liang",
      "Zixin Zhang"
    ],
    "published": "2024-12-10T06:18:07+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2412.07220v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2412.07220v1_chunk_2",
    "chunk_text": "This allows for a more meaningful representation of relationships between sentences. To evaluate the performance of our proposed model, we conducted extensive experiments on ten public real-world datasets and robustness testing. Experimental results show that our method achieves consistent improvements.",
    "original_url": "http://arxiv.org/pdf/2412.07220v1",
    "original_title": "Comateformer: Combined Attention Transformer for Semantic Sentence Matching",
    "source": "arxiv",
    "authors": [
      "Bo Li",
      "Di Liang",
      "Zixin Zhang"
    ],
    "published": "2024-12-10T06:18:07+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2412.07220v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2305.03210v2_chunk_0",
    "chunk_text": "AttentionViz: A Global View of Transformer Attention\n\nTransformer models are revolutionizing machine learning, but their inner workings remain mysterious. In this work, we present a new visualization technique designed to help researchers understand the self-attention mechanism in transformers that allows these models to learn rich, contextual relationships between elements of a sequence. The main idea behind our method is to visualize a joint embedding of the query and key vectors used by transformer models to compute attention. Unlike previous attention visualization techniques, our approach enables the analysis of global patterns across multiple input sequences. We create an interactive visualization tool, AttentionViz (demo: http://attentionviz.com), based on these joint query-key embeddings, and use it to study attention mechanisms in both language and vision transformers.",
    "original_url": "http://arxiv.org/pdf/2305.03210v2",
    "original_title": "AttentionViz: A Global View of Transformer Attention",
    "source": "arxiv",
    "authors": [
      "Catherine Yeh",
      "Yida Chen",
      "Aoyu Wu",
      "Cynthia Chen",
      "Fernanda Viégas",
      "Martin Wattenberg"
    ],
    "published": "2023-05-04T23:46:49+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2305.03210v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2305.03210v2_chunk_1",
    "chunk_text": "We create an interactive visualization tool, AttentionViz (demo: http://attentionviz.com), based on these joint query-key embeddings, and use it to study attention mechanisms in both language and vision transformers. We demonstrate the utility of our approach in improving model understanding and offering new insights about query-key interactions through several application scenarios and expert feedback.",
    "original_url": "http://arxiv.org/pdf/2305.03210v2",
    "original_title": "AttentionViz: A Global View of Transformer Attention",
    "source": "arxiv",
    "authors": [
      "Catherine Yeh",
      "Yida Chen",
      "Aoyu Wu",
      "Cynthia Chen",
      "Fernanda Viégas",
      "Martin Wattenberg"
    ],
    "published": "2023-05-04T23:46:49+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2305.03210v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2309.12862v4_chunk_0",
    "chunk_text": "Associative Transformer\n\nEmerging from the pairwise attention in conventional Transformers, there is a growing interest in sparse attention mechanisms that align more closely with localized, contextual learning in the biological brain. Existing studies such as the Coordination method employ iterative cross-attention mechanisms with a bottleneck to enable the sparse association of inputs. However, these methods are parameter inefficient and fail in more complex relational reasoning tasks. To this end, we propose Associative Transformer (AiT) to enhance the association among sparsely attended input tokens, improving parameter efficiency and performance in various vision tasks such as classification and relational reasoning. AiT leverages a learnable explicit memory comprising specialized priors that guide bottleneck attentions to facilitate the extraction of diverse localized tokens.",
    "original_url": "http://arxiv.org/pdf/2309.12862v4",
    "original_title": "Associative Transformer",
    "source": "arxiv",
    "authors": [
      "Yuwei Sun",
      "Hideya Ochiai",
      "Zhirong Wu",
      "Stephen Lin",
      "Ryota Kanai"
    ],
    "published": "2023-09-22T13:37:10+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2309.12862v4"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2309.12862v4_chunk_1",
    "chunk_text": "AiT leverages a learnable explicit memory comprising specialized priors that guide bottleneck attentions to facilitate the extraction of diverse localized tokens. Moreover, AiT employs an associative memory-based token reconstruction using a Hopfield energy function. The extensive empirical experiments demonstrate that AiT requires significantly fewer parameters and attention layers outperforming a broad range of sparse Transformer models. Additionally, AiT outperforms the SOTA sparse Transformer models including the Coordination method on the Sort-of-CLEVR dataset.",
    "original_url": "http://arxiv.org/pdf/2309.12862v4",
    "original_title": "Associative Transformer",
    "source": "arxiv",
    "authors": [
      "Yuwei Sun",
      "Hideya Ochiai",
      "Zhirong Wu",
      "Stephen Lin",
      "Ryota Kanai"
    ],
    "published": "2023-09-22T13:37:10+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2309.12862v4"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2503.00687v2_chunk_0",
    "chunk_text": "Transformer Meets Twicing: Harnessing Unattended Residual Information\n\nTransformer-based deep learning models have achieved state-of-the-art performance across numerous language and vision tasks. While the self-attention mechanism, a core component of transformers, has proven capable of handling complex data patterns, it has been observed that the representational capacity of the attention matrix degrades significantly across transformer layers, thereby hurting its overall performance. In this work, we leverage the connection between self-attention computations and low-pass non-local means (NLM) smoothing filters and propose the Twicing Attention, a novel attention mechanism that uses kernel twicing procedure in nonparametric regression to alleviate the low-pass behavior of associated NLM smoothing with compelling theoretical guarantees and enhanced adversarial robustness. This approach enables the extraction and reuse of meaningful information retained in the residuals following the imperfect smoothing operation at each layer. Our proposed method offers two key advantages over standard self-attention: 1) a provably slower decay of representational capacity and 2) improved robustness and accuracy across various data modalities and tasks.",
    "original_url": "http://arxiv.org/pdf/2503.00687v2",
    "original_title": "Transformer Meets Twicing: Harnessing Unattended Residual Information",
    "source": "arxiv",
    "authors": [
      "Laziz Abdullaev",
      "Tan M. Nguyen"
    ],
    "published": "2025-03-02T01:56:35+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2503.00687v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2503.00687v2_chunk_1",
    "chunk_text": "Our proposed method offers two key advantages over standard self-attention: 1) a provably slower decay of representational capacity and 2) improved robustness and accuracy across various data modalities and tasks. We empirically demonstrate the performance gains of our model over baseline transformers on multiple tasks and benchmarks, including image classification and language modeling, on both clean and corrupted data.",
    "original_url": "http://arxiv.org/pdf/2503.00687v2",
    "original_title": "Transformer Meets Twicing: Harnessing Unattended Residual Information",
    "source": "arxiv",
    "authors": [
      "Laziz Abdullaev",
      "Tan M. Nguyen"
    ],
    "published": "2025-03-02T01:56:35+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2503.00687v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2106.07550v1_chunk_0",
    "chunk_text": "Attention mechanisms and deep learning for machine vision: A survey of the state of the art\n\nWith the advent of state of the art nature-inspired pure attention based models i.e. transformers, and their success in natural language processing (NLP), their extension to machine vision (MV) tasks was inevitable and much felt. Subsequently, vision transformers (ViTs) were introduced which are giving quite a challenge to the established deep learning based machine vision techniques. However, pure attention based models/architectures like transformers require huge data, large training times and large computational resources. Some recent works suggest that combinations of these two varied fields can prove to build systems which have the advantages of both these fields.",
    "original_url": "http://arxiv.org/pdf/2106.07550v1",
    "original_title": "Attention mechanisms and deep learning for machine vision: A survey of the state of the art",
    "source": "arxiv",
    "authors": [
      "Abdul Mueed Hafiz",
      "Shabir Ahmad Parah",
      "Rouf Ul Alam Bhat"
    ],
    "published": "2021-06-03T10:23:32+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2106.07550v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2106.07550v1_chunk_1",
    "chunk_text": "Some recent works suggest that combinations of these two varied fields can prove to build systems which have the advantages of both these fields. Accordingly, this state of the art survey paper is introduced which hopefully will help readers get useful information about this interesting and potential research area. A gentle introduction to attention mechanisms is given, followed by a discussion of the popular attention based deep architectures. Subsequently, the major categories of the intersection of attention mechanisms and deep learning for machine vision (MV) based are discussed. Afterwards, the major algorithms, issues and trends within the scope of the paper are discussed.",
    "original_url": "http://arxiv.org/pdf/2106.07550v1",
    "original_title": "Attention mechanisms and deep learning for machine vision: A survey of the state of the art",
    "source": "arxiv",
    "authors": [
      "Abdul Mueed Hafiz",
      "Shabir Ahmad Parah",
      "Rouf Ul Alam Bhat"
    ],
    "published": "2021-06-03T10:23:32+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2106.07550v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2106.07550v1_chunk_2",
    "chunk_text": "Afterwards, the major algorithms, issues and trends within the scope of the paper are discussed.",
    "original_url": "http://arxiv.org/pdf/2106.07550v1",
    "original_title": "Attention mechanisms and deep learning for machine vision: A survey of the state of the art",
    "source": "arxiv",
    "authors": [
      "Abdul Mueed Hafiz",
      "Shabir Ahmad Parah",
      "Rouf Ul Alam Bhat"
    ],
    "published": "2021-06-03T10:23:32+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2106.07550v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2110.02453v2_chunk_0",
    "chunk_text": "Ripple Attention for Visual Perception with Sub-quadratic Complexity\n\nTransformer architectures are now central to sequence modeling tasks. At its heart is the attention mechanism, which enables effective modeling of long-term dependencies in a sequence. Recently, transformers have been successfully applied in the computer vision domain, where 2D images are first segmented into patches and then treated as 1D sequences. Such linearization, however, impairs the notion of spatial locality in images, which bears important visual clues. To bridge the gap, we propose ripple attention, a sub-quadratic attention mechanism for vision transformers.",
    "original_url": "http://arxiv.org/pdf/2110.02453v2",
    "original_title": "Ripple Attention for Visual Perception with Sub-quadratic Complexity",
    "source": "arxiv",
    "authors": [
      "Lin Zheng",
      "Huijie Pan",
      "Lingpeng Kong"
    ],
    "published": "2021-10-06T02:00:38+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2110.02453v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2110.02453v2_chunk_1",
    "chunk_text": "To bridge the gap, we propose ripple attention, a sub-quadratic attention mechanism for vision transformers. Built upon the recent kernel-based efficient attention mechanisms, we design a novel dynamic programming algorithm that weights contributions of different tokens to a query with respect to their relative spatial distances in the 2D space in linear observed time. Extensive experiments and analyses demonstrate the effectiveness of ripple attention on various visual tasks.",
    "original_url": "http://arxiv.org/pdf/2110.02453v2",
    "original_title": "Ripple Attention for Visual Perception with Sub-quadratic Complexity",
    "source": "arxiv",
    "authors": [
      "Lin Zheng",
      "Huijie Pan",
      "Lingpeng Kong"
    ],
    "published": "2021-10-06T02:00:38+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2110.02453v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2106.01540v2_chunk_0",
    "chunk_text": "Luna: Linear Unified Nested Attention\n\nThe quadratic computational and memory complexities of the Transformer's attention mechanism have limited its scalability for modeling long sequences. In this paper, we propose Luna, a linear unified nested attention mechanism that approximates softmax attention with two nested linear attention functions, yielding only linear (as opposed to quadratic) time and space complexity. Specifically, with the first attention function, Luna packs the input sequence into a sequence of fixed length. Then, the packed sequence is unpacked using the second attention function. As compared to a more traditional attention mechanism, Luna introduces an additional sequence with a fixed length as input and an additional corresponding output, which allows Luna to perform attention operation linearly, while also storing adequate contextual information.",
    "original_url": "http://arxiv.org/pdf/2106.01540v2",
    "original_title": "Luna: Linear Unified Nested Attention",
    "source": "arxiv",
    "authors": [
      "Xuezhe Ma",
      "Xiang Kong",
      "Sinong Wang",
      "Chunting Zhou",
      "Jonathan May",
      "Hao Ma",
      "Luke Zettlemoyer"
    ],
    "published": "2021-06-03T01:47:26+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2106.01540v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2106.01540v2_chunk_1",
    "chunk_text": "As compared to a more traditional attention mechanism, Luna introduces an additional sequence with a fixed length as input and an additional corresponding output, which allows Luna to perform attention operation linearly, while also storing adequate contextual information. We perform extensive evaluations on three benchmarks of sequence modeling tasks: long-context sequence modeling, neural machine translation and masked language modeling for large-scale pretraining. Competitive or even better experimental results demonstrate both the effectiveness and efficiency of Luna compared to a variety",
    "original_url": "http://arxiv.org/pdf/2106.01540v2",
    "original_title": "Luna: Linear Unified Nested Attention",
    "source": "arxiv",
    "authors": [
      "Xuezhe Ma",
      "Xiang Kong",
      "Sinong Wang",
      "Chunting Zhou",
      "Jonathan May",
      "Hao Ma",
      "Luke Zettlemoyer"
    ],
    "published": "2021-06-03T01:47:26+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2106.01540v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2212.06267v1_chunk_0",
    "chunk_text": "Mortality Prediction Models with Clinical Notes Using Sparse Attention at the Word and Sentence Levels\n\nIntensive Care in-hospital mortality prediction has various clinical applications. Neural prediction models, especially when capitalising on clinical notes, have been put forward as improvement on currently existing models. However, to be acceptable these models should be performant and transparent. This work studies different attention mechanisms for clinical neural prediction models in terms of their discrimination and calibration. Specifically, we investigate sparse attention as an alternative to dense attention weights in the task of in-hospital mortality prediction from clinical notes.",
    "original_url": "http://arxiv.org/pdf/2212.06267v1",
    "original_title": "Mortality Prediction Models with Clinical Notes Using Sparse Attention at the Word and Sentence Levels",
    "source": "arxiv",
    "authors": [
      "Miguel Rios",
      "Ameen Abu-Hanna"
    ],
    "published": "2022-12-12T22:08:45+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2212.06267v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2212.06267v1_chunk_1",
    "chunk_text": "Specifically, we investigate sparse attention as an alternative to dense attention weights in the task of in-hospital mortality prediction from clinical notes. We evaluate the attention mechanisms based on: i) local self-attention over words in a sentence, and ii) global self-attention with a transformer architecture across sentences. We demonstrate that the sparse mechanism approach outperforms the dense one for the local self-attention in terms of predictive performance with a publicly available dataset, and puts higher attention to prespecified relevant directive words. The performance at the sentence level, however, deteriorates as sentences including the influential directive words tend to be dropped all together.",
    "original_url": "http://arxiv.org/pdf/2212.06267v1",
    "original_title": "Mortality Prediction Models with Clinical Notes Using Sparse Attention at the Word and Sentence Levels",
    "source": "arxiv",
    "authors": [
      "Miguel Rios",
      "Ameen Abu-Hanna"
    ],
    "published": "2022-12-12T22:08:45+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2212.06267v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2012.02030v3_chunk_0",
    "chunk_text": "Data-Informed Global Sparseness in Attention Mechanisms for Deep Neural Networks\n\nAttention mechanisms play a crucial role in the neural revolution of Natural Language Processing (NLP). With the growth of attention-based models, several pruning techniques have been developed to identify and exploit sparseness, making these models more efficient. Most efforts focus on hard-coding attention patterns or pruning attention weights based on training data. We propose Attention Pruning (AP), a framework that observes attention patterns in a fixed dataset and generates a global sparseness mask. AP saves 90% of attention computation for language modeling and about 50% for machine translation and GLUE tasks, maintaining result quality.",
    "original_url": "http://arxiv.org/pdf/2012.02030v3",
    "original_title": "Data-Informed Global Sparseness in Attention Mechanisms for Deep Neural Networks",
    "source": "arxiv",
    "authors": [
      "Ileana Rugina",
      "Rumen Dangovski",
      "Li Jing",
      "Preslav Nakov",
      "Marin Soljačić"
    ],
    "published": "2020-11-20T13:58:21+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2012.02030v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2012.02030v3_chunk_1",
    "chunk_text": "AP saves 90% of attention computation for language modeling and about 50% for machine translation and GLUE tasks, maintaining result quality. Our method reveals important distinctions between self- and cross-attention patterns, guiding future NLP research. Our framework can reduce both latency and memory requirements for any attention-based model, aiding in the development of improved models for existing or new NLP applications. We have demonstrated this with encoder and autoregressive transformer models using Triton GPU kernels and make our code publicly available at https://github.com/irugina/AP.",
    "original_url": "http://arxiv.org/pdf/2012.02030v3",
    "original_title": "Data-Informed Global Sparseness in Attention Mechanisms for Deep Neural Networks",
    "source": "arxiv",
    "authors": [
      "Ileana Rugina",
      "Rumen Dangovski",
      "Li Jing",
      "Preslav Nakov",
      "Marin Soljačić"
    ],
    "published": "2020-11-20T13:58:21+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2012.02030v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2006.04768v3_chunk_0",
    "chunk_text": "Linformer: Self-Attention with Linear Complexity\n\nLarge transformer models have shown extraordinary success in achieving state-of-the-art results in many natural language processing applications. However, training and deploying these models can be prohibitively costly for long sequences, as the standard self-attention mechanism of the Transformer uses $O(n^2)$ time and space with respect to sequence length. In this paper, we demonstrate that the self-attention mechanism can be approximated by a low-rank matrix. We further exploit this finding to propose a new self-attention mechanism, which reduces the overall self-attention complexity from $O(n^2)$ to $O(n)$ in both time and space. The resulting linear transformer, the \\textit{Linformer}, performs on par with standard Transformer models, while being much more memory- and time-efficient.",
    "original_url": "http://arxiv.org/pdf/2006.04768v3",
    "original_title": "Linformer: Self-Attention with Linear Complexity",
    "source": "arxiv",
    "authors": [
      "Sinong Wang",
      "Belinda Z. Li",
      "Madian Khabsa",
      "Han Fang",
      "Hao Ma"
    ],
    "published": "2020-06-08T17:37:52+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2006.04768v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2006.04768v3_chunk_1",
    "chunk_text": "The resulting linear transformer, the \\textit{Linformer}, performs on par with standard Transformer models, while being much more memory- and time-efficient.",
    "original_url": "http://arxiv.org/pdf/2006.04768v3",
    "original_title": "Linformer: Self-Attention with Linear Complexity",
    "source": "arxiv",
    "authors": [
      "Sinong Wang",
      "Belinda Z. Li",
      "Madian Khabsa",
      "Han Fang",
      "Hao Ma"
    ],
    "published": "2020-06-08T17:37:52+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2006.04768v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2202.02093v2_chunk_0",
    "chunk_text": "Temporal Attention for Language Models\n\nPretrained language models based on the transformer architecture have shown great success in NLP. Textual training data often comes from the web and is thus tagged with time-specific information, but most language models ignore this information. They are trained on the textual data alone, limiting their ability to generalize temporally. In this work, we extend the key component of the transformer architecture, i.e., the self-attention mechanism, and propose temporal attention - a time-aware self-attention mechanism. Temporal attention can be applied to any transformer model and requires the input texts to be accompanied with their relevant time points.",
    "original_url": "http://arxiv.org/pdf/2202.02093v2",
    "original_title": "Temporal Attention for Language Models",
    "source": "arxiv",
    "authors": [
      "Guy D. Rosin",
      "Kira Radinsky"
    ],
    "published": "2022-02-04T11:55:34+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2202.02093v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2202.02093v2_chunk_1",
    "chunk_text": "Temporal attention can be applied to any transformer model and requires the input texts to be accompanied with their relevant time points. It allows the transformer to capture this temporal information and create time-specific contextualized word representations. We leverage these representations for the task of semantic change detection; we apply our proposed mechanism to BERT and experiment on three datasets in different languages (English, German, and Latin) that also vary in time, size, and genre. Our proposed model achieves state-of-the-art results on all the datasets.",
    "original_url": "http://arxiv.org/pdf/2202.02093v2",
    "original_title": "Temporal Attention for Language Models",
    "source": "arxiv",
    "authors": [
      "Guy D. Rosin",
      "Kira Radinsky"
    ],
    "published": "2022-02-04T11:55:34+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2202.02093v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2311.10642v4_chunk_0",
    "chunk_text": "Rethinking Attention: Exploring Shallow Feed-Forward Neural Networks as an Alternative to Attention Layers in Transformers\n\nThis work presents an analysis of the effectiveness of using standard shallow feed-forward networks to mimic the behavior of the attention mechanism in the original Transformer model, a state-of-the-art architecture for sequence-to-sequence tasks. We substitute key elements of the attention mechanism in the Transformer with simple feed-forward networks, trained using the original components via knowledge distillation. Our experiments, conducted on the IWSLT2017 dataset, reveal the capacity of these \"attentionless Transformers\" to rival the performance of the original architecture. Through rigorous ablation studies, and experimenting with various replacement network types and sizes, we offer insights that support the viability of our approach. This not only sheds light on the adaptability of shallow feed-forward networks in emulating attention mechanisms but also underscores their potential to streamline complex architectures for sequence-to-sequence tasks.",
    "original_url": "http://arxiv.org/pdf/2311.10642v4",
    "original_title": "Rethinking Attention: Exploring Shallow Feed-Forward Neural Networks as an Alternative to Attention Layers in Transformers",
    "source": "arxiv",
    "authors": [
      "Vukasin Bozic",
      "Danilo Dordevic",
      "Daniele Coppola",
      "Joseph Thommes",
      "Sidak Pal Singh"
    ],
    "published": "2023-11-17T16:58:52+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2311.10642v4"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2311.10642v4_chunk_1",
    "chunk_text": "This not only sheds light on the adaptability of shallow feed-forward networks in emulating attention mechanisms but also underscores their potential to streamline complex architectures for sequence-to-sequence tasks.",
    "original_url": "http://arxiv.org/pdf/2311.10642v4",
    "original_title": "Rethinking Attention: Exploring Shallow Feed-Forward Neural Networks as an Alternative to Attention Layers in Transformers",
    "source": "arxiv",
    "authors": [
      "Vukasin Bozic",
      "Danilo Dordevic",
      "Daniele Coppola",
      "Joseph Thommes",
      "Sidak Pal Singh"
    ],
    "published": "2023-11-17T16:58:52+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2311.10642v4"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2106.03959v4_chunk_0",
    "chunk_text": "Generative Flows with Invertible Attentions\n\nFlow-based generative models have shown an excellent ability to explicitly learn the probability density function of data via a sequence of invertible transformations. Yet, learning attentions in generative flows remains understudied, while it has made breakthroughs in other domains. To fill the gap, this paper introduces two types of invertible attention mechanisms, i.e., map-based and transformer-based attentions, for both unconditional and conditional generative flows. The key idea is to exploit a masked scheme of these two attentions to learn long-range data dependencies in the context of generative flows. The masked scheme allows for invertible attention modules with tractable Jacobian determinants, enabling its seamless integration at any positions of the flow-based models.",
    "original_url": "http://arxiv.org/pdf/2106.03959v4",
    "original_title": "Generative Flows with Invertible Attentions",
    "source": "arxiv",
    "authors": [
      "Rhea Sanjay Sukthanker",
      "Zhiwu Huang",
      "Suryansh Kumar",
      "Radu Timofte",
      "Luc Van Gool"
    ],
    "published": "2021-06-07T20:43:04+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2106.03959v4"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2106.03959v4_chunk_1",
    "chunk_text": "The masked scheme allows for invertible attention modules with tractable Jacobian determinants, enabling its seamless integration at any positions of the flow-based models. The proposed attention mechanisms lead to more efficient generative flows, due to their capability of modeling the long-term data dependencies. Evaluation on multiple image synthesis tasks shows that the proposed attention flows result in efficient models and compare favorably against the state-of-the-art unconditional and conditional generative flows.",
    "original_url": "http://arxiv.org/pdf/2106.03959v4",
    "original_title": "Generative Flows with Invertible Attentions",
    "source": "arxiv",
    "authors": [
      "Rhea Sanjay Sukthanker",
      "Zhiwu Huang",
      "Suryansh Kumar",
      "Radu Timofte",
      "Luc Van Gool"
    ],
    "published": "2021-06-07T20:43:04+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2106.03959v4"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2201.12854v1_chunk_0",
    "chunk_text": "Fast Monte-Carlo Approximation of the Attention Mechanism\n\nWe introduce Monte-Carlo Attention (MCA), a randomized approximation method for reducing the computational cost of self-attention mechanisms in Transformer architectures. MCA exploits the fact that the importance of each token in an input sequence varies with respect to their attention scores; thus, some degree of error can be tolerable when encoding tokens with low attention. Using approximate matrix multiplication, MCA applies different error bounds to encode input tokens such that those with low attention scores are computed with relaxed precision, whereas errors of salient elements are minimized. MCA can operate in parallel with other attention optimization schemes and does not require model modification. We study the theoretical error bounds and demonstrate that MCA reduces attention complexity (in FLOPS) for various Transformer models by up to 11$\\times$ in GLUE benchmarks without compromising model accuracy.",
    "original_url": "http://arxiv.org/pdf/2201.12854v1",
    "original_title": "Fast Monte-Carlo Approximation of the Attention Mechanism",
    "source": "arxiv",
    "authors": [
      "Hyunjun Kim",
      "JeongGil Ko"
    ],
    "published": "2022-01-30T16:02:35+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2201.12854v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2201.12854v1_chunk_1",
    "chunk_text": "We study the theoretical error bounds and demonstrate that MCA reduces attention complexity (in FLOPS) for various Transformer models by up to 11$\\times$ in GLUE benchmarks without compromising model accuracy.",
    "original_url": "http://arxiv.org/pdf/2201.12854v1",
    "original_title": "Fast Monte-Carlo Approximation of the Attention Mechanism",
    "source": "arxiv",
    "authors": [
      "Hyunjun Kim",
      "JeongGil Ko"
    ],
    "published": "2022-01-30T16:02:35+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2201.12854v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2310.16270v1_chunk_0",
    "chunk_text": "Attention Lens: A Tool for Mechanistically Interpreting the Attention Head Information Retrieval Mechanism\n\nTransformer-based Large Language Models (LLMs) are the state-of-the-art for natural language tasks. Recent work has attempted to decode, by reverse engineering the role of linear layers, the internal mechanisms by which LLMs arrive at their final predictions for text completion tasks. Yet little is known about the specific role of attention heads in producing the final token prediction. We propose Attention Lens, a tool that enables researchers to translate the outputs of attention heads into vocabulary tokens via learned attention-head-specific transformations called lenses. Preliminary findings from our trained lenses indicate that attention heads play highly specialized roles in language models.",
    "original_url": "http://arxiv.org/pdf/2310.16270v1",
    "original_title": "Attention Lens: A Tool for Mechanistically Interpreting the Attention Head Information Retrieval Mechanism",
    "source": "arxiv",
    "authors": [
      "Mansi Sakarvadia",
      "Arham Khan",
      "Aswathy Ajith",
      "Daniel Grzenda",
      "Nathaniel Hudson",
      "André Bauer",
      "Kyle Chard",
      "Ian Foster"
    ],
    "published": "2023-10-25T01:03:35+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2310.16270v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2310.16270v1_chunk_1",
    "chunk_text": "Preliminary findings from our trained lenses indicate that attention heads play highly specialized roles in language models. The code for Attention Lens is available at github.com/msakarvadia/AttentionLens.",
    "original_url": "http://arxiv.org/pdf/2310.16270v1",
    "original_title": "Attention Lens: A Tool for Mechanistically Interpreting the Attention Head Information Retrieval Mechanism",
    "source": "arxiv",
    "authors": [
      "Mansi Sakarvadia",
      "Arham Khan",
      "Aswathy Ajith",
      "Daniel Grzenda",
      "Nathaniel Hudson",
      "André Bauer",
      "Kyle Chard",
      "Ian Foster"
    ],
    "published": "2023-10-25T01:03:35+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2310.16270v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2407.10855v1_chunk_0",
    "chunk_text": "Weighted Grouped Query Attention in Transformers\n\nThe attention mechanism forms the foundational blocks for transformer language models. Recent approaches show that scaling the model achieves human-level performance. However, with increasing demands for scaling and constraints on hardware memory, the inference costs of these models remain high. To reduce the inference time, Multi-Query Attention (MQA) and Grouped-Query Attention (GQA) were proposed in (Shazeer, 2019) and (Ainslieet al., 2023) respectively. In this paper, we propose a variation of Grouped-Query Attention, termed Weighted Grouped-Query Attention (WGQA).",
    "original_url": "http://arxiv.org/pdf/2407.10855v1",
    "original_title": "Weighted Grouped Query Attention in Transformers",
    "source": "arxiv",
    "authors": [
      "Sai Sena Chinnakonduru",
      "Astarag Mohapatra"
    ],
    "published": "2024-07-15T16:07:13+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2407.10855v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2407.10855v1_chunk_1",
    "chunk_text": "In this paper, we propose a variation of Grouped-Query Attention, termed Weighted Grouped-Query Attention (WGQA). We introduced new learnable parameters for each key and value head in the T5 decoder attention blocks, enabling the model to take a weighted average during finetuning. Our model achieves an average of 0.53% improvement over GQA, and the performance converges to traditional Multi-head attention (MHA) with no additional overhead during inference. We evaluated the introduction of these parameters and subsequent finetuning informs the model about the grouping mechanism during training, thereby enhancing performance. Additionally, we demonstrate the scaling laws in our analysis by comparing the results between T5-small and T5-base architecture.",
    "original_url": "http://arxiv.org/pdf/2407.10855v1",
    "original_title": "Weighted Grouped Query Attention in Transformers",
    "source": "arxiv",
    "authors": [
      "Sai Sena Chinnakonduru",
      "Astarag Mohapatra"
    ],
    "published": "2024-07-15T16:07:13+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2407.10855v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2407.10855v1_chunk_2",
    "chunk_text": "Additionally, we demonstrate the scaling laws in our analysis by comparing the results between T5-small and T5-base architecture.",
    "original_url": "http://arxiv.org/pdf/2407.10855v1",
    "original_title": "Weighted Grouped Query Attention in Transformers",
    "source": "arxiv",
    "authors": [
      "Sai Sena Chinnakonduru",
      "Astarag Mohapatra"
    ],
    "published": "2024-07-15T16:07:13+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2407.10855v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2407.16153v1_chunk_0",
    "chunk_text": "On the Benefits of Rank in Attention Layers\n\nAttention-based mechanisms are widely used in machine learning, most prominently in transformers. However, hyperparameters such as the rank of the attention matrices and the number of heads are scaled nearly the same way in all realizations of this architecture, without theoretical justification. In this work we show that there are dramatic trade-offs between the rank and number of heads of the attention mechanism. Specifically, we present a simple and natural target function that can be represented using a single full-rank attention head for any context length, but that cannot be approximated by low-rank attention unless the number of heads is exponential in the embedding dimension, even for short context lengths. Moreover, we prove that, for short context lengths, adding depth allows the target to be approximated by low-rank attention.",
    "original_url": "http://arxiv.org/pdf/2407.16153v1",
    "original_title": "On the Benefits of Rank in Attention Layers",
    "source": "arxiv",
    "authors": [
      "Noah Amsel",
      "Gilad Yehudai",
      "Joan Bruna"
    ],
    "published": "2024-07-23T03:40:24+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2407.16153v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2407.16153v1_chunk_1",
    "chunk_text": "Moreover, we prove that, for short context lengths, adding depth allows the target to be approximated by low-rank attention. For long contexts, we conjecture that full-rank attention is necessary. Finally, we present experiments with off-the-shelf transformers that validate our theoretical findings.",
    "original_url": "http://arxiv.org/pdf/2407.16153v1",
    "original_title": "On the Benefits of Rank in Attention Layers",
    "source": "arxiv",
    "authors": [
      "Noah Amsel",
      "Gilad Yehudai",
      "Joan Bruna"
    ],
    "published": "2024-07-23T03:40:24+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2407.16153v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1905.02719v2_chunk_0",
    "chunk_text": "Intentional Attention Mask Transformation for Robust CNN Classification\n\nConvolutional Neural Networks have achieved impressive results in various tasks, but interpreting the internal mechanism is a challenging problem. To tackle this problem, we exploit a multi-channel attention mechanism in feature space. Our network architecture allows us to obtain an attention mask for each feature while existing CNN visualization methods provide only a common attention mask for all features. We apply the proposed multi-channel attention mechanism to multi-attribute recognition task. We can obtain different attention mask for each feature and for each attribute.",
    "original_url": "http://arxiv.org/pdf/1905.02719v2",
    "original_title": "Intentional Attention Mask Transformation for Robust CNN Classification",
    "source": "arxiv",
    "authors": [
      "Masanari Kimura",
      "Masayuki Tanaka"
    ],
    "published": "2019-05-07T08:16:46+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1905.02719v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1905.02719v2_chunk_1",
    "chunk_text": "We can obtain different attention mask for each feature and for each attribute. Those analyses give us deeper insight into the feature space of CNNs. Furthermore, our proposed attention mechanism naturally derives a method for improving the robustness of CNNs. From the observation of feature space based on the proposed attention mask, we demonstrate that we can obtain robust CNNs by intentionally emphasizing features that are important for attributes. The experimental results for the benchmark dataset show that the proposed method gives high human interpretability while accurately grasping the attributes of the data, and improves network robustness.",
    "original_url": "http://arxiv.org/pdf/1905.02719v2",
    "original_title": "Intentional Attention Mask Transformation for Robust CNN Classification",
    "source": "arxiv",
    "authors": [
      "Masanari Kimura",
      "Masayuki Tanaka"
    ],
    "published": "2019-05-07T08:16:46+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1905.02719v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1905.02719v2_chunk_2",
    "chunk_text": "The experimental results for the benchmark dataset show that the proposed method gives high human interpretability while accurately grasping the attributes of the data, and improves network robustness.",
    "original_url": "http://arxiv.org/pdf/1905.02719v2",
    "original_title": "Intentional Attention Mask Transformation for Robust CNN Classification",
    "source": "arxiv",
    "authors": [
      "Masanari Kimura",
      "Masayuki Tanaka"
    ],
    "published": "2019-05-07T08:16:46+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1905.02719v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2503.00226v2_chunk_0",
    "chunk_text": "Spiking Transformer:Introducing Accurate Addition-Only Spiking Self-Attention for Transformer\n\nTransformers have demonstrated outstanding performance across a wide range of tasks, owing to their self-attention mechanism, but they are highly energy-consuming. Spiking Neural Networks have emerged as a promising energy-efficient alternative to traditional Artificial Neural Networks, leveraging event-driven computation and binary spikes for information transfer. The combination of Transformers' capabilities with the energy efficiency of SNNs offers a compelling opportunity. This paper addresses the challenge of adapting the self-attention mechanism of Transformers to the spiking paradigm by introducing a novel approach: Accurate Addition-Only Spiking Self-Attention (A$^2$OS$^2$A). Unlike existing methods that rely solely on binary spiking neurons for all components of the self-attention mechanism, our approach integrates binary, ReLU, and ternary spiking neurons.",
    "original_url": "http://arxiv.org/pdf/2503.00226v2",
    "original_title": "Spiking Transformer:Introducing Accurate Addition-Only Spiking Self-Attention for Transformer",
    "source": "arxiv",
    "authors": [
      "Yufei Guo",
      "Xiaode Liu",
      "Yuanpei Chen",
      "Weihang Peng",
      "Yuhan Zhang",
      "Zhe Ma"
    ],
    "published": "2025-02-28T22:23:29+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2503.00226v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2503.00226v2_chunk_1",
    "chunk_text": "Unlike existing methods that rely solely on binary spiking neurons for all components of the self-attention mechanism, our approach integrates binary, ReLU, and ternary spiking neurons. This hybrid strategy significantly improves accuracy while preserving non-multiplicative computations. Moreover, our method eliminates the need for softmax and scaling operations. Extensive experiments show that the A$^2$OS$^2$A-based Spiking Transformer outperforms existing SNN-based Transformers on several datasets, even achieving an accuracy of 78.66\\% on ImageNet-1K. Our work represents a significant advancement in SNN-based Transformer models, offering a more accurate and efficient solution for real-world applications.",
    "original_url": "http://arxiv.org/pdf/2503.00226v2",
    "original_title": "Spiking Transformer:Introducing Accurate Addition-Only Spiking Self-Attention for Transformer",
    "source": "arxiv",
    "authors": [
      "Yufei Guo",
      "Xiaode Liu",
      "Yuanpei Chen",
      "Weihang Peng",
      "Yuhan Zhang",
      "Zhe Ma"
    ],
    "published": "2025-02-28T22:23:29+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2503.00226v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2503.00226v2_chunk_2",
    "chunk_text": "Our work represents a significant advancement in SNN-based Transformer models, offering a more accurate and efficient solution for real-world applications.",
    "original_url": "http://arxiv.org/pdf/2503.00226v2",
    "original_title": "Spiking Transformer:Introducing Accurate Addition-Only Spiking Self-Attention for Transformer",
    "source": "arxiv",
    "authors": [
      "Yufei Guo",
      "Xiaode Liu",
      "Yuanpei Chen",
      "Weihang Peng",
      "Yuhan Zhang",
      "Zhe Ma"
    ],
    "published": "2025-02-28T22:23:29+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2503.00226v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2502.11178v1_chunk_0",
    "chunk_text": "DAViMNet: SSMs-Based Domain Adaptive Object Detection\n\nUnsupervised domain adaptation (UDA) for object detection adapts models trained on labeled source domains to unlabeled target domains, ensuring robust performance across domain shifts. Transformer-based architectures excel at capturing long-range dependencies but face efficiency challenges due to their quadratic attention complexity, which limits scalability in UDA tasks. To address these issues, we propose a hybrid domain-adaptive Mamba Transformer architecture that combines Mamba's efficient state-space modeling with attention mechanisms to tackle domain-specific spatial and channel-wise variations. Each hybrid block integrates domain-adaptive Mamba blocks and attention mechanisms: Domain-Adaptive Mamba employs spatial and channel state-space models to adaptively model domain variations, while attention mechanisms leverage self-attention for intra-domain feature enhancement and cross-attention for effective source-target alignment. Our approach processes both shallow and deeper features, employing an entropy-based knowledge distillation framework with margin ReLU to emphasize discriminative features and suppress noise.",
    "original_url": "http://arxiv.org/pdf/2502.11178v1",
    "original_title": "DAViMNet: SSMs-Based Domain Adaptive Object Detection",
    "source": "arxiv",
    "authors": [
      "A. Enes Doruk",
      "Hasan F. Ates"
    ],
    "published": "2025-02-16T15:58:54+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2502.11178v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2502.11178v1_chunk_1",
    "chunk_text": "Our approach processes both shallow and deeper features, employing an entropy-based knowledge distillation framework with margin ReLU to emphasize discriminative features and suppress noise. Gradient Reversal Layers enable adversarial alignment across network layers, while entropy-driven gating attention with random perturbations refines target features and mitigates overfitting. By unifying these components, our architecture achieves state-of-the-art performance in UDA object detection, balancing efficiency with robust generalization.",
    "original_url": "http://arxiv.org/pdf/2502.11178v1",
    "original_title": "DAViMNet: SSMs-Based Domain Adaptive Object Detection",
    "source": "arxiv",
    "authors": [
      "A. Enes Doruk",
      "Hasan F. Ates"
    ],
    "published": "2025-02-16T15:58:54+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2502.11178v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2102.03951v1_chunk_0",
    "chunk_text": "End-to-End Multi-Channel Transformer for Speech Recognition\n\nTransformers are powerful neural architectures that allow integrating different modalities using attention mechanisms. In this paper, we leverage the neural transformer architectures for multi-channel speech recognition systems, where the spectral and spatial information collected from different microphones are integrated using attention layers. Our multi-channel transformer network mainly consists of three parts: channel-wise self attention layers (CSA), cross-channel attention layers (CCA), and multi-channel encoder-decoder attention layers (EDA). The CSA and CCA layers encode the contextual relationship within and between channels and across time, respectively. The channel-attended outputs from CSA and CCA are then fed into the EDA layers to help decode the next token given the preceding ones.",
    "original_url": "http://arxiv.org/pdf/2102.03951v1",
    "original_title": "End-to-End Multi-Channel Transformer for Speech Recognition",
    "source": "arxiv",
    "authors": [
      "Feng-Ju Chang",
      "Martin Radfar",
      "Athanasios Mouchtaris",
      "Brian King",
      "Siegfried Kunzmann"
    ],
    "published": "2021-02-08T00:12:44+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2102.03951v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2102.03951v1_chunk_1",
    "chunk_text": "The channel-attended outputs from CSA and CCA are then fed into the EDA layers to help decode the next token given the preceding ones. The experiments show that in a far-field in-house dataset, our method outperforms the baseline single-channel transformer, as well as the super-directive and neural beamformers cascaded with the transformers.",
    "original_url": "http://arxiv.org/pdf/2102.03951v1",
    "original_title": "End-to-End Multi-Channel Transformer for Speech Recognition",
    "source": "arxiv",
    "authors": [
      "Feng-Ju Chang",
      "Martin Radfar",
      "Athanasios Mouchtaris",
      "Brian King",
      "Siegfried Kunzmann"
    ],
    "published": "2021-02-08T00:12:44+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2102.03951v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2110.11299v1_chunk_0",
    "chunk_text": "Transformer Acceleration with Dynamic Sparse Attention\n\nTransformers are the mainstream of NLP applications and are becoming increasingly popular in other domains such as Computer Vision. Despite the improvements in model quality, the enormous computation costs make Transformers difficult at deployment, especially when the sequence length is large in emerging applications. Processing attention mechanism as the essential component of Transformer is the bottleneck of execution due to the quadratic complexity. Prior art explores sparse patterns in attention to support long sequence modeling, but those pieces of work are on static or fixed patterns. We demonstrate that the sparse patterns are dynamic, depending on input sequences.",
    "original_url": "http://arxiv.org/pdf/2110.11299v1",
    "original_title": "Transformer Acceleration with Dynamic Sparse Attention",
    "source": "arxiv",
    "authors": [
      "Liu Liu",
      "Zheng Qu",
      "Zhaodong Chen",
      "Yufei Ding",
      "Yuan Xie"
    ],
    "published": "2021-10-21T17:31:57+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2110.11299v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2110.11299v1_chunk_1",
    "chunk_text": "We demonstrate that the sparse patterns are dynamic, depending on input sequences. Thus, we propose the Dynamic Sparse Attention (DSA) that can efficiently exploit the dynamic sparsity in the attention of Transformers. Compared with other methods, our approach can achieve better trade-offs between accuracy and model complexity. Moving forward, we identify challenges and provide solutions to implement DSA on existing hardware (GPUs) and specialized hardware in order to achieve practical speedup and efficiency improvements for Transformer execution.",
    "original_url": "http://arxiv.org/pdf/2110.11299v1",
    "original_title": "Transformer Acceleration with Dynamic Sparse Attention",
    "source": "arxiv",
    "authors": [
      "Liu Liu",
      "Zheng Qu",
      "Zhaodong Chen",
      "Yufei Ding",
      "Yuan Xie"
    ],
    "published": "2021-10-21T17:31:57+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2110.11299v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2205.00434v1_chunk_0",
    "chunk_text": "Reinforced Swin-Convs Transformer for Underwater Image Enhancement\n\nUnderwater Image Enhancement (UIE) technology aims to tackle the challenge of restoring the degraded underwater images due to light absorption and scattering. To address problems, a novel U-Net based Reinforced Swin-Convs Transformer for the Underwater Image Enhancement method (URSCT-UIE) is proposed. Specifically, with the deficiency of U-Net based on pure convolutions, we embedded the Swin Transformer into U-Net for improving the ability to capture the global dependency. Then, given the inadequacy of the Swin Transformer capturing the local attention, the reintroduction of convolutions may capture more local attention. Thus, we provide an ingenious manner for the fusion of convolutions and the core attention mechanism to build a Reinforced Swin-Convs Transformer Block (RSCTB) for capturing more local attention, which is reinforced in the channel and the spatial attention of the Swin Transformer.",
    "original_url": "http://arxiv.org/pdf/2205.00434v1",
    "original_title": "Reinforced Swin-Convs Transformer for Underwater Image Enhancement",
    "source": "arxiv",
    "authors": [
      "Tingdi Ren",
      "Haiyong Xu",
      "Gangyi Jiang",
      "Mei Yu",
      "Ting Luo"
    ],
    "published": "2022-05-01T09:46:33+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2205.00434v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2205.00434v1_chunk_1",
    "chunk_text": "Thus, we provide an ingenious manner for the fusion of convolutions and the core attention mechanism to build a Reinforced Swin-Convs Transformer Block (RSCTB) for capturing more local attention, which is reinforced in the channel and the spatial attention of the Swin Transformer. Finally, the experimental results on available datasets demonstrate that the proposed URSCT-UIE achieves state-of-the-art performance compared with other methods in terms of both subjective and objective evaluations. The code will be released on GitHub after acceptance.",
    "original_url": "http://arxiv.org/pdf/2205.00434v1",
    "original_title": "Reinforced Swin-Convs Transformer for Underwater Image Enhancement",
    "source": "arxiv",
    "authors": [
      "Tingdi Ren",
      "Haiyong Xu",
      "Gangyi Jiang",
      "Mei Yu",
      "Ting Luo"
    ],
    "published": "2022-05-01T09:46:33+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2205.00434v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2503.15986v1_chunk_0",
    "chunk_text": "SpiLiFormer: Enhancing Spiking Transformers with Lateral Inhibition\n\nSpiking Neural Networks (SNNs) based on Transformers have garnered significant attention due to their superior performance and high energy efficiency. However, the spiking attention modules of most existing Transformer-based SNNs are adapted from those of analog Transformers, failing to fully address the issue of over-allocating attention to irrelevant contexts. To fix this fundamental yet overlooked issue, we propose a Lateral Inhibition-inspired Spiking Transformer (SpiLiFormer). It emulates the brain's lateral inhibition mechanism, guiding the model to enhance attention to relevant tokens while suppressing attention to irrelevant ones. Our model achieves state-of-the-art (SOTA) performance across multiple datasets, including CIFAR-10 (+0.45%), CIFAR-100 (+0.48%), CIFAR10-DVS (+2.70%), N-Caltech101 (+1.94%), and ImageNet-1K (+1.6%).",
    "original_url": "http://arxiv.org/pdf/2503.15986v1",
    "original_title": "SpiLiFormer: Enhancing Spiking Transformers with Lateral Inhibition",
    "source": "arxiv",
    "authors": [
      "Zeqi Zheng",
      "Yanchen Huang",
      "Yingchao Yu",
      "Zizheng Zhu",
      "Junfeng Tang",
      "Zhaofei Yu",
      "Yaochu Jin"
    ],
    "published": "2025-03-20T09:36:31+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2503.15986v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2503.15986v1_chunk_1",
    "chunk_text": "Our model achieves state-of-the-art (SOTA) performance across multiple datasets, including CIFAR-10 (+0.45%), CIFAR-100 (+0.48%), CIFAR10-DVS (+2.70%), N-Caltech101 (+1.94%), and ImageNet-1K (+1.6%). Notably, on the ImageNet-1K dataset, SpiLiFormer (69.9M parameters, 4 time steps, 384 resolution) outperforms E-SpikeFormer (173.0M parameters, 8 time steps, 384 resolution), a SOTA spiking Transformer, by 0.46% using only 39% of the parameters and half the time steps. Our code and training checkpoints will be released upon acceptance.",
    "original_url": "http://arxiv.org/pdf/2503.15986v1",
    "original_title": "SpiLiFormer: Enhancing Spiking Transformers with Lateral Inhibition",
    "source": "arxiv",
    "authors": [
      "Zeqi Zheng",
      "Yanchen Huang",
      "Yingchao Yu",
      "Zizheng Zhu",
      "Junfeng Tang",
      "Zhaofei Yu",
      "Yaochu Jin"
    ],
    "published": "2025-03-20T09:36:31+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2503.15986v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2501.06098v3_chunk_0",
    "chunk_text": "ELFATT: Efficient Linear Fast Attention for Vision Transformers\n\nThe attention mechanism is the key to the success of transformers in different machine learning tasks. However, the quadratic complexity with respect to the sequence length of the vanilla softmax-based attention mechanism becomes the major bottleneck for the application of long sequence tasks, such as vision tasks. Although various efficient linear attention mechanisms have been proposed, they need to sacrifice performance to achieve high efficiency. What's more, memory-efficient methods, such as FlashAttention-1-3, still have quadratic computation complexity which can be further improved. In this paper, we propose a novel efficient linear fast attention (ELFATT) mechanism to achieve low memory input/output operations, linear computational complexity, and high performance at the same time.",
    "original_url": "http://arxiv.org/pdf/2501.06098v3",
    "original_title": "ELFATT: Efficient Linear Fast Attention for Vision Transformers",
    "source": "arxiv",
    "authors": [
      "Chong Wu",
      "Maolin Che",
      "Renjie Xu",
      "Zhuoheng Ran",
      "Hong Yan"
    ],
    "published": "2025-01-10T16:51:19+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2501.06098v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2501.06098v3_chunk_1",
    "chunk_text": "In this paper, we propose a novel efficient linear fast attention (ELFATT) mechanism to achieve low memory input/output operations, linear computational complexity, and high performance at the same time. ELFATT offers 4-7x speedups over the vanilla softmax-based attention mechanism in high-resolution vision tasks without losing performance. ELFATT is FlashAttention friendly. Using FlashAttention-2 acceleration, ELFATT still offers 2-3x speedups over the vanilla softmax-based attention mechanism on high-resolution vision tasks without losing performance. Even on edge GPUs, ELFATT still offers 1.6x to 2.0x speedups compared to state-of-the-art attention mechanisms in various power modes from 5W to 60W.",
    "original_url": "http://arxiv.org/pdf/2501.06098v3",
    "original_title": "ELFATT: Efficient Linear Fast Attention for Vision Transformers",
    "source": "arxiv",
    "authors": [
      "Chong Wu",
      "Maolin Che",
      "Renjie Xu",
      "Zhuoheng Ran",
      "Hong Yan"
    ],
    "published": "2025-01-10T16:51:19+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2501.06098v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2501.06098v3_chunk_2",
    "chunk_text": "Even on edge GPUs, ELFATT still offers 1.6x to 2.0x speedups compared to state-of-the-art attention mechanisms in various power modes from 5W to 60W. Furthermore, ELFATT can be used to enhance and accelerate diffusion tasks directly without training.",
    "original_url": "http://arxiv.org/pdf/2501.06098v3",
    "original_title": "ELFATT: Efficient Linear Fast Attention for Vision Transformers",
    "source": "arxiv",
    "authors": [
      "Chong Wu",
      "Maolin Che",
      "Renjie Xu",
      "Zhuoheng Ran",
      "Hong Yan"
    ],
    "published": "2025-01-10T16:51:19+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2501.06098v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1912.00349v1_chunk_0",
    "chunk_text": "Not All Attention Is Needed: Gated Attention Network for Sequence Data\n\nAlthough deep neural networks generally have fixed network structures, the concept of dynamic mechanism has drawn more and more attention in recent years. Attention mechanisms compute input-dependent dynamic attention weights for aggregating a sequence of hidden states. Dynamic network configuration in convolutional neural networks (CNNs) selectively activates only part of the network at a time for different inputs. In this paper, we combine the two dynamic mechanisms for text classification tasks. Traditional attention mechanisms attend to the whole sequence of hidden states for an input sentence, while in most cases not all attention is needed especially for long sequences.",
    "original_url": "http://arxiv.org/pdf/1912.00349v1",
    "original_title": "Not All Attention Is Needed: Gated Attention Network for Sequence Data",
    "source": "arxiv",
    "authors": [
      "Lanqing Xue",
      "Xiaopeng Li",
      "Nevin L. Zhang"
    ],
    "published": "2019-12-01T07:57:41+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1912.00349v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1912.00349v1_chunk_1",
    "chunk_text": "Traditional attention mechanisms attend to the whole sequence of hidden states for an input sentence, while in most cases not all attention is needed especially for long sequences. We propose a novel method called Gated Attention Network (GA-Net) to dynamically select a subset of elements to attend to using an auxiliary network, and compute attention weights to aggregate the selected elements. It avoids a significant amount of unnecessary computation on unattended elements, and allows the model to pay attention to important parts of the sequence. Experiments in various datasets show that the proposed method achieves better performance compared with all baseline models with global or local attention while requiring less computation and achieving better interpretability. It is also promising to extend the idea to more complex attention-based models, such as transformers and seq-to-seq models.",
    "original_url": "http://arxiv.org/pdf/1912.00349v1",
    "original_title": "Not All Attention Is Needed: Gated Attention Network for Sequence Data",
    "source": "arxiv",
    "authors": [
      "Lanqing Xue",
      "Xiaopeng Li",
      "Nevin L. Zhang"
    ],
    "published": "2019-12-01T07:57:41+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1912.00349v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1912.00349v1_chunk_2",
    "chunk_text": "It is also promising to extend the idea to more complex attention-based models, such as transformers and seq-to-seq models.",
    "original_url": "http://arxiv.org/pdf/1912.00349v1",
    "original_title": "Not All Attention Is Needed: Gated Attention Network for Sequence Data",
    "source": "arxiv",
    "authors": [
      "Lanqing Xue",
      "Xiaopeng Li",
      "Nevin L. Zhang"
    ],
    "published": "2019-12-01T07:57:41+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1912.00349v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2504.09088v1_chunk_0",
    "chunk_text": "Multi-Modal Brain Tumor Segmentation via 3D Multi-Scale Self-attention and Cross-attention\n\nDue to the success of CNN-based and Transformer-based models in various computer vision tasks, recent works study the applicability of CNN-Transformer hybrid architecture models in 3D multi-modality medical segmentation tasks. Introducing Transformer brings long-range dependent information modeling ability in 3D medical images to hybrid models via the self-attention mechanism. However, these models usually employ fixed receptive fields of 3D volumetric features within each self-attention layer, ignoring the multi-scale volumetric lesion features. To address this issue, we propose a CNN-Transformer hybrid 3D medical image segmentation model, named TMA-TransBTS, based on an encoder-decoder structure. TMA-TransBTS realizes simultaneous extraction of multi-scale 3D features and modeling of long-distance dependencies by multi-scale division and aggregation of 3D tokens in a self-attention layer.",
    "original_url": "http://arxiv.org/pdf/2504.09088v1",
    "original_title": "Multi-Modal Brain Tumor Segmentation via 3D Multi-Scale Self-attention and Cross-attention",
    "source": "arxiv",
    "authors": [
      "Yonghao Huang",
      "Leiting Chen",
      "Chuan Zhou"
    ],
    "published": "2025-04-12T05:53:59+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2504.09088v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2504.09088v1_chunk_1",
    "chunk_text": "TMA-TransBTS realizes simultaneous extraction of multi-scale 3D features and modeling of long-distance dependencies by multi-scale division and aggregation of 3D tokens in a self-attention layer. Furthermore, TMA-TransBTS proposes a 3D multi-scale cross-attention module to establish a link between the encoder and the decoder for extracting rich volume representations by exploiting the mutual attention mechanism of cross-attention and multi-scale aggregation of 3D tokens. Extensive experimental results on three public 3D medical segmentation datasets show that TMA-TransBTS achieves higher averaged segmentation results than previous state-of-the-art CNN-based 3D methods and CNN-Transform hybrid 3D methods for the segmentation of 3D multi-modality brain tumors.",
    "original_url": "http://arxiv.org/pdf/2504.09088v1",
    "original_title": "Multi-Modal Brain Tumor Segmentation via 3D Multi-Scale Self-attention and Cross-attention",
    "source": "arxiv",
    "authors": [
      "Yonghao Huang",
      "Leiting Chen",
      "Chuan Zhou"
    ],
    "published": "2025-04-12T05:53:59+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2504.09088v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2103.03493v1_chunk_0",
    "chunk_text": "Causal Attention for Vision-Language Tasks\n\nWe present a novel attention mechanism: Causal Attention (CATT), to remove the ever-elusive confounding effect in existing attention-based vision-language models. This effect causes harmful bias that misleads the attention module to focus on the spurious correlations in training data, damaging the model generalization. As the confounder is unobserved in general, we use the front-door adjustment to realize the causal intervention, which does not require any knowledge on the confounder. Specifically, CATT is implemented as a combination of 1) In-Sample Attention (IS-ATT) and 2) Cross-Sample Attention (CS-ATT), where the latter forcibly brings other samples into every IS-ATT, mimicking the causal intervention. CATT abides by the Q-K-V convention and hence can replace any attention module such as top-down attention and self-attention in Transformers.",
    "original_url": "http://arxiv.org/pdf/2103.03493v1",
    "original_title": "Causal Attention for Vision-Language Tasks",
    "source": "arxiv",
    "authors": [
      "Xu Yang",
      "Hanwang Zhang",
      "Guojun Qi",
      "Jianfei Cai"
    ],
    "published": "2021-03-05T06:38:25+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2103.03493v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2103.03493v1_chunk_1",
    "chunk_text": "CATT abides by the Q-K-V convention and hence can replace any attention module such as top-down attention and self-attention in Transformers. CATT improves various popular attention-based vision-language models by considerable margins. In particular, we show that CATT has great potential in large-scale pre-training, e.g., it can promote the lighter LXMERT~\\cite{tan2019lxmert}, which uses fewer data and less computational power, comparable to the heavier UNITER~\\cite{chen2020uniter}. Code is published in \\url{https://github.com/yangxuntu/catt}.",
    "original_url": "http://arxiv.org/pdf/2103.03493v1",
    "original_title": "Causal Attention for Vision-Language Tasks",
    "source": "arxiv",
    "authors": [
      "Xu Yang",
      "Hanwang Zhang",
      "Guojun Qi",
      "Jianfei Cai"
    ],
    "published": "2021-03-05T06:38:25+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2103.03493v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2411.17116v2_chunk_0",
    "chunk_text": "Star Attention: Efficient LLM Inference over Long Sequences\n\nInference with Transformer-based Large Language Models (LLMs) on long sequences is both costly and slow due to the quadratic complexity of the self-attention mechanism. We introduce Star Attention, a two-phase block-sparse approximation that improves computational efficiency by sharding attention across multiple hosts while minimizing communication overhead. In the first phase, the context is processed using blockwise-local attention across hosts, in parallel. In the second phase, query and response tokens attend to all prior cached tokens through sequence-global attention. Star Attention integrates seamlessly with most Transformer-based LLMs trained with global attention, reducing memory requirements and inference time by up to 11x while preserving 97-100% of accuracy.",
    "original_url": "http://arxiv.org/pdf/2411.17116v2",
    "original_title": "Star Attention: Efficient LLM Inference over Long Sequences",
    "source": "arxiv",
    "authors": [
      "Shantanu Acharya",
      "Fei Jia",
      "Boris Ginsburg"
    ],
    "published": "2024-11-26T05:10:04+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2411.17116v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2411.17116v2_chunk_1",
    "chunk_text": "Star Attention integrates seamlessly with most Transformer-based LLMs trained with global attention, reducing memory requirements and inference time by up to 11x while preserving 97-100% of accuracy.",
    "original_url": "http://arxiv.org/pdf/2411.17116v2",
    "original_title": "Star Attention: Efficient LLM Inference over Long Sequences",
    "source": "arxiv",
    "authors": [
      "Shantanu Acharya",
      "Fei Jia",
      "Boris Ginsburg"
    ],
    "published": "2024-11-26T05:10:04+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2411.17116v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2502.08363v1_chunk_0",
    "chunk_text": "Top-Theta Attention: Sparsifying Transformers by Compensated Thresholding\n\nThe attention mechanism is essential for the impressive capabilities of transformer-based Large Language Models (LLMs). However, calculating attention is computationally intensive due to its quadratic dependency on the sequence length. We introduce a novel approach called Top-Theta Attention, or simply Top-$\\theta$, which selectively prunes less essential attention elements by comparing them against carefully calibrated thresholds. This method greatly improves the efficiency of self-attention matrix multiplication while preserving model accuracy, reducing the number of required V cache rows by 3x during generative decoding and the number of attention elements by 10x during the prefill phase. Our method does not require model retraining; instead, it requires only a brief calibration phase to be resilient to distribution shifts, thus not requiring the thresholds for different datasets to be recalibrated.",
    "original_url": "http://arxiv.org/pdf/2502.08363v1",
    "original_title": "Top-Theta Attention: Sparsifying Transformers by Compensated Thresholding",
    "source": "arxiv",
    "authors": [
      "Konstantin Berestizshevsky",
      "Renzo Andri",
      "Lukas Cavigelli"
    ],
    "published": "2025-02-12T12:50:15+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2502.08363v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2502.08363v1_chunk_1",
    "chunk_text": "Our method does not require model retraining; instead, it requires only a brief calibration phase to be resilient to distribution shifts, thus not requiring the thresholds for different datasets to be recalibrated. Unlike top-k attention, Top-$\\theta$ eliminates full-vector dependency, making it suitable for tiling and scale-out and avoiding costly top-k search. A key innovation of our approach is the development of efficient numerical compensation techniques, which help preserve model accuracy even under aggressive pruning of attention scores.",
    "original_url": "http://arxiv.org/pdf/2502.08363v1",
    "original_title": "Top-Theta Attention: Sparsifying Transformers by Compensated Thresholding",
    "source": "arxiv",
    "authors": [
      "Konstantin Berestizshevsky",
      "Renzo Andri",
      "Lukas Cavigelli"
    ],
    "published": "2025-02-12T12:50:15+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2502.08363v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2212.12552v1_chunk_0",
    "chunk_text": "A Close Look at Spatial Modeling: From Attention to Convolution\n\nVision Transformers have shown great promise recently for many vision tasks due to the insightful architecture design and attention mechanism. By revisiting the self-attention responses in Transformers, we empirically observe two interesting issues. First, Vision Transformers present a queryirrelevant behavior at deep layers, where the attention maps exhibit nearly consistent contexts in global scope, regardless of the query patch position (also head-irrelevant). Second, the attention maps are intrinsically sparse, few tokens dominate the attention weights; introducing the knowledge from ConvNets would largely smooth the attention and enhance the performance. Motivated by above observations, we generalize self-attention formulation to abstract a queryirrelevant global context directly and further integrate the global context into convolutions.",
    "original_url": "http://arxiv.org/pdf/2212.12552v1",
    "original_title": "A Close Look at Spatial Modeling: From Attention to Convolution",
    "source": "arxiv",
    "authors": [
      "Xu Ma",
      "Huan Wang",
      "Can Qin",
      "Kunpeng Li",
      "Xingchen Zhao",
      "Jie Fu",
      "Yun Fu"
    ],
    "published": "2022-12-23T19:13:43+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2212.12552v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2212.12552v1_chunk_1",
    "chunk_text": "Motivated by above observations, we generalize self-attention formulation to abstract a queryirrelevant global context directly and further integrate the global context into convolutions. The resulting model, a Fully Convolutional Vision Transformer (i.e., FCViT), purely consists of convolutional layers and firmly inherits the merits of both attention mechanism and convolutions, including dynamic property, weight sharing, and short- and long-range feature modeling, etc. Experimental results demonstrate the effectiveness of FCViT. With less than 14M parameters, our FCViT-S12 outperforms related work ResT-Lite by 3.7% top1 accuracy on ImageNet-1K. When scaling FCViT to larger models, we still perform better than previous state-of-the-art ConvNeXt with even fewer parameters.",
    "original_url": "http://arxiv.org/pdf/2212.12552v1",
    "original_title": "A Close Look at Spatial Modeling: From Attention to Convolution",
    "source": "arxiv",
    "authors": [
      "Xu Ma",
      "Huan Wang",
      "Can Qin",
      "Kunpeng Li",
      "Xingchen Zhao",
      "Jie Fu",
      "Yun Fu"
    ],
    "published": "2022-12-23T19:13:43+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2212.12552v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2212.12552v1_chunk_2",
    "chunk_text": "When scaling FCViT to larger models, we still perform better than previous state-of-the-art ConvNeXt with even fewer parameters. FCViT-based models also demonstrate promising transferability to downstream tasks, like object detection, instance segmentation, and semantic segmentation. Codes and models are made available at: https://github.com/ma-xu/FCViT.",
    "original_url": "http://arxiv.org/pdf/2212.12552v1",
    "original_title": "A Close Look at Spatial Modeling: From Attention to Convolution",
    "source": "arxiv",
    "authors": [
      "Xu Ma",
      "Huan Wang",
      "Can Qin",
      "Kunpeng Li",
      "Xingchen Zhao",
      "Jie Fu",
      "Yun Fu"
    ],
    "published": "2022-12-23T19:13:43+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2212.12552v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2409.10715v2_chunk_0",
    "chunk_text": "Self-Attention Limits Working Memory Capacity of Transformer-Based Models\n\nRecent work on Transformer-based large language models (LLMs) has revealed striking limits in their working memory capacity, similar to what has been found in human behavioral studies. Specifically, these models' performance drops significantly on N-back tasks as N increases. However, there is still a lack of mechanistic interpretability as to why this phenomenon would arise. Inspired by the executive attention theory from behavioral sciences, we hypothesize that the self-attention mechanism within Transformer-based models might be responsible for their working memory capacity limits. To test this hypothesis, we train vanilla decoder-only transformers to perform N-back tasks and find that attention scores gradually aggregate to the N-back positions over training, suggesting that the model masters the task by learning a strategy to pay attention to the relationship between the current position and the N-back position.",
    "original_url": "http://arxiv.org/pdf/2409.10715v2",
    "original_title": "Self-Attention Limits Working Memory Capacity of Transformer-Based Models",
    "source": "arxiv",
    "authors": [
      "Dongyu Gong",
      "Hantao Zhang"
    ],
    "published": "2024-09-16T20:38:35+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2409.10715v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2409.10715v2_chunk_1",
    "chunk_text": "To test this hypothesis, we train vanilla decoder-only transformers to perform N-back tasks and find that attention scores gradually aggregate to the N-back positions over training, suggesting that the model masters the task by learning a strategy to pay attention to the relationship between the current position and the N-back position. Critically, we find that the total entropy of the attention score matrix increases as N increases, suggesting that the dispersion of attention scores might be the cause of the capacity limit observed in N-back tasks. Our findings thus offer insights into the shared role of attention in both human and artificial intelligence. Moreover, the limitations of the self-attention mechanism revealed in the current study could inform future efforts to design more powerful model architectures with enhanced working memory capacity and cognitive capabilities.",
    "original_url": "http://arxiv.org/pdf/2409.10715v2",
    "original_title": "Self-Attention Limits Working Memory Capacity of Transformer-Based Models",
    "source": "arxiv",
    "authors": [
      "Dongyu Gong",
      "Hantao Zhang"
    ],
    "published": "2024-09-16T20:38:35+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2409.10715v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2107.12600v1_chunk_0",
    "chunk_text": "PiSLTRc: Position-informed Sign Language Transformer with Content-aware Convolution\n\nSince the superiority of Transformer in learning long-term dependency, the sign language Transformer model achieves remarkable progress in Sign Language Recognition (SLR) and Translation (SLT). However, there are several issues with the Transformer that prevent it from better sign language understanding. The first issue is that the self-attention mechanism learns sign video representation in a frame-wise manner, neglecting the temporal semantic structure of sign gestures. Secondly, the attention mechanism with absolute position encoding is direction and distance unaware, thus limiting its ability. To address these issues, we propose a new model architecture, namely PiSLTRc, with two distinctive characteristics: (i) content-aware and position-aware convolution layers.",
    "original_url": "http://arxiv.org/pdf/2107.12600v1",
    "original_title": "PiSLTRc: Position-informed Sign Language Transformer with Content-aware Convolution",
    "source": "arxiv",
    "authors": [
      "Pan Xie",
      "Mengyi Zhao",
      "Xiaohui Hu"
    ],
    "published": "2021-07-27T05:01:27+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2107.12600v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2107.12600v1_chunk_1",
    "chunk_text": "To address these issues, we propose a new model architecture, namely PiSLTRc, with two distinctive characteristics: (i) content-aware and position-aware convolution layers. Specifically, we explicitly select relevant features using a novel content-aware neighborhood gathering method. Then we aggregate these features with position-informed temporal convolution layers, thus generating robust neighborhood-enhanced sign representation. (ii) injecting the relative position information to the attention mechanism in the encoder, decoder, and even encoder-decoder cross attention. Compared with the vanilla Transformer model, our model performs consistently better on three large-scale sign language benchmarks: PHOENIX-2014, PHOENIX-2014-T and CSL.",
    "original_url": "http://arxiv.org/pdf/2107.12600v1",
    "original_title": "PiSLTRc: Position-informed Sign Language Transformer with Content-aware Convolution",
    "source": "arxiv",
    "authors": [
      "Pan Xie",
      "Mengyi Zhao",
      "Xiaohui Hu"
    ],
    "published": "2021-07-27T05:01:27+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2107.12600v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2107.12600v1_chunk_2",
    "chunk_text": "Compared with the vanilla Transformer model, our model performs consistently better on three large-scale sign language benchmarks: PHOENIX-2014, PHOENIX-2014-T and CSL. Furthermore, extensive experiments demonstrate that the proposed method achieves state-of-the-art performance on translation quality with $+1.6$ BLEU improvements.",
    "original_url": "http://arxiv.org/pdf/2107.12600v1",
    "original_title": "PiSLTRc: Position-informed Sign Language Transformer with Content-aware Convolution",
    "source": "arxiv",
    "authors": [
      "Pan Xie",
      "Mengyi Zhao",
      "Xiaohui Hu"
    ],
    "published": "2021-07-27T05:01:27+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2107.12600v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2403.14302v2_chunk_0",
    "chunk_text": "SpikingResformer: Bridging ResNet and Vision Transformer in Spiking Neural Networks\n\nThe remarkable success of Vision Transformers in Artificial Neural Networks (ANNs) has led to a growing interest in incorporating the self-attention mechanism and transformer-based architecture into Spiking Neural Networks (SNNs). While existing methods propose spiking self-attention mechanisms that are compatible with SNNs, they lack reasonable scaling methods, and the overall architectures proposed by these methods suffer from a bottleneck in effectively extracting local features. To address these challenges, we propose a novel spiking self-attention mechanism named Dual Spike Self-Attention (DSSA) with a reasonable scaling method. Based on DSSA, we propose a novel spiking Vision Transformer architecture called SpikingResformer, which combines the ResNet-based multi-stage architecture with our proposed DSSA to improve both performance and energy efficiency while reducing parameters. Experimental results show that SpikingResformer achieves higher accuracy with fewer parameters and lower energy consumption than other spiking Vision Transformer counterparts.",
    "original_url": "http://arxiv.org/pdf/2403.14302v2",
    "original_title": "SpikingResformer: Bridging ResNet and Vision Transformer in Spiking Neural Networks",
    "source": "arxiv",
    "authors": [
      "Xinyu Shi",
      "Zecheng Hao",
      "Zhaofei Yu"
    ],
    "published": "2024-03-21T11:16:42+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2403.14302v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2403.14302v2_chunk_1",
    "chunk_text": "Experimental results show that SpikingResformer achieves higher accuracy with fewer parameters and lower energy consumption than other spiking Vision Transformer counterparts. Notably, our SpikingResformer-L achieves 79.40% top-1 accuracy on ImageNet with 4 time-steps, which is the state-of-the-art result in the SNN field.",
    "original_url": "http://arxiv.org/pdf/2403.14302v2",
    "original_title": "SpikingResformer: Bridging ResNet and Vision Transformer in Spiking Neural Networks",
    "source": "arxiv",
    "authors": [
      "Xinyu Shi",
      "Zecheng Hao",
      "Zhaofei Yu"
    ],
    "published": "2024-03-21T11:16:42+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2403.14302v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2201.05887v2_chunk_0",
    "chunk_text": "Domain Adaptation via Bidirectional Cross-Attention Transformer\n\nDomain Adaptation (DA) aims to leverage the knowledge learned from a source domain with ample labeled data to a target domain with unlabeled data only. Most existing studies on DA contribute to learning domain-invariant feature representations for both domains by minimizing the domain gap based on convolution-based neural networks. Recently, vision transformers significantly improved performance in multiple vision tasks. Built on vision transformers, in this paper we propose a Bidirectional Cross-Attention Transformer (BCAT) for DA with the aim to improve the performance. In the proposed BCAT, the attention mechanism can extract implicit source and target mixup feature representations to narrow the domain discrepancy.",
    "original_url": "http://arxiv.org/pdf/2201.05887v2",
    "original_title": "Domain Adaptation via Bidirectional Cross-Attention Transformer",
    "source": "arxiv",
    "authors": [
      "Xiyu Wang",
      "Pengxin Guo",
      "Yu Zhang"
    ],
    "published": "2022-01-15T16:49:56+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2201.05887v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2201.05887v2_chunk_1",
    "chunk_text": "In the proposed BCAT, the attention mechanism can extract implicit source and target mixup feature representations to narrow the domain discrepancy. Specifically, in BCAT, we design a weight-sharing quadruple-branch transformer with a bidirectional cross-attention mechanism to learn domain-invariant feature representations. Extensive experiments demonstrate that the proposed BCAT model achieves superior performance on four benchmark datasets over existing state-of-the-art DA methods that are based on convolutions or transformers.",
    "original_url": "http://arxiv.org/pdf/2201.05887v2",
    "original_title": "Domain Adaptation via Bidirectional Cross-Attention Transformer",
    "source": "arxiv",
    "authors": [
      "Xiyu Wang",
      "Pengxin Guo",
      "Yu Zhang"
    ],
    "published": "2022-01-15T16:49:56+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2201.05887v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2307.16144v1_chunk_0",
    "chunk_text": "Video Frame Interpolation with Flow Transformer\n\nVideo frame interpolation has been actively studied with the development of convolutional neural networks. However, due to the intrinsic limitations of kernel weight sharing in convolution, the interpolated frame generated by it may lose details. In contrast, the attention mechanism in Transformer can better distinguish the contribution of each pixel, and it can also capture long-range pixel dependencies, which provides great potential for video interpolation. Nevertheless, the original Transformer is commonly used for 2D images; how to develop a Transformer-based framework with consideration of temporal self-attention for video frame interpolation remains an open issue. In this paper, we propose Video Frame Interpolation Flow Transformer to incorporate motion dynamics from optical flows into the self-attention mechanism.",
    "original_url": "http://arxiv.org/pdf/2307.16144v1",
    "original_title": "Video Frame Interpolation with Flow Transformer",
    "source": "arxiv",
    "authors": [
      "Pan Gao",
      "Haoyue Tian",
      "Jie Qin"
    ],
    "published": "2023-07-30T06:44:37+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2307.16144v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2307.16144v1_chunk_1",
    "chunk_text": "In this paper, we propose Video Frame Interpolation Flow Transformer to incorporate motion dynamics from optical flows into the self-attention mechanism. Specifically, we design a Flow Transformer Block that calculates the temporal self-attention in a matched local area with the guidance of flow, making our framework suitable for interpolating frames with large motion while maintaining reasonably low complexity. In addition, we construct a multi-scale architecture to account for multi-scale motion, further improving the overall performance. Extensive experiments on three benchmarks demonstrate that the proposed method can generate interpolated frames with better visual quality than state-of-the-art methods.",
    "original_url": "http://arxiv.org/pdf/2307.16144v1",
    "original_title": "Video Frame Interpolation with Flow Transformer",
    "source": "arxiv",
    "authors": [
      "Pan Gao",
      "Haoyue Tian",
      "Jie Qin"
    ],
    "published": "2023-07-30T06:44:37+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2307.16144v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2309.13600v1_chunk_0",
    "chunk_text": "Multi-Dimensional Hyena for Spatial Inductive Bias\n\nIn recent years, Vision Transformers have attracted increasing interest from computer vision researchers. However, the advantage of these transformers over CNNs is only fully manifested when trained over a large dataset, mainly due to the reduced inductive bias towards spatial locality within the transformer's self-attention mechanism. In this work, we present a data-efficient vision transformer that does not rely on self-attention. Instead, it employs a novel generalization to multiple axes of the very recent Hyena layer. We propose several alternative approaches for obtaining this generalization and delve into their unique distinctions and considerations from both empirical and theoretical perspectives.",
    "original_url": "http://arxiv.org/pdf/2309.13600v1",
    "original_title": "Multi-Dimensional Hyena for Spatial Inductive Bias",
    "source": "arxiv",
    "authors": [
      "Itamar Zimerman",
      "Lior Wolf"
    ],
    "published": "2023-09-24T10:22:35+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2309.13600v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2309.13600v1_chunk_1",
    "chunk_text": "We propose several alternative approaches for obtaining this generalization and delve into their unique distinctions and considerations from both empirical and theoretical perspectives. Our empirical findings indicate that the proposed Hyena N-D layer boosts the performance of various Vision Transformer architectures, such as ViT, Swin, and DeiT across multiple datasets. Furthermore, in the small dataset regime, our Hyena-based ViT is favorable to ViT variants from the recent literature that are specifically designed for solving the same challenge, i.e., working with small datasets or incorporating image-specific inductive bias into the self-attention mechanism. Finally, we show that a hybrid approach that is based on Hyena N-D for the first layers in ViT, followed by layers that incorporate conventional attention, consistently boosts the performance of various vision transformer architectures.",
    "original_url": "http://arxiv.org/pdf/2309.13600v1",
    "original_title": "Multi-Dimensional Hyena for Spatial Inductive Bias",
    "source": "arxiv",
    "authors": [
      "Itamar Zimerman",
      "Lior Wolf"
    ],
    "published": "2023-09-24T10:22:35+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2309.13600v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2404.09365v1_chunk_0",
    "chunk_text": "Hierarchical Attention Models for Multi-Relational Graphs\n\nWe present Bi-Level Attention-Based Relational Graph Convolutional Networks (BR-GCN), unique neural network architectures that utilize masked self-attentional layers with relational graph convolutions, to effectively operate on highly multi-relational data. BR-GCN models use bi-level attention to learn node embeddings through (1) node-level attention, and (2) relation-level attention. The node-level self-attentional layers use intra-relational graph interactions to learn relation-specific node embeddings using a weighted aggregation of neighborhood features in a sparse subgraph region. The relation-level self-attentional layers use inter-relational graph interactions to learn the final node embeddings using a weighted aggregation of relation-specific node embeddings. The BR-GCN bi-level attention mechanism extends Transformer-based multiplicative attention from the natural language processing (NLP) domain, and Graph Attention Networks (GAT)-based attention, to large-scale heterogeneous graphs (HGs).",
    "original_url": "http://arxiv.org/pdf/2404.09365v1",
    "original_title": "Hierarchical Attention Models for Multi-Relational Graphs",
    "source": "arxiv",
    "authors": [
      "Roshni G. Iyer",
      "Wei Wang",
      "Yizhou Sun"
    ],
    "published": "2024-04-14T21:37:39+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2404.09365v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2404.09365v1_chunk_1",
    "chunk_text": "The BR-GCN bi-level attention mechanism extends Transformer-based multiplicative attention from the natural language processing (NLP) domain, and Graph Attention Networks (GAT)-based attention, to large-scale heterogeneous graphs (HGs). On node classification, BR-GCN outperforms baselines from 0.29% to 14.95% as a stand-alone model, and on link prediction, BR-GCN outperforms baselines from 0.02% to 7.40% as an auto-encoder model. We also conduct ablation studies to evaluate the quality of BR-GCN's relation-level attention and discuss how its learning of graph structure may be transferred to enrich other graph neural networks (GNNs). Through various experiments, we show that BR-GCN's attention mechanism is both scalable and more effective in learning compared to state-of-the-art GNNs.",
    "original_url": "http://arxiv.org/pdf/2404.09365v1",
    "original_title": "Hierarchical Attention Models for Multi-Relational Graphs",
    "source": "arxiv",
    "authors": [
      "Roshni G. Iyer",
      "Wei Wang",
      "Yizhou Sun"
    ],
    "published": "2024-04-14T21:37:39+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2404.09365v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2301.10938v1_chunk_0",
    "chunk_text": "Compact Transformer Tracker with Correlative Masked Modeling\n\nTransformer framework has been showing superior performances in visual object tracking for its great strength in information aggregation across the template and search image with the well-known attention mechanism. Most recent advances focus on exploring attention mechanism variants for better information aggregation. We find these schemes are equivalent to or even just a subset of the basic self-attention mechanism. In this paper, we prove that the vanilla self-attention structure is sufficient for information aggregation, and structural adaption is unnecessary. The key is not the attention structure, but how to extract the discriminative feature for tracking and enhance the communication between the target and search image.",
    "original_url": "http://arxiv.org/pdf/2301.10938v1",
    "original_title": "Compact Transformer Tracker with Correlative Masked Modeling",
    "source": "arxiv",
    "authors": [
      "Zikai Song",
      "Run Luo",
      "Junqing Yu",
      "Yi-Ping Phoebe Chen",
      "Wei Yang"
    ],
    "published": "2023-01-26T04:58:08+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2301.10938v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2301.10938v1_chunk_1",
    "chunk_text": "The key is not the attention structure, but how to extract the discriminative feature for tracking and enhance the communication between the target and search image. Based on this finding, we adopt the basic vision transformer (ViT) architecture as our main tracker and concatenate the template and search image for feature embedding. To guide the encoder to capture the invariant feature for tracking, we attach a lightweight correlative masked decoder which reconstructs the original template and search image from the corresponding masked tokens. The correlative masked decoder serves as a plugin for the compact transform tracker and is skipped in inference. Our compact tracker uses the most simple structure which only consists of a ViT backbone and a box head, and can run at 40 fps.",
    "original_url": "http://arxiv.org/pdf/2301.10938v1",
    "original_title": "Compact Transformer Tracker with Correlative Masked Modeling",
    "source": "arxiv",
    "authors": [
      "Zikai Song",
      "Run Luo",
      "Junqing Yu",
      "Yi-Ping Phoebe Chen",
      "Wei Yang"
    ],
    "published": "2023-01-26T04:58:08+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2301.10938v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2301.10938v1_chunk_2",
    "chunk_text": "Our compact tracker uses the most simple structure which only consists of a ViT backbone and a box head, and can run at 40 fps. Extensive experiments show the proposed compact transform tracker outperforms existing approaches, including advanced attention variants, and demonstrates the sufficiency of self-attention in tracking tasks. Our method achieves state-of-the-art performance on five challenging datasets, along with the VOT2020, UAV123, LaSOT, TrackingNet, and GOT-10k benchmarks. Our project is available at https://github.com/HUSTDML/CTTrack.",
    "original_url": "http://arxiv.org/pdf/2301.10938v1",
    "original_title": "Compact Transformer Tracker with Correlative Masked Modeling",
    "source": "arxiv",
    "authors": [
      "Zikai Song",
      "Run Luo",
      "Junqing Yu",
      "Yi-Ping Phoebe Chen",
      "Wei Yang"
    ],
    "published": "2023-01-26T04:58:08+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2301.10938v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2310.11025v1_chunk_0",
    "chunk_text": "SignGT: Signed Attention-based Graph Transformer for Graph Representation Learning\n\nThe emerging graph Transformers have achieved impressive performance for graph representation learning over graph neural networks (GNNs). In this work, we regard the self-attention mechanism, the core module of graph Transformers, as a two-step aggregation operation on a fully connected graph. Due to the property of generating positive attention values, the self-attention mechanism is equal to conducting a smooth operation on all nodes, preserving the low-frequency information. However, only capturing the low-frequency information is inefficient in learning complex relations of nodes on diverse graphs, such as heterophily graphs where the high-frequency information is crucial. To this end, we propose a Signed Attention-based Graph Transformer (SignGT) to adaptively capture various frequency information from the graphs.",
    "original_url": "http://arxiv.org/pdf/2310.11025v1",
    "original_title": "SignGT: Signed Attention-based Graph Transformer for Graph Representation Learning",
    "source": "arxiv",
    "authors": [
      "Jinsong Chen",
      "Gaichao Li",
      "John E. Hopcroft",
      "Kun He"
    ],
    "published": "2023-10-17T06:42:11+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2310.11025v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2310.11025v1_chunk_1",
    "chunk_text": "To this end, we propose a Signed Attention-based Graph Transformer (SignGT) to adaptively capture various frequency information from the graphs. Specifically, SignGT develops a new signed self-attention mechanism (SignSA) that produces signed attention values according to the semantic relevance of node pairs. Hence, the diverse frequency information between different node pairs could be carefully preserved. Besides, SignGT proposes a structure-aware feed-forward network (SFFN) that introduces the neighborhood bias to preserve the local topology information. In this way, SignGT could learn informative node representations from both long-range dependencies and local topology information.",
    "original_url": "http://arxiv.org/pdf/2310.11025v1",
    "original_title": "SignGT: Signed Attention-based Graph Transformer for Graph Representation Learning",
    "source": "arxiv",
    "authors": [
      "Jinsong Chen",
      "Gaichao Li",
      "John E. Hopcroft",
      "Kun He"
    ],
    "published": "2023-10-17T06:42:11+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2310.11025v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2310.11025v1_chunk_2",
    "chunk_text": "In this way, SignGT could learn informative node representations from both long-range dependencies and local topology information. Extensive empirical results on both node-level and graph-level tasks indicate the superiority of SignGT against state-of-the-art graph Transformers as well as advanced GNNs.",
    "original_url": "http://arxiv.org/pdf/2310.11025v1",
    "original_title": "SignGT: Signed Attention-based Graph Transformer for Graph Representation Learning",
    "source": "arxiv",
    "authors": [
      "Jinsong Chen",
      "Gaichao Li",
      "John E. Hopcroft",
      "Kun He"
    ],
    "published": "2023-10-17T06:42:11+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2310.11025v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2410.04271v1_chunk_0",
    "chunk_text": "Fundamental Limitations on Subquadratic Alternatives to Transformers\n\nThe Transformer architecture is widely deployed in many popular and impactful Large Language Models. At its core is the attention mechanism for calculating correlations between pairs of tokens. Performing an attention computation takes quadratic time in the input size, and had become the time bottleneck for transformer operations. In order to circumvent this, researchers have used a variety of approaches, including designing heuristic algorithms for performing attention computations faster, and proposing alternatives to the attention mechanism which can be computed more quickly. For instance, state space models such as Mamba were designed to replace attention with an almost linear time alternative.",
    "original_url": "http://arxiv.org/pdf/2410.04271v1",
    "original_title": "Fundamental Limitations on Subquadratic Alternatives to Transformers",
    "source": "arxiv",
    "authors": [
      "Josh Alman",
      "Hantao Yu"
    ],
    "published": "2024-10-05T19:21:13+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2410.04271v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2410.04271v1_chunk_1",
    "chunk_text": "For instance, state space models such as Mamba were designed to replace attention with an almost linear time alternative. In this paper, we prove that any such approach cannot perform important tasks that Transformer is able to perform (assuming a popular conjecture from fine-grained complexity theory). We focus on document similarity tasks, where one is given as input many documents and would like to find a pair which is (approximately) the most similar. We prove that Transformer is able to perform this task, and we prove that this task cannot be performed in truly subquadratic time by any algorithm. Thus, any model which can be evaluated in subquadratic time - whether because of subquadratic-time heuristics for attention, faster attention replacements like Mamba, or any other reason - cannot perform this task.",
    "original_url": "http://arxiv.org/pdf/2410.04271v1",
    "original_title": "Fundamental Limitations on Subquadratic Alternatives to Transformers",
    "source": "arxiv",
    "authors": [
      "Josh Alman",
      "Hantao Yu"
    ],
    "published": "2024-10-05T19:21:13+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2410.04271v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2410.04271v1_chunk_2",
    "chunk_text": "Thus, any model which can be evaluated in subquadratic time - whether because of subquadratic-time heuristics for attention, faster attention replacements like Mamba, or any other reason - cannot perform this task. In other words, in order to perform tasks that (implicitly or explicitly) involve document similarity, one may as well use Transformer and cannot avoid its quadratic running time.",
    "original_url": "http://arxiv.org/pdf/2410.04271v1",
    "original_title": "Fundamental Limitations on Subquadratic Alternatives to Transformers",
    "source": "arxiv",
    "authors": [
      "Josh Alman",
      "Hantao Yu"
    ],
    "published": "2024-10-05T19:21:13+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2410.04271v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2204.13465v1_chunk_0",
    "chunk_text": "Attention Based Neural Networks for Wireless Channel Estimation\n\nIn this paper, we deploy the self-attention mechanism to achieve improved channel estimation for orthogonal frequency-division multiplexing waveforms in the downlink. Specifically, we propose a new hybrid encoder-decoder structure (called HA02) for the first time which exploits the attention mechanism to focus on the most important input information. In particular, we implement a transformer encoder block as the encoder to achieve the sparsity in the input features and a residual neural network as the decoder respectively, inspired by the success of the attention mechanism. Using 3GPP channel models, our simulations show superior estimation performance compared with other candidate neural network methods for channel estimation.",
    "original_url": "http://arxiv.org/pdf/2204.13465v1",
    "original_title": "Attention Based Neural Networks for Wireless Channel Estimation",
    "source": "arxiv",
    "authors": [
      "Dianxin Luan",
      "John Thompson"
    ],
    "published": "2022-04-28T12:54:19+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2204.13465v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2204.07143v5_chunk_0",
    "chunk_text": "Neighborhood Attention Transformer\n\nWe present Neighborhood Attention (NA), the first efficient and scalable sliding-window attention mechanism for vision. NA is a pixel-wise operation, localizing self attention (SA) to the nearest neighboring pixels, and therefore enjoys a linear time and space complexity compared to the quadratic complexity of SA. The sliding-window pattern allows NA's receptive field to grow without needing extra pixel shifts, and preserves translational equivariance, unlike Swin Transformer's Window Self Attention (WSA). We develop NATTEN (Neighborhood Attention Extension), a Python package with efficient C++ and CUDA kernels, which allows NA to run up to 40% faster than Swin's WSA while using up to 25% less memory. We further present Neighborhood Attention Transformer (NAT), a new hierarchical transformer design based on NA that boosts image classification and downstream vision performance.",
    "original_url": "http://arxiv.org/pdf/2204.07143v5",
    "original_title": "Neighborhood Attention Transformer",
    "source": "arxiv",
    "authors": [
      "Ali Hassani",
      "Steven Walton",
      "Jiachen Li",
      "Shen Li",
      "Humphrey Shi"
    ],
    "published": "2022-04-14T17:55:15+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2204.07143v5"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2204.07143v5_chunk_1",
    "chunk_text": "We further present Neighborhood Attention Transformer (NAT), a new hierarchical transformer design based on NA that boosts image classification and downstream vision performance. Experimental results on NAT are competitive; NAT-Tiny reaches 83.2% top-1 accuracy on ImageNet, 51.4% mAP on MS-COCO and 48.4% mIoU on ADE20K, which is 1.9% ImageNet accuracy, 1.0% COCO mAP, and 2.6% ADE20K mIoU improvement over a Swin model with similar size. To support more research based on sliding-window attention, we open source our project and release our checkpoints at: https://github.com/SHI-Labs/Neighborhood-Attention-Transformer .",
    "original_url": "http://arxiv.org/pdf/2204.07143v5",
    "original_title": "Neighborhood Attention Transformer",
    "source": "arxiv",
    "authors": [
      "Ali Hassani",
      "Steven Walton",
      "Jiachen Li",
      "Shen Li",
      "Humphrey Shi"
    ],
    "published": "2022-04-14T17:55:15+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2204.07143v5"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2303.12068v1_chunk_0",
    "chunk_text": "Machine Learning for Brain Disorders: Transformers and Visual Transformers\n\nTransformers were initially introduced for natural language processing (NLP) tasks, but fast they were adopted by most deep learning fields, including computer vision. They measure the relationships between pairs of input tokens (words in the case of text strings, parts of images for visual Transformers), termed attention. The cost is exponential with the number of tokens. For image classification, the most common Transformer Architecture uses only the Transformer Encoder in order to transform the various input tokens. However, there are also numerous other applications in which the decoder part of the traditional Transformer Architecture is also used.",
    "original_url": "http://arxiv.org/pdf/2303.12068v1",
    "original_title": "Machine Learning for Brain Disorders: Transformers and Visual Transformers",
    "source": "arxiv",
    "authors": [
      "Robin Courant",
      "Maika Edberg",
      "Nicolas Dufour",
      "Vicky Kalogeiton"
    ],
    "published": "2023-03-21T17:57:33+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2303.12068v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2303.12068v1_chunk_1",
    "chunk_text": "However, there are also numerous other applications in which the decoder part of the traditional Transformer Architecture is also used. Here, we first introduce the Attention mechanism (Section 1), and then the Basic Transformer Block including the Vision Transformer (Section 2). Next, we discuss some improvements of visual Transformers to account for small datasets or less computation(Section 3). Finally, we introduce Visual Transformers applied to tasks other than image classification, such as detection, segmentation, generation and training without labels (Section 4) and other domains, such as video or multimodality using text or audio data (Section 5).",
    "original_url": "http://arxiv.org/pdf/2303.12068v1",
    "original_title": "Machine Learning for Brain Disorders: Transformers and Visual Transformers",
    "source": "arxiv",
    "authors": [
      "Robin Courant",
      "Maika Edberg",
      "Nicolas Dufour",
      "Vicky Kalogeiton"
    ],
    "published": "2023-03-21T17:57:33+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2303.12068v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1909.00015v2_chunk_0",
    "chunk_text": "Adaptively Sparse Transformers\n\nAttention mechanisms have become ubiquitous in NLP. Recent architectures, notably the Transformer, learn powerful context-aware word representations through layered, multi-headed attention. The multiple heads learn diverse types of word relationships. However, with standard softmax attention, all attention heads are dense, assigning a non-zero weight to all context words. In this work, we introduce the adaptively sparse Transformer, wherein attention heads have flexible, context-dependent sparsity patterns.",
    "original_url": "http://arxiv.org/pdf/1909.00015v2",
    "original_title": "Adaptively Sparse Transformers",
    "source": "arxiv",
    "authors": [
      "Gonçalo M. Correia",
      "Vlad Niculae",
      "André F. T. Martins"
    ],
    "published": "2019-08-30T18:06:14+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1909.00015v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1909.00015v2_chunk_1",
    "chunk_text": "In this work, we introduce the adaptively sparse Transformer, wherein attention heads have flexible, context-dependent sparsity patterns. This sparsity is accomplished by replacing softmax with $\\alpha$-entmax: a differentiable generalization of softmax that allows low-scoring words to receive precisely zero weight. Moreover, we derive a method to automatically learn the $\\alpha$ parameter -- which controls the shape and sparsity of $\\alpha$-entmax -- allowing attention heads to choose between focused or spread-out behavior. Our adaptively sparse Transformer improves interpretability and head diversity when compared to softmax Transformers on machine translation datasets. Findings of the quantitative and qualitative analysis of our approach include that heads in different layers learn different sparsity preferences and tend to be more diverse in their attention distributions than softmax Transformers.",
    "original_url": "http://arxiv.org/pdf/1909.00015v2",
    "original_title": "Adaptively Sparse Transformers",
    "source": "arxiv",
    "authors": [
      "Gonçalo M. Correia",
      "Vlad Niculae",
      "André F. T. Martins"
    ],
    "published": "2019-08-30T18:06:14+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1909.00015v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1909.00015v2_chunk_2",
    "chunk_text": "Findings of the quantitative and qualitative analysis of our approach include that heads in different layers learn different sparsity preferences and tend to be more diverse in their attention distributions than softmax Transformers. Furthermore, at no cost in accuracy, sparsity in attention heads helps to uncover different head specializations.",
    "original_url": "http://arxiv.org/pdf/1909.00015v2",
    "original_title": "Adaptively Sparse Transformers",
    "source": "arxiv",
    "authors": [
      "Gonçalo M. Correia",
      "Vlad Niculae",
      "André F. T. Martins"
    ],
    "published": "2019-08-30T18:06:14+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1909.00015v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2004.13342v2_chunk_0",
    "chunk_text": "Scheduled DropHead: A Regularization Method for Transformer Models\n\nIn this paper, we introduce DropHead, a structured dropout method specifically designed for regularizing the multi-head attention mechanism, which is a key component of transformer, a state-of-the-art model for various NLP tasks. In contrast to the conventional dropout mechanisms which randomly drop units or connections, the proposed DropHead is a structured dropout method. It drops entire attention-heads during training and It prevents the multi-head attention model from being dominated by a small portion of attention heads while also reduces the risk of overfitting the training data, thus making use of the multi-head attention mechanism more efficiently. Motivated by recent studies about the learning dynamic of the multi-head attention mechanism, we propose a specific dropout rate schedule to adaptively adjust the dropout rate of DropHead and achieve better regularization effect. Experimental results on both machine translation and text classification benchmark datasets demonstrate the effectiveness of the proposed approach.",
    "original_url": "http://arxiv.org/pdf/2004.13342v2",
    "original_title": "Scheduled DropHead: A Regularization Method for Transformer Models",
    "source": "arxiv",
    "authors": [
      "Wangchunshu Zhou",
      "Tao Ge",
      "Ke Xu",
      "Furu Wei",
      "Ming Zhou"
    ],
    "published": "2020-04-28T07:33:14+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2004.13342v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2004.13342v2_chunk_1",
    "chunk_text": "Experimental results on both machine translation and text classification benchmark datasets demonstrate the effectiveness of the proposed approach.",
    "original_url": "http://arxiv.org/pdf/2004.13342v2",
    "original_title": "Scheduled DropHead: A Regularization Method for Transformer Models",
    "source": "arxiv",
    "authors": [
      "Wangchunshu Zhou",
      "Tao Ge",
      "Ke Xu",
      "Furu Wei",
      "Ming Zhou"
    ],
    "published": "2020-04-28T07:33:14+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2004.13342v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2108.01377v1_chunk_0",
    "chunk_text": "A Dynamic Head Importance Computation Mechanism for Neural Machine Translation\n\nMultiple parallel attention mechanisms that use multiple attention heads facilitate greater performance of the Transformer model for various applications e.g., Neural Machine Translation (NMT), text classification. In multi-head attention mechanism, different heads attend to different parts of the input. However, the limitation is that multiple heads might attend to the same part of the input, resulting in multiple heads being redundant. Thus, the model resources are under-utilized. One approach to avoid this is to prune least important heads based on certain importance score.",
    "original_url": "http://arxiv.org/pdf/2108.01377v1",
    "original_title": "A Dynamic Head Importance Computation Mechanism for Neural Machine Translation",
    "source": "arxiv",
    "authors": [
      "Akshay Goindani",
      "Manish Shrivastava"
    ],
    "published": "2021-08-03T09:16:55+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2108.01377v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2108.01377v1_chunk_1",
    "chunk_text": "One approach to avoid this is to prune least important heads based on certain importance score. In this work, we focus on designing a Dynamic Head Importance Computation Mechanism (DHICM) to dynamically calculate the importance of a head with respect to the input. Our insight is to design an additional attention layer together with multi-head attention, and utilize the outputs of the multi-head attention along with the input, to compute the importance for each head. Additionally, we add an extra loss function to prevent the model from assigning same score to all heads, to identify more important heads and improvise performance. We analyzed performance of DHICM for NMT with different languages.",
    "original_url": "http://arxiv.org/pdf/2108.01377v1",
    "original_title": "A Dynamic Head Importance Computation Mechanism for Neural Machine Translation",
    "source": "arxiv",
    "authors": [
      "Akshay Goindani",
      "Manish Shrivastava"
    ],
    "published": "2021-08-03T09:16:55+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2108.01377v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2108.01377v1_chunk_2",
    "chunk_text": "We analyzed performance of DHICM for NMT with different languages. Experiments on different datasets show that DHICM outperforms traditional Transformer-based approach by large margin, especially, when less training data is available.",
    "original_url": "http://arxiv.org/pdf/2108.01377v1",
    "original_title": "A Dynamic Head Importance Computation Mechanism for Neural Machine Translation",
    "source": "arxiv",
    "authors": [
      "Akshay Goindani",
      "Manish Shrivastava"
    ],
    "published": "2021-08-03T09:16:55+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2108.01377v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2403.18276v2_chunk_0",
    "chunk_text": "RankMamba: Benchmarking Mamba's Document Ranking Performance in the Era of Transformers\n\nTransformer structure has achieved great success in multiple applied machine learning communities, such as natural language processing (NLP), computer vision (CV) and information retrieval (IR). Transformer architecture's core mechanism -- attention requires $O(n^2)$ time complexity in training and $O(n)$ time complexity in inference. Many works have been proposed to improve the attention mechanism's scalability, such as Flash Attention and Multi-query Attention. A different line of work aims to design new mechanisms to replace attention. Recently, a notable model structure -- Mamba, which is based on state space models, has achieved transformer-equivalent performance in multiple sequence modeling tasks.",
    "original_url": "http://arxiv.org/pdf/2403.18276v2",
    "original_title": "RankMamba: Benchmarking Mamba's Document Ranking Performance in the Era of Transformers",
    "source": "arxiv",
    "authors": [
      "Zhichao Xu"
    ],
    "published": "2024-03-27T06:07:05+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2403.18276v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2403.18276v2_chunk_1",
    "chunk_text": "Recently, a notable model structure -- Mamba, which is based on state space models, has achieved transformer-equivalent performance in multiple sequence modeling tasks. In this work, we examine \\mamba's efficacy through the lens of a classical IR task -- document ranking. A reranker model takes a query and a document as input, and predicts a scalar relevance score. This task demands the language model's ability to comprehend lengthy contextual inputs and to capture the interaction between query and document tokens. We find that (1) Mamba models achieve competitive performance compared to transformer-based models with the same training recipe; (2) but also have a lower training throughput in comparison to efficient transformer implementations such as flash attention.",
    "original_url": "http://arxiv.org/pdf/2403.18276v2",
    "original_title": "RankMamba: Benchmarking Mamba's Document Ranking Performance in the Era of Transformers",
    "source": "arxiv",
    "authors": [
      "Zhichao Xu"
    ],
    "published": "2024-03-27T06:07:05+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2403.18276v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2403.18276v2_chunk_2",
    "chunk_text": "We find that (1) Mamba models achieve competitive performance compared to transformer-based models with the same training recipe; (2) but also have a lower training throughput in comparison to efficient transformer implementations such as flash attention. We hope this study can serve as a starting point to explore Mamba models in other classical IR tasks. Our code implementation and trained checkpoints are made public to facilitate reproducibility (https://github.com/zhichaoxu-shufe/RankMamba).",
    "original_url": "http://arxiv.org/pdf/2403.18276v2",
    "original_title": "RankMamba: Benchmarking Mamba's Document Ranking Performance in the Era of Transformers",
    "source": "arxiv",
    "authors": [
      "Zhichao Xu"
    ],
    "published": "2024-03-27T06:07:05+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2403.18276v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2412.00085v1_chunk_0",
    "chunk_text": "Residual Attention Single-Head Vision Transformer Network for Rolling Bearing Fault Diagnosis in Noisy Environments\n\nRolling bearings play a crucial role in industrial machinery, directly influencing equipment performance, durability, and safety. However, harsh operating conditions, such as high speeds and temperatures, often lead to bearing malfunctions, resulting in downtime, economic losses, and safety hazards. This paper proposes the Residual Attention Single-Head Vision Transformer Network (RA-SHViT-Net) for fault diagnosis in rolling bearings. Vibration signals are transformed from the time to frequency domain using the Fast Fourier Transform (FFT) before being processed by RA-SHViT-Net. The model employs the Single-Head Vision Transformer (SHViT) to capture local and global features, balancing computational efficiency and predictive accuracy.",
    "original_url": "http://arxiv.org/pdf/2412.00085v1",
    "original_title": "Residual Attention Single-Head Vision Transformer Network for Rolling Bearing Fault Diagnosis in Noisy Environments",
    "source": "arxiv",
    "authors": [
      "Songjiang Lai",
      "Tsun-Hin Cheung",
      "Jiayi Zhao",
      "Kaiwen Xue",
      "Ka-Chun Fung",
      "Kin-Man Lam"
    ],
    "published": "2024-11-27T02:46:54+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2412.00085v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2412.00085v1_chunk_1",
    "chunk_text": "The model employs the Single-Head Vision Transformer (SHViT) to capture local and global features, balancing computational efficiency and predictive accuracy. To enhance feature extraction, the Adaptive Hybrid Attention Block (AHAB) integrates channel and spatial attention mechanisms. The network architecture includes Depthwise Convolution, Single-Head Self-Attention, Residual Feed-Forward Networks (Res-FFN), and AHAB modules, ensuring robust feature representation and mitigating gradient vanishing issues. Evaluation on the Case Western Reserve University and Paderborn University datasets demonstrates the RA-SHViT-Net's superior accuracy and robustness in complex, noisy environments. Ablation studies further validate the contributions of individual components, establishing RA-SHViT-Net as an effective tool for early fault detection and classification, promoting efficient maintenance strategies in industrial settings.",
    "original_url": "http://arxiv.org/pdf/2412.00085v1",
    "original_title": "Residual Attention Single-Head Vision Transformer Network for Rolling Bearing Fault Diagnosis in Noisy Environments",
    "source": "arxiv",
    "authors": [
      "Songjiang Lai",
      "Tsun-Hin Cheung",
      "Jiayi Zhao",
      "Kaiwen Xue",
      "Ka-Chun Fung",
      "Kin-Man Lam"
    ],
    "published": "2024-11-27T02:46:54+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2412.00085v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2412.00085v1_chunk_2",
    "chunk_text": "Ablation studies further validate the contributions of individual components, establishing RA-SHViT-Net as an effective tool for early fault detection and classification, promoting efficient maintenance strategies in industrial settings. Keywords: rolling bearings, fault diagnosis, Vision Transformer, attention mechanism, noisy environments, Fast Fourier Transform (FFT)",
    "original_url": "http://arxiv.org/pdf/2412.00085v1",
    "original_title": "Residual Attention Single-Head Vision Transformer Network for Rolling Bearing Fault Diagnosis in Noisy Environments",
    "source": "arxiv",
    "authors": [
      "Songjiang Lai",
      "Tsun-Hin Cheung",
      "Jiayi Zhao",
      "Kaiwen Xue",
      "Ka-Chun Fung",
      "Kin-Man Lam"
    ],
    "published": "2024-11-27T02:46:54+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2412.00085v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2411.14662v1_chunk_0",
    "chunk_text": "Multiset Transformer: Advancing Representation Learning in Persistence Diagrams\n\nTo improve persistence diagram representation learning, we propose Multiset Transformer. This is the first neural network that utilizes attention mechanisms specifically designed for multisets as inputs and offers rigorous theoretical guarantees of permutation invariance. The architecture integrates multiset-enhanced attentions with a pool-decomposition scheme, allowing multiplicities to be preserved across equivariant layers. This capability enables full leverage of multiplicities while significantly reducing both computational and spatial complexity compared to the Set Transformer. Additionally, our method can greatly benefit from clustering as a preprocessing step to further minimize complexity, an advantage not possessed by the Set Transformer.",
    "original_url": "http://arxiv.org/pdf/2411.14662v1",
    "original_title": "Multiset Transformer: Advancing Representation Learning in Persistence Diagrams",
    "source": "arxiv",
    "authors": [
      "Minghua Wang",
      "Ziyun Huang",
      "Jinhui Xu"
    ],
    "published": "2024-11-22T01:38:47+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2411.14662v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2411.14662v1_chunk_1",
    "chunk_text": "Additionally, our method can greatly benefit from clustering as a preprocessing step to further minimize complexity, an advantage not possessed by the Set Transformer. Experimental results demonstrate that the Multiset Transformer outperforms existing neural network methods in the realm of persistence diagram representation learning.",
    "original_url": "http://arxiv.org/pdf/2411.14662v1",
    "original_title": "Multiset Transformer: Advancing Representation Learning in Persistence Diagrams",
    "source": "arxiv",
    "authors": [
      "Minghua Wang",
      "Ziyun Huang",
      "Jinhui Xu"
    ],
    "published": "2024-11-22T01:38:47+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2411.14662v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2311.11959v1_chunk_0",
    "chunk_text": "Correlated Attention in Transformers for Multivariate Time Series\n\nMultivariate time series (MTS) analysis prevails in real-world applications such as finance, climate science and healthcare. The various self-attention mechanisms, the backbone of the state-of-the-art Transformer-based models, efficiently discover the temporal dependencies, yet cannot well capture the intricate cross-correlation between different features of MTS data, which inherently stems from complex dynamical systems in practice. To this end, we propose a novel correlated attention mechanism, which not only efficiently captures feature-wise dependencies, but can also be seamlessly integrated within the encoder blocks of existing well-known Transformers to gain efficiency improvement. In particular, correlated attention operates across feature channels to compute cross-covariance matrices between queries and keys with different lag values, and selectively aggregate representations at the sub-series level. This architecture facilitates automated discovery and representation learning of not only instantaneous but also lagged cross-correlations, while inherently capturing time series auto-correlation.",
    "original_url": "http://arxiv.org/pdf/2311.11959v1",
    "original_title": "Correlated Attention in Transformers for Multivariate Time Series",
    "source": "arxiv",
    "authors": [
      "Quang Minh Nguyen",
      "Lam M. Nguyen",
      "Subhro Das"
    ],
    "published": "2023-11-20T17:35:44+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2311.11959v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2311.11959v1_chunk_1",
    "chunk_text": "This architecture facilitates automated discovery and representation learning of not only instantaneous but also lagged cross-correlations, while inherently capturing time series auto-correlation. When combined with prevalent Transformer baselines, correlated attention mechanism constitutes a better alternative for encoder-only architectures, which are suitable for a wide range of tasks including imputation, anomaly detection and classification. Extensive experiments on the aforementioned tasks consistently underscore the advantages of correlated attention mechanism in enhancing base Transformer models, and demonstrate our state-of-the-art results in imputation, anomaly detection and classification.",
    "original_url": "http://arxiv.org/pdf/2311.11959v1",
    "original_title": "Correlated Attention in Transformers for Multivariate Time Series",
    "source": "arxiv",
    "authors": [
      "Quang Minh Nguyen",
      "Lam M. Nguyen",
      "Subhro Das"
    ],
    "published": "2023-11-20T17:35:44+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2311.11959v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2010.14816v1_chunk_0",
    "chunk_text": "Higher Order Linear Transformer\n\nFollowing up on the linear transformer part of the article from Katharopoulos et al., that takes this idea from Shen et al., the trick that produces a linear complexity for the attention mechanism is re-used and extended to a second-order approximation of the softmax normalization.",
    "original_url": "http://arxiv.org/pdf/2010.14816v1",
    "original_title": "Higher Order Linear Transformer",
    "source": "arxiv",
    "authors": [
      "Jean Mercat"
    ],
    "published": "2020-10-28T08:48:23+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2010.14816v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1908.11775v4_chunk_0",
    "chunk_text": "Transformer Dissection: A Unified Understanding of Transformer's Attention via the Lens of Kernel\n\nTransformer is a powerful architecture that achieves superior performance on various sequence learning tasks, including neural machine translation, language understanding, and sequence prediction. At the core of the Transformer is the attention mechanism, which concurrently processes all inputs in the streams. In this paper, we present a new formulation of attention via the lens of the kernel. To be more precise, we realize that the attention can be seen as applying kernel smoother over the inputs with the kernel scores being the similarities between inputs. This new formulation gives us a better way to understand individual components of the Transformer's attention, such as the better way to integrate the positional embedding.",
    "original_url": "http://arxiv.org/pdf/1908.11775v4",
    "original_title": "Transformer Dissection: A Unified Understanding of Transformer's Attention via the Lens of Kernel",
    "source": "arxiv",
    "authors": [
      "Yao-Hung Hubert Tsai",
      "Shaojie Bai",
      "Makoto Yamada",
      "Louis-Philippe Morency",
      "Ruslan Salakhutdinov"
    ],
    "published": "2019-08-30T15:05:02+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1908.11775v4"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1908.11775v4_chunk_1",
    "chunk_text": "This new formulation gives us a better way to understand individual components of the Transformer's attention, such as the better way to integrate the positional embedding. Another important advantage of our kernel-based formulation is that it paves the way to a larger space of composing Transformer's attention. As an example, we propose a new variant of Transformer's attention which models the input as a product of symmetric kernels. This approach achieves competitive performance to the current state of the art model with less computation. In our experiments, we empirically study different kernel construction strategies on two widely used tasks: neural machine translation and sequence prediction.",
    "original_url": "http://arxiv.org/pdf/1908.11775v4",
    "original_title": "Transformer Dissection: A Unified Understanding of Transformer's Attention via the Lens of Kernel",
    "source": "arxiv",
    "authors": [
      "Yao-Hung Hubert Tsai",
      "Shaojie Bai",
      "Makoto Yamada",
      "Louis-Philippe Morency",
      "Ruslan Salakhutdinov"
    ],
    "published": "2019-08-30T15:05:02+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1908.11775v4"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1908.11775v4_chunk_2",
    "chunk_text": "In our experiments, we empirically study different kernel construction strategies on two widely used tasks: neural machine translation and sequence prediction.",
    "original_url": "http://arxiv.org/pdf/1908.11775v4",
    "original_title": "Transformer Dissection: A Unified Understanding of Transformer's Attention via the Lens of Kernel",
    "source": "arxiv",
    "authors": [
      "Yao-Hung Hubert Tsai",
      "Shaojie Bai",
      "Makoto Yamada",
      "Louis-Philippe Morency",
      "Ruslan Salakhutdinov"
    ],
    "published": "2019-08-30T15:05:02+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1908.11775v4"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2306.01610v1_chunk_0",
    "chunk_text": "Centered Self-Attention Layers\n\nThe self-attention mechanism in transformers and the message-passing mechanism in graph neural networks are repeatedly applied within deep learning architectures. We show that this application inevitably leads to oversmoothing, i.e., to similar representations at the deeper layers for different tokens in transformers and different nodes in graph neural networks. Based on our analysis, we present a correction term to the aggregating operator of these mechanisms. Empirically, this simple term eliminates much of the oversmoothing problem in visual transformers, obtaining performance in weakly supervised segmentation that surpasses elaborate baseline methods that introduce multiple auxiliary networks and training phrases. In graph neural networks, the correction term enables the training of very deep architectures more effectively than many recent solutions to the same problem.",
    "original_url": "http://arxiv.org/pdf/2306.01610v1",
    "original_title": "Centered Self-Attention Layers",
    "source": "arxiv",
    "authors": [
      "Ameen Ali",
      "Tomer Galanti",
      "Lior Wolf"
    ],
    "published": "2023-06-02T15:19:08+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2306.01610v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2306.01610v1_chunk_1",
    "chunk_text": "In graph neural networks, the correction term enables the training of very deep architectures more effectively than many recent solutions to the same problem.",
    "original_url": "http://arxiv.org/pdf/2306.01610v1",
    "original_title": "Centered Self-Attention Layers",
    "source": "arxiv",
    "authors": [
      "Ameen Ali",
      "Tomer Galanti",
      "Lior Wolf"
    ],
    "published": "2023-06-02T15:19:08+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2306.01610v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1910.11871v1_chunk_0",
    "chunk_text": "Towards Online End-to-end Transformer Automatic Speech Recognition\n\nThe Transformer self-attention network has recently shown promising performance as an alternative to recurrent neural networks in end-to-end (E2E) automatic speech recognition (ASR) systems. However, Transformer has a drawback in that the entire input sequence is required to compute self-attention. We have proposed a block processing method for the Transformer encoder by introducing a context-aware inheritance mechanism. An additional context embedding vector handed over from the previously processed block helps to encode not only local acoustic information but also global linguistic, channel, and speaker attributes. In this paper, we extend it towards an entire online E2E ASR system by introducing an online decoding process inspired by monotonic chunkwise attention (MoChA) into the Transformer decoder.",
    "original_url": "http://arxiv.org/pdf/1910.11871v1",
    "original_title": "Towards Online End-to-end Transformer Automatic Speech Recognition",
    "source": "arxiv",
    "authors": [
      "Emiru Tsunoo",
      "Yosuke Kashiwagi",
      "Toshiyuki Kumakura",
      "Shinji Watanabe"
    ],
    "published": "2019-10-25T05:28:17+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1910.11871v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1910.11871v1_chunk_1",
    "chunk_text": "In this paper, we extend it towards an entire online E2E ASR system by introducing an online decoding process inspired by monotonic chunkwise attention (MoChA) into the Transformer decoder. Our novel MoChA training and inference algorithms exploit the unique properties of Transformer, whose attentions are not always monotonic or peaky, and have multiple heads and residual connections of the decoder layers. Evaluations of the Wall Street Journal (WSJ) and AISHELL-1 show that our proposed online Transformer decoder outperforms conventional chunkwise approaches.",
    "original_url": "http://arxiv.org/pdf/1910.11871v1",
    "original_title": "Towards Online End-to-end Transformer Automatic Speech Recognition",
    "source": "arxiv",
    "authors": [
      "Emiru Tsunoo",
      "Yosuke Kashiwagi",
      "Toshiyuki Kumakura",
      "Shinji Watanabe"
    ],
    "published": "2019-10-25T05:28:17+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1910.11871v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2310.07323v1_chunk_0",
    "chunk_text": "Multichannel consecutive data cross-extraction with 1DCNN-attention for diagnosis of power transformer\n\nPower transformer plays a critical role in grid infrastructure, and its diagnosis is paramount for maintaining stable operation. However, the current methods for transformer diagnosis focus on discrete dissolved gas analysis, neglecting deep feature extraction of multichannel consecutive data. The unutilized sequential data contains the significant temporal information reflecting the transformer condition. In light of this, the structure of multichannel consecutive data cross-extraction (MCDC) is proposed in this article in order to comprehensively exploit the intrinsic characteristic and evaluate the states of transformer. Moreover, for the better accommodation in scenario of transformer diagnosis, one dimensional convolution neural network attention (1DCNN-attention) mechanism is introduced and offers a more efficient solution given the simplified spatial complexity.",
    "original_url": "http://arxiv.org/pdf/2310.07323v1",
    "original_title": "Multichannel consecutive data cross-extraction with 1DCNN-attention for diagnosis of power transformer",
    "source": "arxiv",
    "authors": [
      "Wei Zheng",
      "Guogang Zhang",
      "Chenchen Zhao",
      "Qianqian Zhu"
    ],
    "published": "2023-10-11T09:14:17+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2310.07323v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2310.07323v1_chunk_1",
    "chunk_text": "Moreover, for the better accommodation in scenario of transformer diagnosis, one dimensional convolution neural network attention (1DCNN-attention) mechanism is introduced and offers a more efficient solution given the simplified spatial complexity. Finally, the effectiveness of MCDC and the superior generalization ability, compared with other algorithms, are validated in experiments conducted on a dataset collected from real operation cases of power transformer. Additionally, the better stability of 1DCNN-attention has also been certified.",
    "original_url": "http://arxiv.org/pdf/2310.07323v1",
    "original_title": "Multichannel consecutive data cross-extraction with 1DCNN-attention for diagnosis of power transformer",
    "source": "arxiv",
    "authors": [
      "Wei Zheng",
      "Guogang Zhang",
      "Chenchen Zhao",
      "Qianqian Zhu"
    ],
    "published": "2023-10-11T09:14:17+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2310.07323v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2409.19764v3_chunk_0",
    "chunk_text": "Spiking Transformer with Spatial-Temporal Attention\n\nSpike-based Transformer presents a compelling and energy-efficient alternative to traditional Artificial Neural Network (ANN)-based Transformers, achieving impressive results through sparse binary computations. However, existing spike-based transformers predominantly focus on spatial attention while neglecting crucial temporal dependencies inherent in spike-based processing, leading to suboptimal feature representation and limited performance. To address this limitation, we propose Spiking Transformer with Spatial-Temporal Attention (STAtten), a simple and straightforward architecture that efficiently integrates both spatial and temporal information in the self-attention mechanism. STAtten introduces a block-wise computation strategy that processes information in spatial-temporal chunks, enabling comprehensive feature capture while maintaining the same computational complexity as previous spatial-only approaches. Our method can be seamlessly integrated into existing spike-based transformers without architectural overhaul.",
    "original_url": "http://arxiv.org/pdf/2409.19764v3",
    "original_title": "Spiking Transformer with Spatial-Temporal Attention",
    "source": "arxiv",
    "authors": [
      "Donghyun Lee",
      "Yuhang Li",
      "Youngeun Kim",
      "Shiting Xiao",
      "Priyadarshini Panda"
    ],
    "published": "2024-09-29T20:29:39+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2409.19764v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2409.19764v3_chunk_1",
    "chunk_text": "Our method can be seamlessly integrated into existing spike-based transformers without architectural overhaul. Extensive experiments demonstrate that STAtten significantly improves the performance of existing spike-based transformers across both static and neuromorphic datasets, including CIFAR10/100, ImageNet, CIFAR10-DVS, and N-Caltech101. The code is available at https://github.com/Intelligent-Computing-Lab-Yale/STAtten",
    "original_url": "http://arxiv.org/pdf/2409.19764v3",
    "original_title": "Spiking Transformer with Spatial-Temporal Attention",
    "source": "arxiv",
    "authors": [
      "Donghyun Lee",
      "Yuhang Li",
      "Youngeun Kim",
      "Shiting Xiao",
      "Priyadarshini Panda"
    ],
    "published": "2024-09-29T20:29:39+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2409.19764v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2108.12784v2_chunk_0",
    "chunk_text": "TCCT: Tightly-Coupled Convolutional Transformer on Time Series Forecasting\n\nTime series forecasting is essential for a wide range of real-world applications. Recent studies have shown the superiority of Transformer in dealing with such problems, especially long sequence time series input(LSTI) and long sequence time series forecasting(LSTF) problems. To improve the efficiency and enhance the locality of Transformer, these studies combine Transformer with CNN in varying degrees. However, their combinations are loosely-coupled and do not make full use of CNN. To address this issue, we propose the concept of tightly-coupled convolutional Transformer(TCCT) and three TCCT architectures which apply transformed CNN architectures into Transformer: (1) CSPAttention: through fusing CSPNet with self-attention mechanism, the computation cost of self-attention mechanism is reduced by 30% and the memory usage is reduced by 50% while achieving equivalent or beyond prediction accuracy.",
    "original_url": "http://arxiv.org/pdf/2108.12784v2",
    "original_title": "TCCT: Tightly-Coupled Convolutional Transformer on Time Series Forecasting",
    "source": "arxiv",
    "authors": [
      "Li Shen",
      "Yangzhu Wang"
    ],
    "published": "2021-08-29T08:49:31+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2108.12784v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2108.12784v2_chunk_1",
    "chunk_text": "To address this issue, we propose the concept of tightly-coupled convolutional Transformer(TCCT) and three TCCT architectures which apply transformed CNN architectures into Transformer: (1) CSPAttention: through fusing CSPNet with self-attention mechanism, the computation cost of self-attention mechanism is reduced by 30% and the memory usage is reduced by 50% while achieving equivalent or beyond prediction accuracy. (2) Dilated causal convolution: this method is to modify the distilling operation proposed by Informer through replacing canonical convolutional layers with dilated causal convolutional layers to gain exponentially receptive field growth. (3) Passthrough mechanism: the application of passthrough mechanism to stack of self-attention blocks helps Transformer-like models get more fine-grained information with negligible extra computation costs. Our experiments on real-world datasets show that our TCCT architectures could greatly improve the performance of existing state-of-art Transformer models on time series forecasting with much lower computation and memory costs, including canonical Transformer, LogTrans and Informer.",
    "original_url": "http://arxiv.org/pdf/2108.12784v2",
    "original_title": "TCCT: Tightly-Coupled Convolutional Transformer on Time Series Forecasting",
    "source": "arxiv",
    "authors": [
      "Li Shen",
      "Yangzhu Wang"
    ],
    "published": "2021-08-29T08:49:31+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2108.12784v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2411.07176v3_chunk_0",
    "chunk_text": "More Expressive Attention with Negative Weights\n\nWe propose a novel attention mechanism, named Cog Attention, that enables attention weights to be negative for enhanced expressiveness, which stems from two key factors: (1) Cog Attention enhances parameter flexibility. For example, unlike traditional softmax attention heads that use a static output-value (OV) matrix to delete or copy inputs that the heads attend to, Cog Attention naturally learns to use the sign of dynamic query-key (QK) inner products to represent these operations. This enables Cog Attention to perform multiple operations simultaneously within a single head. Meanwhile, Cog Attention's OV matrix can focus more on refinement or modification. (2) Cog Attention enhances the model's robustness against representational collapse by preventing the ``over-squashing'' of earlier tokens into later positions.",
    "original_url": "http://arxiv.org/pdf/2411.07176v3",
    "original_title": "More Expressive Attention with Negative Weights",
    "source": "arxiv",
    "authors": [
      "Ang Lv",
      "Ruobing Xie",
      "Shuaipeng Li",
      "Jiayi Liao",
      "Xingwu Sun",
      "Zhanhui Kang",
      "Di Wang",
      "Rui Yan"
    ],
    "published": "2024-11-11T17:56:28+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2411.07176v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2411.07176v3_chunk_1",
    "chunk_text": "(2) Cog Attention enhances the model's robustness against representational collapse by preventing the ``over-squashing'' of earlier tokens into later positions. We develop Transformer-like models which use Cog Attention as attention modules, including decoder-only models at various scales for language modeling and U-ViT diffusion models for image generation. Experiments show that models using Cog Attention exhibit superior performance compared to those employing traditional softmax attention modules. Our approach suggests a promising research direction for rethinking and breaking the entrenched constraints of traditional softmax attention, such as the requirement for non-negative weights.",
    "original_url": "http://arxiv.org/pdf/2411.07176v3",
    "original_title": "More Expressive Attention with Negative Weights",
    "source": "arxiv",
    "authors": [
      "Ang Lv",
      "Ruobing Xie",
      "Shuaipeng Li",
      "Jiayi Liao",
      "Xingwu Sun",
      "Zhanhui Kang",
      "Di Wang",
      "Rui Yan"
    ],
    "published": "2024-11-11T17:56:28+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2411.07176v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2405.18781v2_chunk_0",
    "chunk_text": "On the Role of Attention Masks and LayerNorm in Transformers\n\nSelf-attention is the key mechanism of transformers, which are the essential building blocks of modern foundation models. Recent studies have shown that pure self-attention suffers from an increasing degree of rank collapse as depth increases, limiting model expressivity and further utilization of model depth. The existing literature on rank collapse, however, has mostly overlooked other critical components in transformers that may alleviate the rank collapse issue. In this paper, we provide a general analysis of rank collapse under self-attention, taking into account the effects of attention masks and layer normalization (LayerNorm). In particular, we find that although pure masked attention still suffers from exponential collapse to a rank one subspace, sparse or local masked attention can provably slow down the collapse rate.",
    "original_url": "http://arxiv.org/pdf/2405.18781v2",
    "original_title": "On the Role of Attention Masks and LayerNorm in Transformers",
    "source": "arxiv",
    "authors": [
      "Xinyi Wu",
      "Amir Ajorlou",
      "Yifei Wang",
      "Stefanie Jegelka",
      "Ali Jadbabaie"
    ],
    "published": "2024-05-29T05:41:28+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2405.18781v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2405.18781v2_chunk_1",
    "chunk_text": "In particular, we find that although pure masked attention still suffers from exponential collapse to a rank one subspace, sparse or local masked attention can provably slow down the collapse rate. In the case of self-attention with LayerNorm, we first show that for certain classes of value matrices, collapse to a rank one subspace still happens exponentially. However, through construction of nontrivial counterexamples, we then establish that with proper choice of value matrices, a general class of sequences may not converge to a rank one subspace, and the self-attention dynamics with LayerNorm can simultaneously possess a rich set of equilibria with any possible rank between one and full. Our result refutes the previous hypothesis that LayerNorm plays no role in the rank collapse of self-attention and suggests that self-attention with LayerNorm constitutes a much more expressive, versatile nonlinear dynamical system than what was originally thought.",
    "original_url": "http://arxiv.org/pdf/2405.18781v2",
    "original_title": "On the Role of Attention Masks and LayerNorm in Transformers",
    "source": "arxiv",
    "authors": [
      "Xinyi Wu",
      "Amir Ajorlou",
      "Yifei Wang",
      "Stefanie Jegelka",
      "Ali Jadbabaie"
    ],
    "published": "2024-05-29T05:41:28+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2405.18781v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2004.05150v2_chunk_0",
    "chunk_text": "Longformer: The Long-Document Transformer\n\nTransformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks.",
    "original_url": "http://arxiv.org/pdf/2004.05150v2",
    "original_title": "Longformer: The Long-Document Transformer",
    "source": "arxiv",
    "authors": [
      "Iz Beltagy",
      "Matthew E. Peters",
      "Arman Cohan"
    ],
    "published": "2020-04-10T17:54:09+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2004.05150v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2004.05150v2_chunk_1",
    "chunk_text": "In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a Longformer variant for supporting long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization dataset.",
    "original_url": "http://arxiv.org/pdf/2004.05150v2",
    "original_title": "Longformer: The Long-Document Transformer",
    "source": "arxiv",
    "authors": [
      "Iz Beltagy",
      "Matthew E. Peters",
      "Arman Cohan"
    ],
    "published": "2020-04-10T17:54:09+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2004.05150v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2207.10498v1_chunk_0",
    "chunk_text": "Towards Efficient Adversarial Training on Vision Transformers\n\nVision Transformer (ViT), as a powerful alternative to Convolutional Neural Network (CNN), has received much attention. Recent work showed that ViTs are also vulnerable to adversarial examples like CNNs. To build robust ViTs, an intuitive way is to apply adversarial training since it has been shown as one of the most effective ways to accomplish robust CNNs. However, one major limitation of adversarial training is its heavy computational cost. The self-attention mechanism adopted by ViTs is a computationally intense operation whose expense increases quadratically with the number of input patches, making adversarial training on ViTs even more time-consuming.",
    "original_url": "http://arxiv.org/pdf/2207.10498v1",
    "original_title": "Towards Efficient Adversarial Training on Vision Transformers",
    "source": "arxiv",
    "authors": [
      "Boxi Wu",
      "Jindong Gu",
      "Zhifeng Li",
      "Deng Cai",
      "Xiaofei He",
      "Wei Liu"
    ],
    "published": "2022-07-21T14:23:50+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2207.10498v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2207.10498v1_chunk_1",
    "chunk_text": "The self-attention mechanism adopted by ViTs is a computationally intense operation whose expense increases quadratically with the number of input patches, making adversarial training on ViTs even more time-consuming. In this work, we first comprehensively study fast adversarial training on a variety of vision transformers and illustrate the relationship between the efficiency and robustness. Then, to expediate adversarial training on ViTs, we propose an efficient Attention Guided Adversarial Training mechanism. Specifically, relying on the specialty of self-attention, we actively remove certain patch embeddings of each layer with an attention-guided dropping strategy during adversarial training. The slimmed self-attention modules accelerate the adversarial training on ViTs significantly.",
    "original_url": "http://arxiv.org/pdf/2207.10498v1",
    "original_title": "Towards Efficient Adversarial Training on Vision Transformers",
    "source": "arxiv",
    "authors": [
      "Boxi Wu",
      "Jindong Gu",
      "Zhifeng Li",
      "Deng Cai",
      "Xiaofei He",
      "Wei Liu"
    ],
    "published": "2022-07-21T14:23:50+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2207.10498v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2207.10498v1_chunk_2",
    "chunk_text": "The slimmed self-attention modules accelerate the adversarial training on ViTs significantly. With only 65\\% of the fast adversarial training time, we match the state-of-the-art results on the challenging ImageNet benchmark.",
    "original_url": "http://arxiv.org/pdf/2207.10498v1",
    "original_title": "Towards Efficient Adversarial Training on Vision Transformers",
    "source": "arxiv",
    "authors": [
      "Boxi Wu",
      "Jindong Gu",
      "Zhifeng Li",
      "Deng Cai",
      "Xiaofei He",
      "Wei Liu"
    ],
    "published": "2022-07-21T14:23:50+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2207.10498v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2405.21061v2_chunk_0",
    "chunk_text": "Graph External Attention Enhanced Transformer\n\nThe Transformer architecture has recently gained considerable attention in the field of graph representation learning, as it naturally overcomes several limitations of Graph Neural Networks (GNNs) with customized attention mechanisms or positional and structural encodings. Despite making some progress, existing works tend to overlook external information of graphs, specifically the correlation between graphs. Intuitively, graphs with similar structures should have similar representations. Therefore, we propose Graph External Attention (GEA) -- a novel attention mechanism that leverages multiple external node/edge key-value units to capture inter-graph correlations implicitly. On this basis, we design an effective architecture called Graph External Attention Enhanced Transformer (GEAET), which integrates local structure and global interaction information for more comprehensive graph representations.",
    "original_url": "http://arxiv.org/pdf/2405.21061v2",
    "original_title": "Graph External Attention Enhanced Transformer",
    "source": "arxiv",
    "authors": [
      "Jianqing Liang",
      "Min Chen",
      "Jiye Liang"
    ],
    "published": "2024-05-31T17:50:27+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2405.21061v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2405.21061v2_chunk_1",
    "chunk_text": "On this basis, we design an effective architecture called Graph External Attention Enhanced Transformer (GEAET), which integrates local structure and global interaction information for more comprehensive graph representations. Extensive experiments on benchmark datasets demonstrate that GEAET achieves state-of-the-art empirical performance. The source code is available for reproducibility at: https://github.com/icm1018/GEAET.",
    "original_url": "http://arxiv.org/pdf/2405.21061v2",
    "original_title": "Graph External Attention Enhanced Transformer",
    "source": "arxiv",
    "authors": [
      "Jianqing Liang",
      "Min Chen",
      "Jiye Liang"
    ],
    "published": "2024-05-31T17:50:27+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2405.21061v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2408.11656v1_chunk_0",
    "chunk_text": "Macformer: Transformer with Random Maclaurin Feature Attention\n\nRandom feature attention (RFA) adopts random fourier feature (RFF) methods to approximate the softmax function, resulting in a linear time and space attention mechanism that enables the construction of an efficient Transformer. Inspired by RFA, we propose Macformer, a Transformer architecture that employs random Maclaurin features (RMF) to approximate various dot-product kernels, thereby accelerating attention computations for long sequence. Macformer consists of Random Maclaurin Feature Attention (RMFA) and pre-post Scaling Batch Normalization (ppSBN), the former is an unbiased approximation for dot-product kernelized attention and the later is a two-stage regularization mechanism guaranteeing the error of RMFA. We conducted toy experiments to demonstrate the efficiency of RMFA and ppSBN, and experiments on long range arena (LRA) benchmark to validate the acceleration and accuracy of Macformer with different dot-product kernels. Experiment results of Macformer are consistent with our theoretical analysis.",
    "original_url": "http://arxiv.org/pdf/2408.11656v1",
    "original_title": "Macformer: Transformer with Random Maclaurin Feature Attention",
    "source": "arxiv",
    "authors": [
      "Yuhan Guo",
      "Lizhong Ding",
      "Ye Yuan",
      "Guoren Wang"
    ],
    "published": "2024-08-21T14:27:36+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2408.11656v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2408.11656v1_chunk_1",
    "chunk_text": "Experiment results of Macformer are consistent with our theoretical analysis.",
    "original_url": "http://arxiv.org/pdf/2408.11656v1",
    "original_title": "Macformer: Transformer with Random Maclaurin Feature Attention",
    "source": "arxiv",
    "authors": [
      "Yuhan Guo",
      "Lizhong Ding",
      "Ye Yuan",
      "Guoren Wang"
    ],
    "published": "2024-08-21T14:27:36+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2408.11656v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2502.11469v1_chunk_0",
    "chunk_text": "If Attention Serves as a Cognitive Model of Human Memory Retrieval, What is the Plausible Memory Representation? Recent work in computational psycholinguistics has revealed intriguing parallels between attention mechanisms and human memory retrieval, focusing primarily on Transformer architectures that operate on token-level representations. However, computational psycholinguistic research has also established that syntactic structures provide compelling explanations for human sentence processing that word-level factors alone cannot fully account for. In this study, we investigate whether the attention mechanism of Transformer Grammar (TG), which uniquely operates on syntactic structures as representational units, can serve as a cognitive model of human memory retrieval, using Normalized Attention Entropy (NAE) as a linking hypothesis between model behavior and human processing difficulty. Our experiments demonstrate that TG's attention achieves superior predictive power for self-paced reading times compared to vanilla Transformer's, with further analyses revealing independent contributions from both models.",
    "original_url": "http://arxiv.org/pdf/2502.11469v1",
    "original_title": "If Attention Serves as a Cognitive Model of Human Memory Retrieval, What is the Plausible Memory Representation?",
    "source": "arxiv",
    "authors": [
      "Ryo Yoshida",
      "Shinnosuke Isono",
      "Kohei Kajikawa",
      "Taiga Someya",
      "Yushi Sugimito",
      "Yohei Oseki"
    ],
    "published": "2025-02-17T05:58:25+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2502.11469v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2502.11469v1_chunk_1",
    "chunk_text": "Our experiments demonstrate that TG's attention achieves superior predictive power for self-paced reading times compared to vanilla Transformer's, with further analyses revealing independent contributions from both models. These findings suggest that human sentence processing involves dual memory representations -- one based on syntactic structures and another on token sequences -- with attention serving as the general retrieval algorithm, while highlighting the importance of incorporating syntactic structures as representational units.",
    "original_url": "http://arxiv.org/pdf/2502.11469v1",
    "original_title": "If Attention Serves as a Cognitive Model of Human Memory Retrieval, What is the Plausible Memory Representation?",
    "source": "arxiv",
    "authors": [
      "Ryo Yoshida",
      "Shinnosuke Isono",
      "Kohei Kajikawa",
      "Taiga Someya",
      "Yushi Sugimito",
      "Yohei Oseki"
    ],
    "published": "2025-02-17T05:58:25+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2502.11469v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2503.11108v2_chunk_0",
    "chunk_text": "Time and Memory Trade-off of KV-Cache Compression in Tensor Transformer Decoding\n\nThe key-value (KV) cache in the tensor version of transformers presents a significant bottleneck during inference. While previous work analyzes the fundamental space complexity barriers in standard attention mechanisms [Haris and Onak, 2025], our work generalizes the space complexity barriers result to tensor attention version. Our theoretical contributions rely on a reduction from communication complexity and deduce the memory lower bound for tensor-structured attention mechanisms when $d = \\Omega(\\log n)$. Furthermore, we introduce two types of tensor attention cache and present a trade-off between time and memory for two scenarios. Overall, our work provides a theoretical foundation for us to understand the time-memory tradeoff of KV-Cache compression in tensor attention decoding and offers more perspectives in developing more memory-efficient tensor attention Transformer architectures.",
    "original_url": "http://arxiv.org/pdf/2503.11108v2",
    "original_title": "Time and Memory Trade-off of KV-Cache Compression in Tensor Transformer Decoding",
    "source": "arxiv",
    "authors": [
      "Yifang Chen",
      "Xiaoyu Li",
      "Yingyu Liang",
      "Zhenmei Shi",
      "Zhao Song",
      "Yu Tian"
    ],
    "published": "2025-03-14T06:01:42+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2503.11108v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2503.11108v2_chunk_1",
    "chunk_text": "Overall, our work provides a theoretical foundation for us to understand the time-memory tradeoff of KV-Cache compression in tensor attention decoding and offers more perspectives in developing more memory-efficient tensor attention Transformer architectures.",
    "original_url": "http://arxiv.org/pdf/2503.11108v2",
    "original_title": "Time and Memory Trade-off of KV-Cache Compression in Tensor Transformer Decoding",
    "source": "arxiv",
    "authors": [
      "Yifang Chen",
      "Xiaoyu Li",
      "Yingyu Liang",
      "Zhenmei Shi",
      "Zhao Song",
      "Yu Tian"
    ],
    "published": "2025-03-14T06:01:42+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2503.11108v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2010.03045v2_chunk_0",
    "chunk_text": "Rotate to Attend: Convolutional Triplet Attention Module\n\nBenefiting from the capability of building inter-dependencies among channels or spatial locations, attention mechanisms have been extensively studied and broadly used in a variety of computer vision tasks recently. In this paper, we investigate light-weight but effective attention mechanisms and present triplet attention, a novel method for computing attention weights by capturing cross-dimension interaction using a three-branch structure. For an input tensor, triplet attention builds inter-dimensional dependencies by the rotation operation followed by residual transformations and encodes inter-channel and spatial information with negligible computational overhead. Our method is simple as well as efficient and can be easily plugged into classic backbone networks as an add-on module. We demonstrate the effectiveness of our method on various challenging tasks including image classification on ImageNet-1k and object detection on MSCOCO and PASCAL VOC datasets.",
    "original_url": "http://arxiv.org/pdf/2010.03045v2",
    "original_title": "Rotate to Attend: Convolutional Triplet Attention Module",
    "source": "arxiv",
    "authors": [
      "Diganta Misra",
      "Trikay Nalamada",
      "Ajay Uppili Arasanipalai",
      "Qibin Hou"
    ],
    "published": "2020-10-06T21:31:00+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2010.03045v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2010.03045v2_chunk_1",
    "chunk_text": "We demonstrate the effectiveness of our method on various challenging tasks including image classification on ImageNet-1k and object detection on MSCOCO and PASCAL VOC datasets. Furthermore, we provide extensive in-sight into the performance of triplet attention by visually inspecting the GradCAM and GradCAM++ results. The empirical evaluation of our method supports our intuition on the importance of capturing dependencies across dimensions when computing attention weights. Code for this paper can be publicly accessed at https://github.com/LandskapeAI/triplet-attention",
    "original_url": "http://arxiv.org/pdf/2010.03045v2",
    "original_title": "Rotate to Attend: Convolutional Triplet Attention Module",
    "source": "arxiv",
    "authors": [
      "Diganta Misra",
      "Trikay Nalamada",
      "Ajay Uppili Arasanipalai",
      "Qibin Hou"
    ],
    "published": "2020-10-06T21:31:00+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2010.03045v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2107.06493v1_chunk_0",
    "chunk_text": "Serialized Multi-Layer Multi-Head Attention for Neural Speaker Embedding\n\nThis paper proposes a serialized multi-layer multi-head attention for neural speaker embedding in text-independent speaker verification. In prior works, frame-level features from one layer are aggregated to form an utterance-level representation. Inspired by the Transformer network, our proposed method utilizes the hierarchical architecture of stacked self-attention mechanisms to derive refined features that are more correlated with speakers. Serialized attention mechanism contains a stack of self-attention modules to create fixed-dimensional representations of speakers. Instead of utilizing multi-head attention in parallel, the proposed serialized multi-layer multi-head attention is designed to aggregate and propagate attentive statistics from one layer to the next in a serialized manner.",
    "original_url": "http://arxiv.org/pdf/2107.06493v1",
    "original_title": "Serialized Multi-Layer Multi-Head Attention for Neural Speaker Embedding",
    "source": "arxiv",
    "authors": [
      "Hongning Zhu",
      "Kong Aik Lee",
      "Haizhou Li"
    ],
    "published": "2021-07-14T05:38:48+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2107.06493v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2107.06493v1_chunk_1",
    "chunk_text": "Instead of utilizing multi-head attention in parallel, the proposed serialized multi-layer multi-head attention is designed to aggregate and propagate attentive statistics from one layer to the next in a serialized manner. In addition, we employ an input-aware query for each utterance with the statistics pooling. With more layers stacked, the neural network can learn more discriminative speaker embeddings. Experiment results on VoxCeleb1 dataset and SITW dataset show that our proposed method outperforms other baseline methods, including x-vectors and other x-vectors + conventional attentive pooling approaches by 9.7% in EER and 8.1% in DCF0.01.",
    "original_url": "http://arxiv.org/pdf/2107.06493v1",
    "original_title": "Serialized Multi-Layer Multi-Head Attention for Neural Speaker Embedding",
    "source": "arxiv",
    "authors": [
      "Hongning Zhu",
      "Kong Aik Lee",
      "Haizhou Li"
    ],
    "published": "2021-07-14T05:38:48+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2107.06493v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2211.13940v1_chunk_0",
    "chunk_text": "Spatial-Temporal Attention Network for Open-Set Fine-Grained Image Recognition\n\nTriggered by the success of transformers in various visual tasks, the spatial self-attention mechanism has recently attracted more and more attention in the computer vision community. However, we empirically found that a typical vision transformer with the spatial self-attention mechanism could not learn accurate attention maps for distinguishing different categories of fine-grained images. To address this problem, motivated by the temporal attention mechanism in brains, we propose a spatial-temporal attention network for learning fine-grained feature representations, called STAN, where the features learnt by implementing a sequence of spatial self-attention operations corresponding to multiple moments are aggregated progressively. The proposed STAN consists of four modules: a self-attention backbone module for learning a sequence of features with self-attention operations, a spatial feature self-organizing module for facilitating the model training, a spatial-temporal feature learning module for aggregating the re-organized features via a Long Short-Term Memory network, and a context-aware module that is implemented as the forget block of the spatial-temporal feature learning module for preserving/forgetting the long-term memory by utilizing contextual information. Then, we propose a STAN-based method for open-set fine-grained recognition by integrating the proposed STAN network with a linear classifier, called STAN-OSFGR.",
    "original_url": "http://arxiv.org/pdf/2211.13940v1",
    "original_title": "Spatial-Temporal Attention Network for Open-Set Fine-Grained Image Recognition",
    "source": "arxiv",
    "authors": [
      "Jiayin Sun",
      "Hong Wang",
      "Qiulei Dong"
    ],
    "published": "2022-11-25T07:46:42+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2211.13940v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2211.13940v1_chunk_1",
    "chunk_text": "Then, we propose a STAN-based method for open-set fine-grained recognition by integrating the proposed STAN network with a linear classifier, called STAN-OSFGR. Extensive experimental results on 3 fine-grained datasets and 2 coarse-grained datasets demonstrate that the proposed STAN-OSFGR outperforms 9 state-of-the-art open-set recognition methods significantly in most cases.",
    "original_url": "http://arxiv.org/pdf/2211.13940v1",
    "original_title": "Spatial-Temporal Attention Network for Open-Set Fine-Grained Image Recognition",
    "source": "arxiv",
    "authors": [
      "Jiayin Sun",
      "Hong Wang",
      "Qiulei Dong"
    ],
    "published": "2022-11-25T07:46:42+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2211.13940v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1911.03897v1_chunk_0",
    "chunk_text": "Two-Headed Monster And Crossed Co-Attention Networks\n\nThis paper presents some preliminary investigations of a new co-attention mechanism in neural transduction models. We propose a paradigm, termed Two-Headed Monster (THM), which consists of two symmetric encoder modules and one decoder module connected with co-attention. As a specific and concrete implementation of THM, Crossed Co-Attention Networks (CCNs) are designed based on the Transformer model. We demonstrate CCNs on WMT 2014 EN-DE and WMT 2016 EN-FI translation tasks and our model outperforms the strong Transformer baseline by 0.51 (big) and 0.74 (base) BLEU points on EN-DE and by 0.17 (big) and 0.47 (base) BLEU points on EN-FI.",
    "original_url": "http://arxiv.org/pdf/1911.03897v1",
    "original_title": "Two-Headed Monster And Crossed Co-Attention Networks",
    "source": "arxiv",
    "authors": [
      "Yaoyiran Li",
      "Jing Jiang"
    ],
    "published": "2019-11-10T10:55:12+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1911.03897v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2210.04243v1_chunk_0",
    "chunk_text": "Fine-Tuning Pre-trained Transformers into Decaying Fast Weights\n\nAutoregressive Transformers are strong language models but incur O(T) complexity during per-token generation due to the self-attention mechanism. Recent work proposes kernel-based methods to approximate causal self-attention by replacing it with recurrent formulations with various update rules and feature maps to achieve O(1) time and memory complexity. We explore these approaches and find that they are unnecessarily complex, and propose a simple alternative - decaying fast weights - that runs fast on GPU, outperforms prior methods, and retains 99% of attention's performance for GPT-2. We also show competitive performance on WikiText-103 against more complex attention substitutes.",
    "original_url": "http://arxiv.org/pdf/2210.04243v1",
    "original_title": "Fine-Tuning Pre-trained Transformers into Decaying Fast Weights",
    "source": "arxiv",
    "authors": [
      "Huanru Henry Mao"
    ],
    "published": "2022-10-09T12:27:25+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2210.04243v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2211.04076v1_chunk_0",
    "chunk_text": "Linear Self-Attention Approximation via Trainable Feedforward Kernel\n\nIn pursuit of faster computation, Efficient Transformers demonstrate an impressive variety of approaches -- models attaining sub-quadratic attention complexity can utilize a notion of sparsity or a low-rank approximation of inputs to reduce the number of attended keys; other ways to reduce complexity include locality-sensitive hashing, key pooling, additional memory to store information in compacted or hybridization with other architectures, such as CNN. Often based on a strong mathematical basis, kernelized approaches allow for the approximation of attention with linear complexity while retaining high accuracy. Therefore, in the present paper, we aim to expand the idea of trainable kernel methods to approximate the self-attention mechanism of the Transformer architecture.",
    "original_url": "http://arxiv.org/pdf/2211.04076v1",
    "original_title": "Linear Self-Attention Approximation via Trainable Feedforward Kernel",
    "source": "arxiv",
    "authors": [
      "Uladzislau Yorsh",
      "Alexander Kovalenko"
    ],
    "published": "2022-11-08T08:14:11+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2211.04076v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2002.10260v3_chunk_0",
    "chunk_text": "Fixed Encoder Self-Attention Patterns in Transformer-Based Machine Translation\n\nTransformer-based models have brought a radical change to neural machine translation. A key feature of the Transformer architecture is the so-called multi-head attention mechanism, which allows the model to focus simultaneously on different parts of the input. However, recent works have shown that most attention heads learn simple, and often redundant, positional patterns. In this paper, we propose to replace all but one attention head of each encoder layer with simple fixed -- non-learnable -- attentive patterns that are solely based on position and do not require any external knowledge. Our experiments with different data sizes and multiple language pairs show that fixing the attention heads on the encoder side of the Transformer at training time does not impact the translation quality and even increases BLEU scores by up to 3 points in low-resource scenarios.",
    "original_url": "http://arxiv.org/pdf/2002.10260v3",
    "original_title": "Fixed Encoder Self-Attention Patterns in Transformer-Based Machine Translation",
    "source": "arxiv",
    "authors": [
      "Alessandro Raganato",
      "Yves Scherrer",
      "Jörg Tiedemann"
    ],
    "published": "2020-02-24T13:53:06+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2002.10260v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2002.10260v3_chunk_1",
    "chunk_text": "Our experiments with different data sizes and multiple language pairs show that fixing the attention heads on the encoder side of the Transformer at training time does not impact the translation quality and even increases BLEU scores by up to 3 points in low-resource scenarios.",
    "original_url": "http://arxiv.org/pdf/2002.10260v3",
    "original_title": "Fixed Encoder Self-Attention Patterns in Transformer-Based Machine Translation",
    "source": "arxiv",
    "authors": [
      "Alessandro Raganato",
      "Yves Scherrer",
      "Jörg Tiedemann"
    ],
    "published": "2020-02-24T13:53:06+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2002.10260v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2311.05988v1_chunk_0",
    "chunk_text": "Vision Big Bird: Random Sparsification for Full Attention\n\nRecently, Transformers have shown promising performance in various vision tasks. However, the high costs of global self-attention remain challenging for Transformers, especially for high-resolution vision tasks. Inspired by one of the most successful transformers-based models for NLP: Big Bird, we propose a novel sparse attention mechanism for Vision Transformers (ViT). Specifically, we separate the heads into three groups, the first group used convolutional neural network (CNN) to extract local features and provide positional information for the model, the second group used Random Sampling Windows (RS-Win) for sparse self-attention calculation, and the third group reduces the resolution of the keys and values by average pooling for global attention. Based on these components, ViT maintains the sparsity of self-attention while maintaining the merits of Big Bird (i.e., the model is a universal approximator of sequence functions and is Turing complete).",
    "original_url": "http://arxiv.org/pdf/2311.05988v1",
    "original_title": "Vision Big Bird: Random Sparsification for Full Attention",
    "source": "arxiv",
    "authors": [
      "Zhemin Zhang",
      "Xun Gong"
    ],
    "published": "2023-11-10T11:00:25+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2311.05988v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2311.05988v1_chunk_1",
    "chunk_text": "Based on these components, ViT maintains the sparsity of self-attention while maintaining the merits of Big Bird (i.e., the model is a universal approximator of sequence functions and is Turing complete). Moreover, our results show that the positional encoding, a crucial component in ViTs, can be safely removed in our model. Experiments show that Vision Big Bird demonstrates competitive performance on common vision tasks.",
    "original_url": "http://arxiv.org/pdf/2311.05988v1",
    "original_title": "Vision Big Bird: Random Sparsification for Full Attention",
    "source": "arxiv",
    "authors": [
      "Zhemin Zhang",
      "Xun Gong"
    ],
    "published": "2023-11-10T11:00:25+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2311.05988v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2410.12850v1_chunk_0",
    "chunk_text": "RecurFormer: Not All Transformer Heads Need Self-Attention\n\nTransformer-based large language models (LLMs) excel in modeling complex language patterns but face significant computational costs during inference, especially with long inputs due to the attention mechanism's memory overhead. We observe that certain attention heads exhibit a distribution where the attention weights concentrate on tokens near the query token, termed as recency aware, which focuses on local and short-range dependencies. Leveraging this insight, we propose RecurFormer, a novel architecture that replaces these attention heads with linear recurrent neural networks (RNNs), specifically the Mamba architecture. This replacement reduces the cache size without evicting tokens, thus maintaining generation quality. RecurFormer retains the ability to model long-range dependencies through the remaining attention heads and allows for reusing pre-trained Transformer-based LLMs weights with continual training.",
    "original_url": "http://arxiv.org/pdf/2410.12850v1",
    "original_title": "RecurFormer: Not All Transformer Heads Need Self-Attention",
    "source": "arxiv",
    "authors": [
      "Ruiqing Yan",
      "Linghan Zheng",
      "Xingbo Du",
      "Han Zou",
      "Yufeng Guo",
      "Jianfei Yang"
    ],
    "published": "2024-10-10T15:24:12+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2410.12850v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2410.12850v1_chunk_1",
    "chunk_text": "RecurFormer retains the ability to model long-range dependencies through the remaining attention heads and allows for reusing pre-trained Transformer-based LLMs weights with continual training. Experiments demonstrate that RecurFormer matches the original model's performance while significantly enhancing inference efficiency. Our approach provides a practical solution to the computational challenges of Transformer-based LLMs inference, making it highly attractive for tasks involving long inputs.",
    "original_url": "http://arxiv.org/pdf/2410.12850v1",
    "original_title": "RecurFormer: Not All Transformer Heads Need Self-Attention",
    "source": "arxiv",
    "authors": [
      "Ruiqing Yan",
      "Linghan Zheng",
      "Xingbo Du",
      "Han Zou",
      "Yufeng Guo",
      "Jianfei Yang"
    ],
    "published": "2024-10-10T15:24:12+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2410.12850v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2204.09593v2_chunk_0",
    "chunk_text": "COOL, a Context Outlooker, and its Application to Question Answering and other Natural Language Processing Tasks\n\nVision outlooker improves the performance of vision transformers, which implements a self-attention mechanism by adding an outlook attention, a form of local attention. In natural language processing, as has been the case in computer vision and other domains, transformer-based models constitute the state-of-the-art for most processing tasks. In this domain, too, many authors have argued and demonstrated the importance of local context. We present an outlook attention mechanism, COOL, for natural language processing. COOL, added on top of the self-attention layers of a transformer-based model, encodes local syntactic context considering word proximity and more pair-wise constraints than dynamic convolution used by existing approaches.",
    "original_url": "http://arxiv.org/pdf/2204.09593v2",
    "original_title": "COOL, a Context Outlooker, and its Application to Question Answering and other Natural Language Processing Tasks",
    "source": "arxiv",
    "authors": [
      "Fangyi Zhu",
      "See-Kiong Ng",
      "Stéphane Bressan"
    ],
    "published": "2022-04-01T07:03:40+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2204.09593v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2204.09593v2_chunk_1",
    "chunk_text": "COOL, added on top of the self-attention layers of a transformer-based model, encodes local syntactic context considering word proximity and more pair-wise constraints than dynamic convolution used by existing approaches. A comparative empirical performance evaluation of an implementation of COOL with different transformer-based models confirms the opportunity for improvement over a baseline using the original models alone for various natural language processing tasks, including question answering. The proposed approach achieves competitive performance with existing state-of-the-art methods on some tasks.",
    "original_url": "http://arxiv.org/pdf/2204.09593v2",
    "original_title": "COOL, a Context Outlooker, and its Application to Question Answering and other Natural Language Processing Tasks",
    "source": "arxiv",
    "authors": [
      "Fangyi Zhu",
      "See-Kiong Ng",
      "Stéphane Bressan"
    ],
    "published": "2022-04-01T07:03:40+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2204.09593v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2310.02041v1_chunk_0",
    "chunk_text": "The Inhibitor: ReLU and Addition-Based Attention for Efficient Transformers\n\nTo enhance the computational efficiency of quantized Transformers, we replace the dot-product and Softmax-based attention with an alternative mechanism involving addition and ReLU activation only. This side-steps the expansion to double precision often required by matrix multiplication and avoids costly Softmax evaluations but maintains much of the core functionality of conventional dot-product attention. It can enable more efficient execution and support larger quantized Transformer models on resource-constrained hardware or alternative arithmetic systems like homomorphic encryption. Training experiments on four common benchmark tasks show test set prediction scores comparable to those of conventional Transformers with dot-product attention. Our scaling experiments also suggest significant computational savings, both in plaintext and under encryption.",
    "original_url": "http://arxiv.org/pdf/2310.02041v1",
    "original_title": "The Inhibitor: ReLU and Addition-Based Attention for Efficient Transformers",
    "source": "arxiv",
    "authors": [
      "Rickard Brännvall"
    ],
    "published": "2023-10-03T13:34:21+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2310.02041v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2310.02041v1_chunk_1",
    "chunk_text": "Our scaling experiments also suggest significant computational savings, both in plaintext and under encryption. In particular, we believe that the ReLU and addition-based attention mechanism introduced in this paper may enable privacy-preserving AI applications operating under homomorphic encryption by avoiding the costly multiplication of encrypted variables.",
    "original_url": "http://arxiv.org/pdf/2310.02041v1",
    "original_title": "The Inhibitor: ReLU and Addition-Based Attention for Efficient Transformers",
    "source": "arxiv",
    "authors": [
      "Rickard Brännvall"
    ],
    "published": "2023-10-03T13:34:21+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2310.02041v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2312.04234v5_chunk_0",
    "chunk_text": "Graph Convolutions Enrich the Self-Attention in Transformers! Transformers, renowned for their self-attention mechanism, have achieved state-of-the-art performance across various tasks in natural language processing, computer vision, time-series modeling, etc. However, one of the challenges with deep Transformer models is the oversmoothing problem, where representations across layers converge to indistinguishable values, leading to significant performance degradation. We interpret the original self-attention as a simple graph filter and redesign it from a graph signal processing (GSP) perspective. We propose a graph-filter-based self-attention (GFSA) to learn a general yet effective one, whose complexity, however, is slightly larger than that of the original self-attention mechanism.",
    "original_url": "http://arxiv.org/pdf/2312.04234v5",
    "original_title": "Graph Convolutions Enrich the Self-Attention in Transformers!",
    "source": "arxiv",
    "authors": [
      "Jeongwhan Choi",
      "Hyowon Wi",
      "Jayoung Kim",
      "Yehjin Shin",
      "Kookjin Lee",
      "Nathaniel Trask",
      "Noseong Park"
    ],
    "published": "2023-12-07T11:40:32+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2312.04234v5"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2312.04234v5_chunk_1",
    "chunk_text": "We propose a graph-filter-based self-attention (GFSA) to learn a general yet effective one, whose complexity, however, is slightly larger than that of the original self-attention mechanism. We demonstrate that GFSA improves the performance of Transformers in various fields, including computer vision, natural language processing, graph-level tasks, speech recognition, and code classification.",
    "original_url": "http://arxiv.org/pdf/2312.04234v5",
    "original_title": "Graph Convolutions Enrich the Self-Attention in Transformers!",
    "source": "arxiv",
    "authors": [
      "Jeongwhan Choi",
      "Hyowon Wi",
      "Jayoung Kim",
      "Yehjin Shin",
      "Kookjin Lee",
      "Nathaniel Trask",
      "Noseong Park"
    ],
    "published": "2023-12-07T11:40:32+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2312.04234v5"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2502.12507v1_chunk_0",
    "chunk_text": "Mixture of Attention Yields Accurate Results for Tabular Data\n\nTabular data inherently exhibits significant feature heterogeneity, but existing transformer-based methods lack specialized mechanisms to handle this property. To bridge the gap, we propose MAYA, an encoder-decoder transformer-based framework. In the encoder, we design a Mixture of Attention (MOA) that constructs multiple parallel attention branches and averages the features at each branch, effectively fusing heterogeneous features while limiting parameter growth. Additionally, we employ collaborative learning with a dynamic consistency weight constraint to produce more robust representations. In the decoder stage, cross-attention is utilized to seamlessly integrate tabular data with corresponding label features.",
    "original_url": "http://arxiv.org/pdf/2502.12507v1",
    "original_title": "Mixture of Attention Yields Accurate Results for Tabular Data",
    "source": "arxiv",
    "authors": [
      "Xuechen Li",
      "Yupeng Li",
      "Jian Liu",
      "Xiaolin Jin",
      "Tian Yang",
      "Xin Hu"
    ],
    "published": "2025-02-18T03:43:42+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2502.12507v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2502.12507v1_chunk_1",
    "chunk_text": "In the decoder stage, cross-attention is utilized to seamlessly integrate tabular data with corresponding label features. This dual-attention mechanism effectively captures both intra-instance and inter-instance interactions. We evaluate the proposed method on a wide range of datasets and compare it with other state-of-the-art transformer-based methods. Extensive experiments demonstrate that our model achieves superior performance among transformer-based methods in both tabular classification and regression tasks.",
    "original_url": "http://arxiv.org/pdf/2502.12507v1",
    "original_title": "Mixture of Attention Yields Accurate Results for Tabular Data",
    "source": "arxiv",
    "authors": [
      "Xuechen Li",
      "Yupeng Li",
      "Jian Liu",
      "Xiaolin Jin",
      "Tian Yang",
      "Xin Hu"
    ],
    "published": "2025-02-18T03:43:42+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2502.12507v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2504.05336v1_chunk_0",
    "chunk_text": "Quantum Adaptive Self-Attention for Quantum Transformer Models\n\nTransformer models have revolutionized sequential learning across various domains, yet their self-attention mechanism incurs quadratic computational cost, posing limitations for real-time and resource-constrained tasks. To address this, we propose Quantum Adaptive Self-Attention (QASA), a novel hybrid architecture that enhances classical Transformer models with a quantum attention mechanism. QASA replaces dot-product attention with a parameterized quantum circuit (PQC) that adaptively captures inter-token relationships in the quantum Hilbert space. Additionally, a residual quantum projection module is introduced before the feedforward network to further refine temporal features. Our design retains classical efficiency in earlier layers while injecting quantum expressiveness in the final encoder block, ensuring compatibility with current NISQ hardware.",
    "original_url": "http://arxiv.org/pdf/2504.05336v1",
    "original_title": "Quantum Adaptive Self-Attention for Quantum Transformer Models",
    "source": "arxiv",
    "authors": [
      "Chi-Sheng Chen",
      "En-Jui Kuo"
    ],
    "published": "2025-04-05T02:52:37+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2504.05336v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2504.05336v1_chunk_1",
    "chunk_text": "Our design retains classical efficiency in earlier layers while injecting quantum expressiveness in the final encoder block, ensuring compatibility with current NISQ hardware. Experiments on synthetic time-series tasks demonstrate that QASA achieves faster convergence and superior generalization compared to both standard Transformers and reduced classical variants. Preliminary complexity analysis suggests potential quantum advantages in gradient computation, opening new avenues for efficient quantum deep learning models.",
    "original_url": "http://arxiv.org/pdf/2504.05336v1",
    "original_title": "Quantum Adaptive Self-Attention for Quantum Transformer Models",
    "source": "arxiv",
    "authors": [
      "Chi-Sheng Chen",
      "En-Jui Kuo"
    ],
    "published": "2025-04-05T02:52:37+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2504.05336v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2010.03766v1_chunk_0",
    "chunk_text": "Improving Attention Mechanism with Query-Value Interaction\n\nAttention mechanism has played critical roles in various state-of-the-art NLP models such as Transformer and BERT. It can be formulated as a ternary function that maps the input queries, keys and values into an output by using a summation of values weighted by the attention weights derived from the interactions between queries and keys. Similar with query-key interactions, there is also inherent relatedness between queries and values, and incorporating query-value interactions has the potential to enhance the output by learning customized values according to the characteristics of queries. However, the query-value interactions are ignored by existing attention methods, which may be not optimal. In this paper, we propose to improve the existing attention mechanism by incorporating query-value interactions.",
    "original_url": "http://arxiv.org/pdf/2010.03766v1",
    "original_title": "Improving Attention Mechanism with Query-Value Interaction",
    "source": "arxiv",
    "authors": [
      "Chuhan Wu",
      "Fangzhao Wu",
      "Tao Qi",
      "Yongfeng Huang"
    ],
    "published": "2020-10-08T05:12:52+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2010.03766v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2010.03766v1_chunk_1",
    "chunk_text": "In this paper, we propose to improve the existing attention mechanism by incorporating query-value interactions. We propose a query-value interaction function which can learn query-aware attention values, and combine them with the original values and attention weights to form the final output. Extensive experiments on four datasets for different tasks show that our approach can consistently improve the performance of many attention-based models by incorporating query-value interactions.",
    "original_url": "http://arxiv.org/pdf/2010.03766v1",
    "original_title": "Improving Attention Mechanism with Query-Value Interaction",
    "source": "arxiv",
    "authors": [
      "Chuhan Wu",
      "Fangzhao Wu",
      "Tao Qi",
      "Yongfeng Huang"
    ],
    "published": "2020-10-08T05:12:52+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2010.03766v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2010.12416v1_chunk_0",
    "chunk_text": "SAHDL: Sparse Attention Hypergraph Regularized Dictionary Learning\n\nIn recent years, the attention mechanism contributes significantly to hypergraph based neural networks. However, these methods update the attention weights with the network propagating. That is to say, this type of attention mechanism is only suitable for deep learning-based methods while not applicable to the traditional machine learning approaches. In this paper, we propose a hypergraph based sparse attention mechanism to tackle this issue and embed it into dictionary learning. More specifically, we first construct a sparse attention hypergraph, asset attention weights to samples by employing the $\\ell_1$-norm sparse regularization to mine the high-order relationship among sample features.",
    "original_url": "http://arxiv.org/pdf/2010.12416v1",
    "original_title": "SAHDL: Sparse Attention Hypergraph Regularized Dictionary Learning",
    "source": "arxiv",
    "authors": [
      "Shuai Shao",
      "Rui Xu",
      "Yan-Jiang Wang",
      "Weifeng Liu",
      "Bao-Di Liu"
    ],
    "published": "2020-10-23T14:07:00+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2010.12416v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2010.12416v1_chunk_1",
    "chunk_text": "More specifically, we first construct a sparse attention hypergraph, asset attention weights to samples by employing the $\\ell_1$-norm sparse regularization to mine the high-order relationship among sample features. Then, we introduce the hypergraph Laplacian operator to preserve the local structure for subspace transformation in dictionary learning. Besides, we incorporate the discriminative information into the hypergraph as the guidance to aggregate samples. Unlike previous works, our method updates attention weights independently, does not rely on the deep network. We demonstrate the efficacy of our approach on four benchmark datasets.",
    "original_url": "http://arxiv.org/pdf/2010.12416v1",
    "original_title": "SAHDL: Sparse Attention Hypergraph Regularized Dictionary Learning",
    "source": "arxiv",
    "authors": [
      "Shuai Shao",
      "Rui Xu",
      "Yan-Jiang Wang",
      "Weifeng Liu",
      "Bao-Di Liu"
    ],
    "published": "2020-10-23T14:07:00+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2010.12416v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2010.12416v1_chunk_2",
    "chunk_text": "We demonstrate the efficacy of our approach on four benchmark datasets.",
    "original_url": "http://arxiv.org/pdf/2010.12416v1",
    "original_title": "SAHDL: Sparse Attention Hypergraph Regularized Dictionary Learning",
    "source": "arxiv",
    "authors": [
      "Shuai Shao",
      "Rui Xu",
      "Yan-Jiang Wang",
      "Weifeng Liu",
      "Bao-Di Liu"
    ],
    "published": "2020-10-23T14:07:00+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2010.12416v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2106.09236v1_chunk_0",
    "chunk_text": "Efficient Conformer with Prob-Sparse Attention Mechanism for End-to-EndSpeech Recognition\n\nEnd-to-end models are favored in automatic speech recognition (ASR) because of their simplified system structure and superior performance. Among these models, Transformer and Conformer have achieved state-of-the-art recognition accuracy in which self-attention plays a vital role in capturing important global information. However, the time and memory complexity of self-attention increases squarely with the length of the sentence. In this paper, a prob-sparse self-attention mechanism is introduced into Conformer to sparse the computing process of self-attention in order to accelerate inference speed and reduce space consumption. Specifically, we adopt a Kullback-Leibler divergence based sparsity measurement for each query to decide whether we compute the attention function on this query.",
    "original_url": "http://arxiv.org/pdf/2106.09236v1",
    "original_title": "Efficient Conformer with Prob-Sparse Attention Mechanism for End-to-EndSpeech Recognition",
    "source": "arxiv",
    "authors": [
      "Xiong Wang",
      "Sining Sun",
      "Lei Xie",
      "Long Ma"
    ],
    "published": "2021-06-17T04:04:04+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2106.09236v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2106.09236v1_chunk_1",
    "chunk_text": "Specifically, we adopt a Kullback-Leibler divergence based sparsity measurement for each query to decide whether we compute the attention function on this query. By using the prob-sparse attention mechanism, we achieve impressively 8% to 45% inference speed-up and 15% to 45% memory usage reduction of the self-attention module of Conformer Transducer while maintaining the same level of error rate.",
    "original_url": "http://arxiv.org/pdf/2106.09236v1",
    "original_title": "Efficient Conformer with Prob-Sparse Attention Mechanism for End-to-EndSpeech Recognition",
    "source": "arxiv",
    "authors": [
      "Xiong Wang",
      "Sining Sun",
      "Lei Xie",
      "Long Ma"
    ],
    "published": "2021-06-17T04:04:04+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2106.09236v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2304.04556v1_chunk_0",
    "chunk_text": "Attention: Marginal Probability is All You Need? Attention mechanisms are a central property of cognitive systems allowing them to selectively deploy cognitive resources in a flexible manner. Attention has been long studied in the neurosciences and there are numerous phenomenological models that try to capture its core properties. Recently attentional mechanisms have become a dominating architectural choice of machine learning and are the central innovation of Transformers. The dominant intuition and formalism underlying their development has drawn on ideas of keys and queries in database management systems.",
    "original_url": "http://arxiv.org/pdf/2304.04556v1",
    "original_title": "Attention: Marginal Probability is All You Need?",
    "source": "arxiv",
    "authors": [
      "Ryan Singh",
      "Christopher L. Buckley"
    ],
    "published": "2023-04-07T14:38:39+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2304.04556v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2304.04556v1_chunk_1",
    "chunk_text": "The dominant intuition and formalism underlying their development has drawn on ideas of keys and queries in database management systems. In this work, we propose an alternative Bayesian foundation for attentional mechanisms and show how this unifies different attentional architectures in machine learning. This formulation allows to to identify commonality across different attention ML architectures as well as suggest a bridge to those developed in neuroscience. We hope this work will guide more sophisticated intuitions into the key properties of attention architectures and suggest new ones.",
    "original_url": "http://arxiv.org/pdf/2304.04556v1",
    "original_title": "Attention: Marginal Probability is All You Need?",
    "source": "arxiv",
    "authors": [
      "Ryan Singh",
      "Christopher L. Buckley"
    ],
    "published": "2023-04-07T14:38:39+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2304.04556v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2005.08514v2_chunk_0",
    "chunk_text": "Spatio-Temporal Graph Transformer Networks for Pedestrian Trajectory Prediction\n\nUnderstanding crowd motion dynamics is critical to real-world applications, e.g., surveillance systems and autonomous driving. This is challenging because it requires effectively modeling the socially aware crowd spatial interaction and complex temporal dependencies. We believe attention is the most important factor for trajectory prediction. In this paper, we present STAR, a Spatio-Temporal grAph tRansformer framework, which tackles trajectory prediction by only attention mechanisms. STAR models intra-graph crowd interaction by TGConv, a novel Transformer-based graph convolution mechanism.",
    "original_url": "http://arxiv.org/pdf/2005.08514v2",
    "original_title": "Spatio-Temporal Graph Transformer Networks for Pedestrian Trajectory Prediction",
    "source": "arxiv",
    "authors": [
      "Cunjun Yu",
      "Xiao Ma",
      "Jiawei Ren",
      "Haiyu Zhao",
      "Shuai Yi"
    ],
    "published": "2020-05-18T08:08:09+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2005.08514v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2005.08514v2_chunk_1",
    "chunk_text": "STAR models intra-graph crowd interaction by TGConv, a novel Transformer-based graph convolution mechanism. The inter-graph temporal dependencies are modeled by separate temporal Transformers. STAR captures complex spatio-temporal interactions by interleaving between spatial and temporal Transformers. To calibrate the temporal prediction for the long-lasting effect of disappeared pedestrians, we introduce a read-writable external memory module, consistently being updated by the temporal Transformer. We show that with only attention mechanism, STAR achieves state-of-the-art performance on 5 commonly used real-world pedestrian prediction datasets.",
    "original_url": "http://arxiv.org/pdf/2005.08514v2",
    "original_title": "Spatio-Temporal Graph Transformer Networks for Pedestrian Trajectory Prediction",
    "source": "arxiv",
    "authors": [
      "Cunjun Yu",
      "Xiao Ma",
      "Jiawei Ren",
      "Haiyu Zhao",
      "Shuai Yi"
    ],
    "published": "2020-05-18T08:08:09+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2005.08514v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2005.08514v2_chunk_2",
    "chunk_text": "We show that with only attention mechanism, STAR achieves state-of-the-art performance on 5 commonly used real-world pedestrian prediction datasets.",
    "original_url": "http://arxiv.org/pdf/2005.08514v2",
    "original_title": "Spatio-Temporal Graph Transformer Networks for Pedestrian Trajectory Prediction",
    "source": "arxiv",
    "authors": [
      "Cunjun Yu",
      "Xiao Ma",
      "Jiawei Ren",
      "Haiyu Zhao",
      "Shuai Yi"
    ],
    "published": "2020-05-18T08:08:09+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2005.08514v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2405.01102v2_chunk_0",
    "chunk_text": "Less is More: on the Over-Globalizing Problem in Graph Transformers\n\nGraph Transformer, due to its global attention mechanism, has emerged as a new tool in dealing with graph-structured data. It is well recognized that the global attention mechanism considers a wider receptive field in a fully connected graph, leading many to believe that useful information can be extracted from all the nodes. In this paper, we challenge this belief: does the globalizing property always benefit Graph Transformers? We reveal the over-globalizing problem in Graph Transformer by presenting both empirical evidence and theoretical analysis, i.e., the current attention mechanism overly focuses on those distant nodes, while the near nodes, which actually contain most of the useful information, are relatively weakened. Then we propose a novel Bi-Level Global Graph Transformer with Collaborative Training (CoBFormer), including the inter-cluster and intra-cluster Transformers, to prevent the over-globalizing problem while keeping the ability to extract valuable information from distant nodes.",
    "original_url": "http://arxiv.org/pdf/2405.01102v2",
    "original_title": "Less is More: on the Over-Globalizing Problem in Graph Transformers",
    "source": "arxiv",
    "authors": [
      "Yujie Xing",
      "Xiao Wang",
      "Yibo Li",
      "Hai Huang",
      "Chuan Shi"
    ],
    "published": "2024-05-02T09:12:22+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2405.01102v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2405.01102v2_chunk_1",
    "chunk_text": "Then we propose a novel Bi-Level Global Graph Transformer with Collaborative Training (CoBFormer), including the inter-cluster and intra-cluster Transformers, to prevent the over-globalizing problem while keeping the ability to extract valuable information from distant nodes. Moreover, the collaborative training is proposed to improve the model's generalization ability with a theoretical guarantee. Extensive experiments on various graphs well validate the effectiveness of our proposed CoBFormer.",
    "original_url": "http://arxiv.org/pdf/2405.01102v2",
    "original_title": "Less is More: on the Over-Globalizing Problem in Graph Transformers",
    "source": "arxiv",
    "authors": [
      "Yujie Xing",
      "Xiao Wang",
      "Yibo Li",
      "Hai Huang",
      "Chuan Shi"
    ],
    "published": "2024-05-02T09:12:22+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2405.01102v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2106.08823v1_chunk_0",
    "chunk_text": "Eigen Analysis of Self-Attention and its Reconstruction from Partial Computation\n\nState-of-the-art transformer models use pairwise dot-product based self-attention, which comes at a computational cost quadratic in the input sequence length. In this paper, we investigate the global structure of attention scores computed using this dot product mechanism on a typical distribution of inputs, and study the principal components of their variation. Through eigen analysis of full attention score matrices, as well as of their individual rows, we find that most of the variation among attention scores lie in a low-dimensional eigenspace. Moreover, we find significant overlap between these eigenspaces for different layers and even different transformer models. Based on this, we propose to compute scores only for a partial subset of token pairs, and use them to estimate scores for the remaining pairs.",
    "original_url": "http://arxiv.org/pdf/2106.08823v1",
    "original_title": "Eigen Analysis of Self-Attention and its Reconstruction from Partial Computation",
    "source": "arxiv",
    "authors": [
      "Srinadh Bhojanapalli",
      "Ayan Chakrabarti",
      "Himanshu Jain",
      "Sanjiv Kumar",
      "Michal Lukasik",
      "Andreas Veit"
    ],
    "published": "2021-06-16T14:38:42+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2106.08823v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2106.08823v1_chunk_1",
    "chunk_text": "Based on this, we propose to compute scores only for a partial subset of token pairs, and use them to estimate scores for the remaining pairs. Beyond investigating the accuracy of reconstructing attention scores themselves, we investigate training transformer models that employ these approximations, and analyze the effect on overall accuracy. Our analysis and the proposed method provide insights into how to balance the benefits of exact pair-wise attention and its significant computational expense.",
    "original_url": "http://arxiv.org/pdf/2106.08823v1",
    "original_title": "Eigen Analysis of Self-Attention and its Reconstruction from Partial Computation",
    "source": "arxiv",
    "authors": [
      "Srinadh Bhojanapalli",
      "Ayan Chakrabarti",
      "Himanshu Jain",
      "Sanjiv Kumar",
      "Michal Lukasik",
      "Andreas Veit"
    ],
    "published": "2021-06-16T14:38:42+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2106.08823v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2307.08837v1_chunk_0",
    "chunk_text": "DARTS: Double Attention Reference-based Transformer for Super-resolution\n\nWe present DARTS, a transformer model for reference-based image super-resolution. DARTS learns joint representations of two image distributions to enhance the content of low-resolution input images through matching correspondences learned from high-resolution reference images. Current state-of-the-art techniques in reference-based image super-resolution are based on a multi-network, multi-stage architecture. In this work, we adapt the double attention block from the GAN literature, processing the two visual streams separately and combining self-attention and cross-attention blocks through a gating attention strategy. Our work demonstrates how the attention mechanism can be adapted for the particular requirements of reference-based image super-resolution, significantly simplifying the architecture and training pipeline.",
    "original_url": "http://arxiv.org/pdf/2307.08837v1",
    "original_title": "DARTS: Double Attention Reference-based Transformer for Super-resolution",
    "source": "arxiv",
    "authors": [
      "Masoomeh Aslahishahri",
      "Jordan Ubbens",
      "Ian Stavness"
    ],
    "published": "2023-07-17T20:57:16+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2307.08837v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2307.08837v1_chunk_1",
    "chunk_text": "Our work demonstrates how the attention mechanism can be adapted for the particular requirements of reference-based image super-resolution, significantly simplifying the architecture and training pipeline. We show that our transformer-based model performs competitively with state-of-the-art models, while maintaining a simpler overall architecture and training process. In particular, we obtain state-of-the-art on the SUN80 dataset, with a PSNR/SSIM of 29.83 / .809. These results show that attention alone is sufficient for the RSR task, without multiple purpose-built subnetworks, knowledge distillation, or multi-stage training.",
    "original_url": "http://arxiv.org/pdf/2307.08837v1",
    "original_title": "DARTS: Double Attention Reference-based Transformer for Super-resolution",
    "source": "arxiv",
    "authors": [
      "Masoomeh Aslahishahri",
      "Jordan Ubbens",
      "Ian Stavness"
    ],
    "published": "2023-07-17T20:57:16+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2307.08837v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2408.16959v1_chunk_0",
    "chunk_text": "HiTSR: A Hierarchical Transformer for Reference-based Super-Resolution\n\nIn this paper, we propose HiTSR, a hierarchical transformer model for reference-based image super-resolution, which enhances low-resolution input images by learning matching correspondences from high-resolution reference images. Diverging from existing multi-network, multi-stage approaches, we streamline the architecture and training pipeline by incorporating the double attention block from GAN literature. Processing two visual streams independently, we fuse self-attention and cross-attention blocks through a gating attention strategy. The model integrates a squeeze-and-excitation module to capture global context from the input images, facilitating long-range spatial interactions within window-based attention blocks. Long skip connections between shallow and deep layers further enhance information flow.",
    "original_url": "http://arxiv.org/pdf/2408.16959v1",
    "original_title": "HiTSR: A Hierarchical Transformer for Reference-based Super-Resolution",
    "source": "arxiv",
    "authors": [
      "Masoomeh Aslahishahri",
      "Jordan Ubbens",
      "Ian Stavness"
    ],
    "published": "2024-08-30T01:16:29+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2408.16959v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2408.16959v1_chunk_1",
    "chunk_text": "Long skip connections between shallow and deep layers further enhance information flow. Our model demonstrates superior performance across three datasets including SUN80, Urban100, and Manga109. Specifically, on the SUN80 dataset, our model achieves PSNR/SSIM values of 30.24/0.821. These results underscore the effectiveness of attention mechanisms in reference-based image super-resolution. The transformer-based model attains state-of-the-art results without the need for purpose-built subnetworks, knowledge distillation, or multi-stage training, emphasizing the potency of attention in meeting reference-based image super-resolution requirements.",
    "original_url": "http://arxiv.org/pdf/2408.16959v1",
    "original_title": "HiTSR: A Hierarchical Transformer for Reference-based Super-Resolution",
    "source": "arxiv",
    "authors": [
      "Masoomeh Aslahishahri",
      "Jordan Ubbens",
      "Ian Stavness"
    ],
    "published": "2024-08-30T01:16:29+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2408.16959v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2408.16959v1_chunk_2",
    "chunk_text": "The transformer-based model attains state-of-the-art results without the need for purpose-built subnetworks, knowledge distillation, or multi-stage training, emphasizing the potency of attention in meeting reference-based image super-resolution requirements.",
    "original_url": "http://arxiv.org/pdf/2408.16959v1",
    "original_title": "HiTSR: A Hierarchical Transformer for Reference-based Super-Resolution",
    "source": "arxiv",
    "authors": [
      "Masoomeh Aslahishahri",
      "Jordan Ubbens",
      "Ian Stavness"
    ],
    "published": "2024-08-30T01:16:29+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2408.16959v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2201.10801v1_chunk_0",
    "chunk_text": "When Shift Operation Meets Vision Transformer: An Extremely Simple Alternative to Attention Mechanism\n\nAttention mechanism has been widely believed as the key to success of vision transformers (ViTs), since it provides a flexible and powerful way to model spatial relationships. However, is the attention mechanism truly an indispensable part of ViT? Can it be replaced by some other alternatives? To demystify the role of attention mechanism, we simplify it into an extremely simple case: ZERO FLOP and ZERO parameter. Concretely, we revisit the shift operation.",
    "original_url": "http://arxiv.org/pdf/2201.10801v1",
    "original_title": "When Shift Operation Meets Vision Transformer: An Extremely Simple Alternative to Attention Mechanism",
    "source": "arxiv",
    "authors": [
      "Guangting Wang",
      "Yucheng Zhao",
      "Chuanxin Tang",
      "Chong Luo",
      "Wenjun Zeng"
    ],
    "published": "2022-01-26T08:17:06+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2201.10801v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2201.10801v1_chunk_1",
    "chunk_text": "Concretely, we revisit the shift operation. It does not contain any parameter or arithmetic calculation. The only operation is to exchange a small portion of the channels between neighboring features. Based on this simple operation, we construct a new backbone network, namely ShiftViT, where the attention layers in ViT are substituted by shift operations. Surprisingly, ShiftViT works quite well in several mainstream tasks, e.g., classification, detection, and segmentation.",
    "original_url": "http://arxiv.org/pdf/2201.10801v1",
    "original_title": "When Shift Operation Meets Vision Transformer: An Extremely Simple Alternative to Attention Mechanism",
    "source": "arxiv",
    "authors": [
      "Guangting Wang",
      "Yucheng Zhao",
      "Chuanxin Tang",
      "Chong Luo",
      "Wenjun Zeng"
    ],
    "published": "2022-01-26T08:17:06+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2201.10801v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2201.10801v1_chunk_2",
    "chunk_text": "Surprisingly, ShiftViT works quite well in several mainstream tasks, e.g., classification, detection, and segmentation. The performance is on par with or even better than the strong baseline Swin Transformer. These results suggest that the attention mechanism might not be the vital factor that makes ViT successful. It can be even replaced by a zero-parameter operation. We should pay more attentions to the remaining parts of ViT in the future work.",
    "original_url": "http://arxiv.org/pdf/2201.10801v1",
    "original_title": "When Shift Operation Meets Vision Transformer: An Extremely Simple Alternative to Attention Mechanism",
    "source": "arxiv",
    "authors": [
      "Guangting Wang",
      "Yucheng Zhao",
      "Chuanxin Tang",
      "Chong Luo",
      "Wenjun Zeng"
    ],
    "published": "2022-01-26T08:17:06+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2201.10801v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2201.10801v1_chunk_3",
    "chunk_text": "We should pay more attentions to the remaining parts of ViT in the future work. Code is available at github.com/microsoft/SPACH.",
    "original_url": "http://arxiv.org/pdf/2201.10801v1",
    "original_title": "When Shift Operation Meets Vision Transformer: An Extremely Simple Alternative to Attention Mechanism",
    "source": "arxiv",
    "authors": [
      "Guangting Wang",
      "Yucheng Zhao",
      "Chuanxin Tang",
      "Chong Luo",
      "Wenjun Zeng"
    ],
    "published": "2022-01-26T08:17:06+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2201.10801v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2405.18874v2_chunk_0",
    "chunk_text": "Are queries and keys always relevant? A case study on Transformer wave functions\n\nThe dot product attention mechanism, originally designed for natural language processing tasks, is a cornerstone of modern Transformers. It adeptly captures semantic relationships between word pairs in sentences by computing a similarity overlap between queries and keys. In this work, we explore the suitability of Transformers, focusing on their attention mechanisms, in the specific domain of the parametrization of variational wave functions to approximate ground states of quantum many-body spin Hamiltonians. Specifically, we perform numerical simulations on the two-dimensional $J_1$-$J_2$ Heisenberg model, a common benchmark in the field of quantum many-body systems on lattice.",
    "original_url": "http://arxiv.org/pdf/2405.18874v2",
    "original_title": "Are queries and keys always relevant? A case study on Transformer wave functions",
    "source": "arxiv",
    "authors": [
      "Riccardo Rende",
      "Luciano Loris Viteritti"
    ],
    "published": "2024-05-29T08:32:37+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2405.18874v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2405.18874v2_chunk_1",
    "chunk_text": "Specifically, we perform numerical simulations on the two-dimensional $J_1$-$J_2$ Heisenberg model, a common benchmark in the field of quantum many-body systems on lattice. By comparing the performance of standard attention mechanisms with a simplified version that excludes queries and keys, relying solely on positions, we achieve competitive results while reducing computational cost and parameter usage. Furthermore, through the analysis of the attention maps generated by standard attention mechanisms, we show that the attention weights become effectively input-independent at the end of the optimization. We support the numerical results with analytical calculations, providing physical insights of why queries and keys should be, in principle, omitted from the attention mechanism when studying large systems.",
    "original_url": "http://arxiv.org/pdf/2405.18874v2",
    "original_title": "Are queries and keys always relevant? A case study on Transformer wave functions",
    "source": "arxiv",
    "authors": [
      "Riccardo Rende",
      "Luciano Loris Viteritti"
    ],
    "published": "2024-05-29T08:32:37+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2405.18874v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2405.13901v3_chunk_0",
    "chunk_text": "Discrete Cosine Transform Based Decorrelated Attention for Vision Transformers\n\nCentral to the Transformer architectures' effectiveness is the self-attention mechanism, a function that maps queries, keys, and values into a high-dimensional vector space. However, training the attention weights of queries, keys, and values is non-trivial from a state of random initialization. In this paper, we propose two methods. (i) We first address the initialization problem of Vision Transformers by introducing a simple, yet highly innovative, initialization approach utilizing discrete cosine transform (DCT) coefficients. Our proposed DCT-based \\textit{attention} initialization marks a significant gain compared to traditional initialization strategies; offering a robust foundation for the attention mechanism.",
    "original_url": "http://arxiv.org/pdf/2405.13901v3",
    "original_title": "Discrete Cosine Transform Based Decorrelated Attention for Vision Transformers",
    "source": "arxiv",
    "authors": [
      "Hongyi Pan",
      "Emadeldeen Hamdan",
      "Xin Zhu",
      "Ahmet Enis Cetin",
      "Ulas Bagci"
    ],
    "published": "2024-05-22T18:15:42+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2405.13901v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2405.13901v3_chunk_1",
    "chunk_text": "Our proposed DCT-based \\textit{attention} initialization marks a significant gain compared to traditional initialization strategies; offering a robust foundation for the attention mechanism. Our experiments reveal that the DCT-based initialization enhances the accuracy of Vision Transformers in classification tasks. (ii) We also recognize that since DCT effectively decorrelates image information in the frequency domain, this decorrelation is useful for compression because it allows the quantization step to discard many of the higher-frequency components. Based on this observation, we propose a novel DCT-based compression technique for the attention function of Vision Transformers. Since high-frequency DCT coefficients usually correspond to noise, we truncate the high-frequency DCT components of the input patches.",
    "original_url": "http://arxiv.org/pdf/2405.13901v3",
    "original_title": "Discrete Cosine Transform Based Decorrelated Attention for Vision Transformers",
    "source": "arxiv",
    "authors": [
      "Hongyi Pan",
      "Emadeldeen Hamdan",
      "Xin Zhu",
      "Ahmet Enis Cetin",
      "Ulas Bagci"
    ],
    "published": "2024-05-22T18:15:42+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2405.13901v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2405.13901v3_chunk_2",
    "chunk_text": "Since high-frequency DCT coefficients usually correspond to noise, we truncate the high-frequency DCT components of the input patches. Our DCT-based compression reduces the size of weight matrices for queries, keys, and values. While maintaining the same level of accuracy, our DCT compressed Swin Transformers obtain a considerable decrease in the computational overhead.",
    "original_url": "http://arxiv.org/pdf/2405.13901v3",
    "original_title": "Discrete Cosine Transform Based Decorrelated Attention for Vision Transformers",
    "source": "arxiv",
    "authors": [
      "Hongyi Pan",
      "Emadeldeen Hamdan",
      "Xin Zhu",
      "Ahmet Enis Cetin",
      "Ulas Bagci"
    ],
    "published": "2024-05-22T18:15:42+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2405.13901v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2110.09419v2_chunk_0",
    "chunk_text": "Compositional Attention: Disentangling Search and Retrieval\n\nMulti-head, key-value attention is the backbone of the widely successful Transformer model and its variants. This attention mechanism uses multiple parallel key-value attention blocks (called heads), each performing two fundamental computations: (1) search - selection of a relevant entity from a set via query-key interactions, and (2) retrieval - extraction of relevant features from the selected entity via a value matrix. Importantly, standard attention heads learn a rigid mapping between search and retrieval. In this work, we first highlight how this static nature of the pairing can potentially: (a) lead to learning of redundant parameters in certain tasks, and (b) hinder generalization. To alleviate this problem, we propose a novel attention mechanism, called Compositional Attention, that replaces the standard head structure.",
    "original_url": "http://arxiv.org/pdf/2110.09419v2",
    "original_title": "Compositional Attention: Disentangling Search and Retrieval",
    "source": "arxiv",
    "authors": [
      "Sarthak Mittal",
      "Sharath Chandra Raparthy",
      "Irina Rish",
      "Yoshua Bengio",
      "Guillaume Lajoie"
    ],
    "published": "2021-10-18T15:47:38+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2110.09419v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2110.09419v2_chunk_1",
    "chunk_text": "To alleviate this problem, we propose a novel attention mechanism, called Compositional Attention, that replaces the standard head structure. The proposed mechanism disentangles search and retrieval and composes them in a dynamic, flexible and context-dependent manner through an additional soft competition stage between the query-key combination and value pairing. Through a series of numerical experiments, we show that it outperforms standard multi-head attention on a variety of tasks, including some out-of-distribution settings. Through our qualitative analysis, we demonstrate that Compositional Attention leads to dynamic specialization based on the type of retrieval needed. Our proposed mechanism generalizes multi-head attention, allows independent scaling of search and retrieval, and can easily be implemented in lieu of standard attention heads in any network architecture.",
    "original_url": "http://arxiv.org/pdf/2110.09419v2",
    "original_title": "Compositional Attention: Disentangling Search and Retrieval",
    "source": "arxiv",
    "authors": [
      "Sarthak Mittal",
      "Sharath Chandra Raparthy",
      "Irina Rish",
      "Yoshua Bengio",
      "Guillaume Lajoie"
    ],
    "published": "2021-10-18T15:47:38+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2110.09419v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2110.09419v2_chunk_2",
    "chunk_text": "Our proposed mechanism generalizes multi-head attention, allows independent scaling of search and retrieval, and can easily be implemented in lieu of standard attention heads in any network architecture.",
    "original_url": "http://arxiv.org/pdf/2110.09419v2",
    "original_title": "Compositional Attention: Disentangling Search and Retrieval",
    "source": "arxiv",
    "authors": [
      "Sarthak Mittal",
      "Sharath Chandra Raparthy",
      "Irina Rish",
      "Yoshua Bengio",
      "Guillaume Lajoie"
    ],
    "published": "2021-10-18T15:47:38+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2110.09419v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2310.02565v1_chunk_0",
    "chunk_text": "Improving Drumming Robot Via Attention Transformer Network\n\nRobotic technology has been widely used in nowadays society, which has made great progress in various fields such as agriculture, manufacturing and entertainment. In this paper, we focus on the topic of drumming robots in entertainment. To this end, we introduce an improving drumming robot that can automatically complete music transcription based on the popular vision transformer network based on the attention mechanism. Equipped with the attention transformer network, our method can efficiently handle the sequential audio embedding input and model their global long-range dependencies. Massive experimental results demonstrate that the improving algorithm can help the drumming robot promote drum classification performance, which can also help the robot to enjoy a variety of smart applications and services.",
    "original_url": "http://arxiv.org/pdf/2310.02565v1",
    "original_title": "Improving Drumming Robot Via Attention Transformer Network",
    "source": "arxiv",
    "authors": [
      "Yang Yi",
      "Zonghan Li"
    ],
    "published": "2023-10-04T03:55:43+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2310.02565v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2310.02565v1_chunk_1",
    "chunk_text": "Massive experimental results demonstrate that the improving algorithm can help the drumming robot promote drum classification performance, which can also help the robot to enjoy a variety of smart applications and services.",
    "original_url": "http://arxiv.org/pdf/2310.02565v1",
    "original_title": "Improving Drumming Robot Via Attention Transformer Network",
    "source": "arxiv",
    "authors": [
      "Yang Yi",
      "Zonghan Li"
    ],
    "published": "2023-10-04T03:55:43+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2310.02565v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1911.05990v1_chunk_0",
    "chunk_text": "Attention on Abstract Visual Reasoning\n\nAttention mechanisms have been boosting the performance of deep learning models on a wide range of applications, ranging from speech understanding to program induction. However, despite experiments from psychology which suggest that attention plays an essential role in visual reasoning, the full potential of attention mechanisms has so far not been explored to solve abstract cognitive tasks on image data. In this work, we propose a hybrid network architecture, grounded on self-attention and relational reasoning. We call this new model Attention Relation Network (ARNe). ARNe combines features from the recently introduced Transformer and the Wild Relation Network (WReN).",
    "original_url": "http://arxiv.org/pdf/1911.05990v1",
    "original_title": "Attention on Abstract Visual Reasoning",
    "source": "arxiv",
    "authors": [
      "Lukas Hahne",
      "Timo Lüddecke",
      "Florentin Wörgötter",
      "David Kappel"
    ],
    "published": "2019-11-14T08:33:40+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1911.05990v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1911.05990v1_chunk_1",
    "chunk_text": "ARNe combines features from the recently introduced Transformer and the Wild Relation Network (WReN). We test ARNe on the Procedurally Generated Matrices (PGMs) datasets for abstract visual reasoning. ARNe excels the WReN model on this task by 11.28 ppt. Relational concepts between objects are efficiently learned demanding only 35% of the training samples to surpass reported accuracy of the base line model. Our proposed hybrid model, represents an alternative on learning abstract relations using self-attention and demonstrates that the Transformer network is also well suited for abstract visual reasoning.",
    "original_url": "http://arxiv.org/pdf/1911.05990v1",
    "original_title": "Attention on Abstract Visual Reasoning",
    "source": "arxiv",
    "authors": [
      "Lukas Hahne",
      "Timo Lüddecke",
      "Florentin Wörgötter",
      "David Kappel"
    ],
    "published": "2019-11-14T08:33:40+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1911.05990v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1911.05990v1_chunk_2",
    "chunk_text": "Our proposed hybrid model, represents an alternative on learning abstract relations using self-attention and demonstrates that the Transformer network is also well suited for abstract visual reasoning.",
    "original_url": "http://arxiv.org/pdf/1911.05990v1",
    "original_title": "Attention on Abstract Visual Reasoning",
    "source": "arxiv",
    "authors": [
      "Lukas Hahne",
      "Timo Lüddecke",
      "Florentin Wörgötter",
      "David Kappel"
    ],
    "published": "2019-11-14T08:33:40+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1911.05990v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1906.04972v1_chunk_0",
    "chunk_text": "Toward Interpretable Music Tagging with Self-Attention\n\nSelf-attention is an attention mechanism that learns a representation by relating different positions in the sequence. The transformer, which is a sequence model solely based on self-attention, and its variants achieved state-of-the-art results in many natural language processing tasks. Since music composes its semantics based on the relations between components in sparse positions, adopting the self-attention mechanism to solve music information retrieval (MIR) problems can be beneficial. Hence, we propose a self-attention based deep sequence model for music tagging. The proposed architecture consists of shallow convolutional layers followed by stacked Transformer encoders.",
    "original_url": "http://arxiv.org/pdf/1906.04972v1",
    "original_title": "Toward Interpretable Music Tagging with Self-Attention",
    "source": "arxiv",
    "authors": [
      "Minz Won",
      "Sanghyuk Chun",
      "Xavier Serra"
    ],
    "published": "2019-06-12T07:08:01+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1906.04972v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1906.04972v1_chunk_1",
    "chunk_text": "The proposed architecture consists of shallow convolutional layers followed by stacked Transformer encoders. Compared to conventional approaches using fully convolutional or recurrent neural networks, our model is more interpretable while reporting competitive results. We validate the performance of our model with the MagnaTagATune and the Million Song Dataset. In addition, we demonstrate the interpretability of the proposed architecture with a heat map visualization.",
    "original_url": "http://arxiv.org/pdf/1906.04972v1",
    "original_title": "Toward Interpretable Music Tagging with Self-Attention",
    "source": "arxiv",
    "authors": [
      "Minz Won",
      "Sanghyuk Chun",
      "Xavier Serra"
    ],
    "published": "2019-06-12T07:08:01+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1906.04972v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2306.10128v1_chunk_0",
    "chunk_text": "Systematic Architectural Design of Scale Transformed Attention Condenser DNNs via Multi-Scale Class Representational Response Similarity Analysis\n\nSelf-attention mechanisms are commonly included in a convolutional neural networks to achieve an improved efficiency performance balance. However, adding self-attention mechanisms adds additional hyperparameters to tune for the application at hand. In this work we propose a novel type of DNN analysis called Multi-Scale Class Representational Response Similarity Analysis (ClassRepSim) which can be used to identify specific design interventions that lead to more efficient self-attention convolutional neural network architectures. Using insights grained from ClassRepSim we propose the Spatial Transformed Attention Condenser (STAC) module, a novel attention-condenser based self-attention module. We show that adding STAC modules to ResNet style architectures can result in up to a 1.6% increase in top-1 accuracy compared to vanilla ResNet models and up to a 0.5% increase in top-1 accuracy compared to SENet models on the ImageNet64x64 dataset, at the cost of up to 1.7% increase in FLOPs and 2x the number of parameters.",
    "original_url": "http://arxiv.org/pdf/2306.10128v1",
    "original_title": "Systematic Architectural Design of Scale Transformed Attention Condenser DNNs via Multi-Scale Class Representational Response Similarity Analysis",
    "source": "arxiv",
    "authors": [
      "Andre Hryniowski",
      "Alexander Wong"
    ],
    "published": "2023-06-16T18:29:26+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2306.10128v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2306.10128v1_chunk_1",
    "chunk_text": "We show that adding STAC modules to ResNet style architectures can result in up to a 1.6% increase in top-1 accuracy compared to vanilla ResNet models and up to a 0.5% increase in top-1 accuracy compared to SENet models on the ImageNet64x64 dataset, at the cost of up to 1.7% increase in FLOPs and 2x the number of parameters. In addition, we demonstrate that results from ClassRepSim analysis can be used to select an effective parameterization of the STAC module resulting in competitive performance compared to an extensive parameter search.",
    "original_url": "http://arxiv.org/pdf/2306.10128v1",
    "original_title": "Systematic Architectural Design of Scale Transformed Attention Condenser DNNs via Multi-Scale Class Representational Response Similarity Analysis",
    "source": "arxiv",
    "authors": [
      "Andre Hryniowski",
      "Alexander Wong"
    ],
    "published": "2023-06-16T18:29:26+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2306.10128v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2406.16747v1_chunk_0",
    "chunk_text": "Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers\n\nAccommodating long sequences efficiently in autoregressive Transformers, especially within an extended context window, poses significant challenges due to the quadratic computational complexity and substantial KV memory requirements inherent in self-attention mechanisms. In this work, we introduce SPARSEK Attention, a novel sparse attention mechanism designed to overcome these computational and memory obstacles while maintaining performance. Our approach integrates a scoring network and a differentiable top-k mask operator, SPARSEK, to select a constant number of KV pairs for each query, thereby enabling gradient-based optimization. As a result, SPARSEK Attention offers linear time complexity and constant memory footprint during generation. Experimental results reveal that SPARSEK Attention outperforms previous sparse attention methods and provides significant speed improvements during both training and inference, particularly in language modeling and downstream tasks.",
    "original_url": "http://arxiv.org/pdf/2406.16747v1",
    "original_title": "Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers",
    "source": "arxiv",
    "authors": [
      "Chao Lou",
      "Zixia Jia",
      "Zilong Zheng",
      "Kewei Tu"
    ],
    "published": "2024-06-24T15:55:59+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2406.16747v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2406.16747v1_chunk_1",
    "chunk_text": "Experimental results reveal that SPARSEK Attention outperforms previous sparse attention methods and provides significant speed improvements during both training and inference, particularly in language modeling and downstream tasks. Furthermore, our method can be seamlessly integrated into pre-trained Large Language Models (LLMs) with minimal fine-tuning, offering a practical solution for effectively managing long-range dependencies in diverse applications.",
    "original_url": "http://arxiv.org/pdf/2406.16747v1",
    "original_title": "Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers",
    "source": "arxiv",
    "authors": [
      "Chao Lou",
      "Zixia Jia",
      "Zilong Zheng",
      "Kewei Tu"
    ],
    "published": "2024-06-24T15:55:59+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2406.16747v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2502.20764v1_chunk_0",
    "chunk_text": "Visual Attention Exploration in Vision-Based Mamba Models\n\nState space models (SSMs) have emerged as an efficient alternative to transformer-based models, offering linear complexity that scales better than transformers. One of the latest advances in SSMs, Mamba, introduces a selective scan mechanism that assigns trainable weights to input tokens, effectively mimicking the attention mechanism. Mamba has also been successfully extended to the vision domain by decomposing 2D images into smaller patches and arranging them as 1D sequences. However, it remains unclear how these patches interact with (or attend to) each other in relation to their original 2D spatial location. Additionally, the order used to arrange the patches into a sequence also significantly impacts their attention distribution.",
    "original_url": "http://arxiv.org/pdf/2502.20764v1",
    "original_title": "Visual Attention Exploration in Vision-Based Mamba Models",
    "source": "arxiv",
    "authors": [
      "Junpeng Wang",
      "Chin-Chia Michael Yeh",
      "Uday Singh Saini",
      "Mahashweta Das"
    ],
    "published": "2025-02-28T06:33:18+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2502.20764v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2502.20764v1_chunk_1",
    "chunk_text": "Additionally, the order used to arrange the patches into a sequence also significantly impacts their attention distribution. To better understand the attention between patches and explore the attention patterns, we introduce a visual analytics tool specifically designed for vision-based Mamba models. This tool enables a deeper understanding of how attention is distributed across patches in different Mamba blocks and how it evolves throughout a Mamba model. Using the tool, we also investigate the impact of different patch-ordering strategies on the learned attention, offering further insights into the model's behavior.",
    "original_url": "http://arxiv.org/pdf/2502.20764v1",
    "original_title": "Visual Attention Exploration in Vision-Based Mamba Models",
    "source": "arxiv",
    "authors": [
      "Junpeng Wang",
      "Chin-Chia Michael Yeh",
      "Uday Singh Saini",
      "Mahashweta Das"
    ],
    "published": "2025-02-28T06:33:18+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2502.20764v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2503.22796v1_chunk_0",
    "chunk_text": "DiTFastAttnV2: Head-wise Attention Compression for Multi-Modality Diffusion Transformers\n\nText-to-image generation models, especially Multimodal Diffusion Transformers (MMDiT), have shown remarkable progress in generating high-quality images. However, these models often face significant computational bottlenecks, particularly in attention mechanisms, which hinder their scalability and efficiency. In this paper, we introduce DiTFastAttnV2, a post-training compression method designed to accelerate attention in MMDiT. Through an in-depth analysis of MMDiT's attention patterns, we identify key differences from prior DiT-based methods and propose head-wise arrow attention and caching mechanisms to dynamically adjust attention heads, effectively bridging this gap. We also design an Efficient Fused Kernel for further acceleration.",
    "original_url": "http://arxiv.org/pdf/2503.22796v1",
    "original_title": "DiTFastAttnV2: Head-wise Attention Compression for Multi-Modality Diffusion Transformers",
    "source": "arxiv",
    "authors": [
      "Hanling Zhang",
      "Rundong Su",
      "Zhihang Yuan",
      "Pengtao Chen",
      "Mingzhu Shen Yibo Fan",
      "Shengen Yan",
      "Guohao Dai",
      "Yu Wang"
    ],
    "published": "2025-03-28T18:00:12+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2503.22796v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2503.22796v1_chunk_1",
    "chunk_text": "We also design an Efficient Fused Kernel for further acceleration. By leveraging local metric methods and optimization techniques, our approach significantly reduces the search time for optimal compression schemes to just minutes while maintaining generation quality. Furthermore, with the customized kernel, DiTFastAttnV2 achieves a 68% reduction in attention FLOPs and 1.5x end-to-end speedup on 2K image generation without compromising visual fidelity.",
    "original_url": "http://arxiv.org/pdf/2503.22796v1",
    "original_title": "DiTFastAttnV2: Head-wise Attention Compression for Multi-Modality Diffusion Transformers",
    "source": "arxiv",
    "authors": [
      "Hanling Zhang",
      "Rundong Su",
      "Zhihang Yuan",
      "Pengtao Chen",
      "Mingzhu Shen Yibo Fan",
      "Shengen Yan",
      "Guohao Dai",
      "Yu Wang"
    ],
    "published": "2025-03-28T18:00:12+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2503.22796v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2203.15662v1_chunk_0",
    "chunk_text": "MatteFormer: Transformer-Based Image Matting via Prior-Tokens\n\nIn this paper, we propose a transformer-based image matting model called MatteFormer, which takes full advantage of trimap information in the transformer block. Our method first introduces a prior-token which is a global representation of each trimap region (e.g. foreground, background and unknown). These prior-tokens are used as global priors and participate in the self-attention mechanism of each block. Each stage of the encoder is composed of PAST (Prior-Attentive Swin Transformer) block, which is based on the Swin Transformer block, but differs in a couple of aspects: 1) It has PA-WSA (Prior-Attentive Window Self-Attention) layer, performing self-attention not only with spatial-tokens but also with prior-tokens.",
    "original_url": "http://arxiv.org/pdf/2203.15662v1",
    "original_title": "MatteFormer: Transformer-Based Image Matting via Prior-Tokens",
    "source": "arxiv",
    "authors": [
      "GyuTae Park",
      "SungJoon Son",
      "JaeYoung Yoo",
      "SeHo Kim",
      "Nojun Kwak"
    ],
    "published": "2022-03-29T15:25:56+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2203.15662v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2203.15662v1_chunk_1",
    "chunk_text": "Each stage of the encoder is composed of PAST (Prior-Attentive Swin Transformer) block, which is based on the Swin Transformer block, but differs in a couple of aspects: 1) It has PA-WSA (Prior-Attentive Window Self-Attention) layer, performing self-attention not only with spatial-tokens but also with prior-tokens. 2) It has prior-memory which saves prior-tokens accumulatively from the previous blocks and transfers them to the next block. We evaluate our MatteFormer on the commonly used image matting datasets: Composition-1k and Distinctions-646. Experiment results show that our proposed method achieves state-of-the-art performance with a large margin. Our codes are available at https://github.com/webtoon/matteformer.",
    "original_url": "http://arxiv.org/pdf/2203.15662v1",
    "original_title": "MatteFormer: Transformer-Based Image Matting via Prior-Tokens",
    "source": "arxiv",
    "authors": [
      "GyuTae Park",
      "SungJoon Son",
      "JaeYoung Yoo",
      "SeHo Kim",
      "Nojun Kwak"
    ],
    "published": "2022-03-29T15:25:56+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2203.15662v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2203.15662v1_chunk_2",
    "chunk_text": "Our codes are available at https://github.com/webtoon/matteformer.",
    "original_url": "http://arxiv.org/pdf/2203.15662v1",
    "original_title": "MatteFormer: Transformer-Based Image Matting via Prior-Tokens",
    "source": "arxiv",
    "authors": [
      "GyuTae Park",
      "SungJoon Son",
      "JaeYoung Yoo",
      "SeHo Kim",
      "Nojun Kwak"
    ],
    "published": "2022-03-29T15:25:56+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2203.15662v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2209.07140v1_chunk_0",
    "chunk_text": "Beat Transformer: Demixed Beat and Downbeat Tracking with Dilated Self-Attention\n\nWe propose Beat Transformer, a novel Transformer encoder architecture for joint beat and downbeat tracking. Different from previous models that track beats solely based on the spectrogram of an audio mixture, our model deals with demixed spectrograms with multiple instrument channels. This is inspired by the fact that humans perceive metrical structures from richer musical contexts, such as chord progression and instrumentation. To this end, we develop a Transformer model with both time-wise attention and instrument-wise attention to capture deep-buried metrical cues. Moreover, our model adopts a novel dilated self-attention mechanism, which achieves powerful hierarchical modelling with only linear complexity.",
    "original_url": "http://arxiv.org/pdf/2209.07140v1",
    "original_title": "Beat Transformer: Demixed Beat and Downbeat Tracking with Dilated Self-Attention",
    "source": "arxiv",
    "authors": [
      "Jingwei Zhao",
      "Gus Xia",
      "Ye Wang"
    ],
    "published": "2022-09-15T08:38:58+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2209.07140v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2209.07140v1_chunk_1",
    "chunk_text": "Moreover, our model adopts a novel dilated self-attention mechanism, which achieves powerful hierarchical modelling with only linear complexity. Experiments demonstrate a significant improvement in demixed beat tracking over the non-demixed version. Also, Beat Transformer achieves up to 4% point improvement in downbeat tracking accuracy over the TCN architectures. We further discover an interpretable attention pattern that mirrors our understanding of hierarchical metrical structures.",
    "original_url": "http://arxiv.org/pdf/2209.07140v1",
    "original_title": "Beat Transformer: Demixed Beat and Downbeat Tracking with Dilated Self-Attention",
    "source": "arxiv",
    "authors": [
      "Jingwei Zhao",
      "Gus Xia",
      "Ye Wang"
    ],
    "published": "2022-09-15T08:38:58+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2209.07140v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2304.03012v1_chunk_0",
    "chunk_text": "PointCAT: Cross-Attention Transformer for point cloud\n\nTransformer-based models have significantly advanced natural language processing and computer vision in recent years. However, due to the irregular and disordered structure of point cloud data, transformer-based models for 3D deep learning are still in their infancy compared to other methods. In this paper we present Point Cross-Attention Transformer (PointCAT), a novel end-to-end network architecture using cross-attentions mechanism for point cloud representing. Our approach combines multi-scale features via two seprate cross-attention transformer branches. To reduce the computational increase brought by multi-branch structure, we further introduce an efficient model for shape classification, which only process single class token of one branch as a query to calculate attention map with the other.",
    "original_url": "http://arxiv.org/pdf/2304.03012v1",
    "original_title": "PointCAT: Cross-Attention Transformer for point cloud",
    "source": "arxiv",
    "authors": [
      "Xincheng Yang",
      "Mingze Jin",
      "Weiji He",
      "Qian Chen"
    ],
    "published": "2023-04-06T11:58:18+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2304.03012v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2304.03012v1_chunk_1",
    "chunk_text": "To reduce the computational increase brought by multi-branch structure, we further introduce an efficient model for shape classification, which only process single class token of one branch as a query to calculate attention map with the other. Extensive experiments demonstrate that our method outperforms or achieves comparable performance to several approaches in shape classification, part segmentation and semantic segmentation tasks.",
    "original_url": "http://arxiv.org/pdf/2304.03012v1",
    "original_title": "PointCAT: Cross-Attention Transformer for point cloud",
    "source": "arxiv",
    "authors": [
      "Xincheng Yang",
      "Mingze Jin",
      "Weiji He",
      "Qian Chen"
    ],
    "published": "2023-04-06T11:58:18+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2304.03012v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2309.06891v1_chunk_0",
    "chunk_text": "Keep It SimPool: Who Said Supervised Transformers Suffer from Attention Deficit? Convolutional networks and vision transformers have different forms of pairwise interactions, pooling across layers and pooling at the end of the network. Does the latter really need to be different? As a by-product of pooling, vision transformers provide spatial attention for free, but this is most often of low quality unless self-supervised, which is not well studied. Is supervision really the problem?",
    "original_url": "http://arxiv.org/pdf/2309.06891v1",
    "original_title": "Keep It SimPool: Who Said Supervised Transformers Suffer from Attention Deficit?",
    "source": "arxiv",
    "authors": [
      "Bill Psomas",
      "Ioannis Kakogeorgiou",
      "Konstantinos Karantzalos",
      "Yannis Avrithis"
    ],
    "published": "2023-09-13T11:28:27+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2309.06891v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2309.06891v1_chunk_1",
    "chunk_text": "Is supervision really the problem? In this work, we develop a generic pooling framework and then we formulate a number of existing methods as instantiations. By discussing the properties of each group of methods, we derive SimPool, a simple attention-based pooling mechanism as a replacement of the default one for both convolutional and transformer encoders. We find that, whether supervised or self-supervised, this improves performance on pre-training and downstream tasks and provides attention maps delineating object boundaries in all cases. One could thus call SimPool universal.",
    "original_url": "http://arxiv.org/pdf/2309.06891v1",
    "original_title": "Keep It SimPool: Who Said Supervised Transformers Suffer from Attention Deficit?",
    "source": "arxiv",
    "authors": [
      "Bill Psomas",
      "Ioannis Kakogeorgiou",
      "Konstantinos Karantzalos",
      "Yannis Avrithis"
    ],
    "published": "2023-09-13T11:28:27+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2309.06891v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2309.06891v1_chunk_2",
    "chunk_text": "One could thus call SimPool universal. To our knowledge, we are the first to obtain attention maps in supervised transformers of at least as good quality as self-supervised, without explicit losses or modifying the architecture. Code at: https://github.com/billpsomas/simpool.",
    "original_url": "http://arxiv.org/pdf/2309.06891v1",
    "original_title": "Keep It SimPool: Who Said Supervised Transformers Suffer from Attention Deficit?",
    "source": "arxiv",
    "authors": [
      "Bill Psomas",
      "Ioannis Kakogeorgiou",
      "Konstantinos Karantzalos",
      "Yannis Avrithis"
    ],
    "published": "2023-09-13T11:28:27+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2309.06891v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2311.03235v1_chunk_0",
    "chunk_text": "p-Laplacian Transformer\n\n$p$-Laplacian regularization, rooted in graph and image signal processing, introduces a parameter $p$ to control the regularization effect on these data. Smaller values of $p$ promote sparsity and interpretability, while larger values encourage smoother solutions. In this paper, we first show that the self-attention mechanism obtains the minimal Laplacian regularization ($p=2$) and encourages the smoothness in the architecture. However, the smoothness is not suitable for the heterophilic structure of self-attention in transformers where attention weights between tokens that are in close proximity and non-close ones are assigned indistinguishably. From that insight, we then propose a novel class of transformers, namely the $p$-Laplacian Transformer (p-LaT), which leverages $p$-Laplacian regularization framework to harness the heterophilic features within self-attention layers.",
    "original_url": "http://arxiv.org/pdf/2311.03235v1",
    "original_title": "p-Laplacian Transformer",
    "source": "arxiv",
    "authors": [
      "Tuan Nguyen",
      "Tam Nguyen",
      "Vinh Nguyen",
      "Tan M. Nguyen"
    ],
    "published": "2023-11-06T16:25:56+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2311.03235v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2311.03235v1_chunk_1",
    "chunk_text": "From that insight, we then propose a novel class of transformers, namely the $p$-Laplacian Transformer (p-LaT), which leverages $p$-Laplacian regularization framework to harness the heterophilic features within self-attention layers. In particular, low $p$ values will effectively assign higher attention weights to tokens that are in close proximity to the current token being processed. We empirically demonstrate the advantages of p-LaT over the baseline transformers on a wide range of benchmark datasets.",
    "original_url": "http://arxiv.org/pdf/2311.03235v1",
    "original_title": "p-Laplacian Transformer",
    "source": "arxiv",
    "authors": [
      "Tuan Nguyen",
      "Tam Nguyen",
      "Vinh Nguyen",
      "Tan M. Nguyen"
    ],
    "published": "2023-11-06T16:25:56+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2311.03235v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2404.05657v1_chunk_0",
    "chunk_text": "MLP Can Be A Good Transformer Learner\n\nSelf-attention mechanism is the key of the Transformer but often criticized for its computation demands. Previous token pruning works motivate their methods from the view of computation redundancy but still need to load the full network and require same memory costs. This paper introduces a novel strategy that simplifies vision transformers and reduces computational load through the selective removal of non-essential attention layers, guided by entropy considerations. We identify that regarding the attention layer in bottom blocks, their subsequent MLP layers, i.e. two feed-forward layers, can elicit the same entropy quantity.",
    "original_url": "http://arxiv.org/pdf/2404.05657v1",
    "original_title": "MLP Can Be A Good Transformer Learner",
    "source": "arxiv",
    "authors": [
      "Sihao Lin",
      "Pumeng Lyu",
      "Dongrui Liu",
      "Tao Tang",
      "Xiaodan Liang",
      "Andy Song",
      "Xiaojun Chang"
    ],
    "published": "2024-04-08T16:40:15+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2404.05657v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2404.05657v1_chunk_1",
    "chunk_text": "two feed-forward layers, can elicit the same entropy quantity. Meanwhile, the accompanied MLPs are under-exploited since they exhibit smaller feature entropy compared to those MLPs in the top blocks. Therefore, we propose to integrate the uninformative attention layers into their subsequent counterparts by degenerating them into identical mapping, yielding only MLP in certain transformer blocks. Experimental results on ImageNet-1k show that the proposed method can remove 40% attention layer of DeiT-B, improving throughput and memory bound without performance compromise. Code is available at https://github.com/sihaoevery/lambda_vit.",
    "original_url": "http://arxiv.org/pdf/2404.05657v1",
    "original_title": "MLP Can Be A Good Transformer Learner",
    "source": "arxiv",
    "authors": [
      "Sihao Lin",
      "Pumeng Lyu",
      "Dongrui Liu",
      "Tao Tang",
      "Xiaodan Liang",
      "Andy Song",
      "Xiaojun Chang"
    ],
    "published": "2024-04-08T16:40:15+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2404.05657v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2404.05657v1_chunk_2",
    "chunk_text": "Code is available at https://github.com/sihaoevery/lambda_vit.",
    "original_url": "http://arxiv.org/pdf/2404.05657v1",
    "original_title": "MLP Can Be A Good Transformer Learner",
    "source": "arxiv",
    "authors": [
      "Sihao Lin",
      "Pumeng Lyu",
      "Dongrui Liu",
      "Tao Tang",
      "Xiaodan Liang",
      "Andy Song",
      "Xiaojun Chang"
    ],
    "published": "2024-04-08T16:40:15+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2404.05657v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2405.16877v3_chunk_0",
    "chunk_text": "Are Self-Attentions Effective for Time Series Forecasting? Time series forecasting is crucial for applications across multiple domains and various scenarios. Although Transformer models have dramatically advanced the landscape of forecasting, their effectiveness remains debated. Recent findings have indicated that simpler linear models might outperform complex Transformer-based approaches, highlighting the potential for more streamlined architectures. In this paper, we shift the focus from evaluating the overall Transformer architecture to specifically examining the effectiveness of self-attention for time series forecasting.",
    "original_url": "http://arxiv.org/pdf/2405.16877v3",
    "original_title": "Are Self-Attentions Effective for Time Series Forecasting?",
    "source": "arxiv",
    "authors": [
      "Dongbin Kim",
      "Jinseong Park",
      "Jaewook Lee",
      "Hoki Kim"
    ],
    "published": "2024-05-27T06:49:39+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2405.16877v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2405.16877v3_chunk_1",
    "chunk_text": "In this paper, we shift the focus from evaluating the overall Transformer architecture to specifically examining the effectiveness of self-attention for time series forecasting. To this end, we introduce a new architecture, Cross-Attention-only Time Series transformer (CATS), that rethinks the traditional Transformer framework by eliminating self-attention and leveraging cross-attention mechanisms instead. By establishing future horizon-dependent parameters as queries and enhanced parameter sharing, our model not only improves long-term forecasting accuracy but also reduces the number of parameters and memory usage. Extensive experiment across various datasets demonstrates that our model achieves superior performance with the lowest mean squared error and uses fewer parameters compared to existing models. The implementation of our model is available at: https://github.com/dongbeank/CATS.",
    "original_url": "http://arxiv.org/pdf/2405.16877v3",
    "original_title": "Are Self-Attentions Effective for Time Series Forecasting?",
    "source": "arxiv",
    "authors": [
      "Dongbin Kim",
      "Jinseong Park",
      "Jaewook Lee",
      "Hoki Kim"
    ],
    "published": "2024-05-27T06:49:39+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2405.16877v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2405.16877v3_chunk_2",
    "chunk_text": "The implementation of our model is available at: https://github.com/dongbeank/CATS.",
    "original_url": "http://arxiv.org/pdf/2405.16877v3",
    "original_title": "Are Self-Attentions Effective for Time Series Forecasting?",
    "source": "arxiv",
    "authors": [
      "Dongbin Kim",
      "Jinseong Park",
      "Jaewook Lee",
      "Hoki Kim"
    ],
    "published": "2024-05-27T06:49:39+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2405.16877v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2105.14424v1_chunk_0",
    "chunk_text": "Gaze Estimation using Transformer\n\nRecent work has proven the effectiveness of transformers in many computer vision tasks. However, the performance of transformers in gaze estimation is still unexplored. In this paper, we employ transformers and assess their effectiveness for gaze estimation. We consider two forms of vision transformer which are pure transformers and hybrid transformers. We first follow the popular ViT and employ a pure transformer to estimate gaze from images.",
    "original_url": "http://arxiv.org/pdf/2105.14424v1",
    "original_title": "Gaze Estimation using Transformer",
    "source": "arxiv",
    "authors": [
      "Yihua Cheng",
      "Feng Lu"
    ],
    "published": "2021-05-30T04:06:29+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2105.14424v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2105.14424v1_chunk_1",
    "chunk_text": "We first follow the popular ViT and employ a pure transformer to estimate gaze from images. On the other hand, we preserve the convolutional layers and integrate CNNs as well as transformers. The transformer serves as a component to complement CNNs. We compare the performance of the two transformers in gaze estimation. The Hybrid transformer significantly outperforms the pure transformer in all evaluation datasets with less parameters.",
    "original_url": "http://arxiv.org/pdf/2105.14424v1",
    "original_title": "Gaze Estimation using Transformer",
    "source": "arxiv",
    "authors": [
      "Yihua Cheng",
      "Feng Lu"
    ],
    "published": "2021-05-30T04:06:29+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2105.14424v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2105.14424v1_chunk_2",
    "chunk_text": "The Hybrid transformer significantly outperforms the pure transformer in all evaluation datasets with less parameters. We further conduct experiments to assess the effectiveness of the hybrid transformer and explore the advantage of self-attention mechanism. Experiments show the hybrid transformer can achieve state-of-the-art performance in all benchmarks with pre-training.To facilitate further research, we release codes and models in https://github.com/yihuacheng/GazeTR.",
    "original_url": "http://arxiv.org/pdf/2105.14424v1",
    "original_title": "Gaze Estimation using Transformer",
    "source": "arxiv",
    "authors": [
      "Yihua Cheng",
      "Feng Lu"
    ],
    "published": "2021-05-30T04:06:29+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2105.14424v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2412.08148v1_chunk_0",
    "chunk_text": "A Review of Intelligent Device Fault Diagnosis Technologies Based on Machine Vision\n\nThis paper provides a comprehensive review of mechanical equipment fault diagnosis methods, focusing on the advancements brought by Transformer-based models. It details the structure, working principles, and benefits of Transformers, particularly their self-attention mechanism and parallel computation capabilities, which have propelled their widespread application in natural language processing and computer vision. The discussion highlights key Transformer model variants, such as Vision Transformers (ViT) and their extensions, which leverage self-attention to improve accuracy and efficiency in visual tasks. Furthermore, the paper examines the application of Transformer-based approaches in intelligent fault diagnosis for mechanical systems, showcasing their superior ability to extract and recognize patterns from complex sensor data for precise fault identification. Despite these advancements, challenges remain, including the reliance on extensive labeled datasets, significant computational demands, and difficulties in deploying models on resource-limited devices.",
    "original_url": "http://arxiv.org/pdf/2412.08148v1",
    "original_title": "A Review of Intelligent Device Fault Diagnosis Technologies Based on Machine Vision",
    "source": "arxiv",
    "authors": [
      "Guiran Liu",
      "Binrong Zhu"
    ],
    "published": "2024-12-11T07:06:53+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2412.08148v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2412.08148v1_chunk_1",
    "chunk_text": "Despite these advancements, challenges remain, including the reliance on extensive labeled datasets, significant computational demands, and difficulties in deploying models on resource-limited devices. To address these limitations, the paper proposes future research directions, such as developing lightweight Transformer architectures, integrating multimodal data sources, and enhancing adaptability to diverse operational conditions. These efforts aim to further expand the application of Transformer-based methods in mechanical fault diagnosis, making them more robust, efficient, and suitable for real-world industrial environments.",
    "original_url": "http://arxiv.org/pdf/2412.08148v1",
    "original_title": "A Review of Intelligent Device Fault Diagnosis Technologies Based on Machine Vision",
    "source": "arxiv",
    "authors": [
      "Guiran Liu",
      "Binrong Zhu"
    ],
    "published": "2024-12-11T07:06:53+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2412.08148v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2010.06396v2_chunk_0",
    "chunk_text": "Interpreting Attention Models with Human Visual Attention in Machine Reading Comprehension\n\nWhile neural networks with attention mechanisms have achieved superior performance on many natural language processing tasks, it remains unclear to which extent learned attention resembles human visual attention. In this paper, we propose a new method that leverages eye-tracking data to investigate the relationship between human visual attention and neural attention in machine reading comprehension. To this end, we introduce a novel 23 participant eye tracking dataset - MQA-RC, in which participants read movie plots and answered pre-defined questions. We compare state of the art networks based on long short-term memory (LSTM), convolutional neural models (CNN) and XLNet Transformer architectures. We find that higher similarity to human attention and performance significantly correlates to the LSTM and CNN models.",
    "original_url": "http://arxiv.org/pdf/2010.06396v2",
    "original_title": "Interpreting Attention Models with Human Visual Attention in Machine Reading Comprehension",
    "source": "arxiv",
    "authors": [
      "Ekta Sood",
      "Simon Tannert",
      "Diego Frassinelli",
      "Andreas Bulling",
      "Ngoc Thang Vu"
    ],
    "published": "2020-10-13T13:51:57+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2010.06396v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2010.06396v2_chunk_1",
    "chunk_text": "We find that higher similarity to human attention and performance significantly correlates to the LSTM and CNN models. However, we show this relationship does not hold true for the XLNet models -- despite the fact that the XLNet performs best on this challenging task. Our results suggest that different architectures seem to learn rather different neural attention strategies and similarity of neural to human attention does not guarantee best performance.",
    "original_url": "http://arxiv.org/pdf/2010.06396v2",
    "original_title": "Interpreting Attention Models with Human Visual Attention in Machine Reading Comprehension",
    "source": "arxiv",
    "authors": [
      "Ekta Sood",
      "Simon Tannert",
      "Diego Frassinelli",
      "Andreas Bulling",
      "Ngoc Thang Vu"
    ],
    "published": "2020-10-13T13:51:57+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2010.06396v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2109.02613v1_chunk_0",
    "chunk_text": "Class Semantics-based Attention for Action Detection\n\nAction localization networks are often structured as a feature encoder sub-network and a localization sub-network, where the feature encoder learns to transform an input video to features that are useful for the localization sub-network to generate reliable action proposals. While some of the encoded features may be more useful for generating action proposals, prior action localization approaches do not include any attention mechanism that enables the localization sub-network to attend more to the more important features. In this paper, we propose a novel attention mechanism, the Class Semantics-based Attention (CSA), that learns from the temporal distribution of semantics of action classes present in an input video to find the importance scores of the encoded features, which are used to provide attention to the more useful encoded features. We demonstrate on two popular action detection datasets that incorporating our novel attention mechanism provides considerable performance gains on competitive action detection models (e.g., around 6.2% improvement over BMN action detection baseline to obtain 47.5% mAP on the THUMOS-14 dataset), and a new state-of-the-art of 36.25% mAP on the ActivityNet v1.3 dataset. Further, the CSA localization model family which includes BMN-CSA, was part of the second-placed submission at the 2021 ActivityNet action localization challenge.",
    "original_url": "http://arxiv.org/pdf/2109.02613v1",
    "original_title": "Class Semantics-based Attention for Action Detection",
    "source": "arxiv",
    "authors": [
      "Deepak Sridhar",
      "Niamul Quader",
      "Srikanth Muralidharan",
      "Yaoxin Li",
      "Peng Dai",
      "Juwei Lu"
    ],
    "published": "2021-09-06T17:22:46+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2109.02613v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2109.02613v1_chunk_1",
    "chunk_text": "Further, the CSA localization model family which includes BMN-CSA, was part of the second-placed submission at the 2021 ActivityNet action localization challenge. Our attention mechanism outperforms prior self-attention modules such as the squeeze-and-excitation in action detection task. We also observe that our attention mechanism is complementary to such self-attention modules in that performance improvements are seen when both are used together.",
    "original_url": "http://arxiv.org/pdf/2109.02613v1",
    "original_title": "Class Semantics-based Attention for Action Detection",
    "source": "arxiv",
    "authors": [
      "Deepak Sridhar",
      "Niamul Quader",
      "Srikanth Muralidharan",
      "Yaoxin Li",
      "Peng Dai",
      "Juwei Lu"
    ],
    "published": "2021-09-06T17:22:46+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2109.02613v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2401.01750v2_chunk_0",
    "chunk_text": "Towards Robust Semantic Segmentation against Patch-based Attack via Attention Refinement\n\nThe attention mechanism has been proven effective on various visual tasks in recent years. In the semantic segmentation task, the attention mechanism is applied in various methods, including the case of both Convolution Neural Networks (CNN) and Vision Transformer (ViT) as backbones. However, we observe that the attention mechanism is vulnerable to patch-based adversarial attacks. Through the analysis of the effective receptive field, we attribute it to the fact that the wide receptive field brought by global attention may lead to the spread of the adversarial patch. To address this issue, in this paper, we propose a Robust Attention Mechanism (RAM) to improve the robustness of the semantic segmentation model, which can notably relieve the vulnerability against patch-based attacks.",
    "original_url": "http://arxiv.org/pdf/2401.01750v2",
    "original_title": "Towards Robust Semantic Segmentation against Patch-based Attack via Attention Refinement",
    "source": "arxiv",
    "authors": [
      "Zheng Yuan",
      "Jie Zhang",
      "Yude Wang",
      "Shiguang Shan",
      "Xilin Chen"
    ],
    "published": "2024-01-03T13:58:35+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2401.01750v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2401.01750v2_chunk_1",
    "chunk_text": "To address this issue, in this paper, we propose a Robust Attention Mechanism (RAM) to improve the robustness of the semantic segmentation model, which can notably relieve the vulnerability against patch-based attacks. Compared to the vallina attention mechanism, RAM introduces two novel modules called Max Attention Suppression and Random Attention Dropout, both of which aim to refine the attention matrix and limit the influence of a single adversarial patch on the semantic segmentation results of other positions. Extensive experiments demonstrate the effectiveness of our RAM to improve the robustness of semantic segmentation models against various patch-based attack methods under different attack settings.",
    "original_url": "http://arxiv.org/pdf/2401.01750v2",
    "original_title": "Towards Robust Semantic Segmentation against Patch-based Attack via Attention Refinement",
    "source": "arxiv",
    "authors": [
      "Zheng Yuan",
      "Jie Zhang",
      "Yude Wang",
      "Shiguang Shan",
      "Xilin Chen"
    ],
    "published": "2024-01-03T13:58:35+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2401.01750v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2407.09268v1_chunk_0",
    "chunk_text": "Region Attention Transformer for Medical Image Restoration\n\nTransformer-based methods have demonstrated impressive results in medical image restoration, attributed to the multi-head self-attention (MSA) mechanism in the spatial dimension. However, the majority of existing Transformers conduct attention within fixed and coarsely partitioned regions (\\text{e.g.} the entire image or fixed patches), resulting in interference from irrelevant regions and fragmentation of continuous image content. To overcome these challenges, we introduce a novel Region Attention Transformer (RAT) that utilizes a region-based multi-head self-attention mechanism (R-MSA). The R-MSA dynamically partitions the input image into non-overlapping semantic regions using the robust Segment Anything Model (SAM) and then performs self-attention within these regions.",
    "original_url": "http://arxiv.org/pdf/2407.09268v1",
    "original_title": "Region Attention Transformer for Medical Image Restoration",
    "source": "arxiv",
    "authors": [
      "Zhiwen Yang",
      "Haowei Chen",
      "Ziniu Qian",
      "Yang Zhou",
      "Hui Zhang",
      "Dan Zhao",
      "Bingzheng Wei",
      "Yan Xu"
    ],
    "published": "2024-07-12T13:52:05+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2407.09268v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2407.09268v1_chunk_1",
    "chunk_text": "The R-MSA dynamically partitions the input image into non-overlapping semantic regions using the robust Segment Anything Model (SAM) and then performs self-attention within these regions. This region partitioning is more flexible and interpretable, ensuring that only pixels from similar semantic regions complement each other, thereby eliminating interference from irrelevant regions. Moreover, we introduce a focal region loss to guide our model to adaptively focus on recovering high-difficulty regions. Extensive experiments demonstrate the effectiveness of RAT in various medical image restoration tasks, including PET image synthesis, CT image denoising, and pathological image super-resolution. Code is available at \\href{https://github.com/Yaziwel/Region-Attention-Transformer-for-Medical-Image-Restoration.git}{https://github.com/RAT}.",
    "original_url": "http://arxiv.org/pdf/2407.09268v1",
    "original_title": "Region Attention Transformer for Medical Image Restoration",
    "source": "arxiv",
    "authors": [
      "Zhiwen Yang",
      "Haowei Chen",
      "Ziniu Qian",
      "Yang Zhou",
      "Hui Zhang",
      "Dan Zhao",
      "Bingzheng Wei",
      "Yan Xu"
    ],
    "published": "2024-07-12T13:52:05+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2407.09268v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2407.09268v1_chunk_2",
    "chunk_text": "Code is available at \\href{https://github.com/Yaziwel/Region-Attention-Transformer-for-Medical-Image-Restoration.git}{https://github.com/RAT}.",
    "original_url": "http://arxiv.org/pdf/2407.09268v1",
    "original_title": "Region Attention Transformer for Medical Image Restoration",
    "source": "arxiv",
    "authors": [
      "Zhiwen Yang",
      "Haowei Chen",
      "Ziniu Qian",
      "Yang Zhou",
      "Hui Zhang",
      "Dan Zhao",
      "Bingzheng Wei",
      "Yan Xu"
    ],
    "published": "2024-07-12T13:52:05+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2407.09268v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2206.14263v1_chunk_0",
    "chunk_text": "ZoDIAC: Zoneout Dropout Injection Attention Calculation\n\nRecently the use of self-attention has yielded to state-of-the-art results in vision-language tasks such as image captioning as well as natural language understanding and generation (NLU and NLG) tasks and computer vision tasks such as image classification. This is since self-attention maps the internal interactions among the elements of input source and target sequences. Although self-attention successfully calculates the attention values and maps the relationships among the elements of input source and target sequence, yet there is no mechanism to control the intensity of attention. In real world, when communicating with each other face to face or vocally, we tend to express different visual and linguistic context with various amounts of intensity. Some words might carry (be spoken with) more stress and weight indicating the importance of that word in the context of the whole sentence.",
    "original_url": "http://arxiv.org/pdf/2206.14263v1",
    "original_title": "ZoDIAC: Zoneout Dropout Injection Attention Calculation",
    "source": "arxiv",
    "authors": [
      "Zanyar Zohourianshahzadi",
      "Jugal Kalita"
    ],
    "published": "2022-06-28T19:36:11+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2206.14263v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2206.14263v1_chunk_1",
    "chunk_text": "Some words might carry (be spoken with) more stress and weight indicating the importance of that word in the context of the whole sentence. Based on this intuition, we propose Zoneout Dropout Injection Attention Calculation (ZoDIAC) in which the intensities of attention values in the elements of the input sequence are calculated with respect to the context of the elements of input sequence. The results of our experiments reveal that employing ZoDIAC leads to better performance in comparison with the self-attention module in the Transformer model. The ultimate goal is to find out if we could modify self-attention module in the Transformer model with a method that is potentially extensible to other models that leverage on self-attention at their core. Our findings suggest that this particular goal deserves further attention and investigation by the research community.",
    "original_url": "http://arxiv.org/pdf/2206.14263v1",
    "original_title": "ZoDIAC: Zoneout Dropout Injection Attention Calculation",
    "source": "arxiv",
    "authors": [
      "Zanyar Zohourianshahzadi",
      "Jugal Kalita"
    ],
    "published": "2022-06-28T19:36:11+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2206.14263v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2206.14263v1_chunk_2",
    "chunk_text": "Our findings suggest that this particular goal deserves further attention and investigation by the research community. The code for ZoDIAC is available on www.github.com/zanyarz/zodiac .",
    "original_url": "http://arxiv.org/pdf/2206.14263v1",
    "original_title": "ZoDIAC: Zoneout Dropout Injection Attention Calculation",
    "source": "arxiv",
    "authors": [
      "Zanyar Zohourianshahzadi",
      "Jugal Kalita"
    ],
    "published": "2022-06-28T19:36:11+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2206.14263v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2210.05176v1_chunk_0",
    "chunk_text": "Fine-Grained Image Style Transfer with Visual Transformers\n\nWith the development of the convolutional neural network, image style transfer has drawn increasing attention. However, most existing approaches adopt a global feature transformation to transfer style patterns into content images (e.g., AdaIN and WCT). Such a design usually destroys the spatial information of the input images and fails to transfer fine-grained style patterns into style transfer results. To solve this problem, we propose a novel STyle TRansformer (STTR) network which breaks both content and style images into visual tokens to achieve a fine-grained style transformation. Specifically, two attention mechanisms are adopted in our STTR.",
    "original_url": "http://arxiv.org/pdf/2210.05176v1",
    "original_title": "Fine-Grained Image Style Transfer with Visual Transformers",
    "source": "arxiv",
    "authors": [
      "Jianbo Wang",
      "Huan Yang",
      "Jianlong Fu",
      "Toshihiko Yamasaki",
      "Baining Guo"
    ],
    "published": "2022-10-11T06:26:00+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2210.05176v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2210.05176v1_chunk_1",
    "chunk_text": "Specifically, two attention mechanisms are adopted in our STTR. We first propose to use self-attention to encode content and style tokens such that similar tokens can be grouped and learned together. We then adopt cross-attention between content and style tokens that encourages fine-grained style transformations. To compare STTR with existing approaches, we conduct user studies on Amazon Mechanical Turk (AMT), which are carried out with 50 human subjects with 1,000 votes in total. Extensive evaluations demonstrate the effectiveness and efficiency of the proposed STTR in generating visually pleasing style transfer results.",
    "original_url": "http://arxiv.org/pdf/2210.05176v1",
    "original_title": "Fine-Grained Image Style Transfer with Visual Transformers",
    "source": "arxiv",
    "authors": [
      "Jianbo Wang",
      "Huan Yang",
      "Jianlong Fu",
      "Toshihiko Yamasaki",
      "Baining Guo"
    ],
    "published": "2022-10-11T06:26:00+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2210.05176v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2210.05176v1_chunk_2",
    "chunk_text": "Extensive evaluations demonstrate the effectiveness and efficiency of the proposed STTR in generating visually pleasing style transfer results.",
    "original_url": "http://arxiv.org/pdf/2210.05176v1",
    "original_title": "Fine-Grained Image Style Transfer with Visual Transformers",
    "source": "arxiv",
    "authors": [
      "Jianbo Wang",
      "Huan Yang",
      "Jianlong Fu",
      "Toshihiko Yamasaki",
      "Baining Guo"
    ],
    "published": "2022-10-11T06:26:00+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2210.05176v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2310.10375v3_chunk_0",
    "chunk_text": "GTA: A Geometry-Aware Attention Mechanism for Multi-View Transformers\n\nAs transformers are equivariant to the permutation of input tokens, encoding the positional information of tokens is necessary for many tasks. However, since existing positional encoding schemes have been initially designed for NLP tasks, their suitability for vision tasks, which typically exhibit different structural properties in their data, is questionable. We argue that existing positional encoding schemes are suboptimal for 3D vision tasks, as they do not respect their underlying 3D geometric structure. Based on this hypothesis, we propose a geometry-aware attention mechanism that encodes the geometric structure of tokens as relative transformation determined by the geometric relationship between queries and key-value pairs. By evaluating on multiple novel view synthesis (NVS) datasets in the sparse wide-baseline multi-view setting, we show that our attention, called Geometric Transform Attention (GTA), improves learning efficiency and performance of state-of-the-art transformer-based NVS models without any additional learned parameters and only minor computational overhead.",
    "original_url": "http://arxiv.org/pdf/2310.10375v3",
    "original_title": "GTA: A Geometry-Aware Attention Mechanism for Multi-View Transformers",
    "source": "arxiv",
    "authors": [
      "Takeru Miyato",
      "Bernhard Jaeger",
      "Max Welling",
      "Andreas Geiger"
    ],
    "published": "2023-10-16T13:16:09+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2310.10375v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2310.10375v3_chunk_1",
    "chunk_text": "By evaluating on multiple novel view synthesis (NVS) datasets in the sparse wide-baseline multi-view setting, we show that our attention, called Geometric Transform Attention (GTA), improves learning efficiency and performance of state-of-the-art transformer-based NVS models without any additional learned parameters and only minor computational overhead.",
    "original_url": "http://arxiv.org/pdf/2310.10375v3",
    "original_title": "GTA: A Geometry-Aware Attention Mechanism for Multi-View Transformers",
    "source": "arxiv",
    "authors": [
      "Takeru Miyato",
      "Bernhard Jaeger",
      "Max Welling",
      "Andreas Geiger"
    ],
    "published": "2023-10-16T13:16:09+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2310.10375v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2405.12462v4_chunk_0",
    "chunk_text": "Enhancing Transformer-based models for Long Sequence Time Series Forecasting via Structured Matrix\n\nRecently, Transformer-based models for long sequence time series forecasting have demonstrated promising results. The self-attention mechanism as the core component of these Transformer-based models exhibits great potential in capturing various dependencies among data points. Despite these advancements, it has been a subject of concern to improve the efficiency of the self-attention mechanism. Unfortunately, current specific optimization methods are facing the challenges in applicability and scalability for the future design of long sequence time series forecasting models. Hence, in this article, we propose a novel architectural framework that enhances Transformer-based models through the integration of Surrogate Attention Blocks (SAB) and Surrogate Feed-Forward Neural Network Blocks (SFB).",
    "original_url": "http://arxiv.org/pdf/2405.12462v4",
    "original_title": "Enhancing Transformer-based models for Long Sequence Time Series Forecasting via Structured Matrix",
    "source": "arxiv",
    "authors": [
      "Zhicheng Zhang",
      "Yong Wang",
      "Shaoqi Tan",
      "Bowei Xia",
      "Yujie Luo"
    ],
    "published": "2024-05-21T02:37:47+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2405.12462v4"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2405.12462v4_chunk_1",
    "chunk_text": "Hence, in this article, we propose a novel architectural framework that enhances Transformer-based models through the integration of Surrogate Attention Blocks (SAB) and Surrogate Feed-Forward Neural Network Blocks (SFB). The framework reduces both time and space complexity by the replacement of the self-attention and feed-forward layers with SAB and SFB while maintaining their expressive power and architectural advantages. The equivalence of this substitution is fully demonstrated. The extensive experiments on 10 Transformer-based models across five distinct time series tasks demonstrate an average performance improvement of 12.4%, alongside 61.3% reduction in parameter counts.",
    "original_url": "http://arxiv.org/pdf/2405.12462v4",
    "original_title": "Enhancing Transformer-based models for Long Sequence Time Series Forecasting via Structured Matrix",
    "source": "arxiv",
    "authors": [
      "Zhicheng Zhang",
      "Yong Wang",
      "Shaoqi Tan",
      "Bowei Xia",
      "Yujie Luo"
    ],
    "published": "2024-05-21T02:37:47+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2405.12462v4"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2203.05922v1_chunk_0",
    "chunk_text": "Visualizing and Understanding Patch Interactions in Vision Transformer\n\nVision Transformer (ViT) has become a leading tool in various computer vision tasks, owing to its unique self-attention mechanism that learns visual representations explicitly through cross-patch information interactions. Despite having good success, the literature seldom explores the explainability of vision transformer, and there is no clear picture of how the attention mechanism with respect to the correlation across comprehensive patches will impact the performance and what is the further potential. In this work, we propose a novel explainable visualization approach to analyze and interpret the crucial attention interactions among patches for vision transformer. Specifically, we first introduce a quantification indicator to measure the impact of patch interaction and verify such quantification on attention window design and indiscriminative patches removal. Then, we exploit the effective responsive field of each patch in ViT and devise a window-free transformer architecture accordingly.",
    "original_url": "http://arxiv.org/pdf/2203.05922v1",
    "original_title": "Visualizing and Understanding Patch Interactions in Vision Transformer",
    "source": "arxiv",
    "authors": [
      "Jie Ma",
      "Yalong Bai",
      "Bineng Zhong",
      "Wei Zhang",
      "Ting Yao",
      "Tao Mei"
    ],
    "published": "2022-03-11T13:48:11+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2203.05922v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2203.05922v1_chunk_1",
    "chunk_text": "Then, we exploit the effective responsive field of each patch in ViT and devise a window-free transformer architecture accordingly. Extensive experiments on ImageNet demonstrate that the exquisitely designed quantitative method is shown able to facilitate ViT model learning, leading the top-1 accuracy by 4.28% at most. Moreover, the results on downstream fine-grained recognition tasks further validate the generalization of our proposal.",
    "original_url": "http://arxiv.org/pdf/2203.05922v1",
    "original_title": "Visualizing and Understanding Patch Interactions in Vision Transformer",
    "source": "arxiv",
    "authors": [
      "Jie Ma",
      "Yalong Bai",
      "Bineng Zhong",
      "Wei Zhang",
      "Ting Yao",
      "Tao Mei"
    ],
    "published": "2022-03-11T13:48:11+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2203.05922v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2405.01095v1_chunk_0",
    "chunk_text": "Transformers Fusion across Disjoint Samples for Hyperspectral Image Classification\n\n3D Swin Transformer (3D-ST) known for its hierarchical attention and window-based processing, excels in capturing intricate spatial relationships within images. Spatial-spectral Transformer (SST), meanwhile, specializes in modeling long-range dependencies through self-attention mechanisms. Therefore, this paper introduces a novel method: an attentional fusion of these two transformers to significantly enhance the classification performance of Hyperspectral Images (HSIs). What sets this approach apart is its emphasis on the integration of attentional mechanisms from both architectures. This integration not only refines the modeling of spatial and spectral information but also contributes to achieving more precise and accurate classification results.",
    "original_url": "http://arxiv.org/pdf/2405.01095v1",
    "original_title": "Transformers Fusion across Disjoint Samples for Hyperspectral Image Classification",
    "source": "arxiv",
    "authors": [
      "Muhammad Ahmad",
      "Manuel Mazzara",
      "Salvatore Distifano"
    ],
    "published": "2024-05-02T08:49:01+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2405.01095v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2405.01095v1_chunk_1",
    "chunk_text": "This integration not only refines the modeling of spatial and spectral information but also contributes to achieving more precise and accurate classification results. The experimentation and evaluation of benchmark HSI datasets underscore the importance of employing disjoint training, validation, and test samples. The results demonstrate the effectiveness of the fusion approach, showcasing its superiority over traditional methods and individual transformers. Incorporating disjoint samples enhances the robustness and reliability of the proposed methodology, emphasizing its potential for advancing hyperspectral image classification.",
    "original_url": "http://arxiv.org/pdf/2405.01095v1",
    "original_title": "Transformers Fusion across Disjoint Samples for Hyperspectral Image Classification",
    "source": "arxiv",
    "authors": [
      "Muhammad Ahmad",
      "Manuel Mazzara",
      "Salvatore Distifano"
    ],
    "published": "2024-05-02T08:49:01+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2405.01095v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2103.14625v3_chunk_0",
    "chunk_text": "Dodrio: Exploring Transformer Models with Interactive Visualization\n\nWhy do large pre-trained transformer-based models perform so well across a wide variety of NLP tasks? Recent research suggests the key may lie in multi-headed attention mechanism's ability to learn and represent linguistic information. Understanding how these models represent both syntactic and semantic knowledge is vital to investigate why they succeed and fail, what they have learned, and how they can improve. We present Dodrio, an open-source interactive visualization tool to help NLP researchers and practitioners analyze attention mechanisms in transformer-based models with linguistic knowledge. Dodrio tightly integrates an overview that summarizes the roles of different attention heads, and detailed views that help users compare attention weights with the syntactic structure and semantic information in the input text.",
    "original_url": "http://arxiv.org/pdf/2103.14625v3",
    "original_title": "Dodrio: Exploring Transformer Models with Interactive Visualization",
    "source": "arxiv",
    "authors": [
      "Zijie J. Wang",
      "Robert Turko",
      "Duen Horng Chau"
    ],
    "published": "2021-03-26T17:39:37+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2103.14625v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2103.14625v3_chunk_1",
    "chunk_text": "Dodrio tightly integrates an overview that summarizes the roles of different attention heads, and detailed views that help users compare attention weights with the syntactic structure and semantic information in the input text. To facilitate the visual comparison of attention weights and linguistic knowledge, Dodrio applies different graph visualization techniques to represent attention weights scalable to longer input text. Case studies highlight how Dodrio provides insights into understanding the attention mechanism in transformer-based models. Dodrio is available at https://poloclub.github.io/dodrio/.",
    "original_url": "http://arxiv.org/pdf/2103.14625v3",
    "original_title": "Dodrio: Exploring Transformer Models with Interactive Visualization",
    "source": "arxiv",
    "authors": [
      "Zijie J. Wang",
      "Robert Turko",
      "Duen Horng Chau"
    ],
    "published": "2021-03-26T17:39:37+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2103.14625v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2501.13428v2_chunk_0",
    "chunk_text": "Softplus Attention with Re-weighting Boosts Length Extrapolation in Large Language Models\n\nLarge language models have achieved remarkable success in recent years, primarily due to the implementation of self-attention mechanisms. However, traditional Softmax attention suffers from numerical instability and reduced performance as the length of inference tokens increases. This paper addresses these issues by decomposing the Softmax operation into a non-linear transformation and the $l_1$-norm. We identify the latter as essential for maintaining model performance. By replacing the non-linear transformation with the Softplus activation function and introducing a dynamic scale factor for different token lengths based on invariance entropy, we create a novel attention mechanism with performance better than conventional Softmax attention across various inference lengths.",
    "original_url": "http://arxiv.org/pdf/2501.13428v2",
    "original_title": "Softplus Attention with Re-weighting Boosts Length Extrapolation in Large Language Models",
    "source": "arxiv",
    "authors": [
      "Bo Gao",
      "Michael W. Spratling"
    ],
    "published": "2025-01-23T07:21:08+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2501.13428v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2501.13428v2_chunk_1",
    "chunk_text": "By replacing the non-linear transformation with the Softplus activation function and introducing a dynamic scale factor for different token lengths based on invariance entropy, we create a novel attention mechanism with performance better than conventional Softmax attention across various inference lengths. To further improve the length extrapolation ability of the proposed attention mechanism, we introduce a fine-tuning-free re-weighting mechanism that amplifies significant attention weights while diminishing weaker ones, enabling the model to concentrate more effectively on relevant tokens without requiring retraining. When combined with our proposed attention mechanism, this approach demonstrates significant promise in managing longer sequences, maintaining nearly constant validation loss even at 16$\\times$ the training token length while ensuring numerical stability. Our code is available at: https://github.com/iminfine/freeatten.",
    "original_url": "http://arxiv.org/pdf/2501.13428v2",
    "original_title": "Softplus Attention with Re-weighting Boosts Length Extrapolation in Large Language Models",
    "source": "arxiv",
    "authors": [
      "Bo Gao",
      "Michael W. Spratling"
    ],
    "published": "2025-01-23T07:21:08+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2501.13428v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2106.03714v1_chunk_0",
    "chunk_text": "Refiner: Refining Self-attention for Vision Transformers\n\nVision Transformers (ViTs) have shown competitive accuracy in image classification tasks compared with CNNs. Yet, they generally require much more data for model pre-training. Most of recent works thus are dedicated to designing more complex architectures or training methods to address the data-efficiency issue of ViTs. However, few of them explore improving the self-attention mechanism, a key factor distinguishing ViTs from CNNs. Different from existing works, we introduce a conceptually simple scheme, called refiner, to directly refine the self-attention maps of ViTs.",
    "original_url": "http://arxiv.org/pdf/2106.03714v1",
    "original_title": "Refiner: Refining Self-attention for Vision Transformers",
    "source": "arxiv",
    "authors": [
      "Daquan Zhou",
      "Yujun Shi",
      "Bingyi Kang",
      "Weihao Yu",
      "Zihang Jiang",
      "Yuan Li",
      "Xiaojie Jin",
      "Qibin Hou",
      "Jiashi Feng"
    ],
    "published": "2021-06-07T15:24:54+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2106.03714v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2106.03714v1_chunk_1",
    "chunk_text": "Different from existing works, we introduce a conceptually simple scheme, called refiner, to directly refine the self-attention maps of ViTs. Specifically, refiner explores attention expansion that projects the multi-head attention maps to a higher-dimensional space to promote their diversity. Further, refiner applies convolutions to augment local patterns of the attention maps, which we show is equivalent to a distributed local attention features are aggregated locally with learnable kernels and then globally aggregated with self-attention. Extensive experiments demonstrate that refiner works surprisingly well. Significantly, it enables ViTs to achieve 86% top-1 classification accuracy on ImageNet with only 81M parameters.",
    "original_url": "http://arxiv.org/pdf/2106.03714v1",
    "original_title": "Refiner: Refining Self-attention for Vision Transformers",
    "source": "arxiv",
    "authors": [
      "Daquan Zhou",
      "Yujun Shi",
      "Bingyi Kang",
      "Weihao Yu",
      "Zihang Jiang",
      "Yuan Li",
      "Xiaojie Jin",
      "Qibin Hou",
      "Jiashi Feng"
    ],
    "published": "2021-06-07T15:24:54+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2106.03714v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2106.03714v1_chunk_2",
    "chunk_text": "Significantly, it enables ViTs to achieve 86% top-1 classification accuracy on ImageNet with only 81M parameters.",
    "original_url": "http://arxiv.org/pdf/2106.03714v1",
    "original_title": "Refiner: Refining Self-attention for Vision Transformers",
    "source": "arxiv",
    "authors": [
      "Daquan Zhou",
      "Yujun Shi",
      "Bingyi Kang",
      "Weihao Yu",
      "Zihang Jiang",
      "Yuan Li",
      "Xiaojie Jin",
      "Qibin Hou",
      "Jiashi Feng"
    ],
    "published": "2021-06-07T15:24:54+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2106.03714v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2109.01078v1_chunk_0",
    "chunk_text": "Skim-Attention: Learning to Focus via Document Layout\n\nTransformer-based pre-training techniques of text and layout have proven effective in a number of document understanding tasks. Despite this success, multimodal pre-training models suffer from very high computational and memory costs. Motivated by human reading strategies, this paper presents Skim-Attention, a new attention mechanism that takes advantage of the structure of the document and its layout. Skim-Attention only attends to the 2-dimensional position of the words in a document. Our experiments show that Skim-Attention obtains a lower perplexity than prior works, while being more computationally efficient.",
    "original_url": "http://arxiv.org/pdf/2109.01078v1",
    "original_title": "Skim-Attention: Learning to Focus via Document Layout",
    "source": "arxiv",
    "authors": [
      "Laura Nguyen",
      "Thomas Scialom",
      "Jacopo Staiano",
      "Benjamin Piwowarski"
    ],
    "published": "2021-09-02T16:44:22+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2109.01078v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2109.01078v1_chunk_1",
    "chunk_text": "Our experiments show that Skim-Attention obtains a lower perplexity than prior works, while being more computationally efficient. Skim-Attention can be further combined with long-range Transformers to efficiently process long documents. We also show how Skim-Attention can be used off-the-shelf as a mask for any Pre-trained Language Model, allowing to improve their performance while restricting attention. Finally, we show the emergence of a document structure representation in Skim-Attention.",
    "original_url": "http://arxiv.org/pdf/2109.01078v1",
    "original_title": "Skim-Attention: Learning to Focus via Document Layout",
    "source": "arxiv",
    "authors": [
      "Laura Nguyen",
      "Thomas Scialom",
      "Jacopo Staiano",
      "Benjamin Piwowarski"
    ],
    "published": "2021-09-02T16:44:22+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2109.01078v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2401.08169v2_chunk_0",
    "chunk_text": "Statistical Test for Attention Map in Vision Transformer\n\nThe Vision Transformer (ViT) demonstrates exceptional performance in various computer vision tasks. Attention is crucial for ViT to capture complex wide-ranging relationships among image patches, allowing the model to weigh the importance of image patches and aiding our understanding of the decision-making process. However, when utilizing the attention of ViT as evidence in high-stakes decision-making tasks such as medical diagnostics, a challenge arises due to the potential of attention mechanisms erroneously focusing on irrelevant regions. In this study, we propose a statistical test for ViT's attentions, enabling us to use the attentions as reliable quantitative evidence indicators for ViT's decision-making with a rigorously controlled error rate. Using the framework called selective inference, we quantify the statistical significance of attentions in the form of p-values, which enables the theoretically grounded quantification of the false positive detection probability of attentions.",
    "original_url": "http://arxiv.org/pdf/2401.08169v2",
    "original_title": "Statistical Test for Attention Map in Vision Transformer",
    "source": "arxiv",
    "authors": [
      "Tomohiro Shiraishi",
      "Daiki Miwa",
      "Teruyuki Katsuoka",
      "Vo Nguyen Le Duy",
      "Kouichi Taji",
      "Ichiro Takeuchi"
    ],
    "published": "2024-01-16T07:18:47+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2401.08169v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2401.08169v2_chunk_1",
    "chunk_text": "Using the framework called selective inference, we quantify the statistical significance of attentions in the form of p-values, which enables the theoretically grounded quantification of the false positive detection probability of attentions. We demonstrate the validity and the effectiveness of the proposed method through numerical experiments and applications to brain image diagnoses.",
    "original_url": "http://arxiv.org/pdf/2401.08169v2",
    "original_title": "Statistical Test for Attention Map in Vision Transformer",
    "source": "arxiv",
    "authors": [
      "Tomohiro Shiraishi",
      "Daiki Miwa",
      "Teruyuki Katsuoka",
      "Vo Nguyen Le Duy",
      "Kouichi Taji",
      "Ichiro Takeuchi"
    ],
    "published": "2024-01-16T07:18:47+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2401.08169v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2304.13986v1_chunk_0",
    "chunk_text": "Optimization-Inspired Cross-Attention Transformer for Compressive Sensing\n\nBy integrating certain optimization solvers with deep neural networks, deep unfolding network (DUN) with good interpretability and high performance has attracted growing attention in compressive sensing (CS). However, existing DUNs often improve the visual quality at the price of a large number of parameters and have the problem of feature information loss during iteration. In this paper, we propose an Optimization-inspired Cross-attention Transformer (OCT) module as an iterative process, leading to a lightweight OCT-based Unfolding Framework (OCTUF) for image CS. Specifically, we design a novel Dual Cross Attention (Dual-CA) sub-module, which consists of an Inertia-Supplied Cross Attention (ISCA) block and a Projection-Guided Cross Attention (PGCA) block. ISCA block introduces multi-channel inertia forces and increases the memory effect by a cross attention mechanism between adjacent iterations.",
    "original_url": "http://arxiv.org/pdf/2304.13986v1",
    "original_title": "Optimization-Inspired Cross-Attention Transformer for Compressive Sensing",
    "source": "arxiv",
    "authors": [
      "Jiechong Song",
      "Chong Mou",
      "Shiqi Wang",
      "Siwei Ma",
      "Jian Zhang"
    ],
    "published": "2023-04-27T07:21:30+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2304.13986v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2304.13986v1_chunk_1",
    "chunk_text": "ISCA block introduces multi-channel inertia forces and increases the memory effect by a cross attention mechanism between adjacent iterations. And, PGCA block achieves an enhanced information interaction, which introduces the inertia force into the gradient descent step through a cross attention block. Extensive CS experiments manifest that our OCTUF achieves superior performance compared to state-of-the-art methods while training lower complexity. Codes are available at https://github.com/songjiechong/OCTUF.",
    "original_url": "http://arxiv.org/pdf/2304.13986v1",
    "original_title": "Optimization-Inspired Cross-Attention Transformer for Compressive Sensing",
    "source": "arxiv",
    "authors": [
      "Jiechong Song",
      "Chong Mou",
      "Shiqi Wang",
      "Siwei Ma",
      "Jian Zhang"
    ],
    "published": "2023-04-27T07:21:30+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2304.13986v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2503.11283v1_chunk_0",
    "chunk_text": "Brain Effective Connectivity Estimation via Fourier Spatiotemporal Attention\n\nEstimating brain effective connectivity (EC) from functional magnetic resonance imaging (fMRI) data can aid in comprehending the neural mechanisms underlying human behavior and cognition, providing a foundation for disease diagnosis. However, current spatiotemporal attention modules handle temporal and spatial attention separately, extracting temporal and spatial features either sequentially or in parallel. These approaches overlook the inherent spatiotemporal correlations present in real world fMRI data. Additionally, the presence of noise in fMRI data further limits the performance of existing methods. In this paper, we propose a novel brain effective connectivity estimation method based on Fourier spatiotemporal attention (FSTA-EC), which combines Fourier attention and spatiotemporal attention to simultaneously capture inter-series (spatial) dynamics and intra-series (temporal) dependencies from high-noise fMRI data.",
    "original_url": "http://arxiv.org/pdf/2503.11283v1",
    "original_title": "Brain Effective Connectivity Estimation via Fourier Spatiotemporal Attention",
    "source": "arxiv",
    "authors": [
      "Wen Xiong",
      "Jinduo Liu",
      "Junzhong Ji",
      "Fenglong Ma"
    ],
    "published": "2025-03-14T10:41:27+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2503.11283v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2503.11283v1_chunk_1",
    "chunk_text": "In this paper, we propose a novel brain effective connectivity estimation method based on Fourier spatiotemporal attention (FSTA-EC), which combines Fourier attention and spatiotemporal attention to simultaneously capture inter-series (spatial) dynamics and intra-series (temporal) dependencies from high-noise fMRI data. Specifically, Fourier attention is designed to convert the high-noise fMRI data to frequency domain, and map the denoised fMRI data back to physical domain, and spatiotemporal attention is crafted to simultaneously learn spatiotemporal dynamics. Furthermore, through a series of proofs, we demonstrate that incorporating learnable filter into fast Fourier transform and inverse fast Fourier transform processes is mathematically equivalent to performing cyclic convolution. The experimental results on simulated and real-resting-state fMRI datasets demonstrate that the proposed method exhibits superior performance when compared to state-of-the-art methods.",
    "original_url": "http://arxiv.org/pdf/2503.11283v1",
    "original_title": "Brain Effective Connectivity Estimation via Fourier Spatiotemporal Attention",
    "source": "arxiv",
    "authors": [
      "Wen Xiong",
      "Jinduo Liu",
      "Junzhong Ji",
      "Fenglong Ma"
    ],
    "published": "2025-03-14T10:41:27+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2503.11283v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2410.05565v1_chunk_0",
    "chunk_text": "Chain and Causal Attention for Efficient Entity Tracking\n\nThis paper investigates the limitations of transformers for entity-tracking tasks in large language models. We identify a theoretical constraint, showing that transformers require at least $\\log_2 (n+1)$ layers to handle entity tracking with $n$ state changes. To address this issue, we propose an efficient and frugal enhancement to the standard attention mechanism, enabling it to manage long-term dependencies more efficiently. By considering attention as an adjacency matrix, our model can track entity states with a single layer. Empirical results demonstrate significant improvements in entity tracking datasets while keeping competitive performance on standard natural language modeling.",
    "original_url": "http://arxiv.org/pdf/2410.05565v1",
    "original_title": "Chain and Causal Attention for Efficient Entity Tracking",
    "source": "arxiv",
    "authors": [
      "Erwan Fagnou",
      "Paul Caillon",
      "Blaise Delattre",
      "Alexandre Allauzen"
    ],
    "published": "2024-10-07T23:54:10+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2410.05565v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2410.05565v1_chunk_1",
    "chunk_text": "Empirical results demonstrate significant improvements in entity tracking datasets while keeping competitive performance on standard natural language modeling. Our modified attention allows us to achieve the same performance with drastically fewer layers. Additionally, our enhanced mechanism reveals structured internal representations of attention. Extensive experiments on both toy and complex datasets validate our approach. Our contributions include theoretical insights, an improved attention mechanism, and empirical validation.",
    "original_url": "http://arxiv.org/pdf/2410.05565v1",
    "original_title": "Chain and Causal Attention for Efficient Entity Tracking",
    "source": "arxiv",
    "authors": [
      "Erwan Fagnou",
      "Paul Caillon",
      "Blaise Delattre",
      "Alexandre Allauzen"
    ],
    "published": "2024-10-07T23:54:10+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2410.05565v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2410.05565v1_chunk_2",
    "chunk_text": "Our contributions include theoretical insights, an improved attention mechanism, and empirical validation.",
    "original_url": "http://arxiv.org/pdf/2410.05565v1",
    "original_title": "Chain and Causal Attention for Efficient Entity Tracking",
    "source": "arxiv",
    "authors": [
      "Erwan Fagnou",
      "Paul Caillon",
      "Blaise Delattre",
      "Alexandre Allauzen"
    ],
    "published": "2024-10-07T23:54:10+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2410.05565v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2312.12821v1_chunk_0",
    "chunk_text": "CST-former: Transformer with Channel-Spectro-Temporal Attention for Sound Event Localization and Detection\n\nSound event localization and detection (SELD) is a task for the classification of sound events and the localization of direction of arrival (DoA) utilizing multichannel acoustic signals. Prior studies employ spectral and channel information as the embedding for temporal attention. However, this usage limits the deep neural network from extracting meaningful features from the spectral or spatial domains. Therefore, our investigation in this paper presents a novel framework termed the Channel-Spectro-Temporal Transformer (CST-former) that bolsters SELD performance through the independent application of attention mechanisms to distinct domains. The CST-former architecture employs distinct attention mechanisms to independently process channel, spectral, and temporal information.",
    "original_url": "http://arxiv.org/pdf/2312.12821v1",
    "original_title": "CST-former: Transformer with Channel-Spectro-Temporal Attention for Sound Event Localization and Detection",
    "source": "arxiv",
    "authors": [
      "Yusun Shul",
      "Jung-Woo Choi"
    ],
    "published": "2023-12-20T07:49:43+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2312.12821v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2312.12821v1_chunk_1",
    "chunk_text": "The CST-former architecture employs distinct attention mechanisms to independently process channel, spectral, and temporal information. In addition, we propose an unfolded local embedding (ULE) technique for channel attention (CA) to generate informative embedding vectors including local spectral and temporal information. Empirical validation through experimentation on the 2022 and 2023 DCASE Challenge task3 datasets affirms the efficacy of employing attention mechanisms separated across each domain and the benefit of ULE, in enhancing SELD performance.",
    "original_url": "http://arxiv.org/pdf/2312.12821v1",
    "original_title": "CST-former: Transformer with Channel-Spectro-Temporal Attention for Sound Event Localization and Detection",
    "source": "arxiv",
    "authors": [
      "Yusun Shul",
      "Jung-Woo Choi"
    ],
    "published": "2023-12-20T07:49:43+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2312.12821v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2409.05136v2_chunk_0",
    "chunk_text": "MHS-STMA: Multimodal Hate Speech Detection via Scalable Transformer-Based Multilevel Attention Framework\n\nSocial media has a significant impact on people's lives. Hate speech on social media has emerged as one of society's most serious issues in recent years. Text and pictures are two forms of multimodal data that are distributed within articles. Unimodal analysis has been the primary emphasis of earlier approaches. Additionally, when doing multimodal analysis, researchers neglect to preserve the distinctive qualities associated with each modality.",
    "original_url": "http://arxiv.org/pdf/2409.05136v2",
    "original_title": "MHS-STMA: Multimodal Hate Speech Detection via Scalable Transformer-Based Multilevel Attention Framework",
    "source": "arxiv",
    "authors": [
      "Anusha Chhabra",
      "Dinesh Kumar Vishwakarma"
    ],
    "published": "2024-09-08T15:42:18+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2409.05136v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2409.05136v2_chunk_1",
    "chunk_text": "Additionally, when doing multimodal analysis, researchers neglect to preserve the distinctive qualities associated with each modality. To address these shortcomings, the present article suggests a scalable architecture for multimodal hate content detection called transformer-based multilevel attention (STMA). This architecture consists of three main parts: a combined attention-based deep learning mechanism, a vision attention-mechanism encoder, and a caption attention-mechanism encoder. To identify hate content, each component uses various attention processes and handles multimodal data in a unique way. Several studies employing multiple assessment criteria on three hate speech datasets such as Hateful memes, MultiOff, and MMHS150K, validate the suggested architecture's efficacy.",
    "original_url": "http://arxiv.org/pdf/2409.05136v2",
    "original_title": "MHS-STMA: Multimodal Hate Speech Detection via Scalable Transformer-Based Multilevel Attention Framework",
    "source": "arxiv",
    "authors": [
      "Anusha Chhabra",
      "Dinesh Kumar Vishwakarma"
    ],
    "published": "2024-09-08T15:42:18+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2409.05136v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2409.05136v2_chunk_2",
    "chunk_text": "Several studies employing multiple assessment criteria on three hate speech datasets such as Hateful memes, MultiOff, and MMHS150K, validate the suggested architecture's efficacy. The outcomes demonstrate that on all three datasets, the suggested strategy performs better than the baseline approaches.",
    "original_url": "http://arxiv.org/pdf/2409.05136v2",
    "original_title": "MHS-STMA: Multimodal Hate Speech Detection via Scalable Transformer-Based Multilevel Attention Framework",
    "source": "arxiv",
    "authors": [
      "Anusha Chhabra",
      "Dinesh Kumar Vishwakarma"
    ],
    "published": "2024-09-08T15:42:18+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2409.05136v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2203.12944v1_chunk_0",
    "chunk_text": "Transformers Meet Visual Learning Understanding: A Comprehensive Review\n\nDynamic attention mechanism and global modeling ability make Transformer show strong feature learning ability. In recent years, Transformer has become comparable to CNNs methods in computer vision. This review mainly investigates the current research progress of Transformer in image and video applications, which makes a comprehensive overview of Transformer in visual learning understanding. First, the attention mechanism is reviewed, which plays an essential part in Transformer. And then, the visual Transformer model and the principle of each module are introduced.",
    "original_url": "http://arxiv.org/pdf/2203.12944v1",
    "original_title": "Transformers Meet Visual Learning Understanding: A Comprehensive Review",
    "source": "arxiv",
    "authors": [
      "Yuting Yang",
      "Licheng Jiao",
      "Xu Liu",
      "Fang Liu",
      "Shuyuan Yang",
      "Zhixi Feng",
      "Xu Tang"
    ],
    "published": "2022-03-24T09:09:00+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2203.12944v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2203.12944v1_chunk_1",
    "chunk_text": "And then, the visual Transformer model and the principle of each module are introduced. Thirdly, the existing Transformer-based models are investigated, and their performance is compared in visual learning understanding applications. Three image tasks and two video tasks of computer vision are investigated. The former mainly includes image classification, object detection, and image segmentation. The latter contains object tracking and video classification.",
    "original_url": "http://arxiv.org/pdf/2203.12944v1",
    "original_title": "Transformers Meet Visual Learning Understanding: A Comprehensive Review",
    "source": "arxiv",
    "authors": [
      "Yuting Yang",
      "Licheng Jiao",
      "Xu Liu",
      "Fang Liu",
      "Shuyuan Yang",
      "Zhixi Feng",
      "Xu Tang"
    ],
    "published": "2022-03-24T09:09:00+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2203.12944v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2203.12944v1_chunk_2",
    "chunk_text": "The latter contains object tracking and video classification. It is significant for comparing different models' performance in various tasks on several public benchmark data sets. Finally, ten general problems are summarized, and the developing prospects of the visual Transformer are given in this review.",
    "original_url": "http://arxiv.org/pdf/2203.12944v1",
    "original_title": "Transformers Meet Visual Learning Understanding: A Comprehensive Review",
    "source": "arxiv",
    "authors": [
      "Yuting Yang",
      "Licheng Jiao",
      "Xu Liu",
      "Fang Liu",
      "Shuyuan Yang",
      "Zhixi Feng",
      "Xu Tang"
    ],
    "published": "2022-03-24T09:09:00+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2203.12944v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2307.07240v1_chunk_0",
    "chunk_text": "MaxSR: Image Super-Resolution Using Improved MaxViT\n\nWhile transformer models have been demonstrated to be effective for natural language processing tasks and high-level vision tasks, only a few attempts have been made to use powerful transformer models for single image super-resolution. Because transformer models have powerful representation capacity and the in-built self-attention mechanisms in transformer models help to leverage self-similarity prior in input low-resolution image to improve performance for single image super-resolution, we present a single image super-resolution model based on recent hybrid vision transformer of MaxViT, named as MaxSR. MaxSR consists of four parts, a shallow feature extraction block, multiple cascaded adaptive MaxViT blocks to extract deep hierarchical features and model global self-similarity from low-level features efficiently, a hierarchical feature fusion block, and finally a reconstruction block. The key component of MaxSR, i.e., adaptive MaxViT block, is based on MaxViT block which mixes MBConv with squeeze-and-excitation, block attention and grid attention. In order to achieve better global modelling of self-similarity in input low-resolution image, we improve block attention and grid attention in MaxViT block to adaptive block attention and adaptive grid attention which do self-attention inside each window across all grids and each grid across all windows respectively in the most efficient way.",
    "original_url": "http://arxiv.org/pdf/2307.07240v1",
    "original_title": "MaxSR: Image Super-Resolution Using Improved MaxViT",
    "source": "arxiv",
    "authors": [
      "Bincheng Yang",
      "Gangshan Wu"
    ],
    "published": "2023-07-14T09:26:47+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2307.07240v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2307.07240v1_chunk_1",
    "chunk_text": "In order to achieve better global modelling of self-similarity in input low-resolution image, we improve block attention and grid attention in MaxViT block to adaptive block attention and adaptive grid attention which do self-attention inside each window across all grids and each grid across all windows respectively in the most efficient way. We instantiate proposed model for classical single image super-resolution (MaxSR) and lightweight single image super-resolution (MaxSR-light). Experiments show that our MaxSR and MaxSR-light establish new state-of-the-art performance efficiently.",
    "original_url": "http://arxiv.org/pdf/2307.07240v1",
    "original_title": "MaxSR: Image Super-Resolution Using Improved MaxViT",
    "source": "arxiv",
    "authors": [
      "Bincheng Yang",
      "Gangshan Wu"
    ],
    "published": "2023-07-14T09:26:47+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2307.07240v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2410.04798v3_chunk_0",
    "chunk_text": "DAPE V2: Process Attention Score as Feature Map for Length Extrapolation\n\nThe attention mechanism is a fundamental component of the Transformer model, contributing to interactions among distinct tokens, in contrast to earlier feed-forward neural networks. In general, the attention scores are determined simply by the key-query products. However, this work's occasional trial (combining DAPE and NoPE) of including additional MLPs on attention scores without position encoding indicates that the classical key-query multiplication may limit the performance of Transformers. In this work, we conceptualize attention as a feature map and apply the convolution operator (for neighboring attention scores across different heads) to mimic the processing methods in computer vision. Specifically, the main contribution of this paper is identifying and interpreting the Transformer length extrapolation problem as a result of the limited expressiveness of the naive query and key dot product, and we successfully translate the length extrapolation issue into a well-understood feature map processing problem.",
    "original_url": "http://arxiv.org/pdf/2410.04798v3",
    "original_title": "DAPE V2: Process Attention Score as Feature Map for Length Extrapolation",
    "source": "arxiv",
    "authors": [
      "Chuanyang Zheng",
      "Yihang Gao",
      "Han Shi",
      "Jing Xiong",
      "Jiankai Sun",
      "Jingyao Li",
      "Minbin Huang",
      "Xiaozhe Ren",
      "Michael Ng",
      "Xin Jiang",
      "Zhenguo Li",
      "Yu Li"
    ],
    "published": "2024-10-07T07:21:49+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2410.04798v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2410.04798v3_chunk_1",
    "chunk_text": "Specifically, the main contribution of this paper is identifying and interpreting the Transformer length extrapolation problem as a result of the limited expressiveness of the naive query and key dot product, and we successfully translate the length extrapolation issue into a well-understood feature map processing problem. The novel insight, which can be adapted to various attention-related models, reveals that the current Transformer architecture has the potential for further evolution. Extensive experiments demonstrate that treating attention as a feature map and applying convolution as a processing method significantly enhances Transformer performance.",
    "original_url": "http://arxiv.org/pdf/2410.04798v3",
    "original_title": "DAPE V2: Process Attention Score as Feature Map for Length Extrapolation",
    "source": "arxiv",
    "authors": [
      "Chuanyang Zheng",
      "Yihang Gao",
      "Han Shi",
      "Jing Xiong",
      "Jiankai Sun",
      "Jingyao Li",
      "Minbin Huang",
      "Xiaozhe Ren",
      "Michael Ng",
      "Xin Jiang",
      "Zhenguo Li",
      "Yu Li"
    ],
    "published": "2024-10-07T07:21:49+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2410.04798v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2412.02285v1_chunk_0",
    "chunk_text": "GQWformer: A Quantum-based Transformer for Graph Representation Learning\n\nGraph Transformers (GTs) have demonstrated significant advantages in graph representation learning through their global attention mechanisms. However, the self-attention mechanism in GTs tends to neglect the inductive biases inherent in graph structures, making it chanllenging to effectively capture essential structural information. To address this issue, we propose a novel approach that integrate graph inductive bias into self-attention mechanisms by leveraging quantum technology for structural encoding. In this paper, we introduce the Graph Quantum Walk Transformer (GQWformer), a groundbreaking GNN framework that utilizes quantum walks on attributed graphs to generate node quantum states. These quantum states encapsulate rich structural attributes and serve as inductive biases for the transformer, thereby enabling the generation of more meaningful attention scores.",
    "original_url": "http://arxiv.org/pdf/2412.02285v1",
    "original_title": "GQWformer: A Quantum-based Transformer for Graph Representation Learning",
    "source": "arxiv",
    "authors": [
      "Lei Yu",
      "Hongyang Chen",
      "Jingsong Lv",
      "Linyao Yang"
    ],
    "published": "2024-12-03T09:03:04+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2412.02285v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2412.02285v1_chunk_1",
    "chunk_text": "These quantum states encapsulate rich structural attributes and serve as inductive biases for the transformer, thereby enabling the generation of more meaningful attention scores. By subsequently incorporating a recurrent neural network, our design amplifies the model's ability to focus on both local and global information. We conducted comprehensive experiments across five publicly available datasets to evaluate the effectiveness of our model. These results clearly indicate that GQWformer outperforms existing state-of-the-art graph classification algorithms. These findings highlight the significant potential of integrating quantum computing methodologies with traditional GNNs to advance the field of graph representation learning, providing a promising direction for future research and applications.",
    "original_url": "http://arxiv.org/pdf/2412.02285v1",
    "original_title": "GQWformer: A Quantum-based Transformer for Graph Representation Learning",
    "source": "arxiv",
    "authors": [
      "Lei Yu",
      "Hongyang Chen",
      "Jingsong Lv",
      "Linyao Yang"
    ],
    "published": "2024-12-03T09:03:04+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2412.02285v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2412.02285v1_chunk_2",
    "chunk_text": "These findings highlight the significant potential of integrating quantum computing methodologies with traditional GNNs to advance the field of graph representation learning, providing a promising direction for future research and applications.",
    "original_url": "http://arxiv.org/pdf/2412.02285v1",
    "original_title": "GQWformer: A Quantum-based Transformer for Graph Representation Learning",
    "source": "arxiv",
    "authors": [
      "Lei Yu",
      "Hongyang Chen",
      "Jingsong Lv",
      "Linyao Yang"
    ],
    "published": "2024-12-03T09:03:04+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2412.02285v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2101.04158v1_chunk_0",
    "chunk_text": "BERT-GT: Cross-sentence n-ary relation extraction with BERT and Graph Transformer\n\nA biomedical relation statement is commonly expressed in multiple sentences and consists of many concepts, including gene, disease, chemical, and mutation. To automatically extract information from biomedical literature, existing biomedical text-mining approaches typically formulate the problem as a cross-sentence n-ary relation-extraction task that detects relations among n entities across multiple sentences, and use either a graph neural network (GNN) with long short-term memory (LSTM) or an attention mechanism. Recently, Transformer has been shown to outperform LSTM on many natural language processing (NLP) tasks. In this work, we propose a novel architecture that combines Bidirectional Encoder Representations from Transformers with Graph Transformer (BERT-GT), through integrating a neighbor-attention mechanism into the BERT architecture. Unlike the original Transformer architecture, which utilizes the whole sentence(s) to calculate the attention of the current token, the neighbor-attention mechanism in our method calculates its attention utilizing only its neighbor tokens.",
    "original_url": "http://arxiv.org/pdf/2101.04158v1",
    "original_title": "BERT-GT: Cross-sentence n-ary relation extraction with BERT and Graph Transformer",
    "source": "arxiv",
    "authors": [
      "Po-Ting Lai",
      "Zhiyong Lu"
    ],
    "published": "2021-01-11T19:34:55+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2101.04158v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2101.04158v1_chunk_1",
    "chunk_text": "Unlike the original Transformer architecture, which utilizes the whole sentence(s) to calculate the attention of the current token, the neighbor-attention mechanism in our method calculates its attention utilizing only its neighbor tokens. Thus, each token can pay attention to its neighbor information with little noise. We show that this is critically important when the text is very long, as in cross-sentence or abstract-level relation-extraction tasks. Our benchmarking results show improvements of 5.44% and 3.89% in accuracy and F1-measure over the state-of-the-art on n-ary and chemical-protein relation datasets, suggesting BERT-GT is a robust approach that is applicable to other biomedical relation extraction tasks or datasets.",
    "original_url": "http://arxiv.org/pdf/2101.04158v1",
    "original_title": "BERT-GT: Cross-sentence n-ary relation extraction with BERT and Graph Transformer",
    "source": "arxiv",
    "authors": [
      "Po-Ting Lai",
      "Zhiyong Lu"
    ],
    "published": "2021-01-11T19:34:55+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2101.04158v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1912.00835v2_chunk_0",
    "chunk_text": "Low Rank Factorization for Compact Multi-Head Self-Attention\n\nEffective representation learning from text has been an active area of research in the fields of NLP and text mining. Attention mechanisms have been at the forefront in order to learn contextual sentence representations. Current state-of-the-art approaches for many NLP tasks use large pre-trained language models such as BERT, XLNet and so on for learning representations. These models are based on the Transformer architecture that involves recurrent blocks of computation consisting of multi-head self-attention and feedforward networks. One of the major bottlenecks largely contributing to the computational complexity of the Transformer models is the self-attention layer, that is both computationally expensive and parameter intensive.",
    "original_url": "http://arxiv.org/pdf/1912.00835v2",
    "original_title": "Low Rank Factorization for Compact Multi-Head Self-Attention",
    "source": "arxiv",
    "authors": [
      "Sneha Mehta",
      "Huzefa Rangwala",
      "Naren Ramakrishnan"
    ],
    "published": "2019-11-26T16:01:51+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1912.00835v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1912.00835v2_chunk_1",
    "chunk_text": "One of the major bottlenecks largely contributing to the computational complexity of the Transformer models is the self-attention layer, that is both computationally expensive and parameter intensive. In this work, we introduce a novel multi-head self-attention mechanism operating on GRUs that is shown to be computationally cheaper and more parameter efficient than self-attention mechanism proposed in Transformers for text classification tasks. The efficiency of our approach mainly stems from two optimizations; 1) we use low-rank matrix factorization of the affinity matrix to efficiently get multiple attention distributions instead of having separate parameters for each head 2) attention scores are obtained by querying a global context vector instead of densely querying all the words in the sentence. We evaluate the performance of the proposed model on tasks such as sentiment analysis from movie reviews, predicting business ratings from reviews and classifying news articles into topics. We find that the proposed approach matches or outperforms a series of strong baselines and is more parameter efficient than comparable multi-head approaches.",
    "original_url": "http://arxiv.org/pdf/1912.00835v2",
    "original_title": "Low Rank Factorization for Compact Multi-Head Self-Attention",
    "source": "arxiv",
    "authors": [
      "Sneha Mehta",
      "Huzefa Rangwala",
      "Naren Ramakrishnan"
    ],
    "published": "2019-11-26T16:01:51+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1912.00835v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1912.00835v2_chunk_2",
    "chunk_text": "We find that the proposed approach matches or outperforms a series of strong baselines and is more parameter efficient than comparable multi-head approaches. We also perform qualitative analyses to verify that the proposed approach is interpretable and captures context-dependent word importance.",
    "original_url": "http://arxiv.org/pdf/1912.00835v2",
    "original_title": "Low Rank Factorization for Compact Multi-Head Self-Attention",
    "source": "arxiv",
    "authors": [
      "Sneha Mehta",
      "Huzefa Rangwala",
      "Naren Ramakrishnan"
    ],
    "published": "2019-11-26T16:01:51+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1912.00835v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2406.06486v1_chunk_0",
    "chunk_text": "Continuum Attention for Neural Operators\n\nTransformers, and the attention mechanism in particular, have become ubiquitous in machine learning. Their success in modeling nonlocal, long-range correlations has led to their widespread adoption in natural language processing, computer vision, and time-series problems. Neural operators, which map spaces of functions into spaces of functions, are necessarily both nonlinear and nonlocal if they are universal; it is thus natural to ask whether the attention mechanism can be used in the design of neural operators. Motivated by this, we study transformers in the function space setting. We formulate attention as a map between infinite dimensional function spaces and prove that the attention mechanism as implemented in practice is a Monte Carlo or finite difference approximation of this operator.",
    "original_url": "http://arxiv.org/pdf/2406.06486v1",
    "original_title": "Continuum Attention for Neural Operators",
    "source": "arxiv",
    "authors": [
      "Edoardo Calvello",
      "Nikola B. Kovachki",
      "Matthew E. Levine",
      "Andrew M. Stuart"
    ],
    "published": "2024-06-10T17:25:46+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2406.06486v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2406.06486v1_chunk_1",
    "chunk_text": "We formulate attention as a map between infinite dimensional function spaces and prove that the attention mechanism as implemented in practice is a Monte Carlo or finite difference approximation of this operator. The function space formulation allows for the design of transformer neural operators, a class of architectures designed to learn mappings between function spaces, for which we prove a universal approximation result. The prohibitive cost of applying the attention operator to functions defined on multi-dimensional domains leads to the need for more efficient attention-based architectures. For this reason we also introduce a function space generalization of the patching strategy from computer vision, and introduce a class of associated neural operators. Numerical results, on an array of operator learning problems, demonstrate the promise of our approaches to function space formulations of attention and their use in neural operators.",
    "original_url": "http://arxiv.org/pdf/2406.06486v1",
    "original_title": "Continuum Attention for Neural Operators",
    "source": "arxiv",
    "authors": [
      "Edoardo Calvello",
      "Nikola B. Kovachki",
      "Matthew E. Levine",
      "Andrew M. Stuart"
    ],
    "published": "2024-06-10T17:25:46+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2406.06486v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2406.06486v1_chunk_2",
    "chunk_text": "Numerical results, on an array of operator learning problems, demonstrate the promise of our approaches to function space formulations of attention and their use in neural operators.",
    "original_url": "http://arxiv.org/pdf/2406.06486v1",
    "original_title": "Continuum Attention for Neural Operators",
    "source": "arxiv",
    "authors": [
      "Edoardo Calvello",
      "Nikola B. Kovachki",
      "Matthew E. Levine",
      "Andrew M. Stuart"
    ],
    "published": "2024-06-10T17:25:46+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2406.06486v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2302.11824v1_chunk_0",
    "chunk_text": "MossFormer: Pushing the Performance Limit of Monaural Speech Separation using Gated Single-Head Transformer with Convolution-Augmented Joint Self-Attentions\n\nTransformer based models have provided significant performance improvements in monaural speech separation. However, there is still a performance gap compared to a recent proposed upper bound. The major limitation of the current dual-path Transformer models is the inefficient modelling of long-range elemental interactions and local feature patterns. In this work, we achieve the upper bound by proposing a gated single-head transformer architecture with convolution-augmented joint self-attentions, named \\textit{MossFormer} (\\textit{Mo}naural \\textit{s}peech \\textit{s}eparation Trans\\textit{Former}). To effectively solve the indirect elemental interactions across chunks in the dual-path architecture, MossFormer employs a joint local and global self-attention architecture that simultaneously performs a full-computation self-attention on local chunks and a linearised low-cost self-attention over the full sequence.",
    "original_url": "http://arxiv.org/pdf/2302.11824v1",
    "original_title": "MossFormer: Pushing the Performance Limit of Monaural Speech Separation using Gated Single-Head Transformer with Convolution-Augmented Joint Self-Attentions",
    "source": "arxiv",
    "authors": [
      "Shengkui Zhao",
      "Bin Ma"
    ],
    "published": "2023-02-23T07:17:12+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2302.11824v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2302.11824v1_chunk_1",
    "chunk_text": "To effectively solve the indirect elemental interactions across chunks in the dual-path architecture, MossFormer employs a joint local and global self-attention architecture that simultaneously performs a full-computation self-attention on local chunks and a linearised low-cost self-attention over the full sequence. The joint attention enables MossFormer model full-sequence elemental interaction directly. In addition, we employ a powerful attentive gating mechanism with simplified single-head self-attentions. Besides the attentive long-range modelling, we also augment MossFormer with convolutions for the position-wise local pattern modelling. As a consequence, MossFormer significantly outperforms the previous models and achieves the state-of-the-art results on WSJ0-2/3mix and WHAM!/WHAMR!",
    "original_url": "http://arxiv.org/pdf/2302.11824v1",
    "original_title": "MossFormer: Pushing the Performance Limit of Monaural Speech Separation using Gated Single-Head Transformer with Convolution-Augmented Joint Self-Attentions",
    "source": "arxiv",
    "authors": [
      "Shengkui Zhao",
      "Bin Ma"
    ],
    "published": "2023-02-23T07:17:12+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2302.11824v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2302.11824v1_chunk_2",
    "chunk_text": "As a consequence, MossFormer significantly outperforms the previous models and achieves the state-of-the-art results on WSJ0-2/3mix and WHAM!/WHAMR! benchmarks. Our model achieves the SI-SDRi upper bound of 21.2 dB on WSJ0-3mix and only 0.3 dB below the upper bound of 23.1 dB on WSJ0-2mix.",
    "original_url": "http://arxiv.org/pdf/2302.11824v1",
    "original_title": "MossFormer: Pushing the Performance Limit of Monaural Speech Separation using Gated Single-Head Transformer with Convolution-Augmented Joint Self-Attentions",
    "source": "arxiv",
    "authors": [
      "Shengkui Zhao",
      "Bin Ma"
    ],
    "published": "2023-02-23T07:17:12+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2302.11824v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2105.14432v2_chunk_0",
    "chunk_text": "TransMatcher: Deep Image Matching Through Transformers for Generalizable Person Re-identification\n\nTransformers have recently gained increasing attention in computer vision. However, existing studies mostly use Transformers for feature representation learning, e.g. for image classification and dense predictions, and the generalizability of Transformers is unknown. In this work, we further investigate the possibility of applying Transformers for image matching and metric learning given pairs of images. We find that the Vision Transformer (ViT) and the vanilla Transformer with decoders are not adequate for image matching due to their lack of image-to-image attention.",
    "original_url": "http://arxiv.org/pdf/2105.14432v2",
    "original_title": "TransMatcher: Deep Image Matching Through Transformers for Generalizable Person Re-identification",
    "source": "arxiv",
    "authors": [
      "Shengcai Liao",
      "Ling Shao"
    ],
    "published": "2021-05-30T05:38:33+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2105.14432v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2105.14432v2_chunk_1",
    "chunk_text": "We find that the Vision Transformer (ViT) and the vanilla Transformer with decoders are not adequate for image matching due to their lack of image-to-image attention. Thus, we further design two naive solutions, i.e. query-gallery concatenation in ViT, and query-gallery cross-attention in the vanilla Transformer. The latter improves the performance, but it is still limited. This implies that the attention mechanism in Transformers is primarily designed for global feature aggregation, which is not naturally suitable for image matching.",
    "original_url": "http://arxiv.org/pdf/2105.14432v2",
    "original_title": "TransMatcher: Deep Image Matching Through Transformers for Generalizable Person Re-identification",
    "source": "arxiv",
    "authors": [
      "Shengcai Liao",
      "Ling Shao"
    ],
    "published": "2021-05-30T05:38:33+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2105.14432v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2105.14432v2_chunk_2",
    "chunk_text": "This implies that the attention mechanism in Transformers is primarily designed for global feature aggregation, which is not naturally suitable for image matching. Accordingly, we propose a new simplified decoder, which drops the full attention implementation with the softmax weighting, keeping only the query-key similarity computation. Additionally, global max pooling and a multilayer perceptron (MLP) head are applied to decode the matching result. This way, the simplified decoder is computationally more efficient, while at the same time more effective for image matching. The proposed method, called TransMatcher, achieves state-of-the-art performance in generalizable person re-identification, with up to 6.1% and 5.7% performance gains in Rank-1 and mAP, respectively, on several popular datasets.",
    "original_url": "http://arxiv.org/pdf/2105.14432v2",
    "original_title": "TransMatcher: Deep Image Matching Through Transformers for Generalizable Person Re-identification",
    "source": "arxiv",
    "authors": [
      "Shengcai Liao",
      "Ling Shao"
    ],
    "published": "2021-05-30T05:38:33+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2105.14432v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2105.14432v2_chunk_3",
    "chunk_text": "The proposed method, called TransMatcher, achieves state-of-the-art performance in generalizable person re-identification, with up to 6.1% and 5.7% performance gains in Rank-1 and mAP, respectively, on several popular datasets. Code is available at https://github.com/ShengcaiLiao/QAConv.",
    "original_url": "http://arxiv.org/pdf/2105.14432v2",
    "original_title": "TransMatcher: Deep Image Matching Through Transformers for Generalizable Person Re-identification",
    "source": "arxiv",
    "authors": [
      "Shengcai Liao",
      "Ling Shao"
    ],
    "published": "2021-05-30T05:38:33+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2105.14432v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2407.01602v1_chunk_0",
    "chunk_text": "Clustering in pure-attention hardmax transformers and its role in sentiment analysis\n\nTransformers are extremely successful machine learning models whose mathematical properties remain poorly understood. Here, we rigorously characterize the behavior of transformers with hardmax self-attention and normalization sublayers as the number of layers tends to infinity. By viewing such transformers as discrete-time dynamical systems describing the evolution of points in a Euclidean space, and thanks to a geometric interpretation of the self-attention mechanism based on hyperplane separation, we show that the transformer inputs asymptotically converge to a clustered equilibrium determined by special points called leaders. We then leverage this theoretical understanding to solve sentiment analysis problems from language processing using a fully interpretable transformer model, which effectively captures `context' by clustering meaningless words around leader words carrying the most meaning. Finally, we outline remaining challenges to bridge the gap between the mathematical analysis of transformers and their real-life implementation.",
    "original_url": "http://arxiv.org/pdf/2407.01602v1",
    "original_title": "Clustering in pure-attention hardmax transformers and its role in sentiment analysis",
    "source": "arxiv",
    "authors": [
      "Albert Alcalde",
      "Giovanni Fantuzzi",
      "Enrique Zuazua"
    ],
    "published": "2024-06-26T16:13:35+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2407.01602v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2407.01602v1_chunk_1",
    "chunk_text": "Finally, we outline remaining challenges to bridge the gap between the mathematical analysis of transformers and their real-life implementation.",
    "original_url": "http://arxiv.org/pdf/2407.01602v1",
    "original_title": "Clustering in pure-attention hardmax transformers and its role in sentiment analysis",
    "source": "arxiv",
    "authors": [
      "Albert Alcalde",
      "Giovanni Fantuzzi",
      "Enrique Zuazua"
    ],
    "published": "2024-06-26T16:13:35+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2407.01602v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2204.08175v1_chunk_0",
    "chunk_text": "Usage of specific attention improves change point detection\n\nThe change point is a moment of an abrupt alteration in the data distribution. Current methods for change point detection are based on recurrent neural methods suitable for sequential data. However, recent works show that transformers based on attention mechanisms perform better than standard recurrent models for many tasks. The most benefit is noticeable in the case of longer sequences. In this paper, we investigate different attentions for the change point detection task and proposed specific form of attention related to the task at hand.",
    "original_url": "http://arxiv.org/pdf/2204.08175v1",
    "original_title": "Usage of specific attention improves change point detection",
    "source": "arxiv",
    "authors": [
      "Anna Dmitrienko",
      "Evgenia Romanenkova",
      "Alexey Zaytsev"
    ],
    "published": "2022-04-18T06:05:50+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2204.08175v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2204.08175v1_chunk_1",
    "chunk_text": "In this paper, we investigate different attentions for the change point detection task and proposed specific form of attention related to the task at hand. We show that using a special form of attention outperforms state-of-the-art results.",
    "original_url": "http://arxiv.org/pdf/2204.08175v1",
    "original_title": "Usage of specific attention improves change point detection",
    "source": "arxiv",
    "authors": [
      "Anna Dmitrienko",
      "Evgenia Romanenkova",
      "Alexey Zaytsev"
    ],
    "published": "2022-04-18T06:05:50+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2204.08175v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2102.10662v2_chunk_0",
    "chunk_text": "Medical Transformer: Gated Axial-Attention for Medical Image Segmentation\n\nOver the past decade, Deep Convolutional Neural Networks have been widely adopted for medical image segmentation and shown to achieve adequate performance. However, due to the inherent inductive biases present in the convolutional architectures, they lack understanding of long-range dependencies in the image. Recently proposed Transformer-based architectures that leverage self-attention mechanism encode long-range dependencies and learn representations that are highly expressive. This motivates us to explore Transformer-based solutions and study the feasibility of using Transformer-based network architectures for medical image segmentation tasks. Majority of existing Transformer-based network architectures proposed for vision applications require large-scale datasets to train properly.",
    "original_url": "http://arxiv.org/pdf/2102.10662v2",
    "original_title": "Medical Transformer: Gated Axial-Attention for Medical Image Segmentation",
    "source": "arxiv",
    "authors": [
      "Jeya Maria Jose Valanarasu",
      "Poojan Oza",
      "Ilker Hacihaliloglu",
      "Vishal M. Patel"
    ],
    "published": "2021-02-21T18:35:14+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2102.10662v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2102.10662v2_chunk_1",
    "chunk_text": "Majority of existing Transformer-based network architectures proposed for vision applications require large-scale datasets to train properly. However, compared to the datasets for vision applications, for medical imaging the number of data samples is relatively low, making it difficult to efficiently train transformers for medical applications. To this end, we propose a Gated Axial-Attention model which extends the existing architectures by introducing an additional control mechanism in the self-attention module. Furthermore, to train the model effectively on medical images, we propose a Local-Global training strategy (LoGo) which further improves the performance. Specifically, we operate on the whole image and patches to learn global and local features, respectively.",
    "original_url": "http://arxiv.org/pdf/2102.10662v2",
    "original_title": "Medical Transformer: Gated Axial-Attention for Medical Image Segmentation",
    "source": "arxiv",
    "authors": [
      "Jeya Maria Jose Valanarasu",
      "Poojan Oza",
      "Ilker Hacihaliloglu",
      "Vishal M. Patel"
    ],
    "published": "2021-02-21T18:35:14+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2102.10662v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2102.10662v2_chunk_2",
    "chunk_text": "Specifically, we operate on the whole image and patches to learn global and local features, respectively. The proposed Medical Transformer (MedT) is evaluated on three different medical image segmentation datasets and it is shown that it achieves better performance than the convolutional and other related transformer-based architectures. Code: https://github.com/jeya-maria-jose/Medical-Transformer",
    "original_url": "http://arxiv.org/pdf/2102.10662v2",
    "original_title": "Medical Transformer: Gated Axial-Attention for Medical Image Segmentation",
    "source": "arxiv",
    "authors": [
      "Jeya Maria Jose Valanarasu",
      "Poojan Oza",
      "Ilker Hacihaliloglu",
      "Vishal M. Patel"
    ],
    "published": "2021-02-21T18:35:14+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2102.10662v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2107.00641v1_chunk_0",
    "chunk_text": "Focal Self-attention for Local-Global Interactions in Vision Transformers\n\nRecently, Vision Transformer and its variants have shown great promise on various computer vision tasks. The ability of capturing short- and long-range visual dependencies through self-attention is arguably the main source for the success. But it also brings challenges due to quadratic computational overhead, especially for the high-resolution vision tasks (e.g., object detection). In this paper, we present focal self-attention, a new mechanism that incorporates both fine-grained local and coarse-grained global interactions. Using this new mechanism, each token attends the closest surrounding tokens at fine granularity but the tokens far away at coarse granularity, and thus can capture both short- and long-range visual dependencies efficiently and effectively.",
    "original_url": "http://arxiv.org/pdf/2107.00641v1",
    "original_title": "Focal Self-attention for Local-Global Interactions in Vision Transformers",
    "source": "arxiv",
    "authors": [
      "Jianwei Yang",
      "Chunyuan Li",
      "Pengchuan Zhang",
      "Xiyang Dai",
      "Bin Xiao",
      "Lu Yuan",
      "Jianfeng Gao"
    ],
    "published": "2021-07-01T17:56:09+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2107.00641v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2107.00641v1_chunk_1",
    "chunk_text": "Using this new mechanism, each token attends the closest surrounding tokens at fine granularity but the tokens far away at coarse granularity, and thus can capture both short- and long-range visual dependencies efficiently and effectively. With focal self-attention, we propose a new variant of Vision Transformer models, called Focal Transformer, which achieves superior performance over the state-of-the-art vision Transformers on a range of public image classification and object detection benchmarks. In particular, our Focal Transformer models with a moderate size of 51.1M and a larger size of 89.8M achieve 83.5 and 83.8 Top-1 accuracy, respectively, on ImageNet classification at 224x224 resolution. Using Focal Transformers as the backbones, we obtain consistent and substantial improvements over the current state-of-the-art Swin Transformers for 6 different object detection methods trained with standard 1x and 3x schedules. Our largest Focal Transformer yields 58.7/58.9 box mAPs and 50.9/51.3 mask mAPs on COCO mini-val/test-dev, and 55.4 mIoU on ADE20K for semantic segmentation, creating new SoTA on three of the most challenging computer vision tasks.",
    "original_url": "http://arxiv.org/pdf/2107.00641v1",
    "original_title": "Focal Self-attention for Local-Global Interactions in Vision Transformers",
    "source": "arxiv",
    "authors": [
      "Jianwei Yang",
      "Chunyuan Li",
      "Pengchuan Zhang",
      "Xiyang Dai",
      "Bin Xiao",
      "Lu Yuan",
      "Jianfeng Gao"
    ],
    "published": "2021-07-01T17:56:09+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2107.00641v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2107.00641v1_chunk_2",
    "chunk_text": "Our largest Focal Transformer yields 58.7/58.9 box mAPs and 50.9/51.3 mask mAPs on COCO mini-val/test-dev, and 55.4 mIoU on ADE20K for semantic segmentation, creating new SoTA on three of the most challenging computer vision tasks.",
    "original_url": "http://arxiv.org/pdf/2107.00641v1",
    "original_title": "Focal Self-attention for Local-Global Interactions in Vision Transformers",
    "source": "arxiv",
    "authors": [
      "Jianwei Yang",
      "Chunyuan Li",
      "Pengchuan Zhang",
      "Xiyang Dai",
      "Bin Xiao",
      "Lu Yuan",
      "Jianfeng Gao"
    ],
    "published": "2021-07-01T17:56:09+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2107.00641v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2212.00973v1_chunk_0",
    "chunk_text": "A Domain-Knowledge-Inspired Music Embedding Space and a Novel Attention Mechanism for Symbolic Music Modeling\n\nFollowing the success of the transformer architecture in the natural language domain, transformer-like architectures have been widely applied to the domain of symbolic music recently. Symbolic music and text, however, are two different modalities. Symbolic music contains multiple attributes, both absolute attributes (e.g., pitch) and relative attributes (e.g., pitch interval). These relative attributes shape human perception of musical motifs. These important relative attributes, however, are mostly ignored in existing symbolic music modeling methods with the main reason being the lack of a musically-meaningful embedding space where both the absolute and relative embeddings of the symbolic music tokens can be efficiently represented.",
    "original_url": "http://arxiv.org/pdf/2212.00973v1",
    "original_title": "A Domain-Knowledge-Inspired Music Embedding Space and a Novel Attention Mechanism for Symbolic Music Modeling",
    "source": "arxiv",
    "authors": [
      "Z. Guo",
      "J. Kang",
      "D. Herremans"
    ],
    "published": "2022-12-02T05:04:31+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2212.00973v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2212.00973v1_chunk_1",
    "chunk_text": "These important relative attributes, however, are mostly ignored in existing symbolic music modeling methods with the main reason being the lack of a musically-meaningful embedding space where both the absolute and relative embeddings of the symbolic music tokens can be efficiently represented. In this paper, we propose the Fundamental Music Embedding (FME) for symbolic music based on a bias-adjusted sinusoidal encoding within which both the absolute and the relative attributes can be embedded and the fundamental musical properties (e.g., translational invariance) are explicitly preserved. Taking advantage of the proposed FME, we further propose a novel attention mechanism based on the relative index, pitch and onset embeddings (RIPO attention) such that the musical domain knowledge can be fully utilized for symbolic music modeling. Experiment results show that our proposed model: RIPO transformer which utilizes FME and RIPO attention outperforms the state-of-the-art transformers (i.e., music transformer, linear transformer) in a melody completion task. Moreover, using the RIPO transformer in a downstream music generation task, we notice that the notorious degeneration phenomenon no longer exists and the music generated by the RIPO transformer outperforms the music generated by state-of-the-art transformer models in both subjective and objective evaluations.",
    "original_url": "http://arxiv.org/pdf/2212.00973v1",
    "original_title": "A Domain-Knowledge-Inspired Music Embedding Space and a Novel Attention Mechanism for Symbolic Music Modeling",
    "source": "arxiv",
    "authors": [
      "Z. Guo",
      "J. Kang",
      "D. Herremans"
    ],
    "published": "2022-12-02T05:04:31+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2212.00973v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2212.00973v1_chunk_2",
    "chunk_text": "Moreover, using the RIPO transformer in a downstream music generation task, we notice that the notorious degeneration phenomenon no longer exists and the music generated by the RIPO transformer outperforms the music generated by state-of-the-art transformer models in both subjective and objective evaluations.",
    "original_url": "http://arxiv.org/pdf/2212.00973v1",
    "original_title": "A Domain-Knowledge-Inspired Music Embedding Space and a Novel Attention Mechanism for Symbolic Music Modeling",
    "source": "arxiv",
    "authors": [
      "Z. Guo",
      "J. Kang",
      "D. Herremans"
    ],
    "published": "2022-12-02T05:04:31+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2212.00973v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2407.12322v3_chunk_0",
    "chunk_text": "Frequency Guidance Matters: Skeletal Action Recognition by Frequency-Aware Mixed Transformer\n\nRecently, transformers have demonstrated great potential for modeling long-term dependencies from skeleton sequences and thereby gained ever-increasing attention in skeleton action recognition. However, the existing transformer-based approaches heavily rely on the naive attention mechanism for capturing the spatiotemporal features, which falls short in learning discriminative representations that exhibit similar motion patterns. To address this challenge, we introduce the Frequency-aware Mixed Transformer (FreqMixFormer), specifically designed for recognizing similar skeletal actions with subtle discriminative motions. First, we introduce a frequency-aware attention module to unweave skeleton frequency representations by embedding joint features into frequency attention maps, aiming to distinguish the discriminative movements based on their frequency coefficients. Subsequently, we develop a mixed transformer architecture to incorporate spatial features with frequency features to model the comprehensive frequency-spatial patterns.",
    "original_url": "http://arxiv.org/pdf/2407.12322v3",
    "original_title": "Frequency Guidance Matters: Skeletal Action Recognition by Frequency-Aware Mixed Transformer",
    "source": "arxiv",
    "authors": [
      "Wenhan Wu",
      "Ce Zheng",
      "Zihao Yang",
      "Chen Chen",
      "Srijan Das",
      "Aidong Lu"
    ],
    "published": "2024-07-17T05:47:27+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2407.12322v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2407.12322v3_chunk_1",
    "chunk_text": "Subsequently, we develop a mixed transformer architecture to incorporate spatial features with frequency features to model the comprehensive frequency-spatial patterns. Additionally, a temporal transformer is proposed to extract the global correlations across frames. Extensive experiments show that FreqMiXFormer outperforms SOTA on 3 popular skeleton action recognition datasets, including NTU RGB+D, NTU RGB+D 120, and NW-UCLA datasets.",
    "original_url": "http://arxiv.org/pdf/2407.12322v3",
    "original_title": "Frequency Guidance Matters: Skeletal Action Recognition by Frequency-Aware Mixed Transformer",
    "source": "arxiv",
    "authors": [
      "Wenhan Wu",
      "Ce Zheng",
      "Zihao Yang",
      "Chen Chen",
      "Srijan Das",
      "Aidong Lu"
    ],
    "published": "2024-07-17T05:47:27+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2407.12322v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2112.03254v3_chunk_0",
    "chunk_text": "Human Parity on CommonsenseQA: Augmenting Self-Attention with External Attention\n\nMost of today's AI systems focus on using self-attention mechanisms and transformer architectures on large amounts of diverse data to achieve impressive performance gains. In this paper, we propose to augment the transformer architecture with an external attention mechanism to bring external knowledge and context to bear. By integrating external information into the prediction process, we hope to reduce the need for ever-larger models and increase the democratization of AI systems. We find that the proposed external attention mechanism can significantly improve the performance of existing AI systems, allowing practitioners to easily customize foundation AI models to many diverse downstream applications. In particular, we focus on the task of Commonsense Reasoning, demonstrating that the proposed external attention mechanism can augment existing transformer models and significantly improve the model's reasoning capabilities.",
    "original_url": "http://arxiv.org/pdf/2112.03254v3",
    "original_title": "Human Parity on CommonsenseQA: Augmenting Self-Attention with External Attention",
    "source": "arxiv",
    "authors": [
      "Yichong Xu",
      "Chenguang Zhu",
      "Shuohang Wang",
      "Siqi Sun",
      "Hao Cheng",
      "Xiaodong Liu",
      "Jianfeng Gao",
      "Pengcheng He",
      "Michael Zeng",
      "Xuedong Huang"
    ],
    "published": "2021-12-06T18:59:02+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2112.03254v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2112.03254v3_chunk_1",
    "chunk_text": "In particular, we focus on the task of Commonsense Reasoning, demonstrating that the proposed external attention mechanism can augment existing transformer models and significantly improve the model's reasoning capabilities. The proposed system, Knowledgeable External Attention for commonsense Reasoning (KEAR), reaches human parity on the open CommonsenseQA research benchmark with an accuracy of 89.4\\% in comparison to the human accuracy of 88.9\\%.",
    "original_url": "http://arxiv.org/pdf/2112.03254v3",
    "original_title": "Human Parity on CommonsenseQA: Augmenting Self-Attention with External Attention",
    "source": "arxiv",
    "authors": [
      "Yichong Xu",
      "Chenguang Zhu",
      "Shuohang Wang",
      "Siqi Sun",
      "Hao Cheng",
      "Xiaodong Liu",
      "Jianfeng Gao",
      "Pengcheng He",
      "Michael Zeng",
      "Xuedong Huang"
    ],
    "published": "2021-12-06T18:59:02+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2112.03254v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2306.11307v3_chunk_0",
    "chunk_text": "Transforming Graphs for Enhanced Attribute Clustering: An Innovative Graph Transformer-Based Method\n\nGraph Representation Learning (GRL) is an influential methodology, enabling a more profound understanding of graph-structured data and aiding graph clustering, a critical task across various domains. The recent incursion of attention mechanisms, originally an artifact of Natural Language Processing (NLP), into the realm of graph learning has spearheaded a notable shift in research trends. Consequently, Graph Attention Networks (GATs) and Graph Attention Auto-Encoders have emerged as preferred tools for graph clustering tasks. Yet, these methods primarily employ a local attention mechanism, thereby curbing their capacity to apprehend the intricate global dependencies between nodes within graphs. Addressing these impediments, this study introduces an innovative method known as the Graph Transformer Auto-Encoder for Graph Clustering (GTAGC).",
    "original_url": "http://arxiv.org/pdf/2306.11307v3",
    "original_title": "Transforming Graphs for Enhanced Attribute Clustering: An Innovative Graph Transformer-Based Method",
    "source": "arxiv",
    "authors": [
      "Shuo Han",
      "Jiacheng Liu",
      "Jiayun Wu",
      "Yinan Chen",
      "Li Tao"
    ],
    "published": "2023-06-20T06:04:03+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2306.11307v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2306.11307v3_chunk_1",
    "chunk_text": "Addressing these impediments, this study introduces an innovative method known as the Graph Transformer Auto-Encoder for Graph Clustering (GTAGC). By melding the Graph Auto-Encoder with the Graph Transformer, GTAGC is adept at capturing global dependencies between nodes. This integration amplifies the graph representation and surmounts the constraints posed by the local attention mechanism. The architecture of GTAGC encompasses graph embedding, integration of the Graph Transformer within the autoencoder structure, and a clustering component. It strategically alternates between graph embedding and clustering, thereby tailoring the Graph Transformer for clustering tasks, whilst preserving the graph's global structural information.",
    "original_url": "http://arxiv.org/pdf/2306.11307v3",
    "original_title": "Transforming Graphs for Enhanced Attribute Clustering: An Innovative Graph Transformer-Based Method",
    "source": "arxiv",
    "authors": [
      "Shuo Han",
      "Jiacheng Liu",
      "Jiayun Wu",
      "Yinan Chen",
      "Li Tao"
    ],
    "published": "2023-06-20T06:04:03+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2306.11307v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2306.11307v3_chunk_2",
    "chunk_text": "It strategically alternates between graph embedding and clustering, thereby tailoring the Graph Transformer for clustering tasks, whilst preserving the graph's global structural information. Through extensive experimentation on diverse benchmark datasets, GTAGC has exhibited superior performance against existing state-of-the-art graph clustering methodologies.",
    "original_url": "http://arxiv.org/pdf/2306.11307v3",
    "original_title": "Transforming Graphs for Enhanced Attribute Clustering: An Innovative Graph Transformer-Based Method",
    "source": "arxiv",
    "authors": [
      "Shuo Han",
      "Jiacheng Liu",
      "Jiayun Wu",
      "Yinan Chen",
      "Li Tao"
    ],
    "published": "2023-06-20T06:04:03+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2306.11307v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2110.00860v1_chunk_0",
    "chunk_text": "Implicit and Explicit Attention for Zero-Shot Learning\n\nMost of the existing Zero-Shot Learning (ZSL) methods focus on learning a compatibility function between the image representation and class attributes. Few others concentrate on learning image representation combining local and global features. However, the existing approaches still fail to address the bias issue towards the seen classes. In this paper, we propose implicit and explicit attention mechanisms to address the existing bias problem in ZSL models. We formulate the implicit attention mechanism with a self-supervised image angle rotation task, which focuses on specific image features aiding to solve the task.",
    "original_url": "http://arxiv.org/pdf/2110.00860v1",
    "original_title": "Implicit and Explicit Attention for Zero-Shot Learning",
    "source": "arxiv",
    "authors": [
      "Faisal Alamri",
      "Anjan Dutta"
    ],
    "published": "2021-10-02T18:06:21+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2110.00860v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2110.00860v1_chunk_1",
    "chunk_text": "We formulate the implicit attention mechanism with a self-supervised image angle rotation task, which focuses on specific image features aiding to solve the task. The explicit attention mechanism is composed with the consideration of a multi-headed self-attention mechanism via Vision Transformer model, which learns to map image features to semantic space during the training stage. We conduct comprehensive experiments on three popular benchmarks: AWA2, CUB and SUN. The performance of our proposed attention mechanisms has proved its effectiveness, and has achieved the state-of-the-art harmonic mean on all the three datasets.",
    "original_url": "http://arxiv.org/pdf/2110.00860v1",
    "original_title": "Implicit and Explicit Attention for Zero-Shot Learning",
    "source": "arxiv",
    "authors": [
      "Faisal Alamri",
      "Anjan Dutta"
    ],
    "published": "2021-10-02T18:06:21+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2110.00860v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2306.04927v1_chunk_0",
    "chunk_text": "An Efficient Transformer for Simultaneous Learning of BEV and Lane Representations in 3D Lane Detection\n\nAccurately detecting lane lines in 3D space is crucial for autonomous driving. Existing methods usually first transform image-view features into bird-eye-view (BEV) by aid of inverse perspective mapping (IPM), and then detect lane lines based on the BEV features. However, IPM ignores the changes in road height, leading to inaccurate view transformations. Additionally, the two separate stages of the process can cause cumulative errors and increased complexity. To address these limitations, we propose an efficient transformer for 3D lane detection.",
    "original_url": "http://arxiv.org/pdf/2306.04927v1",
    "original_title": "An Efficient Transformer for Simultaneous Learning of BEV and Lane Representations in 3D Lane Detection",
    "source": "arxiv",
    "authors": [
      "Ziye Chen",
      "Kate Smith-Miles",
      "Bo Du",
      "Guoqi Qian",
      "Mingming Gong"
    ],
    "published": "2023-06-08T04:18:31+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2306.04927v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2306.04927v1_chunk_1",
    "chunk_text": "To address these limitations, we propose an efficient transformer for 3D lane detection. Different from the vanilla transformer, our model contains a decomposed cross-attention mechanism to simultaneously learn lane and BEV representations. The mechanism decomposes the cross-attention between image-view and BEV features into the one between image-view and lane features, and the one between lane and BEV features, both of which are supervised with ground-truth lane lines. Our method obtains 2D and 3D lane predictions by applying the lane features to the image-view and BEV features, respectively. This allows for a more accurate view transformation than IPM-based methods, as the view transformation is learned from data with a supervised cross-attention.",
    "original_url": "http://arxiv.org/pdf/2306.04927v1",
    "original_title": "An Efficient Transformer for Simultaneous Learning of BEV and Lane Representations in 3D Lane Detection",
    "source": "arxiv",
    "authors": [
      "Ziye Chen",
      "Kate Smith-Miles",
      "Bo Du",
      "Guoqi Qian",
      "Mingming Gong"
    ],
    "published": "2023-06-08T04:18:31+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2306.04927v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2306.04927v1_chunk_2",
    "chunk_text": "This allows for a more accurate view transformation than IPM-based methods, as the view transformation is learned from data with a supervised cross-attention. Additionally, the cross-attention between lane and BEV features enables them to adjust to each other, resulting in more accurate lane detection than the two separate stages. Finally, the decomposed cross-attention is more efficient than the original one. Experimental results on OpenLane and ONCE-3DLanes demonstrate the state-of-the-art performance of our method.",
    "original_url": "http://arxiv.org/pdf/2306.04927v1",
    "original_title": "An Efficient Transformer for Simultaneous Learning of BEV and Lane Representations in 3D Lane Detection",
    "source": "arxiv",
    "authors": [
      "Ziye Chen",
      "Kate Smith-Miles",
      "Bo Du",
      "Guoqi Qian",
      "Mingming Gong"
    ],
    "published": "2023-06-08T04:18:31+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2306.04927v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2502.01659v2_chunk_0",
    "chunk_text": "Longer Attention Span: Increasing Transformer Context Length with Sparse Graph Processing Techniques\n\nTransformers have demonstrated great success in numerous domains including natural language processing and bioinformatics. This success stems from the use of the attention mechanism by these models in order to represent and propagate pairwise interactions between individual tokens of sequential data. However, the primary limitation of this operation is its quadratic memory and time complexity in relation to the input's context length - the length of a sequence over which the interactions need to be captured. This significantly limits the length of sequences that can be inferred upon by these models. Extensive research has been conducted to reduce the number of pairwise interactions to sub-quadratic in relation to the context length by introducing sparsity into the attention mechanism through the development of sparse attention masks.",
    "original_url": "http://arxiv.org/pdf/2502.01659v2",
    "original_title": "Longer Attention Span: Increasing Transformer Context Length with Sparse Graph Processing Techniques",
    "source": "arxiv",
    "authors": [
      "Nathaniel Tomczak",
      "Sanmukh Kuppannagari"
    ],
    "published": "2025-01-31T22:05:00+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2502.01659v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2502.01659v2_chunk_1",
    "chunk_text": "Extensive research has been conducted to reduce the number of pairwise interactions to sub-quadratic in relation to the context length by introducing sparsity into the attention mechanism through the development of sparse attention masks. However, efficient implementations that achieve \"true sparsity\" are lacking. In this work, we address this issue by proposing a graph computing view of attention where tokens are perceived as nodes of the graph and the attention mask determines the edges of the graph. Using this view, we develop graph processing algorithms to implement the attention mechanism. Both theoretically and empirically, we demonstrate that our algorithms only perform the needed computations, i.e., they are work optimal.",
    "original_url": "http://arxiv.org/pdf/2502.01659v2",
    "original_title": "Longer Attention Span: Increasing Transformer Context Length with Sparse Graph Processing Techniques",
    "source": "arxiv",
    "authors": [
      "Nathaniel Tomczak",
      "Sanmukh Kuppannagari"
    ],
    "published": "2025-01-31T22:05:00+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2502.01659v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2502.01659v2_chunk_2",
    "chunk_text": "Both theoretically and empirically, we demonstrate that our algorithms only perform the needed computations, i.e., they are work optimal. We also perform extensive experimentation using popular attention masks to explore the impact of sparsity on execution time and achievable context length. Our experiments demonstrate significant speedups in execution times compared to state-of-the-art attention implementations such as FlashAttention for large sequence lengths. We also demonstrate that our algorithms are able to achieve extremely long sequence lengths of as high as 160 million on a single NVIDIA A100 GPU (SXM4 80GB).",
    "original_url": "http://arxiv.org/pdf/2502.01659v2",
    "original_title": "Longer Attention Span: Increasing Transformer Context Length with Sparse Graph Processing Techniques",
    "source": "arxiv",
    "authors": [
      "Nathaniel Tomczak",
      "Sanmukh Kuppannagari"
    ],
    "published": "2025-01-31T22:05:00+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2502.01659v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2411.07635v5_chunk_0",
    "chunk_text": "Breaking the Low-Rank Dilemma of Linear Attention\n\nThe Softmax attention mechanism in Transformer models is notoriously computationally expensive, particularly due to its quadratic complexity, posing significant challenges in vision applications. In contrast, linear attention provides a far more efficient solution by reducing the complexity to linear levels. However, compared to Softmax attention, linear attention often experiences significant performance degradation. Our experiments indicate that this performance drop is due to the low-rank nature of linear attention's feature map, which hinders its ability to adequately model complex spatial information. In this paper, to break the low-rank dilemma of linear attention, we conduct rank analysis from two perspectives: the KV buffer and the output features.",
    "original_url": "http://arxiv.org/pdf/2411.07635v5",
    "original_title": "Breaking the Low-Rank Dilemma of Linear Attention",
    "source": "arxiv",
    "authors": [
      "Qihang Fan",
      "Huaibo Huang",
      "Ran He"
    ],
    "published": "2024-11-12T08:30:59+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2411.07635v5"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2411.07635v5_chunk_1",
    "chunk_text": "In this paper, to break the low-rank dilemma of linear attention, we conduct rank analysis from two perspectives: the KV buffer and the output features. Consequently, we introduce Rank-Augmented Linear Attention (RALA), which rivals the performance of Softmax attention while maintaining linear complexity and high efficiency. Based on RALA, we construct the Rank-Augmented Vision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT achieves excellent performance across various vision tasks. Specifically, without using any additional labels, data, or supervision during training, RAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters and 4.6G FLOPs.",
    "original_url": "http://arxiv.org/pdf/2411.07635v5",
    "original_title": "Breaking the Low-Rank Dilemma of Linear Attention",
    "source": "arxiv",
    "authors": [
      "Qihang Fan",
      "Huaibo Huang",
      "Ran He"
    ],
    "published": "2024-11-12T08:30:59+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2411.07635v5"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2411.07635v5_chunk_2",
    "chunk_text": "Specifically, without using any additional labels, data, or supervision during training, RAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters and 4.6G FLOPs. This result significantly surpasses previous linear attention mechanisms, fully illustrating the potential of RALA. Code will be available at https://github.com/qhfan/RALA.",
    "original_url": "http://arxiv.org/pdf/2411.07635v5",
    "original_title": "Breaking the Low-Rank Dilemma of Linear Attention",
    "source": "arxiv",
    "authors": [
      "Qihang Fan",
      "Huaibo Huang",
      "Ran He"
    ],
    "published": "2024-11-12T08:30:59+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2411.07635v5"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2410.23608v2_chunk_0",
    "chunk_text": "Context-Aware Token Selection and Packing for Enhanced Vision Transformer\n\nIn recent years, the long-range attention mechanism of vision transformers has driven significant performance breakthroughs across various computer vision tasks. However, the traditional self-attention mechanism, which processes both informative and non-informative tokens, suffers from inefficiency and inaccuracies. While sparse attention mechanisms have been introduced to mitigate these issues by pruning tokens involved in attention, they often lack context-awareness and intelligence. These mechanisms frequently apply a uniform token selection strategy across different inputs for batch training or optimize efficiency only for the inference stage. To overcome these challenges, we propose a novel algorithm: Select and Pack Attention (SPA).",
    "original_url": "http://arxiv.org/pdf/2410.23608v2",
    "original_title": "Context-Aware Token Selection and Packing for Enhanced Vision Transformer",
    "source": "arxiv",
    "authors": [
      "Tianyi Zhang",
      "Baoxin Li",
      "Jae-sun Seo",
      "Yu Cao"
    ],
    "published": "2024-10-31T03:47:27+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2410.23608v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2410.23608v2_chunk_1",
    "chunk_text": "To overcome these challenges, we propose a novel algorithm: Select and Pack Attention (SPA). SPA dynamically selects informative tokens using a low-cost gating layer supervised by selection labels and packs these tokens into new batches, enabling a variable number of tokens to be used in parallelized GPU batch training and inference. Extensive experiments across diverse datasets and computer vision tasks demonstrate that SPA delivers superior performance and efficiency, including a 0.6 mAP improvement in object detection and a 16.4% reduction in computational costs.",
    "original_url": "http://arxiv.org/pdf/2410.23608v2",
    "original_title": "Context-Aware Token Selection and Packing for Enhanced Vision Transformer",
    "source": "arxiv",
    "authors": [
      "Tianyi Zhang",
      "Baoxin Li",
      "Jae-sun Seo",
      "Yu Cao"
    ],
    "published": "2024-10-31T03:47:27+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2410.23608v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2503.18565v1_chunk_0",
    "chunk_text": "Distil-xLSTM: Learning Attention Mechanisms through Recurrent Structures\n\nThe current era of Natural Language Processing (NLP) is dominated by Transformer models. However, novel architectures relying on recurrent mechanisms, such as xLSTM and Mamba, have been proposed as alternatives to attention-based models. Although computation is done differently than with the attention mechanism mechanism, these recurrent models yield good results and sometimes even outperform state-of-the-art attention-based models. In this work, we propose Distil-xLSTM, an xLSTM-based Small Language Model (SLM) trained by distilling knowledge from a Large Language Model (LLM) that shows promising results while being compute and scale efficient. Our Distil-xLSTM focuses on approximating a transformer-based model attention parametrization using its recurrent sequence mixing components and shows good results with minimal training.",
    "original_url": "http://arxiv.org/pdf/2503.18565v1",
    "original_title": "Distil-xLSTM: Learning Attention Mechanisms through Recurrent Structures",
    "source": "arxiv",
    "authors": [
      "Abdoul Majid O. Thiombiano",
      "Brahim Hnich",
      "Ali Ben Mrad",
      "Mohamed Wiem Mkaouer"
    ],
    "published": "2025-03-24T11:18:25+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2503.18565v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2503.18565v1_chunk_1",
    "chunk_text": "Our Distil-xLSTM focuses on approximating a transformer-based model attention parametrization using its recurrent sequence mixing components and shows good results with minimal training.",
    "original_url": "http://arxiv.org/pdf/2503.18565v1",
    "original_title": "Distil-xLSTM: Learning Attention Mechanisms through Recurrent Structures",
    "source": "arxiv",
    "authors": [
      "Abdoul Majid O. Thiombiano",
      "Brahim Hnich",
      "Ali Ben Mrad",
      "Mohamed Wiem Mkaouer"
    ],
    "published": "2025-03-24T11:18:25+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2503.18565v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1909.12406v1_chunk_0",
    "chunk_text": "Monotonic Multihead Attention\n\nSimultaneous machine translation models start generating a target sequence before they have encoded or read the source sequence. Recent approaches for this task either apply a fixed policy on a state-of-the art Transformer model, or a learnable monotonic attention on a weaker recurrent neural network-based structure. In this paper, we propose a new attention mechanism, Monotonic Multihead Attention (MMA), which extends the monotonic attention mechanism to multihead attention. We also introduce two novel and interpretable approaches for latency control that are specifically designed for multiple attentions heads. We apply MMA to the simultaneous machine translation task and demonstrate better latency-quality tradeoffs compared to MILk, the previous state-of-the-art approach.",
    "original_url": "http://arxiv.org/pdf/1909.12406v1",
    "original_title": "Monotonic Multihead Attention",
    "source": "arxiv",
    "authors": [
      "Xutai Ma",
      "Juan Pino",
      "James Cross",
      "Liezl Puzon",
      "Jiatao Gu"
    ],
    "published": "2019-09-26T21:32:50+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1909.12406v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1909.12406v1_chunk_1",
    "chunk_text": "We apply MMA to the simultaneous machine translation task and demonstrate better latency-quality tradeoffs compared to MILk, the previous state-of-the-art approach. We also analyze how the latency controls affect the attention span and we motivate the introduction of our model by analyzing the effect of the number of decoder layers and heads on quality and latency.",
    "original_url": "http://arxiv.org/pdf/1909.12406v1",
    "original_title": "Monotonic Multihead Attention",
    "source": "arxiv",
    "authors": [
      "Xutai Ma",
      "Juan Pino",
      "James Cross",
      "Liezl Puzon",
      "Jiatao Gu"
    ],
    "published": "2019-09-26T21:32:50+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1909.12406v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2209.14145v3_chunk_0",
    "chunk_text": "Multi-scale Attention Network for Single Image Super-Resolution\n\nConvNets can compete with transformers in high-level tasks by exploiting larger receptive fields. To unleash the potential of ConvNet in super-resolution, we propose a multi-scale attention network (MAN), by coupling classical multi-scale mechanism with emerging large kernel attention. In particular, we proposed multi-scale large kernel attention (MLKA) and gated spatial attention unit (GSAU). Through our MLKA, we modify large kernel attention with multi-scale and gate schemes to obtain the abundant attention map at various granularity levels, thereby aggregating global and local information and avoiding potential blocking artifacts. In GSAU, we integrate gate mechanism and spatial attention to remove the unnecessary linear layer and aggregate informative spatial context.",
    "original_url": "http://arxiv.org/pdf/2209.14145v3",
    "original_title": "Multi-scale Attention Network for Single Image Super-Resolution",
    "source": "arxiv",
    "authors": [
      "Yan Wang",
      "Yusen Li",
      "Gang Wang",
      "Xiaoguang Liu"
    ],
    "published": "2022-09-28T14:49:28+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2209.14145v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2209.14145v3_chunk_1",
    "chunk_text": "In GSAU, we integrate gate mechanism and spatial attention to remove the unnecessary linear layer and aggregate informative spatial context. To confirm the effectiveness of our designs, we evaluate MAN with multiple complexities by simply stacking different numbers of MLKA and GSAU. Experimental results illustrate that our MAN can perform on par with SwinIR and achieve varied trade-offs between state-of-the-art performance and computations.",
    "original_url": "http://arxiv.org/pdf/2209.14145v3",
    "original_title": "Multi-scale Attention Network for Single Image Super-Resolution",
    "source": "arxiv",
    "authors": [
      "Yan Wang",
      "Yusen Li",
      "Gang Wang",
      "Xiaoguang Liu"
    ],
    "published": "2022-09-28T14:49:28+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2209.14145v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2310.07911v1_chunk_0",
    "chunk_text": "Pit One Against Many: Leveraging Attention-head Embeddings for Parameter-efficient Multi-head Attention\n\nScaling pre-trained language models has resulted in large performance gains in various natural language processing tasks but comes with a large cost in memory requirements. Inspired by the position embeddings in transformers, we aim to simplify and reduce the memory footprint of the multi-head attention (MHA) mechanism. We propose an alternative module that uses only a single shared projection matrix and multiple head embeddings (MHE), i.e. one per head. We empirically demonstrate that our MHE attention is substantially more memory efficient compared to alternative attention mechanisms while achieving high predictive performance retention ratio to vanilla MHA on several downstream tasks.",
    "original_url": "http://arxiv.org/pdf/2310.07911v1",
    "original_title": "Pit One Against Many: Leveraging Attention-head Embeddings for Parameter-efficient Multi-head Attention",
    "source": "arxiv",
    "authors": [
      "Huiyin Xue",
      "Nikolaos Aletras"
    ],
    "published": "2023-10-11T21:38:40+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2310.07911v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2310.07911v1_chunk_1",
    "chunk_text": "We empirically demonstrate that our MHE attention is substantially more memory efficient compared to alternative attention mechanisms while achieving high predictive performance retention ratio to vanilla MHA on several downstream tasks. MHE attention only requires a negligible fraction of additional parameters ($3nd$, where $n$ is the number of attention heads and $d$ the size of the head embeddings) compared to a single-head attention, while MHA requires $(3n^2-3n)d^2-3nd$ additional parameters.",
    "original_url": "http://arxiv.org/pdf/2310.07911v1",
    "original_title": "Pit One Against Many: Leveraging Attention-head Embeddings for Parameter-efficient Multi-head Attention",
    "source": "arxiv",
    "authors": [
      "Huiyin Xue",
      "Nikolaos Aletras"
    ],
    "published": "2023-10-11T21:38:40+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2310.07911v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2504.13981v1_chunk_0",
    "chunk_text": "CacheFormer: High Attention-Based Segment Caching\n\nEfficiently handling long contexts in transformer-based language models with low perplexity is an active area of research. Numerous recent approaches like Linformer, Longformer, Performer, and Structured state space models (SSMs)., have not fully resolved this problem. All these models strive to reduce the quadratic time complexity of the attention mechanism while minimizing the loss in quality due to the effective compression of the long context. Inspired by the cache and virtual memory principle in computers, where in case of a cache miss, not only the needed data is retrieved from the memory, but the adjacent data is also obtained, we apply this concept to handling long contexts by dividing it into small segments. In our design, we retrieve the nearby segments in an uncompressed form when high segment-level attention occurs at the compressed level.",
    "original_url": "http://arxiv.org/pdf/2504.13981v1",
    "original_title": "CacheFormer: High Attention-Based Segment Caching",
    "source": "arxiv",
    "authors": [
      "Sushant Singh",
      "Ausif Mahmood"
    ],
    "published": "2025-04-18T06:34:57+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2504.13981v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2504.13981v1_chunk_1",
    "chunk_text": "In our design, we retrieve the nearby segments in an uncompressed form when high segment-level attention occurs at the compressed level. Our en-hancements for handling long context include aggregating four attention mechanisms consisting of short sliding window attention, long compressed segmented attention, dynamically retrieving top k high attention uncompressed segments, and overlapping segments in long segment attention to avoid segment fragmentation. These enhancements result in an architecture that outperforms ex-isting SOTA architectures with an average perplexity improvement of 8.5% over similar model sizes.",
    "original_url": "http://arxiv.org/pdf/2504.13981v1",
    "original_title": "CacheFormer: High Attention-Based Segment Caching",
    "source": "arxiv",
    "authors": [
      "Sushant Singh",
      "Ausif Mahmood"
    ],
    "published": "2025-04-18T06:34:57+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2504.13981v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2206.06614v1_chunk_0",
    "chunk_text": "Transformers are Meta-Reinforcement Learners\n\nThe transformer architecture and variants presented remarkable success across many machine learning tasks in recent years. This success is intrinsically related to the capability of handling long sequences and the presence of context-dependent weights from the attention mechanism. We argue that these capabilities suit the central role of a Meta-Reinforcement Learning algorithm. Indeed, a meta-RL agent needs to infer the task from a sequence of trajectories. Furthermore, it requires a fast adaptation strategy to adapt its policy for a new task -- which can be achieved using the self-attention mechanism.",
    "original_url": "http://arxiv.org/pdf/2206.06614v1",
    "original_title": "Transformers are Meta-Reinforcement Learners",
    "source": "arxiv",
    "authors": [
      "Luckeciano C. Melo"
    ],
    "published": "2022-06-14T06:21:13+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2206.06614v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2206.06614v1_chunk_1",
    "chunk_text": "Furthermore, it requires a fast adaptation strategy to adapt its policy for a new task -- which can be achieved using the self-attention mechanism. In this work, we present TrMRL (Transformers for Meta-Reinforcement Learning), a meta-RL agent that mimics the memory reinstatement mechanism using the transformer architecture. It associates the recent past of working memories to build an episodic memory recursively through the transformer layers. We show that the self-attention computes a consensus representation that minimizes the Bayes Risk at each layer and provides meaningful features to compute the best actions. We conducted experiments in high-dimensional continuous control environments for locomotion and dexterous manipulation.",
    "original_url": "http://arxiv.org/pdf/2206.06614v1",
    "original_title": "Transformers are Meta-Reinforcement Learners",
    "source": "arxiv",
    "authors": [
      "Luckeciano C. Melo"
    ],
    "published": "2022-06-14T06:21:13+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2206.06614v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2206.06614v1_chunk_2",
    "chunk_text": "We conducted experiments in high-dimensional continuous control environments for locomotion and dexterous manipulation. Results show that TrMRL presents comparable or superior asymptotic performance, sample efficiency, and out-of-distribution generalization compared to the baselines in these environments.",
    "original_url": "http://arxiv.org/pdf/2206.06614v1",
    "original_title": "Transformers are Meta-Reinforcement Learners",
    "source": "arxiv",
    "authors": [
      "Luckeciano C. Melo"
    ],
    "published": "2022-06-14T06:21:13+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2206.06614v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2504.12588v1_chunk_0",
    "chunk_text": "Simplifying Graph Transformers\n\nTransformers have attained outstanding performance across various modalities, employing scaled-dot-product (SDP) attention mechanisms. Researchers have attempted to migrate Transformers to graph learning, but most advanced Graph Transformers are designed with major architectural differences, either integrating message-passing or incorporating sophisticated attention mechanisms. These complexities prevent the easy adoption of Transformer training advances. We propose three simple modifications to the plain Transformer to render it applicable to graphs without introducing major architectural distortions. Specifically, we advocate for the use of (1) simplified $L_2$ attention to measure the magnitude closeness of tokens; (2) adaptive root-mean-square normalization to preserve token magnitude information; and (3) a relative positional encoding bias with a shared encoder.",
    "original_url": "http://arxiv.org/pdf/2504.12588v1",
    "original_title": "Simplifying Graph Transformers",
    "source": "arxiv",
    "authors": [
      "Liheng Ma",
      "Soumyasundar Pal",
      "Yingxue Zhang",
      "Philip H. S. Torr",
      "Mark Coates"
    ],
    "published": "2025-04-17T02:06:50+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2504.12588v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2504.12588v1_chunk_1",
    "chunk_text": "Specifically, we advocate for the use of (1) simplified $L_2$ attention to measure the magnitude closeness of tokens; (2) adaptive root-mean-square normalization to preserve token magnitude information; and (3) a relative positional encoding bias with a shared encoder. Significant performance gains across a variety of graph datasets justify the effectiveness of our proposed modifications. Furthermore, empirical evaluation on the expressiveness benchmark reveals noteworthy realized expressiveness in the graph isomorphism.",
    "original_url": "http://arxiv.org/pdf/2504.12588v1",
    "original_title": "Simplifying Graph Transformers",
    "source": "arxiv",
    "authors": [
      "Liheng Ma",
      "Soumyasundar Pal",
      "Yingxue Zhang",
      "Philip H. S. Torr",
      "Mark Coates"
    ],
    "published": "2025-04-17T02:06:50+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2504.12588v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2410.07063v1_chunk_0",
    "chunk_text": "InAttention: Linear Context Scaling for Transformers\n\nVRAM requirements for transformer models scale quadratically with context length due to the self-attention mechanism. In this paper we modify the decoder-only transformer, replacing self-attention with InAttention, which scales linearly with context length during inference by having tokens attend only to initial states. Benchmarking shows that InAttention significantly reduces VRAM usage during inference, enabling handling of long sequences on consumer GPUs. We corroborate that fine-tuning extends context length efficiently, improving performance on long sequences without high training costs. InAttention offers a scalable solution for long-range dependencies in transformer models, paving the way for further optimization.",
    "original_url": "http://arxiv.org/pdf/2410.07063v1",
    "original_title": "InAttention: Linear Context Scaling for Transformers",
    "source": "arxiv",
    "authors": [
      "Joseph Eisner"
    ],
    "published": "2024-10-09T17:05:15+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2410.07063v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2410.07063v1_chunk_1",
    "chunk_text": "InAttention offers a scalable solution for long-range dependencies in transformer models, paving the way for further optimization.",
    "original_url": "http://arxiv.org/pdf/2410.07063v1",
    "original_title": "InAttention: Linear Context Scaling for Transformers",
    "source": "arxiv",
    "authors": [
      "Joseph Eisner"
    ],
    "published": "2024-10-09T17:05:15+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2410.07063v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2405.15926v2_chunk_0",
    "chunk_text": "Dissecting the Interplay of Attention Paths in a Statistical Mechanics Theory of Transformers\n\nDespite the remarkable empirical performance of Transformers, their theoretical understanding remains elusive. Here, we consider a deep multi-head self-attention network, that is closely related to Transformers yet analytically tractable. We develop a statistical mechanics theory of Bayesian learning in this model, deriving exact equations for the network's predictor statistics under the finite-width thermodynamic limit, i.e., $N,P\\rightarrow\\infty$, $P/N=\\mathcal{O}(1)$, where $N$ is the network width and $P$ is the number of training examples. Our theory shows that the predictor statistics are expressed as a sum of independent kernels, each one pairing different 'attention paths', defined as information pathways through different attention heads across layers. The kernels are weighted according to a 'task-relevant kernel combination' mechanism that aligns the total kernel with the task labels.",
    "original_url": "http://arxiv.org/pdf/2405.15926v2",
    "original_title": "Dissecting the Interplay of Attention Paths in a Statistical Mechanics Theory of Transformers",
    "source": "arxiv",
    "authors": [
      "Lorenzo Tiberi",
      "Francesca Mignacco",
      "Kazuki Irie",
      "Haim Sompolinsky"
    ],
    "published": "2024-05-24T20:34:18+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2405.15926v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2405.15926v2_chunk_1",
    "chunk_text": "The kernels are weighted according to a 'task-relevant kernel combination' mechanism that aligns the total kernel with the task labels. As a consequence, this interplay between attention paths enhances generalization performance. Experiments confirm our findings on both synthetic and real-world sequence classification tasks. Finally, our theory explicitly relates the kernel combination mechanism to properties of the learned weights, allowing for a qualitative transfer of its insights to models trained via gradient descent. As an illustration, we demonstrate an efficient size reduction of the network, by pruning those attention heads that are deemed less relevant by our theory.",
    "original_url": "http://arxiv.org/pdf/2405.15926v2",
    "original_title": "Dissecting the Interplay of Attention Paths in a Statistical Mechanics Theory of Transformers",
    "source": "arxiv",
    "authors": [
      "Lorenzo Tiberi",
      "Francesca Mignacco",
      "Kazuki Irie",
      "Haim Sompolinsky"
    ],
    "published": "2024-05-24T20:34:18+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2405.15926v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2405.15926v2_chunk_2",
    "chunk_text": "As an illustration, we demonstrate an efficient size reduction of the network, by pruning those attention heads that are deemed less relevant by our theory.",
    "original_url": "http://arxiv.org/pdf/2405.15926v2",
    "original_title": "Dissecting the Interplay of Attention Paths in a Statistical Mechanics Theory of Transformers",
    "source": "arxiv",
    "authors": [
      "Lorenzo Tiberi",
      "Francesca Mignacco",
      "Kazuki Irie",
      "Haim Sompolinsky"
    ],
    "published": "2024-05-24T20:34:18+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2405.15926v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2211.14705v1_chunk_0",
    "chunk_text": "Semantic-Aware Local-Global Vision Transformer\n\nVision Transformers have achieved remarkable progresses, among which Swin Transformer has demonstrated the tremendous potential of Transformer for vision tasks. It surmounts the key challenge of high computational complexity by performing local self-attention within shifted windows. In this work we propose the Semantic-Aware Local-Global Vision Transformer (SALG), to further investigate two potential improvements towards Swin Transformer. First, unlike Swin Transformer that performs uniform partition to produce equal size of regular windows for local self-attention, our SALG performs semantic segmentation in an unsupervised way to explore the underlying semantic priors in the image. As a result, each segmented region can correspond to a semantically meaningful part in the image, potentially leading to more effective features within each of segmented regions.",
    "original_url": "http://arxiv.org/pdf/2211.14705v1",
    "original_title": "Semantic-Aware Local-Global Vision Transformer",
    "source": "arxiv",
    "authors": [
      "Jiatong Zhang",
      "Zengwei Yao",
      "Fanglin Chen",
      "Guangming Lu",
      "Wenjie Pei"
    ],
    "published": "2022-11-27T03:16:00+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2211.14705v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2211.14705v1_chunk_1",
    "chunk_text": "As a result, each segmented region can correspond to a semantically meaningful part in the image, potentially leading to more effective features within each of segmented regions. Second, instead of only performing local self-attention within local windows as Swin Transformer does, the proposed SALG performs both 1) local intra-region self-attention for learning fine-grained features within each region and 2) global inter-region feature propagation for modeling global dependencies among all regions. Consequently, our model is able to obtain the global view when learning features for each token, which is the essential advantage of Transformer. Owing to the explicit modeling of the semantic priors and the proposed local-global modeling mechanism, our SALG is particularly advantageous for small-scale models when the modeling capacity is not sufficient for other models to learn semantics implicitly. Extensive experiments across various vision tasks demonstrates the merit of our model over other vision Transformers, especially in the small-scale modeling scenarios.",
    "original_url": "http://arxiv.org/pdf/2211.14705v1",
    "original_title": "Semantic-Aware Local-Global Vision Transformer",
    "source": "arxiv",
    "authors": [
      "Jiatong Zhang",
      "Zengwei Yao",
      "Fanglin Chen",
      "Guangming Lu",
      "Wenjie Pei"
    ],
    "published": "2022-11-27T03:16:00+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2211.14705v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2211.14705v1_chunk_2",
    "chunk_text": "Extensive experiments across various vision tasks demonstrates the merit of our model over other vision Transformers, especially in the small-scale modeling scenarios.",
    "original_url": "http://arxiv.org/pdf/2211.14705v1",
    "original_title": "Semantic-Aware Local-Global Vision Transformer",
    "source": "arxiv",
    "authors": [
      "Jiatong Zhang",
      "Zengwei Yao",
      "Fanglin Chen",
      "Guangming Lu",
      "Wenjie Pei"
    ],
    "published": "2022-11-27T03:16:00+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2211.14705v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2310.14505v4_chunk_0",
    "chunk_text": "Sentiment analysis with adaptive multi-head attention in Transformer\n\nWe propose a novel framework based on the attention mechanism to identify the sentiment of a movie review document. Previous efforts on deep neural networks with attention mechanisms focus on encoder and decoder with fixed numbers of multi-head attention. Therefore, we need a mechanism to stop the attention process automatically if no more useful information can be read from the memory.In this paper, we propose an adaptive multi-head attention architecture (AdaptAttn) which varies the number of attention heads based on length of sentences. AdaptAttn has a data preprocessing step where each document is classified into any one of the three bins small, medium or large based on length of the sentence. The document classified as small goes through two heads in each layer, the medium group passes four heads and the large group is processed by eight heads.",
    "original_url": "http://arxiv.org/pdf/2310.14505v4",
    "original_title": "Sentiment analysis with adaptive multi-head attention in Transformer",
    "source": "arxiv",
    "authors": [
      "Fanfei Meng",
      "Chen-Ao Wang"
    ],
    "published": "2023-10-23T02:32:30+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2310.14505v4"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2310.14505v4_chunk_1",
    "chunk_text": "The document classified as small goes through two heads in each layer, the medium group passes four heads and the large group is processed by eight heads. We examine the merit of our model on the Stanford large movie review dataset. The experimental results show that the F1 score from our model is on par with the baseline model.",
    "original_url": "http://arxiv.org/pdf/2310.14505v4",
    "original_title": "Sentiment analysis with adaptive multi-head attention in Transformer",
    "source": "arxiv",
    "authors": [
      "Fanfei Meng",
      "Chen-Ao Wang"
    ],
    "published": "2023-10-23T02:32:30+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2310.14505v4"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2410.03159v3_chunk_0",
    "chunk_text": "WAVE: Weighted Autoregressive Varying Gate for Time Series Forecasting\n\nWe propose a Weighted Autoregressive Varying gatE (WAVE) attention mechanism equipped with both Autoregressive (AR) and Moving-average (MA) components. It can adapt to various attention mechanisms, enhancing and decoupling their ability to capture long-range and local temporal patterns in time series data. In this paper, we first demonstrate that, for the time series forecasting (TSF) task, the previously overlooked decoder-only autoregressive Transformer model can achieve results comparable to the best baselines when appropriate tokenization and training methods are applied. Moreover, inspired by the ARMA model from statistics and recent advances in linear attention, we introduce the full ARMA structure into existing autoregressive attention mechanisms. By using an indirect MA weight generation method, we incorporate the MA term while maintaining the time complexity and parameter size of the underlying efficient attention models.",
    "original_url": "http://arxiv.org/pdf/2410.03159v3",
    "original_title": "WAVE: Weighted Autoregressive Varying Gate for Time Series Forecasting",
    "source": "arxiv",
    "authors": [
      "Jiecheng Lu",
      "Xu Han",
      "Yan Sun",
      "Shihao Yang"
    ],
    "published": "2024-10-04T05:45:50+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2410.03159v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2410.03159v3_chunk_1",
    "chunk_text": "By using an indirect MA weight generation method, we incorporate the MA term while maintaining the time complexity and parameter size of the underlying efficient attention models. We further explore how indirect parameter generation can produce implicit MA weights that align with the modeling requirements for local temporal impacts. Experimental results show that WAVE attention that incorporates the ARMA structure consistently improves the performance of various AR attentions on TSF tasks, achieving state-of-the-art results.",
    "original_url": "http://arxiv.org/pdf/2410.03159v3",
    "original_title": "WAVE: Weighted Autoregressive Varying Gate for Time Series Forecasting",
    "source": "arxiv",
    "authors": [
      "Jiecheng Lu",
      "Xu Han",
      "Yan Sun",
      "Shihao Yang"
    ],
    "published": "2024-10-04T05:45:50+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2410.03159v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2312.06182v2_chunk_0",
    "chunk_text": "Why \"classic\" Transformers are shallow and how to make them go deep\n\nSince its introduction in 2017, Transformer has emerged as the leading neural network architecture, catalyzing revolutionary advancements in many AI disciplines. The key innovation in Transformer is a Self-Attention (SA) mechanism designed to capture contextual information. However, extending the original Transformer design to models of greater depth has proven exceedingly challenging, if not impossible. Even though various modifications have been proposed in order to stack more layers of SA mechanism into deeper models, a full understanding of this depth problem remains lacking. In this paper, we conduct a comprehensive investigation, both theoretically and empirically, to substantiate the claim that the depth problem is caused by \\emph{token similarity escalation}; that is, tokens grow increasingly alike after repeated applications of the SA mechanism.",
    "original_url": "http://arxiv.org/pdf/2312.06182v2",
    "original_title": "Why \"classic\" Transformers are shallow and how to make them go deep",
    "source": "arxiv",
    "authors": [
      "Yueyao Yu",
      "Yin Zhang"
    ],
    "published": "2023-12-11T07:49:16+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2312.06182v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2312.06182v2_chunk_1",
    "chunk_text": "In this paper, we conduct a comprehensive investigation, both theoretically and empirically, to substantiate the claim that the depth problem is caused by \\emph{token similarity escalation}; that is, tokens grow increasingly alike after repeated applications of the SA mechanism. Our analysis reveals that, driven by the invariant leading eigenspace and large spectral gaps of attention matrices, token similarity provably escalates at a linear rate. Based on the gained insight, we propose a new strategy of surgically removing excessive similarity in contrast to the existing approach of diminishing the SA mechanism explicitly or implicitly (such as in pre-norm transformers). Preliminary experimental results confirm the effectiveness of the proposed strategy in small-scale post-norm Transformer models.",
    "original_url": "http://arxiv.org/pdf/2312.06182v2",
    "original_title": "Why \"classic\" Transformers are shallow and how to make them go deep",
    "source": "arxiv",
    "authors": [
      "Yueyao Yu",
      "Yin Zhang"
    ],
    "published": "2023-12-11T07:49:16+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2312.06182v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2310.07664v1_chunk_0",
    "chunk_text": "Accelerating Vision Transformers Based on Heterogeneous Attention Patterns\n\nRecently, Vision Transformers (ViTs) have attracted a lot of attention in the field of computer vision. Generally, the powerful representative capacity of ViTs mainly benefits from the self-attention mechanism, which has a high computation complexity. To accelerate ViTs, we propose an integrated compression pipeline based on observed heterogeneous attention patterns across layers. On one hand, different images share more similar attention patterns in early layers than later layers, indicating that the dynamic query-by-key self-attention matrix may be replaced with a static self-attention matrix in early layers. Then, we propose a dynamic-guided static self-attention (DGSSA) method where the matrix inherits self-attention information from the replaced dynamic self-attention to effectively improve the feature representation ability of ViTs.",
    "original_url": "http://arxiv.org/pdf/2310.07664v1",
    "original_title": "Accelerating Vision Transformers Based on Heterogeneous Attention Patterns",
    "source": "arxiv",
    "authors": [
      "Deli Yu",
      "Teng Xi",
      "Jianwei Li",
      "Baopu Li",
      "Gang Zhang",
      "Haocheng Feng",
      "Junyu Han",
      "Jingtuo Liu",
      "Errui Ding",
      "Jingdong Wang"
    ],
    "published": "2023-10-11T17:09:19+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2310.07664v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2310.07664v1_chunk_1",
    "chunk_text": "Then, we propose a dynamic-guided static self-attention (DGSSA) method where the matrix inherits self-attention information from the replaced dynamic self-attention to effectively improve the feature representation ability of ViTs. On the other hand, the attention maps have more low-rank patterns, which reflect token redundancy, in later layers than early layers. In a view of linear dimension reduction, we further propose a method of global aggregation pyramid (GLAD) to reduce the number of tokens in later layers of ViTs, such as Deit. Experimentally, the integrated compression pipeline of DGSSA and GLAD can accelerate up to 121% run-time throughput compared with DeiT, which surpasses all SOTA approaches.",
    "original_url": "http://arxiv.org/pdf/2310.07664v1",
    "original_title": "Accelerating Vision Transformers Based on Heterogeneous Attention Patterns",
    "source": "arxiv",
    "authors": [
      "Deli Yu",
      "Teng Xi",
      "Jianwei Li",
      "Baopu Li",
      "Gang Zhang",
      "Haocheng Feng",
      "Junyu Han",
      "Jingtuo Liu",
      "Errui Ding",
      "Jingdong Wang"
    ],
    "published": "2023-10-11T17:09:19+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2310.07664v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2209.01745v1_chunk_0",
    "chunk_text": "SEFormer: Structure Embedding Transformer for 3D Object Detection\n\nEffectively preserving and encoding structure features from objects in irregular and sparse LiDAR points is a key challenge to 3D object detection on point cloud. Recently, Transformer has demonstrated promising performance on many 2D and even 3D vision tasks. Compared with the fixed and rigid convolution kernels, the self-attention mechanism in Transformer can adaptively exclude the unrelated or noisy points and thus suitable for preserving the local spatial structure in irregular LiDAR point cloud. However, Transformer only performs a simple sum on the point features, based on the self-attention mechanism, and all the points share the same transformation for value. Such isotropic operation lacks the ability to capture the direction-distance-oriented local structure which is important for 3D object detection.",
    "original_url": "http://arxiv.org/pdf/2209.01745v1",
    "original_title": "SEFormer: Structure Embedding Transformer for 3D Object Detection",
    "source": "arxiv",
    "authors": [
      "Xiaoyu Feng",
      "Heming Du",
      "Yueqi Duan",
      "Yongpan Liu",
      "Hehe Fan"
    ],
    "published": "2022-09-05T03:38:12+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2209.01745v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2209.01745v1_chunk_1",
    "chunk_text": "Such isotropic operation lacks the ability to capture the direction-distance-oriented local structure which is important for 3D object detection. In this work, we propose a Structure-Embedding transFormer (SEFormer), which can not only preserve local structure as traditional Transformer but also have the ability to encode the local structure. Compared to the self-attention mechanism in traditional Transformer, SEFormer learns different feature transformations for value points based on the relative directions and distances to the query point. Then we propose a SEFormer based network for high-performance 3D object detection. Extensive experiments show that the proposed architecture can achieve SOTA results on Waymo Open Dataset, the largest 3D detection benchmark for autonomous driving.",
    "original_url": "http://arxiv.org/pdf/2209.01745v1",
    "original_title": "SEFormer: Structure Embedding Transformer for 3D Object Detection",
    "source": "arxiv",
    "authors": [
      "Xiaoyu Feng",
      "Heming Du",
      "Yueqi Duan",
      "Yongpan Liu",
      "Hehe Fan"
    ],
    "published": "2022-09-05T03:38:12+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2209.01745v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2209.01745v1_chunk_2",
    "chunk_text": "Extensive experiments show that the proposed architecture can achieve SOTA results on Waymo Open Dataset, the largest 3D detection benchmark for autonomous driving. Specifically, SEFormer achieves 79.02% mAP, which is 1.2% higher than existing works. We will release the codes.",
    "original_url": "http://arxiv.org/pdf/2209.01745v1",
    "original_title": "SEFormer: Structure Embedding Transformer for 3D Object Detection",
    "source": "arxiv",
    "authors": [
      "Xiaoyu Feng",
      "Heming Du",
      "Yueqi Duan",
      "Yongpan Liu",
      "Hehe Fan"
    ],
    "published": "2022-09-05T03:38:12+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2209.01745v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2310.12442v1_chunk_0",
    "chunk_text": "Efficient Long-Range Transformers: You Need to Attend More, but Not Necessarily at Every Layer\n\nPretrained transformer models have demonstrated remarkable performance across various natural language processing tasks. These models leverage the attention mechanism to capture long- and short-range dependencies in the sequence. However, the (full) attention mechanism incurs high computational cost - quadratic in the sequence length, which is not affordable in tasks with long sequences, e.g., inputs with 8k tokens. Although sparse attention can be used to improve computational efficiency, as suggested in existing work, it has limited modeling capacity and often fails to capture complicated dependencies in long sequences. To tackle this challenge, we propose MASFormer, an easy-to-implement transformer variant with Mixed Attention Spans.",
    "original_url": "http://arxiv.org/pdf/2310.12442v1",
    "original_title": "Efficient Long-Range Transformers: You Need to Attend More, but Not Necessarily at Every Layer",
    "source": "arxiv",
    "authors": [
      "Qingru Zhang",
      "Dhananjay Ram",
      "Cole Hawkins",
      "Sheng Zha",
      "Tuo Zhao"
    ],
    "published": "2023-10-19T03:32:05+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2310.12442v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2310.12442v1_chunk_1",
    "chunk_text": "To tackle this challenge, we propose MASFormer, an easy-to-implement transformer variant with Mixed Attention Spans. Specifically, MASFormer is equipped with full attention to capture long-range dependencies, but only at a small number of layers. For the remaining layers, MASformer only employs sparse attention to capture short-range dependencies. Our experiments on natural language modeling and generation tasks show that a decoder-only MASFormer model of 1.3B parameters can achieve competitive performance to vanilla transformers with full attention while significantly reducing computational cost (up to 75%). Additionally, we investigate the effectiveness of continual training with long sequence data and how sequence length impacts downstream generation performance, which may be of independent interest.",
    "original_url": "http://arxiv.org/pdf/2310.12442v1",
    "original_title": "Efficient Long-Range Transformers: You Need to Attend More, but Not Necessarily at Every Layer",
    "source": "arxiv",
    "authors": [
      "Qingru Zhang",
      "Dhananjay Ram",
      "Cole Hawkins",
      "Sheng Zha",
      "Tuo Zhao"
    ],
    "published": "2023-10-19T03:32:05+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2310.12442v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2310.12442v1_chunk_2",
    "chunk_text": "Additionally, we investigate the effectiveness of continual training with long sequence data and how sequence length impacts downstream generation performance, which may be of independent interest.",
    "original_url": "http://arxiv.org/pdf/2310.12442v1",
    "original_title": "Efficient Long-Range Transformers: You Need to Attend More, but Not Necessarily at Every Layer",
    "source": "arxiv",
    "authors": [
      "Qingru Zhang",
      "Dhananjay Ram",
      "Cole Hawkins",
      "Sheng Zha",
      "Tuo Zhao"
    ],
    "published": "2023-10-19T03:32:05+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2310.12442v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2403.01407v1_chunk_0",
    "chunk_text": "Region-Transformer: Self-Attention Region Based Class-Agnostic Point Cloud Segmentation\n\nPoint cloud segmentation, which helps us understand the environment of specific structures and objects, can be performed in class-specific and class-agnostic ways. We propose a novel region-based transformer model called Region-Transformer for performing class-agnostic point cloud segmentation. The model utilizes a region-growth approach and self-attention mechanism to iteratively expand or contract a region by adding or removing points. It is trained on simulated point clouds with instance labels only, avoiding semantic labels. Attention-based networks have succeeded in many previous methods of performing point cloud segmentation.",
    "original_url": "http://arxiv.org/pdf/2403.01407v1",
    "original_title": "Region-Transformer: Self-Attention Region Based Class-Agnostic Point Cloud Segmentation",
    "source": "arxiv",
    "authors": [
      "Dipesh Gyawali",
      "Jian Zhang",
      "BB Karki"
    ],
    "published": "2024-03-03T06:13:43+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2403.01407v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2403.01407v1_chunk_1",
    "chunk_text": "Attention-based networks have succeeded in many previous methods of performing point cloud segmentation. However, a region-growth approach with attention-based networks has yet to be used to explore its performance gain. To our knowledge, we are the first to use a self-attention mechanism in a region-growth approach. With the introduction of self-attention to region-growth that can utilize local contextual information of neighborhood points, our experiments demonstrate that the Region-Transformer model outperforms previous class-agnostic and class-specific methods on indoor datasets regarding clustering metrics. The model generalizes well to large-scale scenes.",
    "original_url": "http://arxiv.org/pdf/2403.01407v1",
    "original_title": "Region-Transformer: Self-Attention Region Based Class-Agnostic Point Cloud Segmentation",
    "source": "arxiv",
    "authors": [
      "Dipesh Gyawali",
      "Jian Zhang",
      "BB Karki"
    ],
    "published": "2024-03-03T06:13:43+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2403.01407v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2403.01407v1_chunk_2",
    "chunk_text": "The model generalizes well to large-scale scenes. Key advantages include capturing long-range dependencies through self-attention, avoiding the need for semantic labels during training, and applicability to a variable number of objects. The Region-Transformer model represents a promising approach for flexible point cloud segmentation with applications in robotics, digital twinning, and autonomous vehicles.",
    "original_url": "http://arxiv.org/pdf/2403.01407v1",
    "original_title": "Region-Transformer: Self-Attention Region Based Class-Agnostic Point Cloud Segmentation",
    "source": "arxiv",
    "authors": [
      "Dipesh Gyawali",
      "Jian Zhang",
      "BB Karki"
    ],
    "published": "2024-03-03T06:13:43+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2403.01407v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2409.04005v2_chunk_0",
    "chunk_text": "Qihoo-T2X: An Efficient Proxy-Tokenized Diffusion Transformer for Text-to-Any-Task\n\nThe global self-attention mechanism in diffusion transformers involves redundant computation due to the sparse and redundant nature of visual information, and the attention map of tokens within a spatial window shows significant similarity. To address this redundancy, we propose the Proxy-Tokenized Diffusion Transformer (PT-DiT), which employs sparse representative token attention (where the number of representative tokens is much smaller than the total number of tokens) to model global visual information efficiently. Specifically, within each transformer block, we compute an averaging token from each spatial-temporal window to serve as a proxy token for that region. The global semantics are captured through the self-attention of these proxy tokens and then injected into all latent tokens via cross-attention. Simultaneously, we introduce window and shift window attention to address the limitations in detail modeling caused by the sparse attention mechanism.",
    "original_url": "http://arxiv.org/pdf/2409.04005v2",
    "original_title": "Qihoo-T2X: An Efficient Proxy-Tokenized Diffusion Transformer for Text-to-Any-Task",
    "source": "arxiv",
    "authors": [
      "Jing Wang",
      "Ao Ma",
      "Jiasong Feng",
      "Dawei Leng",
      "Yuhui Yin",
      "Xiaodan Liang"
    ],
    "published": "2024-09-06T03:13:45+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2409.04005v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2409.04005v2_chunk_1",
    "chunk_text": "Simultaneously, we introduce window and shift window attention to address the limitations in detail modeling caused by the sparse attention mechanism. Building on the well-designed PT-DiT, we further develop the Qihoo-T2X family, which includes a variety of models for T2I, T2V, and T2MV tasks. Experimental results show that PT-DiT achieves competitive performance while reducing the computational complexity in both image and video generation tasks (e.g., a 49% reduction compared to DiT and a 34% reduction compared to PixArt-$\\alpha$). The visual exhibition and source code of Qihoo-T2X is available at https://360cvgroup.github.io/Qihoo-T2X/.",
    "original_url": "http://arxiv.org/pdf/2409.04005v2",
    "original_title": "Qihoo-T2X: An Efficient Proxy-Tokenized Diffusion Transformer for Text-to-Any-Task",
    "source": "arxiv",
    "authors": [
      "Jing Wang",
      "Ao Ma",
      "Jiasong Feng",
      "Dawei Leng",
      "Yuhui Yin",
      "Xiaodan Liang"
    ],
    "published": "2024-09-06T03:13:45+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2409.04005v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2009.14308v1_chunk_0",
    "chunk_text": "Attention that does not Explain Away\n\nModels based on the Transformer architecture have achieved better accuracy than the ones based on competing architectures for a large set of tasks. A unique feature of the Transformer is its universal application of a self-attention mechanism, which allows for free information flow at arbitrary distances. Following a probabilistic view of the attention via the Gaussian mixture model, we find empirical evidence that the Transformer attention tends to \"explain away\" certain input neurons. To compensate for this, we propose a doubly-normalized attention scheme that is simple to implement and provides theoretical guarantees for avoiding the \"explaining away\" effect without introducing significant computational or memory cost. Empirically, we show that the new attention schemes result in improved performance on several well-known benchmarks.",
    "original_url": "http://arxiv.org/pdf/2009.14308v1",
    "original_title": "Attention that does not Explain Away",
    "source": "arxiv",
    "authors": [
      "Nan Ding",
      "Xinjie Fan",
      "Zhenzhong Lan",
      "Dale Schuurmans",
      "Radu Soricut"
    ],
    "published": "2020-09-29T21:05:39+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2009.14308v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2009.14308v1_chunk_1",
    "chunk_text": "Empirically, we show that the new attention schemes result in improved performance on several well-known benchmarks.",
    "original_url": "http://arxiv.org/pdf/2009.14308v1",
    "original_title": "Attention that does not Explain Away",
    "source": "arxiv",
    "authors": [
      "Nan Ding",
      "Xinjie Fan",
      "Zhenzhong Lan",
      "Dale Schuurmans",
      "Radu Soricut"
    ],
    "published": "2020-09-29T21:05:39+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2009.14308v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2008.01547v2_chunk_0",
    "chunk_text": "TensorCoder: Dimension-Wise Attention via Tensor Representation for Natural Language Modeling\n\nTransformer has been widely-used in many Natural Language Processing (NLP) tasks and the scaled dot-product attention between tokens is a core module of Transformer. This attention is a token-wise design and its complexity is quadratic to the length of sequence, limiting its application potential for long sequence tasks. In this paper, we propose a dimension-wise attention mechanism based on which a novel language modeling approach (namely TensorCoder) can be developed. The dimension-wise attention can reduce the attention complexity from the original $O(N^2d)$ to $O(Nd^2)$, where $N$ is the length of the sequence and $d$ is the dimensionality of head. We verify TensorCoder on two tasks including masked language modeling and neural machine translation.",
    "original_url": "http://arxiv.org/pdf/2008.01547v2",
    "original_title": "TensorCoder: Dimension-Wise Attention via Tensor Representation for Natural Language Modeling",
    "source": "arxiv",
    "authors": [
      "Shuai Zhang",
      "Peng Zhang",
      "Xindian Ma",
      "Junqiu Wei",
      "Ningning Wang",
      "Qun Liu"
    ],
    "published": "2020-07-28T13:42:02+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2008.01547v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2008.01547v2_chunk_1",
    "chunk_text": "We verify TensorCoder on two tasks including masked language modeling and neural machine translation. Compared with the original Transformer, TensorCoder not only greatly reduces the calculation of the original model but also obtains improved performance on masked language modeling task (in PTB dataset) and comparable performance on machine translation tasks.",
    "original_url": "http://arxiv.org/pdf/2008.01547v2",
    "original_title": "TensorCoder: Dimension-Wise Attention via Tensor Representation for Natural Language Modeling",
    "source": "arxiv",
    "authors": [
      "Shuai Zhang",
      "Peng Zhang",
      "Xindian Ma",
      "Junqiu Wei",
      "Ningning Wang",
      "Qun Liu"
    ],
    "published": "2020-07-28T13:42:02+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2008.01547v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2110.01240v2_chunk_0",
    "chunk_text": "A free lunch from ViT:Adaptive Attention Multi-scale Fusion Transformer for Fine-grained Visual Recognition\n\nLearning subtle representation about object parts plays a vital role in fine-grained visual recognition (FGVR) field. The vision transformer (ViT) achieves promising results on computer vision due to its attention mechanism. Nonetheless, with the fixed size of patches in ViT, the class token in deep layer focuses on the global receptive field and cannot generate multi-granularity features for FGVR. To capture region attention without box annotations and compensate for ViT shortcomings in FGVR, we propose a novel method named Adaptive attention multi-scale Fusion Transformer (AFTrans). The Selective Attention Collection Module (SACM) in our approach leverages attention weights in ViT and filters them adaptively to correspond with the relative importance of input patches.",
    "original_url": "http://arxiv.org/pdf/2110.01240v2",
    "original_title": "A free lunch from ViT:Adaptive Attention Multi-scale Fusion Transformer for Fine-grained Visual Recognition",
    "source": "arxiv",
    "authors": [
      "Yuan Zhang",
      "Jian Cao",
      "Ling Zhang",
      "Xiangcheng Liu",
      "Zhiyi Wang",
      "Feng Ling",
      "Weiqian Chen"
    ],
    "published": "2021-10-04T08:11:21+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2110.01240v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2110.01240v2_chunk_1",
    "chunk_text": "The Selective Attention Collection Module (SACM) in our approach leverages attention weights in ViT and filters them adaptively to correspond with the relative importance of input patches. The multiple scales (global and local) pipeline is supervised by our weights sharing encoder and can be easily trained end-to-end. Comprehensive experiments demonstrate that AFTrans can achieve SOTA performance on three published fine-grained benchmarks: CUB-200-2011, Stanford Dogs and iNat2017.",
    "original_url": "http://arxiv.org/pdf/2110.01240v2",
    "original_title": "A free lunch from ViT:Adaptive Attention Multi-scale Fusion Transformer for Fine-grained Visual Recognition",
    "source": "arxiv",
    "authors": [
      "Yuan Zhang",
      "Jian Cao",
      "Ling Zhang",
      "Xiangcheng Liu",
      "Zhiyi Wang",
      "Feng Ling",
      "Weiqian Chen"
    ],
    "published": "2021-10-04T08:11:21+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2110.01240v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2110.05841v1_chunk_0",
    "chunk_text": "Relative Molecule Self-Attention Transformer\n\nSelf-supervised learning holds promise to revolutionize molecule property prediction - a central task to drug discovery and many more industries - by enabling data efficient learning from scarce experimental data. Despite significant progress, non-pretrained methods can be still competitive in certain settings. We reason that architecture might be a key bottleneck. In particular, enriching the backbone architecture with domain-specific inductive biases has been key for the success of self-supervised learning in other domains. In this spirit, we methodologically explore the design space of the self-attention mechanism tailored to molecular data.",
    "original_url": "http://arxiv.org/pdf/2110.05841v1",
    "original_title": "Relative Molecule Self-Attention Transformer",
    "source": "arxiv",
    "authors": [
      "Łukasz Maziarka",
      "Dawid Majchrowski",
      "Tomasz Danel",
      "Piotr Gaiński",
      "Jacek Tabor",
      "Igor Podolak",
      "Paweł Morkisz",
      "Stanisław Jastrzębski"
    ],
    "published": "2021-10-12T09:05:26+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2110.05841v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2110.05841v1_chunk_1",
    "chunk_text": "In this spirit, we methodologically explore the design space of the self-attention mechanism tailored to molecular data. We identify a novel variant of self-attention adapted to processing molecules, inspired by the relative self-attention layer, which involves fusing embedded graph and distance relationships between atoms. Our main contribution is Relative Molecule Attention Transformer (R-MAT): a novel Transformer-based model based on the developed self-attention layer that achieves state-of-the-art or very competitive results across a~wide range of molecule property prediction tasks.",
    "original_url": "http://arxiv.org/pdf/2110.05841v1",
    "original_title": "Relative Molecule Self-Attention Transformer",
    "source": "arxiv",
    "authors": [
      "Łukasz Maziarka",
      "Dawid Majchrowski",
      "Tomasz Danel",
      "Piotr Gaiński",
      "Jacek Tabor",
      "Igor Podolak",
      "Paweł Morkisz",
      "Stanisław Jastrzębski"
    ],
    "published": "2021-10-12T09:05:26+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2110.05841v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2303.17882v1_chunk_0",
    "chunk_text": "Visual Anomaly Detection via Dual-Attention Transformer and Discriminative Flow\n\nIn this paper, we introduce the novel state-of-the-art Dual-attention Transformer and Discriminative Flow (DADF) framework for visual anomaly detection. Based on only normal knowledge, visual anomaly detection has wide applications in industrial scenarios and has attracted significant attention. However, most existing methods fail to meet the requirements. In contrast, the proposed DTDF presents a new paradigm: it firstly leverages a pre-trained network to acquire multi-scale prior embeddings, followed by the development of a vision Transformer with dual attention mechanisms, namely self-attention and memorial-attention, to achieve two-level reconstruction for prior embeddings with the sequential and normality association. Additionally, we propose using normalizing flow to establish discriminative likelihood for the joint distribution of prior and reconstructions at each scale.",
    "original_url": "http://arxiv.org/pdf/2303.17882v1",
    "original_title": "Visual Anomaly Detection via Dual-Attention Transformer and Discriminative Flow",
    "source": "arxiv",
    "authors": [
      "Haiming Yao",
      "Wei Luo",
      "Wenyong Yu"
    ],
    "published": "2023-03-31T08:34:07+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2303.17882v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2303.17882v1_chunk_1",
    "chunk_text": "Additionally, we propose using normalizing flow to establish discriminative likelihood for the joint distribution of prior and reconstructions at each scale. The DADF achieves 98.3/98.4 of image/pixel AUROC on Mvtec AD; 83.7 of image AUROC and 67.4 of pixel sPRO on Mvtec LOCO AD benchmarks, demonstrating the effectiveness of our proposed approach.",
    "original_url": "http://arxiv.org/pdf/2303.17882v1",
    "original_title": "Visual Anomaly Detection via Dual-Attention Transformer and Discriminative Flow",
    "source": "arxiv",
    "authors": [
      "Haiming Yao",
      "Wei Luo",
      "Wenyong Yu"
    ],
    "published": "2023-03-31T08:34:07+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2303.17882v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2304.10933v1_chunk_0",
    "chunk_text": "Self-Attention in Colors: Another Take on Encoding Graph Structure in Transformers\n\nWe introduce a novel self-attention mechanism, which we call CSA (Chromatic Self-Attention), which extends the notion of attention scores to attention _filters_, independently modulating the feature channels. We showcase CSA in a fully-attentional graph Transformer CGT (Chromatic Graph Transformer) which integrates both graph structural information and edge features, completely bypassing the need for local message-passing components. Our method flexibly encodes graph structure through node-node interactions, by enriching the original edge features with a relative positional encoding scheme. We propose a new scheme based on random walks that encodes both structural and positional information, and show how to incorporate higher-order topological information, such as rings in molecular graphs. Our approach achieves state-of-the-art results on the ZINC benchmark dataset, while providing a flexible framework for encoding graph structure and incorporating higher-order topology.",
    "original_url": "http://arxiv.org/pdf/2304.10933v1",
    "original_title": "Self-Attention in Colors: Another Take on Encoding Graph Structure in Transformers",
    "source": "arxiv",
    "authors": [
      "Romain Menegaux",
      "Emmanuel Jehanno",
      "Margot Selosse",
      "Julien Mairal"
    ],
    "published": "2023-04-21T13:08:53+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2304.10933v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2304.10933v1_chunk_1",
    "chunk_text": "Our approach achieves state-of-the-art results on the ZINC benchmark dataset, while providing a flexible framework for encoding graph structure and incorporating higher-order topology.",
    "original_url": "http://arxiv.org/pdf/2304.10933v1",
    "original_title": "Self-Attention in Colors: Another Take on Encoding Graph Structure in Transformers",
    "source": "arxiv",
    "authors": [
      "Romain Menegaux",
      "Emmanuel Jehanno",
      "Margot Selosse",
      "Julien Mairal"
    ],
    "published": "2023-04-21T13:08:53+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2304.10933v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2305.13297v2_chunk_0",
    "chunk_text": "Investigating the Role of Feed-Forward Networks in Transformers Using Parallel Attention and Feed-Forward Net Design\n\nThis paper investigates the key role of Feed-Forward Networks (FFNs) in transformer models by utilizing the Parallel Attention and Feed-Forward Net Design (PAF) architecture, and comparing it to their Series Attention and Feed-Forward Net Design (SAF) counterparts. Central to the effectiveness of PAF are two main assumptions regarding the FFN block and the attention block within a layer: 1) the primary function of the FFN block is to maintain isotropy among token embeddings and prevent their degeneration, and 2) the residual norm computed in the attention block is substantially smaller than the input token embedding norm. To empirically validate these assumptions, we train PAF variants of two large language models (RoBERTa-large and bert-large-uncased). Our results demonstrate that both assumptions hold true in the PAF design. This study contributes to a deeper understanding of the roles and interactions between FFNs and self-attention mechanisms in transformer architectures.",
    "original_url": "http://arxiv.org/pdf/2305.13297v2",
    "original_title": "Investigating the Role of Feed-Forward Networks in Transformers Using Parallel Attention and Feed-Forward Net Design",
    "source": "arxiv",
    "authors": [
      "Shashank Sonkar",
      "Richard G. Baraniuk"
    ],
    "published": "2023-05-22T17:56:09+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2305.13297v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2305.13297v2_chunk_1",
    "chunk_text": "This study contributes to a deeper understanding of the roles and interactions between FFNs and self-attention mechanisms in transformer architectures.",
    "original_url": "http://arxiv.org/pdf/2305.13297v2",
    "original_title": "Investigating the Role of Feed-Forward Networks in Transformers Using Parallel Attention and Feed-Forward Net Design",
    "source": "arxiv",
    "authors": [
      "Shashank Sonkar",
      "Richard G. Baraniuk"
    ],
    "published": "2023-05-22T17:56:09+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2305.13297v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2404.05767v1_chunk_0",
    "chunk_text": "CSA-Trans: Code Structure Aware Transformer for AST\n\nWhen applying the Transformer architecture to source code, designing a good self-attention mechanism is critical as it affects how node relationship is extracted from the Abstract Syntax Trees (ASTs) of the source code. We present Code Structure Aware Transformer (CSA-Trans), which uses Code Structure Embedder (CSE) to generate specific PE for each node in AST. CSE generates node Positional Encoding (PE) using disentangled attention. To further extend the self-attention capability, we adopt Stochastic Block Model (SBM) attention. Our evaluation shows that our PE captures the relationships between AST nodes better than other graph-related PE techniques.",
    "original_url": "http://arxiv.org/pdf/2404.05767v1",
    "original_title": "CSA-Trans: Code Structure Aware Transformer for AST",
    "source": "arxiv",
    "authors": [
      "Saeyoon Oh",
      "Shin Yoo"
    ],
    "published": "2024-04-07T10:59:35+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2404.05767v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2404.05767v1_chunk_1",
    "chunk_text": "Our evaluation shows that our PE captures the relationships between AST nodes better than other graph-related PE techniques. We also show through quantitative and qualitative analysis that SBM attention is able to generate more node specific attention coefficients. We demonstrate that CSA-Trans outperforms 14 baselines in code summarization tasks for both Python and Java, while being 41.92% faster and 25.31% memory efficient in Java dataset compared to AST-Trans and SG-Trans respectively.",
    "original_url": "http://arxiv.org/pdf/2404.05767v1",
    "original_title": "CSA-Trans: Code Structure Aware Transformer for AST",
    "source": "arxiv",
    "authors": [
      "Saeyoon Oh",
      "Shin Yoo"
    ],
    "published": "2024-04-07T10:59:35+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2404.05767v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2407.01601v2_chunk_0",
    "chunk_text": "Unveiling and Controlling Anomalous Attention Distribution in Transformers\n\nWith the advent of large models based on the Transformer architecture, researchers have observed an anomalous phenomenon in the Attention mechanism--there is a very high attention on the first element, which is prevalent across Transformer-based models. It is crucial to understand it for the development of techniques focusing on attention distribution, such as Key-Value (KV) Cache compression and infinite extrapolation; however, the latent cause leaves to be unknown. In this paper, we analyze such a phenomenon from the perspective of waiver phenomenon, which involves reducing the internal values of certain elements in the sequence, allowing them to absorb excess attention without affecting their contribution to information. In specific models, due to differences in positional encoding and attention patterns, we have found that the selection of waiver elements by the model can be categorized into two methods: positional-encoding-based and feature-distribution-within-elements-based.",
    "original_url": "http://arxiv.org/pdf/2407.01601v2",
    "original_title": "Unveiling and Controlling Anomalous Attention Distribution in Transformers",
    "source": "arxiv",
    "authors": [
      "Ruiqing Yan",
      "Xingbo Du",
      "Haoyu Deng",
      "Linghan Zheng",
      "Qiuzhuang Sun",
      "Jifang Hu",
      "Yuhang Shao",
      "Penghao Jiang",
      "Jinrong Jiang",
      "Lian Zhao"
    ],
    "published": "2024-06-26T11:53:35+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2407.01601v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2409.09177v1_chunk_0",
    "chunk_text": "Transformer with Controlled Attention for Synchronous Motion Captioning\n\nIn this paper, we address a challenging task, synchronous motion captioning, that aim to generate a language description synchronized with human motion sequences. This task pertains to numerous applications, such as aligned sign language transcription, unsupervised action segmentation and temporal grounding. Our method introduces mechanisms to control self- and cross-attention distributions of the Transformer, allowing interpretability and time-aligned text generation. We achieve this through masking strategies and structuring losses that push the model to maximize attention only on the most important frames contributing to the generation of a motion word. These constraints aim to prevent undesired mixing of information in attention maps and to provide a monotonic attention distribution across tokens.",
    "original_url": "http://arxiv.org/pdf/2409.09177v1",
    "original_title": "Transformer with Controlled Attention for Synchronous Motion Captioning",
    "source": "arxiv",
    "authors": [
      "Karim Radouane",
      "Sylvie Ranwez",
      "Julien Lagarde",
      "Andon Tchechmedjiev"
    ],
    "published": "2024-09-13T20:30:29+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2409.09177v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2409.09177v1_chunk_1",
    "chunk_text": "These constraints aim to prevent undesired mixing of information in attention maps and to provide a monotonic attention distribution across tokens. Thus, the cross attentions of tokens are used for progressive text generation in synchronization with human motion sequences. We demonstrate the superior performance of our approach through evaluation on the two available benchmark datasets, KIT-ML and HumanML3D. As visual evaluation is essential for this task, we provide a comprehensive set of animated visual illustrations in the code repository: https://github.com/rd20karim/Synch-Transformer.",
    "original_url": "http://arxiv.org/pdf/2409.09177v1",
    "original_title": "Transformer with Controlled Attention for Synchronous Motion Captioning",
    "source": "arxiv",
    "authors": [
      "Karim Radouane",
      "Sylvie Ranwez",
      "Julien Lagarde",
      "Andon Tchechmedjiev"
    ],
    "published": "2024-09-13T20:30:29+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2409.09177v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2409.19315v2_chunk_0",
    "chunk_text": "Analog In-Memory Computing Attention Mechanism for Fast and Energy-Efficient Large Language Models\n\nTransformer networks, driven by self-attention, are central to Large Language Models. In generative Transformers, self-attention uses cache memory to store token projections, avoiding recomputation at each time step. However, GPU-stored projections must be loaded into SRAM for each new generation step, causing latency and energy bottlenecks. We present a custom self-attention in-memory computing architecture based on emerging charge-based memories called gain cells, which can be efficiently written to store new tokens during sequence generation and enable parallel analog dot-product computation required for self-attention. However, the analog gain cell circuits introduce non-idealities and constraints preventing the direct mapping of pre-trained models.",
    "original_url": "http://arxiv.org/pdf/2409.19315v2",
    "original_title": "Analog In-Memory Computing Attention Mechanism for Fast and Energy-Efficient Large Language Models",
    "source": "arxiv",
    "authors": [
      "Nathan Leroux",
      "Paul-Philipp Manea",
      "Chirag Sudarshan",
      "Jan Finkbeiner",
      "Sebastian Siegel",
      "John Paul Strachan",
      "Emre Neftci"
    ],
    "published": "2024-09-28T11:00:11+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2409.19315v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2409.19315v2_chunk_1",
    "chunk_text": "However, the analog gain cell circuits introduce non-idealities and constraints preventing the direct mapping of pre-trained models. To circumvent this problem, we design an initialization algorithm achieving text processing performance comparable to GPT-2 without training from scratch. Our architecture respectively reduces attention latency and energy consumption by up to two and five orders of magnitude compared to GPUs, marking a significant step toward ultra-fast, low-power generative Transformers.",
    "original_url": "http://arxiv.org/pdf/2409.19315v2",
    "original_title": "Analog In-Memory Computing Attention Mechanism for Fast and Energy-Efficient Large Language Models",
    "source": "arxiv",
    "authors": [
      "Nathan Leroux",
      "Paul-Philipp Manea",
      "Chirag Sudarshan",
      "Jan Finkbeiner",
      "Sebastian Siegel",
      "John Paul Strachan",
      "Emre Neftci"
    ],
    "published": "2024-09-28T11:00:11+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2409.19315v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2006.10407v2_chunk_0",
    "chunk_text": "Self-and-Mixed Attention Decoder with Deep Acoustic Structure for Transformer-based LVCSR\n\nThe Transformer has shown impressive performance in automatic speech recognition. It uses the encoder-decoder structure with self-attention to learn the relationship between the high-level representation of the source inputs and embedding of the target outputs. In this paper, we propose a novel decoder structure that features a self-and-mixed attention decoder (SMAD) with a deep acoustic structure (DAS) to improve the acoustic representation of Transformer-based LVCSR. Specifically, we introduce a self-attention mechanism to learn a multi-layer deep acoustic structure for multiple levels of acoustic abstraction. We also design a mixed attention mechanism that learns the alignment between different levels of acoustic abstraction and its corresponding linguistic information simultaneously in a shared embedding space.",
    "original_url": "http://arxiv.org/pdf/2006.10407v2",
    "original_title": "Self-and-Mixed Attention Decoder with Deep Acoustic Structure for Transformer-based LVCSR",
    "source": "arxiv",
    "authors": [
      "Xinyuan Zhou",
      "Grandee Lee",
      "Emre Yılmaz",
      "Yanhua Long",
      "Jiaen Liang",
      "Haizhou Li"
    ],
    "published": "2020-06-18T10:24:18+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2006.10407v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2006.10407v2_chunk_1",
    "chunk_text": "We also design a mixed attention mechanism that learns the alignment between different levels of acoustic abstraction and its corresponding linguistic information simultaneously in a shared embedding space. The ASR experiments on Aishell-1 shown that the proposed structure achieves CERs of 4.8% on the dev set and 5.1% on the test set, which are the best results obtained on this task to the best of our knowledge.",
    "original_url": "http://arxiv.org/pdf/2006.10407v2",
    "original_title": "Self-and-Mixed Attention Decoder with Deep Acoustic Structure for Transformer-based LVCSR",
    "source": "arxiv",
    "authors": [
      "Xinyuan Zhou",
      "Grandee Lee",
      "Emre Yılmaz",
      "Yanhua Long",
      "Jiaen Liang",
      "Haizhou Li"
    ],
    "published": "2020-06-18T10:24:18+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2006.10407v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2112.08534v3_chunk_0",
    "chunk_text": "Trading with the Momentum Transformer: An Intelligent and Interpretable Architecture\n\nWe introduce the Momentum Transformer, an attention-based deep-learning architecture, which outperforms benchmark time-series momentum and mean-reversion trading strategies. Unlike state-of-the-art Long Short-Term Memory (LSTM) architectures, which are sequential in nature and tailored to local processing, an attention mechanism provides our architecture with a direct connection to all previous time-steps. Our architecture, an attention-LSTM hybrid, enables us to learn longer-term dependencies, improves performance when considering returns net of transaction costs and naturally adapts to new market regimes, such as during the SARS-CoV-2 crisis. Via the introduction of multiple attention heads, we can capture concurrent regimes, or temporal dynamics, which are occurring at different timescales. The Momentum Transformer is inherently interpretable, providing us with greater insights into our deep-learning momentum trading strategy, including the importance of different factors over time and the past time-steps which are of the greatest significance to the model.",
    "original_url": "http://arxiv.org/pdf/2112.08534v3",
    "original_title": "Trading with the Momentum Transformer: An Intelligent and Interpretable Architecture",
    "source": "arxiv",
    "authors": [
      "Kieran Wood",
      "Sven Giegerich",
      "Stephen Roberts",
      "Stefan Zohren"
    ],
    "published": "2021-12-16T00:04:12+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2112.08534v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2112.08534v3_chunk_1",
    "chunk_text": "The Momentum Transformer is inherently interpretable, providing us with greater insights into our deep-learning momentum trading strategy, including the importance of different factors over time and the past time-steps which are of the greatest significance to the model.",
    "original_url": "http://arxiv.org/pdf/2112.08534v3",
    "original_title": "Trading with the Momentum Transformer: An Intelligent and Interpretable Architecture",
    "source": "arxiv",
    "authors": [
      "Kieran Wood",
      "Sven Giegerich",
      "Stephen Roberts",
      "Stefan Zohren"
    ],
    "published": "2021-12-16T00:04:12+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2112.08534v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2504.02277v1_chunk_0",
    "chunk_text": "Beyond Conventional Transformers: The Medical X-ray Attention (MXA) Block for Improved Multi-Label Diagnosis Using Knowledge Distillation\n\nMedical imaging, particularly X-ray analysis, often involves detecting multiple conditions simultaneously within a single scan, making multi-label classification crucial for real-world clinical applications. We present the Medical X-ray Attention (MXA) block, a novel attention mechanism tailored specifically to address the unique challenges of X-ray abnormality detection. The MXA block enhances traditional Multi-Head Self Attention (MHSA) by integrating a specialized module that efficiently captures both detailed local information and broader global context. To the best of our knowledge, this is the first work to propose a task-specific attention mechanism for diagnosing chest X-rays, as well as to attempt multi-label classification using an Efficient Vision Transformer (EfficientViT). By embedding the MXA block within the EfficientViT architecture and employing knowledge distillation, our proposed model significantly improves performance on the CheXpert dataset, a widely used benchmark for multi-label chest X-ray abnormality detection.",
    "original_url": "http://arxiv.org/pdf/2504.02277v1",
    "original_title": "Beyond Conventional Transformers: The Medical X-ray Attention (MXA) Block for Improved Multi-Label Diagnosis Using Knowledge Distillation",
    "source": "arxiv",
    "authors": [
      "Amit Rand",
      "Hadi Ibrahim"
    ],
    "published": "2025-04-03T04:55:42+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2504.02277v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2504.02277v1_chunk_1",
    "chunk_text": "By embedding the MXA block within the EfficientViT architecture and employing knowledge distillation, our proposed model significantly improves performance on the CheXpert dataset, a widely used benchmark for multi-label chest X-ray abnormality detection. Our approach achieves an area under the curve (AUC) of 0.85, an absolute improvement of 0.19 compared to our baseline model's AUC of 0.66, corresponding to a substantial approximate 233% relative improvement over random guessing (AUC = 0.5).",
    "original_url": "http://arxiv.org/pdf/2504.02277v1",
    "original_title": "Beyond Conventional Transformers: The Medical X-ray Attention (MXA) Block for Improved Multi-Label Diagnosis Using Knowledge Distillation",
    "source": "arxiv",
    "authors": [
      "Amit Rand",
      "Hadi Ibrahim"
    ],
    "published": "2025-04-03T04:55:42+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2504.02277v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2504.00927v1_chunk_0",
    "chunk_text": "Multi-Token Attention\n\nSoft attention is a critical mechanism powering LLMs to locate relevant parts within a given context. However, individual attention weights are determined by the similarity of only a single query and key token vector. This \"single token attention\" bottlenecks the amount of information used in distinguishing a relevant part from the rest of the context. To address this issue, we propose a new attention method, Multi-Token Attention (MTA), which allows LLMs to condition their attention weights on multiple query and key vectors simultaneously. This is achieved by applying convolution operations over queries, keys and heads, allowing nearby queries and keys to affect each other's attention weights for more precise attention.",
    "original_url": "http://arxiv.org/pdf/2504.00927v1",
    "original_title": "Multi-Token Attention",
    "source": "arxiv",
    "authors": [
      "Olga Golovneva",
      "Tianlu Wang",
      "Jason Weston",
      "Sainbayar Sukhbaatar"
    ],
    "published": "2025-04-01T15:59:32+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2504.00927v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2504.00927v1_chunk_1",
    "chunk_text": "This is achieved by applying convolution operations over queries, keys and heads, allowing nearby queries and keys to affect each other's attention weights for more precise attention. As a result, our method can locate relevant context using richer, more nuanced information that can exceed a single vector's capacity. Through extensive evaluations, we demonstrate that MTA achieves enhanced performance on a range of popular benchmarks. Notably, it outperforms Transformer baseline models on standard language modeling tasks, and on tasks that require searching for information within long contexts, where our method's ability to leverage richer information proves particularly beneficial.",
    "original_url": "http://arxiv.org/pdf/2504.00927v1",
    "original_title": "Multi-Token Attention",
    "source": "arxiv",
    "authors": [
      "Olga Golovneva",
      "Tianlu Wang",
      "Jason Weston",
      "Sainbayar Sukhbaatar"
    ],
    "published": "2025-04-01T15:59:32+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2504.00927v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2212.14678v1_chunk_0",
    "chunk_text": "Exploring Transformer Backbones for Image Diffusion Models\n\nWe present an end-to-end Transformer based Latent Diffusion model for image synthesis. On the ImageNet class conditioned generation task we show that a Transformer based Latent Diffusion model achieves a 14.1FID which is comparable to the 13.1FID score of a UNet based architecture. In addition to showing the application of Transformer models for Diffusion based image synthesis this simplification in architecture allows easy fusion and modeling of text and image data. The multi-head attention mechanism of Transformers enables simplified interaction between the image and text features which removes the requirement for crossattention mechanism in UNet based Diffusion models.",
    "original_url": "http://arxiv.org/pdf/2212.14678v1",
    "original_title": "Exploring Transformer Backbones for Image Diffusion Models",
    "source": "arxiv",
    "authors": [
      "Princy Chahal"
    ],
    "published": "2022-12-27T07:05:14+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2212.14678v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2103.02907v1_chunk_0",
    "chunk_text": "Coordinate Attention for Efficient Mobile Network Design\n\nRecent studies on mobile network design have demonstrated the remarkable effectiveness of channel attention (e.g., the Squeeze-and-Excitation attention) for lifting model performance, but they generally neglect the positional information, which is important for generating spatially selective attention maps. In this paper, we propose a novel attention mechanism for mobile networks by embedding positional information into channel attention, which we call \"coordinate attention\". Unlike channel attention that transforms a feature tensor to a single feature vector via 2D global pooling, the coordinate attention factorizes channel attention into two 1D feature encoding processes that aggregate features along the two spatial directions, respectively. In this way, long-range dependencies can be captured along one spatial direction and meanwhile precise positional information can be preserved along the other spatial direction. The resulting feature maps are then encoded separately into a pair of direction-aware and position-sensitive attention maps that can be complementarily applied to the input feature map to augment the representations of the objects of interest.",
    "original_url": "http://arxiv.org/pdf/2103.02907v1",
    "original_title": "Coordinate Attention for Efficient Mobile Network Design",
    "source": "arxiv",
    "authors": [
      "Qibin Hou",
      "Daquan Zhou",
      "Jiashi Feng"
    ],
    "published": "2021-03-04T09:18:02+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2103.02907v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2103.02907v1_chunk_1",
    "chunk_text": "The resulting feature maps are then encoded separately into a pair of direction-aware and position-sensitive attention maps that can be complementarily applied to the input feature map to augment the representations of the objects of interest. Our coordinate attention is simple and can be flexibly plugged into classic mobile networks, such as MobileNetV2, MobileNeXt, and EfficientNet with nearly no computational overhead. Extensive experiments demonstrate that our coordinate attention is not only beneficial to ImageNet classification but more interestingly, behaves better in down-stream tasks, such as object detection and semantic segmentation. Code is available at https://github.com/Andrew-Qibin/CoordAttention.",
    "original_url": "http://arxiv.org/pdf/2103.02907v1",
    "original_title": "Coordinate Attention for Efficient Mobile Network Design",
    "source": "arxiv",
    "authors": [
      "Qibin Hou",
      "Daquan Zhou",
      "Jiashi Feng"
    ],
    "published": "2021-03-04T09:18:02+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2103.02907v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2412.12465v1_chunk_0",
    "chunk_text": "Core Context Aware Attention for Long Context Language Modeling\n\nTransformer-based Large Language Models (LLMs) have exhibited remarkable success in various natural language processing tasks primarily attributed to self-attention mechanism, which requires a token to consider all preceding tokens as its context to compute the attention score. However, when the context length L becomes very large (e.g., 32K), more redundant context information will be included w.r.t. any tokens, making the self-attention suffer from two main limitations: 1) The computational and memory complexity scales quadratically w.r.t. L; 2) The presence of redundant context information may hamper the model to capture dependencies among crucial tokens, which may degrade the representation performance. In this paper, we propose a plug-and-play Core Context Aware (CCA) Attention for efficient long-range context modeling, which consists of two components: 1) Globality-pooling attention that divides input tokens into groups and then dynamically merges tokens within each group into one core token based on their significance; 2) Locality-preserved attention that incorporates neighboring tokens into the attention calculation.",
    "original_url": "http://arxiv.org/pdf/2412.12465v1",
    "original_title": "Core Context Aware Attention for Long Context Language Modeling",
    "source": "arxiv",
    "authors": [
      "Yaofo Chen",
      "Zeng You",
      "Shuhai Zhang",
      "Haokun Li",
      "Yirui Li",
      "Yaowei Wang",
      "Mingkui Tan"
    ],
    "published": "2024-12-17T01:54:08+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2412.12465v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2412.12465v1_chunk_1",
    "chunk_text": "In this paper, we propose a plug-and-play Core Context Aware (CCA) Attention for efficient long-range context modeling, which consists of two components: 1) Globality-pooling attention that divides input tokens into groups and then dynamically merges tokens within each group into one core token based on their significance; 2) Locality-preserved attention that incorporates neighboring tokens into the attention calculation. The two complementary attentions will then be fused to the final attention, maintaining comprehensive modeling ability as the full self-attention. In this way, the core context information w.r.t. a given token will be automatically focused and strengthened, while the context information in redundant groups will be diminished during the learning process. As a result, the computational and memory complexity will be significantly reduced.",
    "original_url": "http://arxiv.org/pdf/2412.12465v1",
    "original_title": "Core Context Aware Attention for Long Context Language Modeling",
    "source": "arxiv",
    "authors": [
      "Yaofo Chen",
      "Zeng You",
      "Shuhai Zhang",
      "Haokun Li",
      "Yirui Li",
      "Yaowei Wang",
      "Mingkui Tan"
    ],
    "published": "2024-12-17T01:54:08+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2412.12465v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2412.12465v1_chunk_2",
    "chunk_text": "As a result, the computational and memory complexity will be significantly reduced. More importantly, the CCA-Attention can improve the long-context modeling ability by diminishing the redundant context information. Extensive experimental results demonstrate that our CCA-Attention significantly outperforms state-of-the-art models in terms of computational efficiency and long-context modeling ability.",
    "original_url": "http://arxiv.org/pdf/2412.12465v1",
    "original_title": "Core Context Aware Attention for Long Context Language Modeling",
    "source": "arxiv",
    "authors": [
      "Yaofo Chen",
      "Zeng You",
      "Shuhai Zhang",
      "Haokun Li",
      "Yirui Li",
      "Yaowei Wang",
      "Mingkui Tan"
    ],
    "published": "2024-12-17T01:54:08+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2412.12465v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2302.13451v2_chunk_0",
    "chunk_text": "A low latency attention module for streaming self-supervised speech representation learning\n\nThe transformer is a fundamental building block in deep learning, and the attention mechanism is the transformer's core component. Self-supervised speech representation learning (SSRL) represents a popular use-case for the transformer architecture. Due to transformers' acausal behavior, the use of transformers for SSRL has been predominantly focused on acausal applications. However, several media processing problems, such as speech processing, require real-time solutions. In this paper, we present an implementation of the attention module that enables training of SSRL architectures with low compute and memory requirements, while allowing real-time inference with low and fixed latency.",
    "original_url": "http://arxiv.org/pdf/2302.13451v2",
    "original_title": "A low latency attention module for streaming self-supervised speech representation learning",
    "source": "arxiv",
    "authors": [
      "Jianbo Ma",
      "Siqi Pan",
      "Deepak Chandran",
      "Andrea Fanelli",
      "Richard Cartwright"
    ],
    "published": "2023-02-27T00:44:22+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2302.13451v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2302.13451v2_chunk_1",
    "chunk_text": "In this paper, we present an implementation of the attention module that enables training of SSRL architectures with low compute and memory requirements, while allowing real-time inference with low and fixed latency. The attention module proposed in this paper includes two components, streaming attention (SA) and low-latency streaming attention (LLSA). The SA represents our proposal for an efficient streaming SSRL implementation, while the LLSA solves the latency build-up problem of other streaming attention architectures, such as the masked acausal attention (MAA), guaranteeing a latency equal to one layer even when multiple layers are stacked. We present a comparative analysis between the vanilla attention, which we will refer here as acausal attention (AA), the SA, and the LLSA, by training a streaming SSRL with automatic speech recognition as downstream task. When training on librispeech-clean-100 and testing on librispeech-test-clean, our low-latency attention module has a word error rate (WER) of 5.84%, which represents a significant improvement over the MAA (WER = 13.82%).",
    "original_url": "http://arxiv.org/pdf/2302.13451v2",
    "original_title": "A low latency attention module for streaming self-supervised speech representation learning",
    "source": "arxiv",
    "authors": [
      "Jianbo Ma",
      "Siqi Pan",
      "Deepak Chandran",
      "Andrea Fanelli",
      "Richard Cartwright"
    ],
    "published": "2023-02-27T00:44:22+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2302.13451v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2302.13451v2_chunk_2",
    "chunk_text": "When training on librispeech-clean-100 and testing on librispeech-test-clean, our low-latency attention module has a word error rate (WER) of 5.84%, which represents a significant improvement over the MAA (WER = 13.82%). Our implementation also reduces the inference latency from 1.92 to 0.16 seconds. The proposed low-latency module preserves many of the benefits of conventional acausal transformers, but also enables latency characteristics that make it applicable to real-time streaming applications.",
    "original_url": "http://arxiv.org/pdf/2302.13451v2",
    "original_title": "A low latency attention module for streaming self-supervised speech representation learning",
    "source": "arxiv",
    "authors": [
      "Jianbo Ma",
      "Siqi Pan",
      "Deepak Chandran",
      "Andrea Fanelli",
      "Richard Cartwright"
    ],
    "published": "2023-02-27T00:44:22+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2302.13451v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2004.01389v1_chunk_0",
    "chunk_text": "LiDAR-based Online 3D Video Object Detection with Graph-based Message Passing and Spatiotemporal Transformer Attention\n\nExisting LiDAR-based 3D object detectors usually focus on the single-frame detection, while ignoring the spatiotemporal information in consecutive point cloud frames. In this paper, we propose an end-to-end online 3D video object detector that operates on point cloud sequences. The proposed model comprises a spatial feature encoding component and a spatiotemporal feature aggregation component. In the former component, a novel Pillar Message Passing Network (PMPNet) is proposed to encode each discrete point cloud frame. It adaptively collects information for a pillar node from its neighbors by iterative message passing, which effectively enlarges the receptive field of the pillar feature.",
    "original_url": "http://arxiv.org/pdf/2004.01389v1",
    "original_title": "LiDAR-based Online 3D Video Object Detection with Graph-based Message Passing and Spatiotemporal Transformer Attention",
    "source": "arxiv",
    "authors": [
      "Junbo Yin",
      "Jianbing Shen",
      "Chenye Guan",
      "Dingfu Zhou",
      "Ruigang Yang"
    ],
    "published": "2020-04-03T06:06:52+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2004.01389v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2004.01389v1_chunk_1",
    "chunk_text": "It adaptively collects information for a pillar node from its neighbors by iterative message passing, which effectively enlarges the receptive field of the pillar feature. In the latter component, we propose an Attentive Spatiotemporal Transformer GRU (AST-GRU) to aggregate the spatiotemporal information, which enhances the conventional ConvGRU with an attentive memory gating mechanism. AST-GRU contains a Spatial Transformer Attention (STA) module and a Temporal Transformer Attention (TTA) module, which can emphasize the foreground objects and align the dynamic objects, respectively. Experimental results demonstrate that the proposed 3D video object detector achieves state-of-the-art performance on the large-scale nuScenes benchmark.",
    "original_url": "http://arxiv.org/pdf/2004.01389v1",
    "original_title": "LiDAR-based Online 3D Video Object Detection with Graph-based Message Passing and Spatiotemporal Transformer Attention",
    "source": "arxiv",
    "authors": [
      "Junbo Yin",
      "Jianbing Shen",
      "Chenye Guan",
      "Dingfu Zhou",
      "Ruigang Yang"
    ],
    "published": "2020-04-03T06:06:52+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2004.01389v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2004.11207v2_chunk_0",
    "chunk_text": "Self-Attention Attribution: Interpreting Information Interactions Inside Transformer\n\nThe great success of Transformer-based models benefits from the powerful multi-head self-attention mechanism, which learns token dependencies and encodes contextual information from the input. Prior work strives to attribute model decisions to individual input features with different saliency measures, but they fail to explain how these input features interact with each other to reach predictions. In this paper, we propose a self-attention attribution method to interpret the information interactions inside Transformer. We take BERT as an example to conduct extensive studies. Firstly, we apply self-attention attribution to identify the important attention heads, while others can be pruned with marginal performance degradation.",
    "original_url": "http://arxiv.org/pdf/2004.11207v2",
    "original_title": "Self-Attention Attribution: Interpreting Information Interactions Inside Transformer",
    "source": "arxiv",
    "authors": [
      "Yaru Hao",
      "Li Dong",
      "Furu Wei",
      "Ke Xu"
    ],
    "published": "2020-04-23T14:58:22+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2004.11207v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2004.11207v2_chunk_1",
    "chunk_text": "Firstly, we apply self-attention attribution to identify the important attention heads, while others can be pruned with marginal performance degradation. Furthermore, we extract the most salient dependencies in each layer to construct an attribution tree, which reveals the hierarchical interactions inside Transformer. Finally, we show that the attribution results can be used as adversarial patterns to implement non-targeted attacks towards BERT.",
    "original_url": "http://arxiv.org/pdf/2004.11207v2",
    "original_title": "Self-Attention Attribution: Interpreting Information Interactions Inside Transformer",
    "source": "arxiv",
    "authors": [
      "Yaru Hao",
      "Li Dong",
      "Furu Wei",
      "Ke Xu"
    ],
    "published": "2020-04-23T14:58:22+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2004.11207v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2401.09596v4_chunk_0",
    "chunk_text": "Efficient generative adversarial networks using linear additive-attention Transformers\n\nAlthough the capacity of deep generative models for image generation, such as Diffusion Models (DMs) and Generative Adversarial Networks (GANs), has dramatically improved in recent years, much of their success can be attributed to computationally expensive architectures. This has limited their adoption and use to research laboratories and companies with large resources, while significantly raising the carbon footprint for training, fine-tuning, and inference. In this work, we present a novel GAN architecture which we call LadaGAN. This architecture is based on a linear attention Transformer block named Ladaformer. The main component of this block is a linear additive-attention mechanism that computes a single attention vector per head instead of the quadratic dot-product attention.",
    "original_url": "http://arxiv.org/pdf/2401.09596v4",
    "original_title": "Efficient generative adversarial networks using linear additive-attention Transformers",
    "source": "arxiv",
    "authors": [
      "Emilio Morales-Juarez",
      "Gibran Fuentes-Pineda"
    ],
    "published": "2024-01-17T21:08:41+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2401.09596v4"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2401.09596v4_chunk_1",
    "chunk_text": "The main component of this block is a linear additive-attention mechanism that computes a single attention vector per head instead of the quadratic dot-product attention. We employ Ladaformer in both the generator and discriminator, which reduces the computational complexity and overcomes the training instabilities often associated with Transformer GANs. LadaGAN consistently outperforms existing convolutional and Transformer GANs on benchmark datasets at different resolutions while being significantly more efficient. Moreover, LadaGAN shows competitive performance compared to state-of-the-art multi-step generative models (e.g. DMs) using orders of magnitude less computational resources.",
    "original_url": "http://arxiv.org/pdf/2401.09596v4",
    "original_title": "Efficient generative adversarial networks using linear additive-attention Transformers",
    "source": "arxiv",
    "authors": [
      "Emilio Morales-Juarez",
      "Gibran Fuentes-Pineda"
    ],
    "published": "2024-01-17T21:08:41+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2401.09596v4"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2401.09596v4_chunk_2",
    "chunk_text": "DMs) using orders of magnitude less computational resources.",
    "original_url": "http://arxiv.org/pdf/2401.09596v4",
    "original_title": "Efficient generative adversarial networks using linear additive-attention Transformers",
    "source": "arxiv",
    "authors": [
      "Emilio Morales-Juarez",
      "Gibran Fuentes-Pineda"
    ],
    "published": "2024-01-17T21:08:41+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2401.09596v4"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2408.07097v1_chunk_0",
    "chunk_text": "Attention Please: What Transformer Models Really Learn for Process Prediction\n\nPredictive process monitoring aims to support the execution of a process during runtime with various predictions about the further evolution of a process instance. In the last years a plethora of deep learning architectures have been established as state-of-the-art for different prediction targets, among others the transformer architecture. The transformer architecture is equipped with a powerful attention mechanism, assigning attention scores to each input part that allows to prioritize most relevant information leading to more accurate and contextual output. However, deep learning models largely represent a black box, i.e., their reasoning or decision-making process cannot be understood in detail. This paper examines whether the attention scores of a transformer based next-activity prediction model can serve as an explanation for its decision-making.",
    "original_url": "http://arxiv.org/pdf/2408.07097v1",
    "original_title": "Attention Please: What Transformer Models Really Learn for Process Prediction",
    "source": "arxiv",
    "authors": [
      "Martin Käppel",
      "Lars Ackermann",
      "Stefan Jablonski",
      "Simon Härtl"
    ],
    "published": "2024-08-12T08:20:38+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2408.07097v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2408.07097v1_chunk_1",
    "chunk_text": "This paper examines whether the attention scores of a transformer based next-activity prediction model can serve as an explanation for its decision-making. We find that attention scores in next-activity prediction models can serve as explainers and exploit this fact in two proposed graph-based explanation approaches. The gained insights could inspire future work on the improvement of predictive business process models as well as enabling a neural network based mining of process models from event logs.",
    "original_url": "http://arxiv.org/pdf/2408.07097v1",
    "original_title": "Attention Please: What Transformer Models Really Learn for Process Prediction",
    "source": "arxiv",
    "authors": [
      "Martin Käppel",
      "Lars Ackermann",
      "Stefan Jablonski",
      "Simon Härtl"
    ],
    "published": "2024-08-12T08:20:38+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2408.07097v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2502.06151v1_chunk_0",
    "chunk_text": "Powerformer: A Transformer with Weighted Causal Attention for Time-series Forecasting\n\nTransformers have recently shown strong performance in time-series forecasting, but their all-to-all attention mechanism overlooks the (temporal) causal and often (temporally) local nature of data. We introduce Powerformer, a novel Transformer variant that replaces noncausal attention weights with causal weights that are reweighted according to a smooth heavy-tailed decay. This simple yet effective modification endows the model with an inductive bias favoring temporally local dependencies, while still allowing sufficient flexibility to learn the unique correlation structure of each dataset. Our empirical results demonstrate that Powerformer not only achieves state-of-the-art accuracy on public time-series benchmarks, but also that it offers improved interpretability of attention patterns. Our analyses show that the model's locality bias is amplified during training, demonstrating an interplay between time-series data and power-law-based attention.",
    "original_url": "http://arxiv.org/pdf/2502.06151v1",
    "original_title": "Powerformer: A Transformer with Weighted Causal Attention for Time-series Forecasting",
    "source": "arxiv",
    "authors": [
      "Kareem Hegazy",
      "Michael W. Mahoney",
      "N. Benjamin Erichson"
    ],
    "published": "2025-02-10T04:42:11+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2502.06151v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2502.06151v1_chunk_1",
    "chunk_text": "Our analyses show that the model's locality bias is amplified during training, demonstrating an interplay between time-series data and power-law-based attention. These findings highlight the importance of domain-specific modifications to the Transformer architecture for time-series forecasting, and they establish Powerformer as a strong, efficient, and principled baseline for future research and real-world applications.",
    "original_url": "http://arxiv.org/pdf/2502.06151v1",
    "original_title": "Powerformer: A Transformer with Weighted Causal Attention for Time-series Forecasting",
    "source": "arxiv",
    "authors": [
      "Kareem Hegazy",
      "Michael W. Mahoney",
      "N. Benjamin Erichson"
    ],
    "published": "2025-02-10T04:42:11+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2502.06151v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2203.05736v1_chunk_0",
    "chunk_text": "Transformer-based Streaming ASR with Cumulative Attention\n\nIn this paper, we propose an online attention mechanism, known as cumulative attention (CA), for streaming Transformer-based automatic speech recognition (ASR). Inspired by monotonic chunkwise attention (MoChA) and head-synchronous decoder-end adaptive computation steps (HS-DACS) algorithms, CA triggers the ASR outputs based on the acoustic information accumulated at each encoding timestep, where the decisions are made using a trainable device, referred to as halting selector. In CA, all the attention heads of the same decoder layer are synchronised to have a unified halting position. This feature effectively alleviates the problem caused by the distinct behaviour of individual heads, which may otherwise give rise to severe latency issues as encountered by MoChA. The ASR experiments conducted on AIShell-1 and Librispeech datasets demonstrate that the proposed CA-based Transformer system can achieve on par or better performance with significant reduction in latency during inference, when compared to other streaming Transformer systems in literature.",
    "original_url": "http://arxiv.org/pdf/2203.05736v1",
    "original_title": "Transformer-based Streaming ASR with Cumulative Attention",
    "source": "arxiv",
    "authors": [
      "Mohan Li",
      "Shucong Zhang",
      "Catalin Zorila",
      "Rama Doddipatla"
    ],
    "published": "2022-03-11T03:22:37+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2203.05736v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2203.05736v1_chunk_1",
    "chunk_text": "The ASR experiments conducted on AIShell-1 and Librispeech datasets demonstrate that the proposed CA-based Transformer system can achieve on par or better performance with significant reduction in latency during inference, when compared to other streaming Transformer systems in literature.",
    "original_url": "http://arxiv.org/pdf/2203.05736v1",
    "original_title": "Transformer-based Streaming ASR with Cumulative Attention",
    "source": "arxiv",
    "authors": [
      "Mohan Li",
      "Shucong Zhang",
      "Catalin Zorila",
      "Rama Doddipatla"
    ],
    "published": "2022-03-11T03:22:37+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2203.05736v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2403.17729v2_chunk_0",
    "chunk_text": "EulerFormer: Sequential User Behavior Modeling with Complex Vector Attention\n\nTo capture user preference, transformer models have been widely applied to model sequential user behavior data. The core of transformer architecture lies in the self-attention mechanism, which computes the pairwise attention scores in a sequence. Due to the permutation-equivariant nature, positional encoding is used to enhance the attention between token representations. In this setting, the pairwise attention scores can be derived by both semantic difference and positional difference. However, prior studies often model the two kinds of difference measurements in different ways, which potentially limits the expressive capacity of sequence modeling.",
    "original_url": "http://arxiv.org/pdf/2403.17729v2",
    "original_title": "EulerFormer: Sequential User Behavior Modeling with Complex Vector Attention",
    "source": "arxiv",
    "authors": [
      "Zhen Tian",
      "Wayne Xin Zhao",
      "Changwang Zhang",
      "Xin Zhao",
      "Zhongrui Ma",
      "Ji-Rong Wen"
    ],
    "published": "2024-03-26T14:18:43+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2403.17729v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2403.17729v2_chunk_1",
    "chunk_text": "However, prior studies often model the two kinds of difference measurements in different ways, which potentially limits the expressive capacity of sequence modeling. To address this issue, this paper proposes a novel transformer variant with complex vector attention, named EulerFormer, which provides a unified theoretical framework to formulate both semantic difference and positional difference. The EulerFormer involves two key technical improvements. First, it employs a new transformation function for efficiently transforming the sequence tokens into polar-form complex vectors using Euler's formula, enabling the unified modeling of both semantic and positional information in a complex rotation form.Secondly, it develops a differential rotation mechanism, where the semantic rotation angles can be controlled by an adaptation function, enabling the adaptive integration of the semantic and positional information according to the semantic contexts.Furthermore, a phase contrastive learning task is proposed to improve the isotropy of contextual representations in EulerFormer. Our theoretical framework possesses a high degree of completeness and generality.",
    "original_url": "http://arxiv.org/pdf/2403.17729v2",
    "original_title": "EulerFormer: Sequential User Behavior Modeling with Complex Vector Attention",
    "source": "arxiv",
    "authors": [
      "Zhen Tian",
      "Wayne Xin Zhao",
      "Changwang Zhang",
      "Xin Zhao",
      "Zhongrui Ma",
      "Ji-Rong Wen"
    ],
    "published": "2024-03-26T14:18:43+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2403.17729v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2403.17729v2_chunk_2",
    "chunk_text": "Our theoretical framework possesses a high degree of completeness and generality. It is more robust to semantic variations and possesses moresuperior theoretical properties in principle. Extensive experiments conducted on four public datasets demonstrate the effectiveness and efficiency of our approach.",
    "original_url": "http://arxiv.org/pdf/2403.17729v2",
    "original_title": "EulerFormer: Sequential User Behavior Modeling with Complex Vector Attention",
    "source": "arxiv",
    "authors": [
      "Zhen Tian",
      "Wayne Xin Zhao",
      "Changwang Zhang",
      "Xin Zhao",
      "Zhongrui Ma",
      "Ji-Rong Wen"
    ],
    "published": "2024-03-26T14:18:43+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2403.17729v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2405.10480v2_chunk_0",
    "chunk_text": "Lean Attention: Hardware-Aware Scalable Attention Mechanism for the Decode-Phase of Transformers\n\nTransformer-based models have emerged as one of the most widely used architectures for natural language processing, natural language generation, and image generation. The size of the state-of-the-art models has increased steadily reaching billions of parameters. These huge models are memory hungry and incur significant inference latency even on cutting edge AI-accelerators, such as GPUs. Specifically, the time and memory complexity of the attention operation is quadratic in terms of the total context length, i.e., prompt and output tokens. Thus, several optimizations such as key-value tensor caching and FlashAttention computation have been proposed to deliver the low latency demands of applications relying on such large models.",
    "original_url": "http://arxiv.org/pdf/2405.10480v2",
    "original_title": "Lean Attention: Hardware-Aware Scalable Attention Mechanism for the Decode-Phase of Transformers",
    "source": "arxiv",
    "authors": [
      "Rya Sanovar",
      "Srikant Bharadwaj",
      "Renee St. Amant",
      "Victor Rühle",
      "Saravan Rajmohan"
    ],
    "published": "2024-05-17T00:52:39+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2405.10480v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2405.10480v2_chunk_1",
    "chunk_text": "Thus, several optimizations such as key-value tensor caching and FlashAttention computation have been proposed to deliver the low latency demands of applications relying on such large models. However, these techniques do not cater to the computationally distinct nature of different phases during inference. To that end, we propose LeanAttention, a scalable technique of computing self-attention for the token-generation phase (decode-phase) of decoder-only transformer models. LeanAttention enables scaling the attention mechanism implementation for the challenging case of long context lengths by re-designing the execution flow for the decode-phase. We identify that the associative property of online softmax can be treated as a reduction operation thus allowing us to parallelize the attention computation over these large context lengths.",
    "original_url": "http://arxiv.org/pdf/2405.10480v2",
    "original_title": "Lean Attention: Hardware-Aware Scalable Attention Mechanism for the Decode-Phase of Transformers",
    "source": "arxiv",
    "authors": [
      "Rya Sanovar",
      "Srikant Bharadwaj",
      "Renee St. Amant",
      "Victor Rühle",
      "Saravan Rajmohan"
    ],
    "published": "2024-05-17T00:52:39+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2405.10480v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2405.10480v2_chunk_2",
    "chunk_text": "We identify that the associative property of online softmax can be treated as a reduction operation thus allowing us to parallelize the attention computation over these large context lengths. We extend the \"stream-K\" style reduction of tiled calculation to self-attention to enable parallel computation resulting in an average of 2.6x attention execution speedup over FlashAttention-2 and up to 8.33x speedup for 512k context lengths.",
    "original_url": "http://arxiv.org/pdf/2405.10480v2",
    "original_title": "Lean Attention: Hardware-Aware Scalable Attention Mechanism for the Decode-Phase of Transformers",
    "source": "arxiv",
    "authors": [
      "Rya Sanovar",
      "Srikant Bharadwaj",
      "Renee St. Amant",
      "Victor Rühle",
      "Saravan Rajmohan"
    ],
    "published": "2024-05-17T00:52:39+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2405.10480v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2407.11306v1_chunk_0",
    "chunk_text": "PADRe: A Unifying Polynomial Attention Drop-in Replacement for Efficient Vision Transformer\n\nWe present Polynomial Attention Drop-in Replacement (PADRe), a novel and unifying framework designed to replace the conventional self-attention mechanism in transformer models. Notably, several recent alternative attention mechanisms, including Hyena, Mamba, SimA, Conv2Former, and Castling-ViT, can be viewed as specific instances of our PADRe framework. PADRe leverages polynomial functions and draws upon established results from approximation theory, enhancing computational efficiency without compromising accuracy. PADRe's key components include multiplicative nonlinearities, which we implement using straightforward, hardware-friendly operations such as Hadamard products, incurring only linear computational and memory costs. PADRe further avoids the need for using complex functions such as Softmax, yet it maintains comparable or superior accuracy compared to traditional self-attention.",
    "original_url": "http://arxiv.org/pdf/2407.11306v1",
    "original_title": "PADRe: A Unifying Polynomial Attention Drop-in Replacement for Efficient Vision Transformer",
    "source": "arxiv",
    "authors": [
      "Pierre-David Letourneau",
      "Manish Kumar Singh",
      "Hsin-Pai Cheng",
      "Shizhong Han",
      "Yunxiao Shi",
      "Dalton Jones",
      "Matthew Harper Langston",
      "Hong Cai",
      "Fatih Porikli"
    ],
    "published": "2024-07-16T01:45:44+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2407.11306v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2407.11306v1_chunk_1",
    "chunk_text": "PADRe further avoids the need for using complex functions such as Softmax, yet it maintains comparable or superior accuracy compared to traditional self-attention. We assess the effectiveness of PADRe as a drop-in replacement for self-attention across diverse computer vision tasks. These tasks include image classification, image-based 2D object detection, and 3D point cloud object detection. Empirical results demonstrate that PADRe runs significantly faster than the conventional self-attention (11x ~ 43x faster on server GPU and mobile NPU) while maintaining similar accuracy when substituting self-attention in the transformer models.",
    "original_url": "http://arxiv.org/pdf/2407.11306v1",
    "original_title": "PADRe: A Unifying Polynomial Attention Drop-in Replacement for Efficient Vision Transformer",
    "source": "arxiv",
    "authors": [
      "Pierre-David Letourneau",
      "Manish Kumar Singh",
      "Hsin-Pai Cheng",
      "Shizhong Han",
      "Yunxiao Shi",
      "Dalton Jones",
      "Matthew Harper Langston",
      "Hong Cai",
      "Fatih Porikli"
    ],
    "published": "2024-07-16T01:45:44+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2407.11306v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2502.01951v1_chunk_0",
    "chunk_text": "On the Emergence of Position Bias in Transformers\n\nRecent studies have revealed various manifestations of position bias in transformer architectures, from the \"lost-in-the-middle\" phenomenon to attention sinks, yet a comprehensive theoretical understanding of how attention masks and positional encodings shape these biases remains elusive. This paper introduces a novel graph-theoretic framework to analyze position bias in multi-layer attention. Modeling attention masks as directed graphs, we quantify how tokens interact with contextual information based on their sequential positions. We uncover two key insights: First, causal masking inherently biases attention toward earlier positions, as tokens in deeper layers attend to increasingly more contextualized representations of earlier tokens. Second, we characterize the competing effects of the causal mask and relative positional encodings, such as the decay mask and rotary positional encoding (RoPE): while both mechanisms introduce distance-based decay within individual attention maps, their aggregate effect across multiple attention layers -- coupled with the causal mask -- leads to a trade-off between the long-term decay effects and the cumulative importance of early sequence positions.",
    "original_url": "http://arxiv.org/pdf/2502.01951v1",
    "original_title": "On the Emergence of Position Bias in Transformers",
    "source": "arxiv",
    "authors": [
      "Xinyi Wu",
      "Yifei Wang",
      "Stefanie Jegelka",
      "Ali Jadbabaie"
    ],
    "published": "2025-02-04T02:53:07+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2502.01951v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2502.01951v1_chunk_1",
    "chunk_text": "Second, we characterize the competing effects of the causal mask and relative positional encodings, such as the decay mask and rotary positional encoding (RoPE): while both mechanisms introduce distance-based decay within individual attention maps, their aggregate effect across multiple attention layers -- coupled with the causal mask -- leads to a trade-off between the long-term decay effects and the cumulative importance of early sequence positions. Through controlled numerical experiments, we not only validate our theoretical findings but also reproduce position biases observed in real-world LLMs. Our framework offers a principled foundation for understanding positional biases in transformers, shedding light on the complex interplay of attention mechanism components and guiding more informed architectural design.",
    "original_url": "http://arxiv.org/pdf/2502.01951v1",
    "original_title": "On the Emergence of Position Bias in Transformers",
    "source": "arxiv",
    "authors": [
      "Xinyi Wu",
      "Yifei Wang",
      "Stefanie Jegelka",
      "Ali Jadbabaie"
    ],
    "published": "2025-02-04T02:53:07+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2502.01951v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2305.09880v4_chunk_0",
    "chunk_text": "A survey of the Vision Transformers and their CNN-Transformer based Variants\n\nVision transformers have become popular as a possible substitute to convolutional neural networks (CNNs) for a variety of computer vision applications. These transformers, with their ability to focus on global relationships in images, offer large learning capacity. However, they may suffer from limited generalization as they do not tend to model local correlation in images. Recently, in vision transformers hybridization of both the convolution operation and self-attention mechanism has emerged, to exploit both the local and global image representations. These hybrid vision transformers, also referred to as CNN-Transformer architectures, have demonstrated remarkable results in vision applications.",
    "original_url": "http://arxiv.org/pdf/2305.09880v4",
    "original_title": "A survey of the Vision Transformers and their CNN-Transformer based Variants",
    "source": "arxiv",
    "authors": [
      "Asifullah Khan",
      "Zunaira Rauf",
      "Anabia Sohail",
      "Abdul Rehman",
      "Hifsa Asif",
      "Aqsa Asif",
      "Umair Farooq"
    ],
    "published": "2023-05-17T01:27:27+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2305.09880v4"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2305.09880v4_chunk_1",
    "chunk_text": "These hybrid vision transformers, also referred to as CNN-Transformer architectures, have demonstrated remarkable results in vision applications. Given the rapidly growing number of hybrid vision transformers, it has become necessary to provide a taxonomy and explanation of these hybrid architectures. This survey presents a taxonomy of the recent vision transformer architectures and more specifically that of the hybrid vision transformers. Additionally, the key features of these architectures such as the attention mechanisms, positional embeddings, multi-scale processing, and convolution are also discussed. In contrast to the previous survey papers that are primarily focused on individual vision transformer architectures or CNNs, this survey uniquely emphasizes the emerging trend of hybrid vision transformers.",
    "original_url": "http://arxiv.org/pdf/2305.09880v4",
    "original_title": "A survey of the Vision Transformers and their CNN-Transformer based Variants",
    "source": "arxiv",
    "authors": [
      "Asifullah Khan",
      "Zunaira Rauf",
      "Anabia Sohail",
      "Abdul Rehman",
      "Hifsa Asif",
      "Aqsa Asif",
      "Umair Farooq"
    ],
    "published": "2023-05-17T01:27:27+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2305.09880v4"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2305.09880v4_chunk_2",
    "chunk_text": "In contrast to the previous survey papers that are primarily focused on individual vision transformer architectures or CNNs, this survey uniquely emphasizes the emerging trend of hybrid vision transformers. By showcasing the potential of hybrid vision transformers to deliver exceptional performance across a range of computer vision tasks, this survey sheds light on the future directions of this rapidly evolving architecture.",
    "original_url": "http://arxiv.org/pdf/2305.09880v4",
    "original_title": "A survey of the Vision Transformers and their CNN-Transformer based Variants",
    "source": "arxiv",
    "authors": [
      "Asifullah Khan",
      "Zunaira Rauf",
      "Anabia Sohail",
      "Abdul Rehman",
      "Hifsa Asif",
      "Aqsa Asif",
      "Umair Farooq"
    ],
    "published": "2023-05-17T01:27:27+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2305.09880v4"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2405.06598v2_chunk_0",
    "chunk_text": "A Lightweight Sparse Focus Transformer for Remote Sensing Image Change Captioning\n\nRemote sensing image change captioning (RSICC) aims to automatically generate sentences that describe content differences in remote sensing bitemporal images. Recently, attention-based transformers have become a prevalent idea for capturing the features of global change. However, existing transformer-based RSICC methods face challenges, e.g., high parameters and high computational complexity caused by the self-attention operation in the transformer encoder component. To alleviate these issues, this paper proposes a Sparse Focus Transformer (SFT) for the RSICC task. Specifically, the SFT network consists of three main components, i.e.",
    "original_url": "http://arxiv.org/pdf/2405.06598v2",
    "original_title": "A Lightweight Sparse Focus Transformer for Remote Sensing Image Change Captioning",
    "source": "arxiv",
    "authors": [
      "Dongwei Sun",
      "Yajie Bao",
      "Junmin Liu",
      "Xiangyong Cao"
    ],
    "published": "2024-05-10T16:56:53+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2405.06598v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2405.06598v2_chunk_1",
    "chunk_text": "Specifically, the SFT network consists of three main components, i.e. a high-level features extractor based on a convolutional neural network (CNN), a sparse focus attention mechanism-based transformer encoder network designed to locate and capture changing regions in dual-temporal images, and a description decoder that embeds images and words to generate sentences for captioning differences. The proposed SFT network can reduce the parameter number and computational complexity by incorporating a sparse attention mechanism within the transformer encoder network. Experimental results on various datasets demonstrate that even with a reduction of over 90\\% in parameters and computational complexity for the transformer encoder, our proposed network can still obtain competitive performance compared to other state-of-the-art RSICC methods. The code is available at \\href{https://github.com/sundongwei/SFT_chag2cap}{Lite\\_Chag2cap}.",
    "original_url": "http://arxiv.org/pdf/2405.06598v2",
    "original_title": "A Lightweight Sparse Focus Transformer for Remote Sensing Image Change Captioning",
    "source": "arxiv",
    "authors": [
      "Dongwei Sun",
      "Yajie Bao",
      "Junmin Liu",
      "Xiangyong Cao"
    ],
    "published": "2024-05-10T16:56:53+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2405.06598v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2405.06598v2_chunk_2",
    "chunk_text": "The code is available at \\href{https://github.com/sundongwei/SFT_chag2cap}{Lite\\_Chag2cap}.",
    "original_url": "http://arxiv.org/pdf/2405.06598v2",
    "original_title": "A Lightweight Sparse Focus Transformer for Remote Sensing Image Change Captioning",
    "source": "arxiv",
    "authors": [
      "Dongwei Sun",
      "Yajie Bao",
      "Junmin Liu",
      "Xiangyong Cao"
    ],
    "published": "2024-05-10T16:56:53+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2405.06598v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2103.00336v1_chunk_0",
    "chunk_text": "Transformers with Competitive Ensembles of Independent Mechanisms\n\nAn important development in deep learning from the earliest MLPs has been a move towards architectures with structural inductive biases which enable the model to keep distinct sources of information and routes of processing well-separated. This structure is linked to the notion of independent mechanisms from the causality literature, in which a mechanism is able to retain the same processing as irrelevant aspects of the world are changed. For example, convnets enable separation over positions, while attention-based architectures (especially Transformers) learn which combination of positions to process dynamically. In this work we explore a way in which the Transformer architecture is deficient: it represents each position with a large monolithic hidden representation and a single set of parameters which are applied over the entire hidden representation. This potentially throws unrelated sources of information together, and limits the Transformer's ability to capture independent mechanisms.",
    "original_url": "http://arxiv.org/pdf/2103.00336v1",
    "original_title": "Transformers with Competitive Ensembles of Independent Mechanisms",
    "source": "arxiv",
    "authors": [
      "Alex Lamb",
      "Di He",
      "Anirudh Goyal",
      "Guolin Ke",
      "Chien-Feng Liao",
      "Mirco Ravanelli",
      "Yoshua Bengio"
    ],
    "published": "2021-02-27T21:48:46+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2103.00336v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2103.00336v1_chunk_1",
    "chunk_text": "This potentially throws unrelated sources of information together, and limits the Transformer's ability to capture independent mechanisms. To address this, we propose Transformers with Independent Mechanisms (TIM), a new Transformer layer which divides the hidden representation and parameters into multiple mechanisms, which only exchange information through attention. Additionally, we propose a competition mechanism which encourages these mechanisms to specialize over time steps, and thus be more independent. We study TIM on a large-scale BERT model, on the Image Transformer, and on speech enhancement and find evidence for semantically meaningful specialization as well as improved performance.",
    "original_url": "http://arxiv.org/pdf/2103.00336v1",
    "original_title": "Transformers with Competitive Ensembles of Independent Mechanisms",
    "source": "arxiv",
    "authors": [
      "Alex Lamb",
      "Di He",
      "Anirudh Goyal",
      "Guolin Ke",
      "Chien-Feng Liao",
      "Mirco Ravanelli",
      "Yoshua Bengio"
    ],
    "published": "2021-02-27T21:48:46+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2103.00336v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2402.16003v2_chunk_0",
    "chunk_text": "Exploring the Power of Pure Attention Mechanisms in Blind Room Parameter Estimation\n\nDynamic parameterization of acoustic environments has drawn widespread attention in the field of audio processing. Precise representation of local room acoustic characteristics is crucial when designing audio filters for various audio rendering applications. Key parameters in this context include reverberation time (RT60) and geometric room volume. In recent years, neural networks have been extensively applied in the task of blind room parameter estimation. However, there remains a question of whether pure attention mechanisms can achieve superior performance in this task.",
    "original_url": "http://arxiv.org/pdf/2402.16003v2",
    "original_title": "Exploring the Power of Pure Attention Mechanisms in Blind Room Parameter Estimation",
    "source": "arxiv",
    "authors": [
      "Chunxi Wang",
      "Maoshen Jia",
      "Meiran Li",
      "Changchun Bao",
      "Wenyu Jin"
    ],
    "published": "2024-02-25T06:32:21+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2402.16003v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2402.16003v2_chunk_1",
    "chunk_text": "However, there remains a question of whether pure attention mechanisms can achieve superior performance in this task. To address this issue, this study employs blind room parameter estimation based on monaural noisy speech signals. Various model architectures are investigated, including a proposed attention-based model. This model is a convolution-free Audio Spectrogram Transformer, utilizing patch splitting, attention mechanisms, and cross-modality transfer learning from a pretrained Vision Transformer. Experimental results suggest that the proposed attention mechanism-based model, relying purely on attention mechanisms without using convolution, exhibits significantly improved performance across various room parameter estimation tasks, especially with the help of dedicated pretraining and data augmentation schemes.",
    "original_url": "http://arxiv.org/pdf/2402.16003v2",
    "original_title": "Exploring the Power of Pure Attention Mechanisms in Blind Room Parameter Estimation",
    "source": "arxiv",
    "authors": [
      "Chunxi Wang",
      "Maoshen Jia",
      "Meiran Li",
      "Changchun Bao",
      "Wenyu Jin"
    ],
    "published": "2024-02-25T06:32:21+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2402.16003v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2402.16003v2_chunk_2",
    "chunk_text": "Experimental results suggest that the proposed attention mechanism-based model, relying purely on attention mechanisms without using convolution, exhibits significantly improved performance across various room parameter estimation tasks, especially with the help of dedicated pretraining and data augmentation schemes. Additionally, the model demonstrates more advantageous adaptability and robustness when handling variable-length audio inputs compared to existing methods.",
    "original_url": "http://arxiv.org/pdf/2402.16003v2",
    "original_title": "Exploring the Power of Pure Attention Mechanisms in Blind Room Parameter Estimation",
    "source": "arxiv",
    "authors": [
      "Chunxi Wang",
      "Maoshen Jia",
      "Meiran Li",
      "Changchun Bao",
      "Wenyu Jin"
    ],
    "published": "2024-02-25T06:32:21+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2402.16003v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2210.08164v1_chunk_0",
    "chunk_text": "Linear Video Transformer with Feature Fixation\n\nVision Transformers have achieved impressive performance in video classification, while suffering from the quadratic complexity caused by the Softmax attention mechanism. Some studies alleviate the computational costs by reducing the number of tokens in attention calculation, but the complexity is still quadratic. Another promising way is to replace Softmax attention with linear attention, which owns linear complexity but presents a clear performance drop. We find that such a drop in linear attention results from the lack of attention concentration on critical features. Therefore, we propose a feature fixation module to reweight the feature importance of the query and key before computing linear attention.",
    "original_url": "http://arxiv.org/pdf/2210.08164v1",
    "original_title": "Linear Video Transformer with Feature Fixation",
    "source": "arxiv",
    "authors": [
      "Kaiyue Lu",
      "Zexiang Liu",
      "Jianyuan Wang",
      "Weixuan Sun",
      "Zhen Qin",
      "Dong Li",
      "Xuyang Shen",
      "Hui Deng",
      "Xiaodong Han",
      "Yuchao Dai",
      "Yiran Zhong"
    ],
    "published": "2022-10-15T02:20:50+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2210.08164v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2210.08164v1_chunk_1",
    "chunk_text": "Therefore, we propose a feature fixation module to reweight the feature importance of the query and key before computing linear attention. Specifically, we regard the query, key, and value as various latent representations of the input token, and learn the feature fixation ratio by aggregating Query-Key-Value information. This is beneficial for measuring the feature importance comprehensively. Furthermore, we enhance the feature fixation by neighborhood association, which leverages additional guidance from spatial and temporal neighbouring tokens. The proposed method significantly improves the linear attention baseline and achieves state-of-the-art performance among linear video Transformers on three popular video classification benchmarks.",
    "original_url": "http://arxiv.org/pdf/2210.08164v1",
    "original_title": "Linear Video Transformer with Feature Fixation",
    "source": "arxiv",
    "authors": [
      "Kaiyue Lu",
      "Zexiang Liu",
      "Jianyuan Wang",
      "Weixuan Sun",
      "Zhen Qin",
      "Dong Li",
      "Xuyang Shen",
      "Hui Deng",
      "Xiaodong Han",
      "Yuchao Dai",
      "Yiran Zhong"
    ],
    "published": "2022-10-15T02:20:50+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2210.08164v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2210.08164v1_chunk_2",
    "chunk_text": "The proposed method significantly improves the linear attention baseline and achieves state-of-the-art performance among linear video Transformers on three popular video classification benchmarks. With fewer parameters and higher efficiency, our performance is even comparable to some Softmax-based quadratic Transformers.",
    "original_url": "http://arxiv.org/pdf/2210.08164v1",
    "original_title": "Linear Video Transformer with Feature Fixation",
    "source": "arxiv",
    "authors": [
      "Kaiyue Lu",
      "Zexiang Liu",
      "Jianyuan Wang",
      "Weixuan Sun",
      "Zhen Qin",
      "Dong Li",
      "Xuyang Shen",
      "Hui Deng",
      "Xiaodong Han",
      "Yuchao Dai",
      "Yiran Zhong"
    ],
    "published": "2022-10-15T02:20:50+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2210.08164v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2405.05219v2_chunk_0",
    "chunk_text": "Conv-Basis: A New Paradigm for Efficient Attention Inference and Gradient Computation in Transformers\n\nThe self-attention mechanism is the key to the success of transformers in recent Large Language Models (LLMs). However, the quadratic computational cost $O(n^2)$ in the input sequence length $n$ is a notorious obstacle for further improvement and scalability in longer contexts. In this work, we leverage the convolution-like structure of attention matrices to develop an efficient approximation method for attention computation using convolution matrices. We propose a $\\mathsf{conv}$ basis system, analogous to the rank basis, and show that any lower triangular matrix can always be decomposed as a sum of structured convolution matrices in this basis. We then design a fast algorithm to approximate the attention matrix via a sum of such $k$ convolution matrices.",
    "original_url": "http://arxiv.org/pdf/2405.05219v2",
    "original_title": "Conv-Basis: A New Paradigm for Efficient Attention Inference and Gradient Computation in Transformers",
    "source": "arxiv",
    "authors": [
      "Yingyu Liang",
      "Heshan Liu",
      "Zhenmei Shi",
      "Zhao Song",
      "Zhuoyan Xu",
      "Junze Yin"
    ],
    "published": "2024-05-08T17:11:38+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2405.05219v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2405.05219v2_chunk_1",
    "chunk_text": "We then design a fast algorithm to approximate the attention matrix via a sum of such $k$ convolution matrices. This allows us to compute the attention {\\it inference} via Fast Fourier Transforms (FFT) in $O(knd \\log n)$ time, where $d$ is the hidden dimension, and thus achieve almost linear time $n^{1+o(1)}$ in the practical scenario where $kd = n^{o(1)}$. Furthermore, the attention {\\it training forward} and {\\it backward gradient} can be computed in $n^{1+o(1)}$ as well. We provide theoretical guarantees on the run time and approximation error and conduct preliminary experiments to evaluate its effectiveness. We hope our new paradigm for accelerating attention computation in transformer models can help their application to longer contexts.",
    "original_url": "http://arxiv.org/pdf/2405.05219v2",
    "original_title": "Conv-Basis: A New Paradigm for Efficient Attention Inference and Gradient Computation in Transformers",
    "source": "arxiv",
    "authors": [
      "Yingyu Liang",
      "Heshan Liu",
      "Zhenmei Shi",
      "Zhao Song",
      "Zhuoyan Xu",
      "Junze Yin"
    ],
    "published": "2024-05-08T17:11:38+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2405.05219v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2405.05219v2_chunk_2",
    "chunk_text": "We hope our new paradigm for accelerating attention computation in transformer models can help their application to longer contexts.",
    "original_url": "http://arxiv.org/pdf/2405.05219v2",
    "original_title": "Conv-Basis: A New Paradigm for Efficient Attention Inference and Gradient Computation in Transformers",
    "source": "arxiv",
    "authors": [
      "Yingyu Liang",
      "Heshan Liu",
      "Zhenmei Shi",
      "Zhao Song",
      "Zhuoyan Xu",
      "Junze Yin"
    ],
    "published": "2024-05-08T17:11:38+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2405.05219v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2504.13480v1_chunk_0",
    "chunk_text": "Integrating Locality-Aware Attention with Transformers for General Geometry PDEs\n\nNeural operators have emerged as promising frameworks for learning mappings governed by partial differential equations (PDEs), serving as data-driven alternatives to traditional numerical methods. While methods such as the Fourier neural operator (FNO) have demonstrated notable performance, their reliance on uniform grids restricts their applicability to complex geometries and irregular meshes. Recently, Transformer-based neural operators with linear attention mechanisms have shown potential in overcoming these limitations for large-scale PDE simulations. However, these approaches predominantly emphasize global feature aggregation, often overlooking fine-scale dynamics and localized PDE behaviors essential for accurate solutions. To address these challenges, we propose the Locality-Aware Attention Transformer (LA2Former), which leverages K-nearest neighbors for dynamic patchifying and integrates global-local attention for enhanced PDE modeling.",
    "original_url": "http://arxiv.org/pdf/2504.13480v1",
    "original_title": "Integrating Locality-Aware Attention with Transformers for General Geometry PDEs",
    "source": "arxiv",
    "authors": [
      "Minsu Koh",
      "Beom-Chul Park",
      "Heejo Kong",
      "Seong-Whan Lee"
    ],
    "published": "2025-04-18T05:43:49+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2504.13480v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2504.13480v1_chunk_1",
    "chunk_text": "To address these challenges, we propose the Locality-Aware Attention Transformer (LA2Former), which leverages K-nearest neighbors for dynamic patchifying and integrates global-local attention for enhanced PDE modeling. By combining linear attention for efficient global context encoding with pairwise attention for capturing intricate local interactions, LA2Former achieves an optimal balance between computational efficiency and predictive accuracy. Extensive evaluations across six benchmark datasets demonstrate that LA2Former improves predictive accuracy by over 50% relative to existing linear attention methods, while also outperforming full pairwise attention under optimal conditions. This work underscores the critical importance of localized feature learning in advancing Transformer-based neural operators for solving PDEs on complex and irregular domains.",
    "original_url": "http://arxiv.org/pdf/2504.13480v1",
    "original_title": "Integrating Locality-Aware Attention with Transformers for General Geometry PDEs",
    "source": "arxiv",
    "authors": [
      "Minsu Koh",
      "Beom-Chul Park",
      "Heejo Kong",
      "Seong-Whan Lee"
    ],
    "published": "2025-04-18T05:43:49+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2504.13480v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2202.01381v2_chunk_0",
    "chunk_text": "ETSformer: Exponential Smoothing Transformers for Time-series Forecasting\n\nTransformers have been actively studied for time-series forecasting in recent years. While often showing promising results in various scenarios, traditional Transformers are not designed to fully exploit the characteristics of time-series data and thus suffer some fundamental limitations, e.g., they generally lack of decomposition capability and interpretability, and are neither effective nor efficient for long-term forecasting. In this paper, we propose ETSFormer, a novel time-series Transformer architecture, which exploits the principle of exponential smoothing in improving Transformers for time-series forecasting. In particular, inspired by the classical exponential smoothing methods in time-series forecasting, we propose the novel exponential smoothing attention (ESA) and frequency attention (FA) to replace the self-attention mechanism in vanilla Transformers, thus improving both accuracy and efficiency. Based on these, we redesign the Transformer architecture with modular decomposition blocks such that it can learn to decompose the time-series data into interpretable time-series components such as level, growth and seasonality.",
    "original_url": "http://arxiv.org/pdf/2202.01381v2",
    "original_title": "ETSformer: Exponential Smoothing Transformers for Time-series Forecasting",
    "source": "arxiv",
    "authors": [
      "Gerald Woo",
      "Chenghao Liu",
      "Doyen Sahoo",
      "Akshat Kumar",
      "Steven Hoi"
    ],
    "published": "2022-02-03T02:50:44+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2202.01381v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2202.01381v2_chunk_1",
    "chunk_text": "Based on these, we redesign the Transformer architecture with modular decomposition blocks such that it can learn to decompose the time-series data into interpretable time-series components such as level, growth and seasonality. Extensive experiments on various time-series benchmarks validate the efficacy and advantages of the proposed method. Code is available at https://github.com/salesforce/ETSformer.",
    "original_url": "http://arxiv.org/pdf/2202.01381v2",
    "original_title": "ETSformer: Exponential Smoothing Transformers for Time-series Forecasting",
    "source": "arxiv",
    "authors": [
      "Gerald Woo",
      "Chenghao Liu",
      "Doyen Sahoo",
      "Akshat Kumar",
      "Steven Hoi"
    ],
    "published": "2022-02-03T02:50:44+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2202.01381v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2501.18401v2_chunk_0",
    "chunk_text": "MatIR: A Hybrid Mamba-Transformer Image Restoration Model\n\nIn recent years, Transformers-based models have made significant progress in the field of image restoration by leveraging their inherent ability to capture complex contextual features. Recently, Mamba models have made a splash in the field of computer vision due to their ability to handle long-range dependencies and their significant computational efficiency compared to Transformers. However, Mamba currently lags behind Transformers in contextual learning capabilities. To overcome the limitations of these two models, we propose a Mamba-Transformer hybrid image restoration model called MatIR. Specifically, MatIR cross-cycles the blocks of the Transformer layer and the Mamba layer to extract features, thereby taking full advantage of the advantages of the two architectures.",
    "original_url": "http://arxiv.org/pdf/2501.18401v2",
    "original_title": "MatIR: A Hybrid Mamba-Transformer Image Restoration Model",
    "source": "arxiv",
    "authors": [
      "Juan Wen",
      "Weiyan Hou",
      "Luc Van Gool",
      "Radu Timofte"
    ],
    "published": "2025-01-30T14:55:40+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2501.18401v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2501.18401v2_chunk_1",
    "chunk_text": "Specifically, MatIR cross-cycles the blocks of the Transformer layer and the Mamba layer to extract features, thereby taking full advantage of the advantages of the two architectures. In the Mamba module, we introduce the Image Inpainting State Space (IRSS) module, which traverses along four scan paths to achieve efficient processing of long sequence data. In the Transformer module, we combine triangular window-based local attention with channel-based global attention to effectively activate the attention mechanism over a wider range of image pixels. Extensive experimental results and ablation studies demonstrate the effectiveness of our approach.",
    "original_url": "http://arxiv.org/pdf/2501.18401v2",
    "original_title": "MatIR: A Hybrid Mamba-Transformer Image Restoration Model",
    "source": "arxiv",
    "authors": [
      "Juan Wen",
      "Weiyan Hou",
      "Luc Van Gool",
      "Radu Timofte"
    ],
    "published": "2025-01-30T14:55:40+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2501.18401v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1803.10916v1_chunk_0",
    "chunk_text": "Attention-based End-to-End Models for Small-Footprint Keyword Spotting\n\nIn this paper, we propose an attention-based end-to-end neural approach for small-footprint keyword spotting (KWS), which aims to simplify the pipelines of building a production-quality KWS system. Our model consists of an encoder and an attention mechanism. The encoder transforms the input signal into a high level representation using RNNs. Then the attention mechanism weights the encoder features and generates a fixed-length vector. Finally, by linear transformation and softmax function, the vector becomes a score used for keyword detection.",
    "original_url": "http://arxiv.org/pdf/1803.10916v1",
    "original_title": "Attention-based End-to-End Models for Small-Footprint Keyword Spotting",
    "source": "arxiv",
    "authors": [
      "Changhao Shan",
      "Junbo Zhang",
      "Yujun Wang",
      "Lei Xie"
    ],
    "published": "2018-03-29T03:32:59+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1803.10916v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1803.10916v1_chunk_1",
    "chunk_text": "Finally, by linear transformation and softmax function, the vector becomes a score used for keyword detection. We also evaluate the performance of different encoder architectures, including LSTM, GRU and CRNN. Experiments on real-world wake-up data show that our approach outperforms the recent Deep KWS approach by a large margin and the best performance is achieved by CRNN. To be more specific, with ~84K parameters, our attention-based model achieves 1.02% false rejection rate (FRR) at 1.0 false alarm (FA) per hour.",
    "original_url": "http://arxiv.org/pdf/1803.10916v1",
    "original_title": "Attention-based End-to-End Models for Small-Footprint Keyword Spotting",
    "source": "arxiv",
    "authors": [
      "Changhao Shan",
      "Junbo Zhang",
      "Yujun Wang",
      "Lei Xie"
    ],
    "published": "2018-03-29T03:32:59+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1803.10916v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2101.07448v1_chunk_0",
    "chunk_text": "Fast Convergence of DETR with Spatially Modulated Co-Attention\n\nThe recently proposed Detection Transformer (DETR) model successfully applies Transformer to objects detection and achieves comparable performance with two-stage object detection frameworks, such as Faster-RCNN. However, DETR suffers from its slow convergence. Training DETR \\cite{carion2020end} from scratch needs 500 epochs to achieve a high accuracy. To accelerate its convergence, we propose a simple yet effective scheme for improving the DETR framework, namely Spatially Modulated Co-Attention (SMCA) mechanism. The core idea of SMCA is to conduct regression-aware co-attention in DETR by constraining co-attention responses to be high near initially estimated bounding box locations.",
    "original_url": "http://arxiv.org/pdf/2101.07448v1",
    "original_title": "Fast Convergence of DETR with Spatially Modulated Co-Attention",
    "source": "arxiv",
    "authors": [
      "Peng Gao",
      "Minghang Zheng",
      "Xiaogang Wang",
      "Jifeng Dai",
      "Hongsheng Li"
    ],
    "published": "2021-01-19T03:52:44+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2101.07448v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2101.07448v1_chunk_1",
    "chunk_text": "The core idea of SMCA is to conduct regression-aware co-attention in DETR by constraining co-attention responses to be high near initially estimated bounding box locations. Our proposed SMCA increases DETR's convergence speed by replacing the original co-attention mechanism in the decoder while keeping other operations in DETR unchanged. Furthermore, by integrating multi-head and scale-selection attention designs into SMCA, our fully-fledged SMCA can achieve better performance compared to DETR with a dilated convolution-based backbone (45.6 mAP at 108 epochs vs. 43.3 mAP at 500 epochs). We perform extensive ablation studies on COCO dataset to validate the effectiveness of the proposed SMCA.",
    "original_url": "http://arxiv.org/pdf/2101.07448v1",
    "original_title": "Fast Convergence of DETR with Spatially Modulated Co-Attention",
    "source": "arxiv",
    "authors": [
      "Peng Gao",
      "Minghang Zheng",
      "Xiaogang Wang",
      "Jifeng Dai",
      "Hongsheng Li"
    ],
    "published": "2021-01-19T03:52:44+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2101.07448v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2108.02404v1_chunk_0",
    "chunk_text": "Fast Convergence of DETR with Spatially Modulated Co-Attention\n\nThe recently proposed Detection Transformer (DETR) model successfully applies Transformer to objects detection and achieves comparable performance with two-stage object detection frameworks, such as Faster-RCNN. However, DETR suffers from its slow convergence. Training DETR from scratch needs 500 epochs to achieve a high accuracy. To accelerate its convergence, we propose a simple yet effective scheme for improving the DETR framework, namely Spatially Modulated Co-Attention (SMCA) mechanism. The core idea of SMCA is to conduct location-aware co-attention in DETR by constraining co-attention responses to be high near initially estimated bounding box locations.",
    "original_url": "http://arxiv.org/pdf/2108.02404v1",
    "original_title": "Fast Convergence of DETR with Spatially Modulated Co-Attention",
    "source": "arxiv",
    "authors": [
      "Peng Gao",
      "Minghang Zheng",
      "Xiaogang Wang",
      "Jifeng Dai",
      "Hongsheng Li"
    ],
    "published": "2021-08-05T06:53:19+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2108.02404v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2108.02404v1_chunk_1",
    "chunk_text": "The core idea of SMCA is to conduct location-aware co-attention in DETR by constraining co-attention responses to be high near initially estimated bounding box locations. Our proposed SMCA increases DETR's convergence speed by replacing the original co-attention mechanism in the decoder while keeping other operations in DETR unchanged. Furthermore, by integrating multi-head and scale-selection attention designs into SMCA, our fully-fledged SMCA can achieve better performance compared to DETR with a dilated convolution-based backbone (45.6 mAP at 108 epochs vs. 43.3 mAP at 500 epochs). We perform extensive ablation studies on COCO dataset to validate SMCA. Code is released at https://github.com/gaopengcuhk/SMCA-DETR .",
    "original_url": "http://arxiv.org/pdf/2108.02404v1",
    "original_title": "Fast Convergence of DETR with Spatially Modulated Co-Attention",
    "source": "arxiv",
    "authors": [
      "Peng Gao",
      "Minghang Zheng",
      "Xiaogang Wang",
      "Jifeng Dai",
      "Hongsheng Li"
    ],
    "published": "2021-08-05T06:53:19+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2108.02404v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2108.02404v1_chunk_2",
    "chunk_text": "Code is released at https://github.com/gaopengcuhk/SMCA-DETR .",
    "original_url": "http://arxiv.org/pdf/2108.02404v1",
    "original_title": "Fast Convergence of DETR with Spatially Modulated Co-Attention",
    "source": "arxiv",
    "authors": [
      "Peng Gao",
      "Minghang Zheng",
      "Xiaogang Wang",
      "Jifeng Dai",
      "Hongsheng Li"
    ],
    "published": "2021-08-05T06:53:19+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2108.02404v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2109.05853v1_chunk_0",
    "chunk_text": "Attention Weights in Transformer NMT Fail Aligning Words Between Sequences but Largely Explain Model Predictions\n\nThis work proposes an extensive analysis of the Transformer architecture in the Neural Machine Translation (NMT) setting. Focusing on the encoder-decoder attention mechanism, we prove that attention weights systematically make alignment errors by relying mainly on uninformative tokens from the source sequence. However, we observe that NMT models assign attention to these tokens to regulate the contribution in the prediction of the two contexts, the source and the prefix of the target sequence. We provide evidence about the influence of wrong alignments on the model behavior, demonstrating that the encoder-decoder attention mechanism is well suited as an interpretability method for NMT. Finally, based on our analysis, we propose methods that largely reduce the word alignment error rate compared to standard induced alignments from attention weights.",
    "original_url": "http://arxiv.org/pdf/2109.05853v1",
    "original_title": "Attention Weights in Transformer NMT Fail Aligning Words Between Sequences but Largely Explain Model Predictions",
    "source": "arxiv",
    "authors": [
      "Javier Ferrando",
      "Marta R. Costa-jussà"
    ],
    "published": "2021-09-13T10:44:02+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2109.05853v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2109.05853v1_chunk_1",
    "chunk_text": "Finally, based on our analysis, we propose methods that largely reduce the word alignment error rate compared to standard induced alignments from attention weights.",
    "original_url": "http://arxiv.org/pdf/2109.05853v1",
    "original_title": "Attention Weights in Transformer NMT Fail Aligning Words Between Sequences but Largely Explain Model Predictions",
    "source": "arxiv",
    "authors": [
      "Javier Ferrando",
      "Marta R. Costa-jussà"
    ],
    "published": "2021-09-13T10:44:02+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2109.05853v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2303.12398v4_chunk_0",
    "chunk_text": "Multiscale Attention via Wavelet Neural Operators for Vision Transformers\n\nTransformers have achieved widespread success in computer vision. At their heart, there is a Self-Attention (SA) mechanism, an inductive bias that associates each token in the input with every other token through a weighted basis. The standard SA mechanism has quadratic complexity with the sequence length, which impedes its utility to long sequences appearing in high resolution vision. Recently, inspired by operator learning for PDEs, Adaptive Fourier Neural Operators (AFNO) were introduced for high resolution attention based on global convolution that is efficiently implemented via FFT. However, the AFNO global filtering cannot well represent small and moderate scale structures that commonly appear in natural images.",
    "original_url": "http://arxiv.org/pdf/2303.12398v4",
    "original_title": "Multiscale Attention via Wavelet Neural Operators for Vision Transformers",
    "source": "arxiv",
    "authors": [
      "Anahita Nekoozadeh",
      "Mohammad Reza Ahmadzadeh",
      "Zahra Mardani"
    ],
    "published": "2023-03-22T09:06:07+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2303.12398v4"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2303.12398v4_chunk_1",
    "chunk_text": "However, the AFNO global filtering cannot well represent small and moderate scale structures that commonly appear in natural images. To leverage the coarse-to-fine scale structures we introduce a Multiscale Wavelet Attention (MWA) by leveraging wavelet neural operators which incurs linear complexity in the sequence size. We replace the attention in ViT with MWA and our experiments with CIFAR and Tiny-ImageNet classification demonstrate significant improvement over alternative Fourier-based attentions such as AFNO and Global Filter Network (GFN).",
    "original_url": "http://arxiv.org/pdf/2303.12398v4",
    "original_title": "Multiscale Attention via Wavelet Neural Operators for Vision Transformers",
    "source": "arxiv",
    "authors": [
      "Anahita Nekoozadeh",
      "Mohammad Reza Ahmadzadeh",
      "Zahra Mardani"
    ],
    "published": "2023-03-22T09:06:07+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2303.12398v4"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2306.03175v1_chunk_0",
    "chunk_text": "Infusing Lattice Symmetry Priors in Attention Mechanisms for Sample-Efficient Abstract Geometric Reasoning\n\nThe Abstraction and Reasoning Corpus (ARC) (Chollet, 2019) and its most recent language-complete instantiation (LARC) has been postulated as an important step towards general AI. Yet, even state-of-the-art machine learning models struggle to achieve meaningful performance on these problems, falling behind non-learning based approaches. We argue that solving these tasks requires extreme generalization that can only be achieved by proper accounting for core knowledge priors. As a step towards this goal, we focus on geometry priors and introduce LatFormer, a model that incorporates lattice symmetry priors in attention masks. We show that, for any transformation of the hypercubic lattice, there exists a binary attention mask that implements that group action.",
    "original_url": "http://arxiv.org/pdf/2306.03175v1",
    "original_title": "Infusing Lattice Symmetry Priors in Attention Mechanisms for Sample-Efficient Abstract Geometric Reasoning",
    "source": "arxiv",
    "authors": [
      "Mattia Atzeni",
      "Mrinmaya Sachan",
      "Andreas Loukas"
    ],
    "published": "2023-06-05T18:32:53+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2306.03175v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2306.03175v1_chunk_1",
    "chunk_text": "We show that, for any transformation of the hypercubic lattice, there exists a binary attention mask that implements that group action. Hence, our study motivates a modification to the standard attention mechanism, where attention weights are scaled using soft masks generated by a convolutional network. Experiments on synthetic geometric reasoning show that LatFormer requires 2 orders of magnitude fewer data than standard attention and transformers. Moreover, our results on ARC and LARC tasks that incorporate geometric priors provide preliminary evidence that these complex datasets do not lie out of the reach of deep learning models.",
    "original_url": "http://arxiv.org/pdf/2306.03175v1",
    "original_title": "Infusing Lattice Symmetry Priors in Attention Mechanisms for Sample-Efficient Abstract Geometric Reasoning",
    "source": "arxiv",
    "authors": [
      "Mattia Atzeni",
      "Mrinmaya Sachan",
      "Andreas Loukas"
    ],
    "published": "2023-06-05T18:32:53+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2306.03175v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2109.02497v2_chunk_0",
    "chunk_text": "Voxel Transformer for 3D Object Detection\n\nWe present Voxel Transformer (VoTr), a novel and effective voxel-based Transformer backbone for 3D object detection from point clouds. Conventional 3D convolutional backbones in voxel-based 3D detectors cannot efficiently capture large context information, which is crucial for object recognition and localization, owing to the limited receptive fields. In this paper, we resolve the problem by introducing a Transformer-based architecture that enables long-range relationships between voxels by self-attention. Given the fact that non-empty voxels are naturally sparse but numerous, directly applying standard Transformer on voxels is non-trivial. To this end, we propose the sparse voxel module and the submanifold voxel module, which can operate on the empty and non-empty voxel positions effectively.",
    "original_url": "http://arxiv.org/pdf/2109.02497v2",
    "original_title": "Voxel Transformer for 3D Object Detection",
    "source": "arxiv",
    "authors": [
      "Jiageng Mao",
      "Yujing Xue",
      "Minzhe Niu",
      "Haoyue Bai",
      "Jiashi Feng",
      "Xiaodan Liang",
      "Hang Xu",
      "Chunjing Xu"
    ],
    "published": "2021-09-06T14:10:22+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2109.02497v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2109.02497v2_chunk_1",
    "chunk_text": "To this end, we propose the sparse voxel module and the submanifold voxel module, which can operate on the empty and non-empty voxel positions effectively. To further enlarge the attention range while maintaining comparable computational overhead to the convolutional counterparts, we propose two attention mechanisms for multi-head attention in those two modules: Local Attention and Dilated Attention, and we further propose Fast Voxel Query to accelerate the querying process in multi-head attention. VoTr contains a series of sparse and submanifold voxel modules and can be applied in most voxel-based detectors. Our proposed VoTr shows consistent improvement over the convolutional baselines while maintaining computational efficiency on the KITTI dataset and the Waymo Open dataset.",
    "original_url": "http://arxiv.org/pdf/2109.02497v2",
    "original_title": "Voxel Transformer for 3D Object Detection",
    "source": "arxiv",
    "authors": [
      "Jiageng Mao",
      "Yujing Xue",
      "Minzhe Niu",
      "Haoyue Bai",
      "Jiashi Feng",
      "Xiaodan Liang",
      "Hang Xu",
      "Chunjing Xu"
    ],
    "published": "2021-09-06T14:10:22+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2109.02497v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2203.03937v4_chunk_0",
    "chunk_text": "Dynamic Group Transformer: A General Vision Transformer Backbone with Dynamic Group Attention\n\nRecently, Transformers have shown promising performance in various vision tasks. To reduce the quadratic computation complexity caused by each query attending to all keys/values, various methods have constrained the range of attention within local regions, where each query only attends to keys/values within a hand-crafted window. However, these hand-crafted window partition mechanisms are data-agnostic and ignore their input content, so it is likely that one query maybe attends to irrelevant keys/values. To address this issue, we propose a Dynamic Group Attention (DG-Attention), which dynamically divides all queries into multiple groups and selects the most relevant keys/values for each group. Our DG-Attention can flexibly model more relevant dependencies without any spatial constraint that is used in hand-crafted window based attention.",
    "original_url": "http://arxiv.org/pdf/2203.03937v4",
    "original_title": "Dynamic Group Transformer: A General Vision Transformer Backbone with Dynamic Group Attention",
    "source": "arxiv",
    "authors": [
      "Kai Liu",
      "Tianyi Wu",
      "Cong Liu",
      "Guodong Guo"
    ],
    "published": "2022-03-08T09:01:41+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2203.03937v4"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2203.03937v4_chunk_1",
    "chunk_text": "Our DG-Attention can flexibly model more relevant dependencies without any spatial constraint that is used in hand-crafted window based attention. Built on the DG-Attention, we develop a general vision transformer backbone named Dynamic Group Transformer (DGT). Extensive experiments show that our models can outperform the state-of-the-art methods on multiple common vision tasks, including image classification, semantic segmentation, object detection, and instance segmentation.",
    "original_url": "http://arxiv.org/pdf/2203.03937v4",
    "original_title": "Dynamic Group Transformer: A General Vision Transformer Backbone with Dynamic Group Attention",
    "source": "arxiv",
    "authors": [
      "Kai Liu",
      "Tianyi Wu",
      "Cong Liu",
      "Guodong Guo"
    ],
    "published": "2022-03-08T09:01:41+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2203.03937v4"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2303.15190v1_chunk_0",
    "chunk_text": "Evaluating self-attention interpretability through human-grounded experimental protocol\n\nAttention mechanisms have played a crucial role in the development of complex architectures such as Transformers in natural language processing. However, Transformers remain hard to interpret and are considered as black-boxes. This paper aims to assess how attention coefficients from Transformers can help in providing interpretability. A new attention-based interpretability method called CLaSsification-Attention (CLS-A) is proposed. CLS-A computes an interpretability score for each word based on the attention coefficient distribution related to the part specific to the classification task within the Transformer architecture.",
    "original_url": "http://arxiv.org/pdf/2303.15190v1",
    "original_title": "Evaluating self-attention interpretability through human-grounded experimental protocol",
    "source": "arxiv",
    "authors": [
      "Milan Bhan",
      "Nina Achache",
      "Victor Legrand",
      "Annabelle Blangero",
      "Nicolas Chesneau"
    ],
    "published": "2023-03-27T13:26:02+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2303.15190v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2303.15190v1_chunk_1",
    "chunk_text": "CLS-A computes an interpretability score for each word based on the attention coefficient distribution related to the part specific to the classification task within the Transformer architecture. A human-grounded experiment is conducted to evaluate and compare CLS-A to other interpretability methods. The experimental protocol relies on the capacity of an interpretability method to provide explanation in line with human reasoning. Experiment design includes measuring reaction times and correct response rates by human subjects. CLS-A performs comparably to usual interpretability methods regarding average participant reaction time and accuracy.",
    "original_url": "http://arxiv.org/pdf/2303.15190v1",
    "original_title": "Evaluating self-attention interpretability through human-grounded experimental protocol",
    "source": "arxiv",
    "authors": [
      "Milan Bhan",
      "Nina Achache",
      "Victor Legrand",
      "Annabelle Blangero",
      "Nicolas Chesneau"
    ],
    "published": "2023-03-27T13:26:02+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2303.15190v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2303.15190v1_chunk_2",
    "chunk_text": "CLS-A performs comparably to usual interpretability methods regarding average participant reaction time and accuracy. The lower computational cost of CLS-A compared to other interpretability methods and its availability by design within the classifier make it particularly interesting. Data analysis also highlights the link between the probability score of a classifier prediction and adequate explanations. Finally, our work confirms the relevancy of the use of CLS-A and shows to which extent self-attention contains rich information to explain Transformer classifiers.",
    "original_url": "http://arxiv.org/pdf/2303.15190v1",
    "original_title": "Evaluating self-attention interpretability through human-grounded experimental protocol",
    "source": "arxiv",
    "authors": [
      "Milan Bhan",
      "Nina Achache",
      "Victor Legrand",
      "Annabelle Blangero",
      "Nicolas Chesneau"
    ],
    "published": "2023-03-27T13:26:02+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2303.15190v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2311.09376v1_chunk_0",
    "chunk_text": "DISTA: Denoising Spiking Transformer with intrinsic plasticity and spatiotemporal attention\n\nAmong the array of neural network architectures, the Vision Transformer (ViT) stands out as a prominent choice, acclaimed for its exceptional expressiveness and consistent high performance in various vision applications. Recently, the emerging Spiking ViT approach has endeavored to harness spiking neurons, paving the way for a more brain-inspired transformer architecture that thrives in ultra-low power operations on dedicated neuromorphic hardware. Nevertheless, this approach remains confined to spatial self-attention and doesn't fully unlock the potential of spiking neural networks. We introduce DISTA, a Denoising Spiking Transformer with Intrinsic Plasticity and SpatioTemporal Attention, designed to maximize the spatiotemporal computational prowess of spiking neurons, particularly for vision applications. DISTA explores two types of spatiotemporal attentions: intrinsic neuron-level attention and network-level attention with explicit memory.",
    "original_url": "http://arxiv.org/pdf/2311.09376v1",
    "original_title": "DISTA: Denoising Spiking Transformer with intrinsic plasticity and spatiotemporal attention",
    "source": "arxiv",
    "authors": [
      "Boxun Xu",
      "Hejia Geng",
      "Yuxuan Yin",
      "Peng Li"
    ],
    "published": "2023-11-15T21:09:08+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2311.09376v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2311.09376v1_chunk_1",
    "chunk_text": "DISTA explores two types of spatiotemporal attentions: intrinsic neuron-level attention and network-level attention with explicit memory. Additionally, DISTA incorporates an efficient nonlinear denoising mechanism to quell the noise inherent in computed spatiotemporal attention maps, thereby resulting in further performance gains. Our DISTA transformer undergoes joint training involving synaptic plasticity (i.e., weight tuning) and intrinsic plasticity (i.e., membrane time constant tuning) and delivers state-of-the-art performances across several static image and dynamic neuromorphic datasets. With only 6 time steps, DISTA achieves remarkable top-1 accuracy on CIFAR10 (96.26%) and CIFAR100 (79.15%), as well as 79.1% on CIFAR10-DVS using 10 time steps.",
    "original_url": "http://arxiv.org/pdf/2311.09376v1",
    "original_title": "DISTA: Denoising Spiking Transformer with intrinsic plasticity and spatiotemporal attention",
    "source": "arxiv",
    "authors": [
      "Boxun Xu",
      "Hejia Geng",
      "Yuxuan Yin",
      "Peng Li"
    ],
    "published": "2023-11-15T21:09:08+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2311.09376v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2407.01375v2_chunk_0",
    "chunk_text": "Transferable-guided Attention Is All You Need for Video Domain Adaptation\n\nUnsupervised domain adaptation (UDA) in videos is a challenging task that remains not well explored compared to image-based UDA techniques. Although vision transformers (ViT) achieve state-of-the-art performance in many computer vision tasks, their use in video UDA has been little explored. Our key idea is to use transformer layers as a feature encoder and incorporate spatial and temporal transferability relationships into the attention mechanism. A Transferable-guided Attention (TransferAttn) framework is then developed to exploit the capacity of the transformer to adapt cross-domain knowledge across different backbones. To improve the transferability of ViT, we introduce a novel and effective module, named Domain Transferable-guided Attention Block (DTAB).",
    "original_url": "http://arxiv.org/pdf/2407.01375v2",
    "original_title": "Transferable-guided Attention Is All You Need for Video Domain Adaptation",
    "source": "arxiv",
    "authors": [
      "André Sacilotti",
      "Samuel Felipe dos Santos",
      "Nicu Sebe",
      "Jurandy Almeida"
    ],
    "published": "2024-07-01T15:29:27+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2407.01375v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2407.01375v2_chunk_1",
    "chunk_text": "To improve the transferability of ViT, we introduce a novel and effective module, named Domain Transferable-guided Attention Block (DTAB). DTAB compels ViT to focus on the spatio-temporal transferability relationship among video frames by changing the self-attention mechanism to a transferability attention mechanism. Extensive experiments were conducted on UCF-HMDB, Kinetics-Gameplay, and Kinetics-NEC Drone datasets, with different backbones, like ResNet101, I3D, and STAM, to verify the effectiveness of TransferAttn compared with state-of-the-art approaches. Also, we demonstrate that DTAB yields performance gains when applied to other state-of-the-art transformer-based UDA methods from both video and image domains. Our code is available at https://github.com/Andre-Sacilotti/transferattn-project-code.",
    "original_url": "http://arxiv.org/pdf/2407.01375v2",
    "original_title": "Transferable-guided Attention Is All You Need for Video Domain Adaptation",
    "source": "arxiv",
    "authors": [
      "André Sacilotti",
      "Samuel Felipe dos Santos",
      "Nicu Sebe",
      "Jurandy Almeida"
    ],
    "published": "2024-07-01T15:29:27+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2407.01375v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2407.01375v2_chunk_2",
    "chunk_text": "Our code is available at https://github.com/Andre-Sacilotti/transferattn-project-code.",
    "original_url": "http://arxiv.org/pdf/2407.01375v2",
    "original_title": "Transferable-guided Attention Is All You Need for Video Domain Adaptation",
    "source": "arxiv",
    "authors": [
      "André Sacilotti",
      "Samuel Felipe dos Santos",
      "Nicu Sebe",
      "Jurandy Almeida"
    ],
    "published": "2024-07-01T15:29:27+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2407.01375v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2501.13989v1_chunk_0",
    "chunk_text": "FreEformer: Frequency Enhanced Transformer for Multivariate Time Series Forecasting\n\nThis paper presents \\textbf{FreEformer}, a simple yet effective model that leverages a \\textbf{Fre}quency \\textbf{E}nhanced Trans\\textbf{former} for multivariate time series forecasting. Our work is based on the assumption that the frequency spectrum provides a global perspective on the composition of series across various frequencies and is highly suitable for robust representation learning. Specifically, we first convert time series into the complex frequency domain using the Discrete Fourier Transform (DFT). The Transformer architecture is then applied to the frequency spectra to capture cross-variate dependencies, with the real and imaginary parts processed independently. However, we observe that the vanilla attention matrix exhibits a low-rank characteristic, thus limiting representation diversity.",
    "original_url": "http://arxiv.org/pdf/2501.13989v1",
    "original_title": "FreEformer: Frequency Enhanced Transformer for Multivariate Time Series Forecasting",
    "source": "arxiv",
    "authors": [
      "Wenzhen Yue",
      "Yong Liu",
      "Xianghua Ying",
      "Bowei Xing",
      "Ruohao Guo",
      "Ji Shi"
    ],
    "published": "2025-01-23T08:53:45+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2501.13989v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2501.13989v1_chunk_1",
    "chunk_text": "However, we observe that the vanilla attention matrix exhibits a low-rank characteristic, thus limiting representation diversity. This could be attributed to the inherent sparsity of the frequency domain and the strong-value-focused nature of Softmax in vanilla attention. To address this, we enhance the vanilla attention mechanism by introducing an additional learnable matrix to the original attention matrix, followed by row-wise L1 normalization. Theoretical analysis~demonstrates that this enhanced attention mechanism improves both feature diversity and gradient flow. Extensive experiments demonstrate that FreEformer consistently outperforms state-of-the-art models on eighteen real-world benchmarks covering electricity, traffic, weather, healthcare and finance.",
    "original_url": "http://arxiv.org/pdf/2501.13989v1",
    "original_title": "FreEformer: Frequency Enhanced Transformer for Multivariate Time Series Forecasting",
    "source": "arxiv",
    "authors": [
      "Wenzhen Yue",
      "Yong Liu",
      "Xianghua Ying",
      "Bowei Xing",
      "Ruohao Guo",
      "Ji Shi"
    ],
    "published": "2025-01-23T08:53:45+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2501.13989v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2501.13989v1_chunk_2",
    "chunk_text": "Extensive experiments demonstrate that FreEformer consistently outperforms state-of-the-art models on eighteen real-world benchmarks covering electricity, traffic, weather, healthcare and finance. Notably, the enhanced attention mechanism also consistently improves the performance of state-of-the-art Transformer-based forecasters.",
    "original_url": "http://arxiv.org/pdf/2501.13989v1",
    "original_title": "FreEformer: Frequency Enhanced Transformer for Multivariate Time Series Forecasting",
    "source": "arxiv",
    "authors": [
      "Wenzhen Yue",
      "Yong Liu",
      "Xianghua Ying",
      "Bowei Xing",
      "Ruohao Guo",
      "Ji Shi"
    ],
    "published": "2025-01-23T08:53:45+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2501.13989v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2012.12556v6_chunk_0",
    "chunk_text": "A Survey on Visual Transformer\n\nTransformer, first applied to the field of natural language processing, is a type of deep neural network mainly based on the self-attention mechanism. Thanks to its strong representation capabilities, researchers are looking at ways to apply transformer to computer vision tasks. In a variety of visual benchmarks, transformer-based models perform similar to or better than other types of networks such as convolutional and recurrent neural networks. Given its high performance and less need for vision-specific inductive bias, transformer is receiving more and more attention from the computer vision community. In this paper, we review these vision transformer models by categorizing them in different tasks and analyzing their advantages and disadvantages.",
    "original_url": "http://arxiv.org/pdf/2012.12556v6",
    "original_title": "A Survey on Visual Transformer",
    "source": "arxiv",
    "authors": [
      "Kai Han",
      "Yunhe Wang",
      "Hanting Chen",
      "Xinghao Chen",
      "Jianyuan Guo",
      "Zhenhua Liu",
      "Yehui Tang",
      "An Xiao",
      "Chunjing Xu",
      "Yixing Xu",
      "Zhaohui Yang",
      "Yiman Zhang",
      "Dacheng Tao"
    ],
    "published": "2020-12-23T09:37:54+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2012.12556v6"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2012.12556v6_chunk_1",
    "chunk_text": "In this paper, we review these vision transformer models by categorizing them in different tasks and analyzing their advantages and disadvantages. The main categories we explore include the backbone network, high/mid-level vision, low-level vision, and video processing. We also include efficient transformer methods for pushing transformer into real device-based applications. Furthermore, we also take a brief look at the self-attention mechanism in computer vision, as it is the base component in transformer. Toward the end of this paper, we discuss the challenges and provide several further research directions for vision transformers.",
    "original_url": "http://arxiv.org/pdf/2012.12556v6",
    "original_title": "A Survey on Visual Transformer",
    "source": "arxiv",
    "authors": [
      "Kai Han",
      "Yunhe Wang",
      "Hanting Chen",
      "Xinghao Chen",
      "Jianyuan Guo",
      "Zhenhua Liu",
      "Yehui Tang",
      "An Xiao",
      "Chunjing Xu",
      "Yixing Xu",
      "Zhaohui Yang",
      "Yiman Zhang",
      "Dacheng Tao"
    ],
    "published": "2020-12-23T09:37:54+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2012.12556v6"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2012.12556v6_chunk_2",
    "chunk_text": "Toward the end of this paper, we discuss the challenges and provide several further research directions for vision transformers.",
    "original_url": "http://arxiv.org/pdf/2012.12556v6",
    "original_title": "A Survey on Visual Transformer",
    "source": "arxiv",
    "authors": [
      "Kai Han",
      "Yunhe Wang",
      "Hanting Chen",
      "Xinghao Chen",
      "Jianyuan Guo",
      "Zhenhua Liu",
      "Yehui Tang",
      "An Xiao",
      "Chunjing Xu",
      "Yixing Xu",
      "Zhaohui Yang",
      "Yiman Zhang",
      "Dacheng Tao"
    ],
    "published": "2020-12-23T09:37:54+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2012.12556v6"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2204.03957v1_chunk_0",
    "chunk_text": "Points to Patches: Enabling the Use of Self-Attention for 3D Shape Recognition\n\nWhile the Transformer architecture has become ubiquitous in the machine learning field, its adaptation to 3D shape recognition is non-trivial. Due to its quadratic computational complexity, the self-attention operator quickly becomes inefficient as the set of input points grows larger. Furthermore, we find that the attention mechanism struggles to find useful connections between individual points on a global scale. In order to alleviate these problems, we propose a two-stage Point Transformer-in-Transformer (Point-TnT) approach which combines local and global attention mechanisms, enabling both individual points and patches of points to attend to each other effectively. Experiments on shape classification show that such an approach provides more useful features for downstream tasks than the baseline Transformer, while also being more computationally efficient.",
    "original_url": "http://arxiv.org/pdf/2204.03957v1",
    "original_title": "Points to Patches: Enabling the Use of Self-Attention for 3D Shape Recognition",
    "source": "arxiv",
    "authors": [
      "Axel Berg",
      "Magnus Oskarsson",
      "Mark O'Connor"
    ],
    "published": "2022-04-08T09:31:24+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2204.03957v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2204.03957v1_chunk_1",
    "chunk_text": "Experiments on shape classification show that such an approach provides more useful features for downstream tasks than the baseline Transformer, while also being more computationally efficient. In addition, we also extend our method to feature matching for scene reconstruction, showing that it can be used in conjunction with existing scene reconstruction pipelines.",
    "original_url": "http://arxiv.org/pdf/2204.03957v1",
    "original_title": "Points to Patches: Enabling the Use of Self-Attention for 3D Shape Recognition",
    "source": "arxiv",
    "authors": [
      "Axel Berg",
      "Magnus Oskarsson",
      "Mark O'Connor"
    ],
    "published": "2022-04-08T09:31:24+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2204.03957v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2208.03369v2_chunk_0",
    "chunk_text": "A Spatially Separable Attention Mechanism for massive MIMO CSI Feedback\n\nChannel State Information (CSI) Feedback plays a crucial role in achieving higher gains through beamforming. However, for a massive MIMO system, this feedback overhead is huge and grows linearly with the number of antennas. To reduce the feedback overhead several compressive sensing (CS) techniques were implemented in recent years but these techniques are often iterative and are computationally complex to realize in power-constrained user equipment (UE). Hence, a data-based deep learning approach took over in these recent years introducing a variety of neural networks for CSI compression. Specifically, transformer-based networks have been shown to achieve state-of-the-art performance.",
    "original_url": "http://arxiv.org/pdf/2208.03369v2",
    "original_title": "A Spatially Separable Attention Mechanism for massive MIMO CSI Feedback",
    "source": "arxiv",
    "authors": [
      "Sharan Mourya",
      "SaiDhiraj Amuru",
      "Kiran Kumar Kuchi"
    ],
    "published": "2022-08-05T19:40:24+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2208.03369v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2208.03369v2_chunk_1",
    "chunk_text": "Specifically, transformer-based networks have been shown to achieve state-of-the-art performance. However, the multi-head attention operation, which is at the core of transformers, is computationally complex making transformers difficult to implement on a UE. In this work, we present a lightweight transformer named STNet which uses a spatially separable attention mechanism that is significantly less complex than the traditional full-attention. Equipped with this, STNet outperformed state-of-the-art models in some scenarios with approximately $1/10^{th}$ of the resources.",
    "original_url": "http://arxiv.org/pdf/2208.03369v2",
    "original_title": "A Spatially Separable Attention Mechanism for massive MIMO CSI Feedback",
    "source": "arxiv",
    "authors": [
      "Sharan Mourya",
      "SaiDhiraj Amuru",
      "Kiran Kumar Kuchi"
    ],
    "published": "2022-08-05T19:40:24+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2208.03369v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2305.17633v1_chunk_0",
    "chunk_text": "DPFormer: Learning Differentially Private Transformer on Long-Tailed Data\n\nThe Transformer has emerged as a versatile and effective architecture with broad applications. However, it still remains an open problem how to efficiently train a Transformer model of high utility with differential privacy guarantees. In this paper, we identify two key challenges in learning differentially private Transformers, i.e., heavy computation overhead due to per-sample gradient clipping and unintentional attention distraction within the attention mechanism. In response, we propose DPFormer, equipped with Phantom Clipping and Re-Attention Mechanism, to address these challenges. Our theoretical analysis shows that DPFormer can reduce computational costs during gradient clipping and effectively mitigate attention distraction (which could obstruct the training process and lead to a significant performance drop, especially in the presence of long-tailed data).",
    "original_url": "http://arxiv.org/pdf/2305.17633v1",
    "original_title": "DPFormer: Learning Differentially Private Transformer on Long-Tailed Data",
    "source": "arxiv",
    "authors": [
      "Youlong Ding",
      "Xueyang Wu",
      "Hao Wang",
      "Weike Pan"
    ],
    "published": "2023-05-28T05:00:07+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2305.17633v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2305.17633v1_chunk_1",
    "chunk_text": "Our theoretical analysis shows that DPFormer can reduce computational costs during gradient clipping and effectively mitigate attention distraction (which could obstruct the training process and lead to a significant performance drop, especially in the presence of long-tailed data). Such analysis is further corroborated by empirical results on two real-world datasets, demonstrating the efficiency and effectiveness of the proposed DPFormer.",
    "original_url": "http://arxiv.org/pdf/2305.17633v1",
    "original_title": "DPFormer: Learning Differentially Private Transformer on Long-Tailed Data",
    "source": "arxiv",
    "authors": [
      "Youlong Ding",
      "Xueyang Wu",
      "Hao Wang",
      "Weike Pan"
    ],
    "published": "2023-05-28T05:00:07+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2305.17633v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2306.00294v1_chunk_0",
    "chunk_text": "Affinity-based Attention in Self-supervised Transformers Predicts Dynamics of Object Grouping in Humans\n\nThe spreading of attention has been proposed as a mechanism for how humans group features to segment objects. However, such a mechanism has not yet been implemented and tested in naturalistic images. Here, we leverage the feature maps from self-supervised vision Transformers and propose a model of human object-based attention spreading and segmentation. Attention spreads within an object through the feature affinity signal between different patches of the image. We also collected behavioral data on people grouping objects in natural images by judging whether two dots are on the same object or on two different objects.",
    "original_url": "http://arxiv.org/pdf/2306.00294v1",
    "original_title": "Affinity-based Attention in Self-supervised Transformers Predicts Dynamics of Object Grouping in Humans",
    "source": "arxiv",
    "authors": [
      "Hossein Adeli",
      "Seoyoung Ahn",
      "Nikolaus Kriegeskorte",
      "Gregory Zelinsky"
    ],
    "published": "2023-06-01T02:25:55+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2306.00294v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2306.00294v1_chunk_1",
    "chunk_text": "We also collected behavioral data on people grouping objects in natural images by judging whether two dots are on the same object or on two different objects. We found that our models of affinity spread that were built on feature maps from the self-supervised Transformers showed significant improvement over baseline and CNN based models on predicting reaction time patterns of humans, despite not being trained on the task or with any other object labels. Our work provides new benchmarks for evaluating models of visual representation learning including Transformers.",
    "original_url": "http://arxiv.org/pdf/2306.00294v1",
    "original_title": "Affinity-based Attention in Self-supervised Transformers Predicts Dynamics of Object Grouping in Humans",
    "source": "arxiv",
    "authors": [
      "Hossein Adeli",
      "Seoyoung Ahn",
      "Nikolaus Kriegeskorte",
      "Gregory Zelinsky"
    ],
    "published": "2023-06-01T02:25:55+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2306.00294v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2401.11673v1_chunk_0",
    "chunk_text": "MVSFormer++: Revealing the Devil in Transformer's Details for Multi-View Stereo\n\nRecent advancements in learning-based Multi-View Stereo (MVS) methods have prominently featured transformer-based models with attention mechanisms. However, existing approaches have not thoroughly investigated the profound influence of transformers on different MVS modules, resulting in limited depth estimation capabilities. In this paper, we introduce MVSFormer++, a method that prudently maximizes the inherent characteristics of attention to enhance various components of the MVS pipeline. Formally, our approach involves infusing cross-view information into the pre-trained DINOv2 model to facilitate MVS learning. Furthermore, we employ different attention mechanisms for the feature encoder and cost volume regularization, focusing on feature and spatial aggregations respectively.",
    "original_url": "http://arxiv.org/pdf/2401.11673v1",
    "original_title": "MVSFormer++: Revealing the Devil in Transformer's Details for Multi-View Stereo",
    "source": "arxiv",
    "authors": [
      "Chenjie Cao",
      "Xinlin Ren",
      "Yanwei Fu"
    ],
    "published": "2024-01-22T03:22:49+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2401.11673v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2401.11673v1_chunk_1",
    "chunk_text": "Furthermore, we employ different attention mechanisms for the feature encoder and cost volume regularization, focusing on feature and spatial aggregations respectively. Additionally, we uncover that some design details would substantially impact the performance of transformer modules in MVS, including normalized 3D positional encoding, adaptive attention scaling, and the position of layer normalization. Comprehensive experiments on DTU, Tanks-and-Temples, BlendedMVS, and ETH3D validate the effectiveness of the proposed method. Notably, MVSFormer++ achieves state-of-the-art performance on the challenging DTU and Tanks-and-Temples benchmarks.",
    "original_url": "http://arxiv.org/pdf/2401.11673v1",
    "original_title": "MVSFormer++: Revealing the Devil in Transformer's Details for Multi-View Stereo",
    "source": "arxiv",
    "authors": [
      "Chenjie Cao",
      "Xinlin Ren",
      "Yanwei Fu"
    ],
    "published": "2024-01-22T03:22:49+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2401.11673v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2502.07553v1_chunk_0",
    "chunk_text": "Attention Learning is Needed to Efficiently Learn Parity Function\n\nTransformers, with their attention mechanisms, have emerged as the state-of-the-art architectures of sequential modeling and empirically outperform feed-forward neural networks (FFNNs) across many fields, such as natural language processing and computer vision. However, their generalization ability, particularly for low-sensitivity functions, remains less studied. We bridge this gap by analyzing transformers on the $k$-parity problem. Daniely and Malach (NeurIPS 2020) show that FFNNs with one hidden layer and $O(nk^7 \\log k)$ parameters can learn $k$-parity, where the input length $n$ is typically much larger than $k$. In this paper, we prove that FFNNs require at least $\\Omega(n)$ parameters to learn $k$-parity, while transformers require only $O(k)$ parameters, surpassing the theoretical lower bound needed by FFNNs.",
    "original_url": "http://arxiv.org/pdf/2502.07553v1",
    "original_title": "Attention Learning is Needed to Efficiently Learn Parity Function",
    "source": "arxiv",
    "authors": [
      "Yaomengxi Han",
      "Debarghya Ghoshdastidar"
    ],
    "published": "2025-02-11T13:41:30+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2502.07553v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2502.07553v1_chunk_1",
    "chunk_text": "In this paper, we prove that FFNNs require at least $\\Omega(n)$ parameters to learn $k$-parity, while transformers require only $O(k)$ parameters, surpassing the theoretical lower bound needed by FFNNs. We further prove that this parameter efficiency cannot be achieved with fixed attention heads. Our work establishes transformers as theoretically superior to FFNNs in learning parity function, showing how their attention mechanisms enable parameter-efficient generalization in functions with low sensitivity.",
    "original_url": "http://arxiv.org/pdf/2502.07553v1",
    "original_title": "Attention Learning is Needed to Efficiently Learn Parity Function",
    "source": "arxiv",
    "authors": [
      "Yaomengxi Han",
      "Debarghya Ghoshdastidar"
    ],
    "published": "2025-02-11T13:41:30+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2502.07553v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1907.04197v1_chunk_0",
    "chunk_text": "Attending to Emotional Narratives\n\nAttention mechanisms in deep neural networks have achieved excellent performance on sequence-prediction tasks. Here, we show that these recently-proposed attention-based mechanisms---in particular, the Transformer with its parallelizable self-attention layers, and the Memory Fusion Network with attention across modalities and time---also generalize well to multimodal time-series emotion recognition. Using a recently-introduced dataset of emotional autobiographical narratives, we adapt and apply these two attention mechanisms to predict emotional valence over time. Our models perform extremely well, in some cases reaching a performance comparable with human raters. We end with a discussion of the implications of attention mechanisms to affective computing.",
    "original_url": "http://arxiv.org/pdf/1907.04197v1",
    "original_title": "Attending to Emotional Narratives",
    "source": "arxiv",
    "authors": [
      "Zhengxuan Wu",
      "Xiyu Zhang",
      "Tan Zhi-Xuan",
      "Jamil Zaki",
      "Desmond C. Ong"
    ],
    "published": "2019-07-08T03:50:43+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1907.04197v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1907.04197v1_chunk_1",
    "chunk_text": "We end with a discussion of the implications of attention mechanisms to affective computing.",
    "original_url": "http://arxiv.org/pdf/1907.04197v1",
    "original_title": "Attending to Emotional Narratives",
    "source": "arxiv",
    "authors": [
      "Zhengxuan Wu",
      "Xiyu Zhang",
      "Tan Zhi-Xuan",
      "Jamil Zaki",
      "Desmond C. Ong"
    ],
    "published": "2019-07-08T03:50:43+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1907.04197v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2502.15349v1_chunk_0",
    "chunk_text": "AttentionEngine: A Versatile Framework for Efficient Attention Mechanisms on Diverse Hardware Platforms\n\nTransformers and large language models (LLMs) have revolutionized machine learning, with attention mechanisms at the core of their success. As the landscape of attention variants expands, so too do the challenges of optimizing their performance, particularly across different hardware platforms. Current optimization strategies are often narrowly focused, requiring extensive manual intervention to accommodate changes in model configurations or hardware environments. In this paper, we introduce AttentionEngine, a comprehensive framework designed to streamline the optimization of attention mechanisms across heterogeneous hardware backends. By decomposing attention computation into modular operations with customizable components, AttentionEngine enables flexible adaptation to diverse algorithmic requirements.",
    "original_url": "http://arxiv.org/pdf/2502.15349v1",
    "original_title": "AttentionEngine: A Versatile Framework for Efficient Attention Mechanisms on Diverse Hardware Platforms",
    "source": "arxiv",
    "authors": [
      "Feiyang Chen",
      "Yu Cheng",
      "Lei Wang",
      "Yuqing Xia",
      "Ziming Miao",
      "Lingxiao Ma",
      "Fan Yang",
      "Jilong Xue",
      "Zhi Yang",
      "Mao Yang",
      "Haibo Chen"
    ],
    "published": "2025-02-21T10:06:41+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2502.15349v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2502.15349v1_chunk_1",
    "chunk_text": "By decomposing attention computation into modular operations with customizable components, AttentionEngine enables flexible adaptation to diverse algorithmic requirements. The framework further automates kernel optimization through a combination of programmable templates and a robust cross-platform scheduling strategy. Empirical results reveal performance gains of up to 10x on configurations beyond the reach of existing methods. AttentionEngine offers a scalable, efficient foundation for developing and deploying attention mechanisms with minimal manual tuning. Our code has been open-sourced and is available at https://github.com/microsoft/AttentionEngine.",
    "original_url": "http://arxiv.org/pdf/2502.15349v1",
    "original_title": "AttentionEngine: A Versatile Framework for Efficient Attention Mechanisms on Diverse Hardware Platforms",
    "source": "arxiv",
    "authors": [
      "Feiyang Chen",
      "Yu Cheng",
      "Lei Wang",
      "Yuqing Xia",
      "Ziming Miao",
      "Lingxiao Ma",
      "Fan Yang",
      "Jilong Xue",
      "Zhi Yang",
      "Mao Yang",
      "Haibo Chen"
    ],
    "published": "2025-02-21T10:06:41+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2502.15349v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2502.15349v1_chunk_2",
    "chunk_text": "Our code has been open-sourced and is available at https://github.com/microsoft/AttentionEngine.",
    "original_url": "http://arxiv.org/pdf/2502.15349v1",
    "original_title": "AttentionEngine: A Versatile Framework for Efficient Attention Mechanisms on Diverse Hardware Platforms",
    "source": "arxiv",
    "authors": [
      "Feiyang Chen",
      "Yu Cheng",
      "Lei Wang",
      "Yuqing Xia",
      "Ziming Miao",
      "Lingxiao Ma",
      "Fan Yang",
      "Jilong Xue",
      "Zhi Yang",
      "Mao Yang",
      "Haibo Chen"
    ],
    "published": "2025-02-21T10:06:41+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2502.15349v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1712.02047v1_chunk_0",
    "chunk_text": "Distance-based Self-Attention Network for Natural Language Inference\n\nAttention mechanism has been used as an ancillary means to help RNN or CNN. However, the Transformer (Vaswani et al., 2017) recently recorded the state-of-the-art performance in machine translation with a dramatic reduction in training time by solely using attention. Motivated by the Transformer, Directional Self Attention Network (Shen et al., 2017), a fully attention-based sentence encoder, was proposed. It showed good performance with various data by using forward and backward directional information in a sentence. But in their study, not considered at all was the distance between words, an important feature when learning the local dependency to help understand the context of input text.",
    "original_url": "http://arxiv.org/pdf/1712.02047v1",
    "original_title": "Distance-based Self-Attention Network for Natural Language Inference",
    "source": "arxiv",
    "authors": [
      "Jinbae Im",
      "Sungzoon Cho"
    ],
    "published": "2017-12-06T05:38:29+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1712.02047v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1712.02047v1_chunk_1",
    "chunk_text": "But in their study, not considered at all was the distance between words, an important feature when learning the local dependency to help understand the context of input text. We propose Distance-based Self-Attention Network, which considers the word distance by using a simple distance mask in order to model the local dependency without losing the ability of modeling global dependency which attention has inherent. Our model shows good performance with NLI data, and it records the new state-of-the-art result with SNLI data. Additionally, we show that our model has a strength in long sentences or documents.",
    "original_url": "http://arxiv.org/pdf/1712.02047v1",
    "original_title": "Distance-based Self-Attention Network for Natural Language Inference",
    "source": "arxiv",
    "authors": [
      "Jinbae Im",
      "Sungzoon Cho"
    ],
    "published": "2017-12-06T05:38:29+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1712.02047v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2103.06104v2_chunk_0",
    "chunk_text": "U-Net Transformer: Self and Cross Attention for Medical Image Segmentation\n\nMedical image segmentation remains particularly challenging for complex and low-contrast anatomical structures. In this paper, we introduce the U-Transformer network, which combines a U-shaped architecture for image segmentation with self- and cross-attention from Transformers. U-Transformer overcomes the inability of U-Nets to model long-range contextual interactions and spatial dependencies, which are arguably crucial for accurate segmentation in challenging contexts. To this end, attention mechanisms are incorporated at two main levels: a self-attention module leverages global interactions between encoder features, while cross-attention in the skip connections allows a fine spatial recovery in the U-Net decoder by filtering out non-semantic features. Experiments on two abdominal CT-image datasets show the large performance gain brought out by U-Transformer compared to U-Net and local Attention U-Nets.",
    "original_url": "http://arxiv.org/pdf/2103.06104v2",
    "original_title": "U-Net Transformer: Self and Cross Attention for Medical Image Segmentation",
    "source": "arxiv",
    "authors": [
      "Olivier Petit",
      "Nicolas Thome",
      "Clément Rambour",
      "Luc Soler"
    ],
    "published": "2021-03-10T14:58:31+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2103.06104v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2103.06104v2_chunk_1",
    "chunk_text": "Experiments on two abdominal CT-image datasets show the large performance gain brought out by U-Transformer compared to U-Net and local Attention U-Nets. We also highlight the importance of using both self- and cross-attention, and the nice interpretability features brought out by U-Transformer.",
    "original_url": "http://arxiv.org/pdf/2103.06104v2",
    "original_title": "U-Net Transformer: Self and Cross Attention for Medical Image Segmentation",
    "source": "arxiv",
    "authors": [
      "Olivier Petit",
      "Nicolas Thome",
      "Clément Rambour",
      "Luc Soler"
    ],
    "published": "2021-03-10T14:58:31+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2103.06104v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2203.09055v1_chunk_0",
    "chunk_text": "Fine- and Coarse-Granularity Hybrid Self-Attention for Efficient BERT\n\nTransformer-based pre-trained models, such as BERT, have shown extraordinary success in achieving state-of-the-art results in many natural language processing applications. However, deploying these models can be prohibitively costly, as the standard self-attention mechanism of the Transformer suffers from quadratic computational cost in the input sequence length. To confront this, we propose FCA, a fine- and coarse-granularity hybrid self-attention that reduces the computation cost through progressively shortening the computational sequence length in self-attention. Specifically, FCA conducts an attention-based scoring strategy to determine the informativeness of tokens at each layer. Then, the informative tokens serve as the fine-granularity computing units in self-attention and the uninformative tokens are replaced with one or several clusters as the coarse-granularity computing units in self-attention.",
    "original_url": "http://arxiv.org/pdf/2203.09055v1",
    "original_title": "Fine- and Coarse-Granularity Hybrid Self-Attention for Efficient BERT",
    "source": "arxiv",
    "authors": [
      "Jing Zhao",
      "Yifan Wang",
      "Junwei Bao",
      "Youzheng Wu",
      "Xiaodong He"
    ],
    "published": "2022-03-17T03:33:47+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2203.09055v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2203.09055v1_chunk_1",
    "chunk_text": "Then, the informative tokens serve as the fine-granularity computing units in self-attention and the uninformative tokens are replaced with one or several clusters as the coarse-granularity computing units in self-attention. Experiments on GLUE and RACE datasets show that BERT with FCA achieves 2x reduction in FLOPs over original BERT with <1% loss in accuracy. We show that FCA offers a significantly better trade-off between accuracy and FLOPs compared to prior methods.",
    "original_url": "http://arxiv.org/pdf/2203.09055v1",
    "original_title": "Fine- and Coarse-Granularity Hybrid Self-Attention for Efficient BERT",
    "source": "arxiv",
    "authors": [
      "Jing Zhao",
      "Yifan Wang",
      "Junwei Bao",
      "Youzheng Wu",
      "Xiaodong He"
    ],
    "published": "2022-03-17T03:33:47+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2203.09055v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2306.01926v1_chunk_0",
    "chunk_text": "RITA: Group Attention is All You Need for Timeseries Analytics\n\nTimeseries analytics is of great importance in many real-world applications. Recently, the Transformer model, popular in natural language processing, has been leveraged to learn high quality feature embeddings from timeseries, core to the performance of various timeseries analytics tasks. However, the quadratic time and space complexities limit Transformers' scalability, especially for long timeseries. To address these issues, we develop a timeseries analytics tool, RITA, which uses a novel attention mechanism, named group attention, to address this scalability issue. Group attention dynamically clusters the objects based on their similarity into a small number of groups and approximately computes the attention at the coarse group granularity.",
    "original_url": "http://arxiv.org/pdf/2306.01926v1",
    "original_title": "RITA: Group Attention is All You Need for Timeseries Analytics",
    "source": "arxiv",
    "authors": [
      "Jiaming Liang",
      "Lei Cao",
      "Samuel Madden",
      "Zachary Ives",
      "Guoliang Li"
    ],
    "published": "2023-06-02T21:45:13+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2306.01926v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2306.01926v1_chunk_1",
    "chunk_text": "Group attention dynamically clusters the objects based on their similarity into a small number of groups and approximately computes the attention at the coarse group granularity. It thus significantly reduces the time and space complexity, yet provides a theoretical guarantee on the quality of the computed attention. The dynamic scheduler of RITA continuously adapts the number of groups and the batch size in the training process, ensuring group attention always uses the fewest groups needed to meet the approximation quality requirement. Extensive experiments on various timeseries datasets and analytics tasks demonstrate that RITA outperforms the state-of-the-art in accuracy and is significantly faster -- with speedups of up to 63X.",
    "original_url": "http://arxiv.org/pdf/2306.01926v1",
    "original_title": "RITA: Group Attention is All You Need for Timeseries Analytics",
    "source": "arxiv",
    "authors": [
      "Jiaming Liang",
      "Lei Cao",
      "Samuel Madden",
      "Zachary Ives",
      "Guoliang Li"
    ],
    "published": "2023-06-02T21:45:13+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2306.01926v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2501.01039v1_chunk_0",
    "chunk_text": "MSWA: Refining Local Attention with Multi-ScaleWindow Attention\n\nTransformer-based LLMs have achieved exceptional performance across a wide range of NLP tasks. However, the standard self-attention mechanism suffers from quadratic time complexity and linearly increased cache size. Sliding window attention (SWA) solves this problem by restricting the attention range to a fixed-size local context window. Nevertheless, SWA employs a uniform window size for each head in each layer, making it inefficient in capturing context of varying scales. To mitigate this limitation, we propose Multi-Scale Window Attention (MSWA) which applies diverse window sizes across heads and layers in the Transformer.",
    "original_url": "http://arxiv.org/pdf/2501.01039v1",
    "original_title": "MSWA: Refining Local Attention with Multi-ScaleWindow Attention",
    "source": "arxiv",
    "authors": [
      "Yixing Xu",
      "Shivank Nag",
      "Dong Li",
      "Lu Tian",
      "Emad Barsoum"
    ],
    "published": "2025-01-02T03:41:32+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2501.01039v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2501.01039v1_chunk_1",
    "chunk_text": "To mitigate this limitation, we propose Multi-Scale Window Attention (MSWA) which applies diverse window sizes across heads and layers in the Transformer. It not only allows for different window sizes among heads within the same layer but also progressively increases window size allocation from shallow to deep layers, thus enabling the model to capture contextual information with different lengths and distances. Experimental results on language modeling and common-sense reasoning tasks substantiate that MSWA outperforms traditional local attention in both effectiveness and efficiency.",
    "original_url": "http://arxiv.org/pdf/2501.01039v1",
    "original_title": "MSWA: Refining Local Attention with Multi-ScaleWindow Attention",
    "source": "arxiv",
    "authors": [
      "Yixing Xu",
      "Shivank Nag",
      "Dong Li",
      "Lu Tian",
      "Emad Barsoum"
    ],
    "published": "2025-01-02T03:41:32+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2501.01039v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2504.15956v1_chunk_0",
    "chunk_text": "Universal Approximation with Softmax Attention\n\nWe prove that with linear transformations, both (i) two-layer self-attention and (ii) one-layer self-attention followed by a softmax function are universal approximators for continuous sequence-to-sequence functions on compact domains. Our main technique is a new interpolation-based method for analyzing attention's internal mechanism. This leads to our key insight: self-attention is able to approximate a generalized version of ReLU to arbitrary precision, and hence subsumes many known universal approximators. Building on these, we show that two-layer multi-head attention alone suffices as a sequence-to-sequence universal approximator. In contrast, prior works rely on feed-forward networks to establish universal approximation in Transformers.",
    "original_url": "http://arxiv.org/pdf/2504.15956v1",
    "original_title": "Universal Approximation with Softmax Attention",
    "source": "arxiv",
    "authors": [
      "Jerry Yao-Chieh Hu",
      "Hude Liu",
      "Hong-Yu Chen",
      "Weimin Wu",
      "Han Liu"
    ],
    "published": "2025-04-22T14:51:33+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2504.15956v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2504.15956v1_chunk_1",
    "chunk_text": "In contrast, prior works rely on feed-forward networks to establish universal approximation in Transformers. Furthermore, we extend our techniques to show that, (softmax-)attention-only layers are capable of approximating various statistical models in-context. We believe these techniques hold independent interest.",
    "original_url": "http://arxiv.org/pdf/2504.15956v1",
    "original_title": "Universal Approximation with Softmax Attention",
    "source": "arxiv",
    "authors": [
      "Jerry Yao-Chieh Hu",
      "Hude Liu",
      "Hong-Yu Chen",
      "Weimin Wu",
      "Han Liu"
    ],
    "published": "2025-04-22T14:51:33+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2504.15956v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2206.00206v1_chunk_0",
    "chunk_text": "Transformer with Fourier Integral Attentions\n\nMulti-head attention empowers the recent success of transformers, the state-of-the-art models that have achieved remarkable success in sequence modeling and beyond. These attention mechanisms compute the pairwise dot products between the queries and keys, which results from the use of unnormalized Gaussian kernels with the assumption that the queries follow a mixture of Gaussian distribution. There is no guarantee that this assumption is valid in practice. In response, we first interpret attention in transformers as a nonparametric kernel regression. We then propose the FourierFormer, a new class of transformers in which the dot-product kernels are replaced by the novel generalized Fourier integral kernels.",
    "original_url": "http://arxiv.org/pdf/2206.00206v1",
    "original_title": "Transformer with Fourier Integral Attentions",
    "source": "arxiv",
    "authors": [
      "Tan Nguyen",
      "Minh Pham",
      "Tam Nguyen",
      "Khai Nguyen",
      "Stanley J. Osher",
      "Nhat Ho"
    ],
    "published": "2022-06-01T03:06:21+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2206.00206v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2206.00206v1_chunk_1",
    "chunk_text": "We then propose the FourierFormer, a new class of transformers in which the dot-product kernels are replaced by the novel generalized Fourier integral kernels. Different from the dot-product kernels, where we need to choose a good covariance matrix to capture the dependency of the features of data, the generalized Fourier integral kernels can automatically capture such dependency and remove the need to tune the covariance matrix. We theoretically prove that our proposed Fourier integral kernels can efficiently approximate any key and query distributions. Compared to the conventional transformers with dot-product attention, FourierFormers attain better accuracy and reduce the redundancy between attention heads. We empirically corroborate the advantages of FourierFormers over the baseline transformers in a variety of practical applications including language modeling and image classification.",
    "original_url": "http://arxiv.org/pdf/2206.00206v1",
    "original_title": "Transformer with Fourier Integral Attentions",
    "source": "arxiv",
    "authors": [
      "Tan Nguyen",
      "Minh Pham",
      "Tam Nguyen",
      "Khai Nguyen",
      "Stanley J. Osher",
      "Nhat Ho"
    ],
    "published": "2022-06-01T03:06:21+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2206.00206v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2206.00206v1_chunk_2",
    "chunk_text": "We empirically corroborate the advantages of FourierFormers over the baseline transformers in a variety of practical applications including language modeling and image classification.",
    "original_url": "http://arxiv.org/pdf/2206.00206v1",
    "original_title": "Transformer with Fourier Integral Attentions",
    "source": "arxiv",
    "authors": [
      "Tan Nguyen",
      "Minh Pham",
      "Tam Nguyen",
      "Khai Nguyen",
      "Stanley J. Osher",
      "Nhat Ho"
    ],
    "published": "2022-06-01T03:06:21+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2206.00206v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2501.01023v2_chunk_0",
    "chunk_text": "Hadamard Attention Recurrent Transformer: A Strong Baseline for Stereo Matching Transformer\n\nIn light of the advancements in transformer technology, extant research posits the construction of stereo transformers as a potential solution to the binocular stereo matching challenge. However, constrained by the low-rank bottleneck and quadratic complexity of attention mechanisms, stereo transformers still fail to demonstrate sufficient nonlinear expressiveness within a reasonable inference time. The lack of focus on key homonymous points renders the representations of such methods vulnerable to challenging conditions, including reflections and weak textures. Furthermore, a slow computing speed is not conducive to the application. To overcome these difficulties, we present the Hadamard Attention Recurrent Stereo Transformer (HART) that incorporates the following components: 1) For faster inference, we present a Hadamard product paradigm for the attention mechanism, achieving linear computational complexity.",
    "original_url": "http://arxiv.org/pdf/2501.01023v2",
    "original_title": "Hadamard Attention Recurrent Transformer: A Strong Baseline for Stereo Matching Transformer",
    "source": "arxiv",
    "authors": [
      "Ziyang Chen",
      "Yongjun Zhang",
      "Wenting Li",
      "Bingshu Wang",
      "Yabo Wu",
      "Yong Zhao",
      "C. L. Philip Chen"
    ],
    "published": "2025-01-02T02:51:16+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2501.01023v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2501.01023v2_chunk_1",
    "chunk_text": "To overcome these difficulties, we present the Hadamard Attention Recurrent Stereo Transformer (HART) that incorporates the following components: 1) For faster inference, we present a Hadamard product paradigm for the attention mechanism, achieving linear computational complexity. 2) We designed a Dense Attention Kernel (DAK) to amplify the differences between relevant and irrelevant feature responses. This allows HART to focus on important details. DAK also converts zero elements to non-zero elements to mitigate the reduced expressiveness caused by the low-rank bottleneck. 3) To compensate for the spatial and channel interaction missing in the Hadamard product, we propose MKOI to capture both global and local information through the interleaving of large and small kernel convolutions.",
    "original_url": "http://arxiv.org/pdf/2501.01023v2",
    "original_title": "Hadamard Attention Recurrent Transformer: A Strong Baseline for Stereo Matching Transformer",
    "source": "arxiv",
    "authors": [
      "Ziyang Chen",
      "Yongjun Zhang",
      "Wenting Li",
      "Bingshu Wang",
      "Yabo Wu",
      "Yong Zhao",
      "C. L. Philip Chen"
    ],
    "published": "2025-01-02T02:51:16+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2501.01023v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2501.01023v2_chunk_2",
    "chunk_text": "3) To compensate for the spatial and channel interaction missing in the Hadamard product, we propose MKOI to capture both global and local information through the interleaving of large and small kernel convolutions. Experimental results demonstrate the effectiveness of our HART. In reflective area, HART ranked 1st on the KITTI 2012 benchmark among all published methods at the time of submission. Code is available at https://github.com/ZYangChen/HART.",
    "original_url": "http://arxiv.org/pdf/2501.01023v2",
    "original_title": "Hadamard Attention Recurrent Transformer: A Strong Baseline for Stereo Matching Transformer",
    "source": "arxiv",
    "authors": [
      "Ziyang Chen",
      "Yongjun Zhang",
      "Wenting Li",
      "Bingshu Wang",
      "Yabo Wu",
      "Yong Zhao",
      "C. L. Philip Chen"
    ],
    "published": "2025-01-02T02:51:16+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2501.01023v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2501.13467v1_chunk_0",
    "chunk_text": "Multi-Level Attention and Contrastive Learning for Enhanced Text Classification with an Optimized Transformer\n\nThis paper studies a text classification algorithm based on an improved Transformer to improve the performance and efficiency of the model in text classification tasks. Aiming at the shortcomings of the traditional Transformer model in capturing deep semantic relationships and optimizing computational complexity, this paper introduces a multi-level attention mechanism and a contrastive learning strategy. The multi-level attention mechanism effectively models the global semantics and local features in the text by combining global attention with local attention; the contrastive learning strategy enhances the model's ability to distinguish between different categories by constructing positive and negative sample pairs while improving the classification effect. In addition, in order to improve the training and inference efficiency of the model on large-scale text data, this paper designs a lightweight module to optimize the feature transformation process and reduce the computational cost. Experimental results on the dataset show that the improved Transformer model outperforms the comparative models such as BiLSTM, CNN, standard Transformer, and BERT in terms of classification accuracy, F1 score, and recall rate, showing stronger semantic representation ability and generalization performance.",
    "original_url": "http://arxiv.org/pdf/2501.13467v1",
    "original_title": "Multi-Level Attention and Contrastive Learning for Enhanced Text Classification with an Optimized Transformer",
    "source": "arxiv",
    "authors": [
      "Jia Gao",
      "Guiran Liu",
      "Binrong Zhu",
      "Shicheng Zhou",
      "Hongye Zheng",
      "Xiaoxuan Liao"
    ],
    "published": "2025-01-23T08:32:27+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2501.13467v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2501.13467v1_chunk_1",
    "chunk_text": "Experimental results on the dataset show that the improved Transformer model outperforms the comparative models such as BiLSTM, CNN, standard Transformer, and BERT in terms of classification accuracy, F1 score, and recall rate, showing stronger semantic representation ability and generalization performance. The method proposed in this paper provides a new idea for algorithm optimization in the field of text classification and has good application potential and practical value. Future work will focus on studying the performance of this model in multi-category imbalanced datasets and cross-domain tasks and explore the integration wi",
    "original_url": "http://arxiv.org/pdf/2501.13467v1",
    "original_title": "Multi-Level Attention and Contrastive Learning for Enhanced Text Classification with an Optimized Transformer",
    "source": "arxiv",
    "authors": [
      "Jia Gao",
      "Guiran Liu",
      "Binrong Zhu",
      "Shicheng Zhou",
      "Hongye Zheng",
      "Xiaoxuan Liao"
    ],
    "published": "2025-01-23T08:32:27+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2501.13467v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2108.00620v2_chunk_0",
    "chunk_text": "Investigating Attention Mechanism in 3D Point Cloud Object Detection\n\nObject detection in three-dimensional (3D) space attracts much interest from academia and industry since it is an essential task in AI-driven applications such as robotics, autonomous driving, and augmented reality. As the basic format of 3D data, the point cloud can provide detailed geometric information about the objects in the original 3D space. However, due to 3D data's sparsity and unorderedness, specially designed networks and modules are needed to process this type of data. Attention mechanism has achieved impressive performance in diverse computer vision tasks; however, it is unclear how attention modules would affect the performance of 3D point cloud object detection and what sort of attention modules could fit with the inherent properties of 3D data. This work investigates the role of the attention mechanism in 3D point cloud object detection and provides insights into the potential of different attention modules.",
    "original_url": "http://arxiv.org/pdf/2108.00620v2",
    "original_title": "Investigating Attention Mechanism in 3D Point Cloud Object Detection",
    "source": "arxiv",
    "authors": [
      "Shi Qiu",
      "Yunfan Wu",
      "Saeed Anwar",
      "Chongyi Li"
    ],
    "published": "2021-08-02T03:54:39+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2108.00620v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2108.00620v2_chunk_1",
    "chunk_text": "This work investigates the role of the attention mechanism in 3D point cloud object detection and provides insights into the potential of different attention modules. To achieve that, we comprehensively investigate classical 2D attentions, novel 3D attentions, including the latest point cloud transformers on SUN RGB-D and ScanNetV2 datasets. Based on the detailed experiments and analysis, we conclude the effects of different attention modules. This paper is expected to serve as a reference source for benefiting attention-embedded 3D point cloud object detection. The code and trained models are available at: https://github.com/ShiQiu0419/attentions_in_3D_detection.",
    "original_url": "http://arxiv.org/pdf/2108.00620v2",
    "original_title": "Investigating Attention Mechanism in 3D Point Cloud Object Detection",
    "source": "arxiv",
    "authors": [
      "Shi Qiu",
      "Yunfan Wu",
      "Saeed Anwar",
      "Chongyi Li"
    ],
    "published": "2021-08-02T03:54:39+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2108.00620v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2108.00620v2_chunk_2",
    "chunk_text": "The code and trained models are available at: https://github.com/ShiQiu0419/attentions_in_3D_detection.",
    "original_url": "http://arxiv.org/pdf/2108.00620v2",
    "original_title": "Investigating Attention Mechanism in 3D Point Cloud Object Detection",
    "source": "arxiv",
    "authors": [
      "Shi Qiu",
      "Yunfan Wu",
      "Saeed Anwar",
      "Chongyi Li"
    ],
    "published": "2021-08-02T03:54:39+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2108.00620v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2201.11037v1_chunk_0",
    "chunk_text": "RTNet: Relation Transformer Network for Diabetic Retinopathy Multi-lesion Segmentation\n\nAutomatic diabetic retinopathy (DR) lesions segmentation makes great sense of assisting ophthalmologists in diagnosis. Although many researches have been conducted on this task, most prior works paid too much attention to the designs of networks instead of considering the pathological association for lesions. Through investigating the pathogenic causes of DR lesions in advance, we found that certain lesions are closed to specific vessels and present relative patterns to each other. Motivated by the observation, we propose a relation transformer block (RTB) to incorporate attention mechanisms at two main levels: a self-attention transformer exploits global dependencies among lesion features, while a cross-attention transformer allows interactions between lesion and vessel features by integrating valuable vascular information to alleviate ambiguity in lesion detection caused by complex fundus structures. In addition, to capture the small lesion patterns first, we propose a global transformer block (GTB) which preserves detailed information in deep network.",
    "original_url": "http://arxiv.org/pdf/2201.11037v1",
    "original_title": "RTNet: Relation Transformer Network for Diabetic Retinopathy Multi-lesion Segmentation",
    "source": "arxiv",
    "authors": [
      "Shiqi Huang",
      "Jianan Li",
      "Yuze Xiao",
      "Ning Shen",
      "Tingfa Xu"
    ],
    "published": "2022-01-26T16:19:04+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2201.11037v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2201.11037v1_chunk_1",
    "chunk_text": "In addition, to capture the small lesion patterns first, we propose a global transformer block (GTB) which preserves detailed information in deep network. By integrating the above blocks of dual-branches, our network segments the four kinds of lesions simultaneously. Comprehensive experiments on IDRiD and DDR datasets well demonstrate the superiority of our approach, which achieves competitive performance compared to state-of-the-arts.",
    "original_url": "http://arxiv.org/pdf/2201.11037v1",
    "original_title": "RTNet: Relation Transformer Network for Diabetic Retinopathy Multi-lesion Segmentation",
    "source": "arxiv",
    "authors": [
      "Shiqi Huang",
      "Jianan Li",
      "Yuze Xiao",
      "Ning Shen",
      "Tingfa Xu"
    ],
    "published": "2022-01-26T16:19:04+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2201.11037v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2203.16795v1_chunk_0",
    "chunk_text": "Deformable Video Transformer\n\nVideo transformers have recently emerged as an effective alternative to convolutional networks for action classification. However, most prior video transformers adopt either global space-time attention or hand-defined strategies to compare patches within and across frames. These fixed attention schemes not only have high computational cost but, by comparing patches at predetermined locations, they neglect the motion dynamics in the video. In this paper, we introduce the Deformable Video Transformer (DVT), which dynamically predicts a small subset of video patches to attend for each query location based on motion information, thus allowing the model to decide where to look in the video based on correspondences across frames. Crucially, these motion-based correspondences are obtained at zero-cost from information stored in the compressed format of the video.",
    "original_url": "http://arxiv.org/pdf/2203.16795v1",
    "original_title": "Deformable Video Transformer",
    "source": "arxiv",
    "authors": [
      "Jue Wang",
      "Lorenzo Torresani"
    ],
    "published": "2022-03-31T04:52:27+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2203.16795v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2203.16795v1_chunk_1",
    "chunk_text": "Crucially, these motion-based correspondences are obtained at zero-cost from information stored in the compressed format of the video. Our deformable attention mechanism is optimised directly with respect to classification performance, thus eliminating the need for suboptimal hand-design of attention strategies. Experiments on four large-scale video benchmarks (Kinetics-400, Something-Something-V2, EPIC-KITCHENS and Diving-48) demonstrate that, compared to existing video transformers, our model achieves higher accuracy at the same or lower computational cost, and it attains state-of-the-art results on these four datasets.",
    "original_url": "http://arxiv.org/pdf/2203.16795v1",
    "original_title": "Deformable Video Transformer",
    "source": "arxiv",
    "authors": [
      "Jue Wang",
      "Lorenzo Torresani"
    ],
    "published": "2022-03-31T04:52:27+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2203.16795v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2307.09084v1_chunk_0",
    "chunk_text": "Attention over pre-trained Sentence Embeddings for Long Document Classification\n\nDespite being the current de-facto models in most NLP tasks, transformers are often limited to short sequences due to their quadratic attention complexity on the number of tokens. Several attempts to address this issue were studied, either by reducing the cost of the self-attention computation or by modeling smaller sequences and combining them through a recurrence mechanism or using a new transformer model. In this paper, we suggest to take advantage of pre-trained sentence transformers to start from semantically meaningful embeddings of the individual sentences, and then combine them through a small attention layer that scales linearly with the document length. We report the results obtained by this simple architecture on three standard document classification datasets. When compared with the current state-of-the-art models using standard fine-tuning, the studied method obtains competitive results (even if there is no clear best model in this configuration).",
    "original_url": "http://arxiv.org/pdf/2307.09084v1",
    "original_title": "Attention over pre-trained Sentence Embeddings for Long Document Classification",
    "source": "arxiv",
    "authors": [
      "Amine Abdaoui",
      "Sourav Dutta"
    ],
    "published": "2023-07-18T09:06:35+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2307.09084v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2307.09084v1_chunk_1",
    "chunk_text": "When compared with the current state-of-the-art models using standard fine-tuning, the studied method obtains competitive results (even if there is no clear best model in this configuration). We also showcase that the studied architecture obtains better results when freezing the underlying transformers. A configuration that is useful when we need to avoid complete fine-tuning (e.g. when the same frozen transformer is shared by different applications). Finally, two additional experiments are provided to further evaluate the relevancy of the studied architecture over simpler baselines.",
    "original_url": "http://arxiv.org/pdf/2307.09084v1",
    "original_title": "Attention over pre-trained Sentence Embeddings for Long Document Classification",
    "source": "arxiv",
    "authors": [
      "Amine Abdaoui",
      "Sourav Dutta"
    ],
    "published": "2023-07-18T09:06:35+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2307.09084v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2307.09084v1_chunk_2",
    "chunk_text": "Finally, two additional experiments are provided to further evaluate the relevancy of the studied architecture over simpler baselines.",
    "original_url": "http://arxiv.org/pdf/2307.09084v1",
    "original_title": "Attention over pre-trained Sentence Embeddings for Long Document Classification",
    "source": "arxiv",
    "authors": [
      "Amine Abdaoui",
      "Sourav Dutta"
    ],
    "published": "2023-07-18T09:06:35+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2307.09084v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2312.07987v3_chunk_0",
    "chunk_text": "SwitchHead: Accelerating Transformers with Mixture-of-Experts Attention\n\nDespite many recent works on Mixture of Experts (MoEs) for resource-efficient Transformer language models, existing methods mostly focus on MoEs for feedforward layers. Previous attempts at extending MoE to the self-attention layer fail to match the performance of the parameter-matched baseline. Our novel SwitchHead is an effective MoE method for the attention layer that successfully reduces both the compute and memory requirements, achieving wall-clock speedup, while matching the language modeling performance of the baseline Transformer. Our novel MoE mechanism allows SwitchHead to compute up to 8 times fewer attention matrices than the standard Transformer. SwitchHead can also be combined with MoE feedforward layers, resulting in fully-MoE \"SwitchAll\" Transformers.",
    "original_url": "http://arxiv.org/pdf/2312.07987v3",
    "original_title": "SwitchHead: Accelerating Transformers with Mixture-of-Experts Attention",
    "source": "arxiv",
    "authors": [
      "Róbert Csordás",
      "Piotr Piękos",
      "Kazuki Irie",
      "Jürgen Schmidhuber"
    ],
    "published": "2023-12-13T09:00:21+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2312.07987v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2312.07987v3_chunk_1",
    "chunk_text": "SwitchHead can also be combined with MoE feedforward layers, resulting in fully-MoE \"SwitchAll\" Transformers. For our 262M parameter model trained on C4, SwitchHead matches the perplexity of standard models with only 44% compute and 27% memory usage. Zero-shot experiments on downstream tasks confirm the performance of SwitchHead, e.g., achieving more than 3.5% absolute improvements on BliMP compared to the baseline with an equal compute resource.",
    "original_url": "http://arxiv.org/pdf/2312.07987v3",
    "original_title": "SwitchHead: Accelerating Transformers with Mixture-of-Experts Attention",
    "source": "arxiv",
    "authors": [
      "Róbert Csordás",
      "Piotr Piękos",
      "Kazuki Irie",
      "Jürgen Schmidhuber"
    ],
    "published": "2023-12-13T09:00:21+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2312.07987v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2504.04869v1_chunk_0",
    "chunk_text": "Content-Aware Transformer for All-in-one Image Restoration\n\nImage restoration has witnessed significant advancements with the development of deep learning models. Although Transformer architectures have progressed considerably in recent years, challenges remain, particularly the limited receptive field in window-based self-attention. In this work, we propose DSwinIR, a Deformable Sliding window Transformer for Image Restoration. DSwinIR introduces a novel deformable sliding window self-attention that adaptively adjusts receptive fields based on image content, enabling the attention mechanism to focus on important regions and enhance feature extraction aligned with salient features. Additionally, we introduce a central ensemble pattern to reduce the inclusion of irrelevant content within attention windows.",
    "original_url": "http://arxiv.org/pdf/2504.04869v1",
    "original_title": "Content-Aware Transformer for All-in-one Image Restoration",
    "source": "arxiv",
    "authors": [
      "Gang Wu",
      "Junjun Jiang",
      "Kui Jiang",
      "Xianming Liu"
    ],
    "published": "2025-04-07T09:24:41+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2504.04869v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2504.04869v1_chunk_1",
    "chunk_text": "Additionally, we introduce a central ensemble pattern to reduce the inclusion of irrelevant content within attention windows. In this way, the proposed DSwinIR model integrates the deformable sliding window Transformer and central ensemble pattern to amplify the strengths of both CNNs and Transformers while mitigating their limitations. Extensive experiments on various image restoration tasks demonstrate that DSwinIR achieves state-of-the-art performance. For example, in image deraining, compared to DRSformer on the SPA dataset, DSwinIR achieves a 0.66 dB PSNR improvement. In all-in-one image restoration, compared to PromptIR, DSwinIR achieves over a 0.66 dB and 1.04 dB improvement on three-task and five-task settings, respectively.",
    "original_url": "http://arxiv.org/pdf/2504.04869v1",
    "original_title": "Content-Aware Transformer for All-in-one Image Restoration",
    "source": "arxiv",
    "authors": [
      "Gang Wu",
      "Junjun Jiang",
      "Kui Jiang",
      "Xianming Liu"
    ],
    "published": "2025-04-07T09:24:41+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2504.04869v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2504.04869v1_chunk_2",
    "chunk_text": "In all-in-one image restoration, compared to PromptIR, DSwinIR achieves over a 0.66 dB and 1.04 dB improvement on three-task and five-task settings, respectively. Pretrained models and code are available at our project https://github.com/Aitical/DSwinIR.",
    "original_url": "http://arxiv.org/pdf/2504.04869v1",
    "original_title": "Content-Aware Transformer for All-in-one Image Restoration",
    "source": "arxiv",
    "authors": [
      "Gang Wu",
      "Junjun Jiang",
      "Kui Jiang",
      "Xianming Liu"
    ],
    "published": "2025-04-07T09:24:41+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2504.04869v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2010.15437v1_chunk_0",
    "chunk_text": "Memory Attentive Fusion: External Language Model Integration for Transformer-based Sequence-to-Sequence Model\n\nThis paper presents a novel fusion method for integrating an external language model (LM) into the Transformer based sequence-to-sequence (seq2seq) model. While paired data are basically required to train the seq2seq model, the external LM can be trained with only unpaired data. Thus, it is important to leverage memorized knowledge in the external LM for building the seq2seq model, since it is hard to prepare a large amount of paired data. However, the existing fusion methods assume that the LM is integrated with recurrent neural network-based seq2seq models instead of the Transformer. Therefore, this paper proposes a fusion method that can explicitly utilize network structures in the Transformer.",
    "original_url": "http://arxiv.org/pdf/2010.15437v1",
    "original_title": "Memory Attentive Fusion: External Language Model Integration for Transformer-based Sequence-to-Sequence Model",
    "source": "arxiv",
    "authors": [
      "Mana Ihori",
      "Ryo Masumura",
      "Naoki Makishima",
      "Tomohiro Tanaka",
      "Akihiko Takashima",
      "Shota Orihashi"
    ],
    "published": "2020-10-29T09:16:23+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2010.15437v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2010.15437v1_chunk_1",
    "chunk_text": "Therefore, this paper proposes a fusion method that can explicitly utilize network structures in the Transformer. The proposed method, called {\\bf memory attentive fusion}, leverages the Transformer-style attention mechanism that repeats source-target attention in a multi-hop manner for reading the memorized knowledge in the LM. Our experiments on two text-style conversion tasks demonstrate that the proposed method performs better than conventional fusion methods.",
    "original_url": "http://arxiv.org/pdf/2010.15437v1",
    "original_title": "Memory Attentive Fusion: External Language Model Integration for Transformer-based Sequence-to-Sequence Model",
    "source": "arxiv",
    "authors": [
      "Mana Ihori",
      "Ryo Masumura",
      "Naoki Makishima",
      "Tomohiro Tanaka",
      "Akihiko Takashima",
      "Shota Orihashi"
    ],
    "published": "2020-10-29T09:16:23+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2010.15437v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2310.11984v3_chunk_0",
    "chunk_text": "From Interpolation to Extrapolation: Complete Length Generalization for Arithmetic Transformers\n\nIn this paper, we investigate the inherent capabilities of transformer models in learning arithmetic algorithms, such as addition and parity. Through experiments and attention analysis, we identify a number of crucial factors for achieving optimal length generalization. We show that transformer models are able to generalize to long lengths with the help of targeted attention biasing. In particular, our solution solves the Parity task, a well-known and theoretically proven failure mode for Transformers. We then introduce Attention Bias Calibration (ABC), a calibration stage that enables the model to automatically learn the proper attention biases, which we show to be connected to mechanisms in relative position encoding.",
    "original_url": "http://arxiv.org/pdf/2310.11984v3",
    "original_title": "From Interpolation to Extrapolation: Complete Length Generalization for Arithmetic Transformers",
    "source": "arxiv",
    "authors": [
      "Shaoxiong Duan",
      "Yining Shi",
      "Wei Xu"
    ],
    "published": "2023-10-18T14:10:47+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2310.11984v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2310.11984v3_chunk_1",
    "chunk_text": "We then introduce Attention Bias Calibration (ABC), a calibration stage that enables the model to automatically learn the proper attention biases, which we show to be connected to mechanisms in relative position encoding. We demonstrate that using ABC, the transformer model can achieve unprecedented near-perfect length generalization on certain arithmetic tasks. In addition, we show that ABC bears remarkable similarities to RPE and LoRA, which may indicate the potential for applications to more complex tasks.",
    "original_url": "http://arxiv.org/pdf/2310.11984v3",
    "original_title": "From Interpolation to Extrapolation: Complete Length Generalization for Arithmetic Transformers",
    "source": "arxiv",
    "authors": [
      "Shaoxiong Duan",
      "Yining Shi",
      "Wei Xu"
    ],
    "published": "2023-10-18T14:10:47+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2310.11984v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2402.15096v1_chunk_0",
    "chunk_text": "Multimodal Transformer With a Low-Computational-Cost Guarantee\n\nTransformer-based models have significantly improved performance across a range of multimodal understanding tasks, such as visual question answering and action recognition. However, multimodal Transformers significantly suffer from a quadratic complexity of the multi-head attention with the input sequence length, especially as the number of modalities increases. To address this, we introduce Low-Cost Multimodal Transformer (LoCoMT), a novel multimodal attention mechanism that aims to reduce computational cost during training and inference with minimal performance loss. Specifically, by assigning different multimodal attention patterns to each attention head, LoCoMT can flexibly control multimodal signals and theoretically ensures a reduced computational cost compared to existing multimodal Transformer variants. Experimental results on two multimodal datasets, namely Audioset and MedVidCL demonstrate that LoCoMT not only reduces GFLOPs but also matches or even outperforms established models.",
    "original_url": "http://arxiv.org/pdf/2402.15096v1",
    "original_title": "Multimodal Transformer With a Low-Computational-Cost Guarantee",
    "source": "arxiv",
    "authors": [
      "Sungjin Park",
      "Edward Choi"
    ],
    "published": "2024-02-23T05:09:35+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2402.15096v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2402.15096v1_chunk_1",
    "chunk_text": "Experimental results on two multimodal datasets, namely Audioset and MedVidCL demonstrate that LoCoMT not only reduces GFLOPs but also matches or even outperforms established models.",
    "original_url": "http://arxiv.org/pdf/2402.15096v1",
    "original_title": "Multimodal Transformer With a Low-Computational-Cost Guarantee",
    "source": "arxiv",
    "authors": [
      "Sungjin Park",
      "Edward Choi"
    ],
    "published": "2024-02-23T05:09:35+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2402.15096v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2503.03321v1_chunk_0",
    "chunk_text": "See What You Are Told: Visual Attention Sink in Large Multimodal Models\n\nLarge multimodal models (LMMs) \"see\" images by leveraging the attention mechanism between text and visual tokens in the transformer decoder. Ideally, these models should focus on key visual information relevant to the text token. However, recent findings indicate that LMMs have an extraordinary tendency to consistently allocate high attention weights to specific visual tokens, even when these tokens are irrelevant to the corresponding text. In this study, we investigate the property behind the appearance of these irrelevant visual tokens and examine their characteristics. Our findings show that this behavior arises due to the massive activation of certain hidden state dimensions, which resembles the attention sink found in language models.",
    "original_url": "http://arxiv.org/pdf/2503.03321v1",
    "original_title": "See What You Are Told: Visual Attention Sink in Large Multimodal Models",
    "source": "arxiv",
    "authors": [
      "Seil Kang",
      "Jinyeong Kim",
      "Junhyeok Kim",
      "Seong Jae Hwang"
    ],
    "published": "2025-03-05T09:55:07+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2503.03321v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2503.03321v1_chunk_1",
    "chunk_text": "Our findings show that this behavior arises due to the massive activation of certain hidden state dimensions, which resembles the attention sink found in language models. Hence, we refer to this phenomenon as the visual attention sink. In particular, our analysis reveals that removing the irrelevant visual sink tokens does not impact model performance, despite receiving high attention weights. Consequently, we recycle the attention to these tokens as surplus resources, redistributing the attention budget to enhance focus on the image. To achieve this, we introduce Visual Attention Redistribution (VAR), a method that redistributes attention in image-centric heads, which we identify as innately focusing on visual information.",
    "original_url": "http://arxiv.org/pdf/2503.03321v1",
    "original_title": "See What You Are Told: Visual Attention Sink in Large Multimodal Models",
    "source": "arxiv",
    "authors": [
      "Seil Kang",
      "Jinyeong Kim",
      "Junhyeok Kim",
      "Seong Jae Hwang"
    ],
    "published": "2025-03-05T09:55:07+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2503.03321v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2503.03321v1_chunk_2",
    "chunk_text": "To achieve this, we introduce Visual Attention Redistribution (VAR), a method that redistributes attention in image-centric heads, which we identify as innately focusing on visual information. VAR can be seamlessly applied across different LMMs to improve performance on a wide range of tasks, including general vision-language tasks, visual hallucination tasks, and vision-centric tasks, all without the need for additional training, models, or inference steps. Experimental results demonstrate that VAR enables LMMs to process visual information more effectively by adjusting their internal attention mechanisms, offering a new direction to enhancing the multimodal capabilities of LMMs.",
    "original_url": "http://arxiv.org/pdf/2503.03321v1",
    "original_title": "See What You Are Told: Visual Attention Sink in Large Multimodal Models",
    "source": "arxiv",
    "authors": [
      "Seil Kang",
      "Jinyeong Kim",
      "Junhyeok Kim",
      "Seong Jae Hwang"
    ],
    "published": "2025-03-05T09:55:07+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2503.03321v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2102.00529v1_chunk_0",
    "chunk_text": "Decoupling the Role of Data, Attention, and Losses in Multimodal Transformers\n\nRecently multimodal transformer models have gained popularity because their performance on language and vision tasks suggest they learn rich visual-linguistic representations. Focusing on zero-shot image retrieval tasks, we study three important factors which can impact the quality of learned representations: pretraining data, the attention mechanism, and loss functions. By pretraining models on six datasets, we observe that dataset noise and language similarity to our downstream task are important indicators of model performance. Through architectural analysis, we learn that models with a multimodal attention mechanism can outperform deeper models with modality specific attention mechanisms. Finally, we show that successful contrastive losses used in the self-supervised learning literature do not yield similar performance gains when used in multimodal transformers",
    "original_url": "http://arxiv.org/pdf/2102.00529v1",
    "original_title": "Decoupling the Role of Data, Attention, and Losses in Multimodal Transformers",
    "source": "arxiv",
    "authors": [
      "Lisa Anne Hendricks",
      "John Mellor",
      "Rosalia Schneider",
      "Jean-Baptiste Alayrac",
      "Aida Nematzadeh"
    ],
    "published": "2021-01-31T20:36:41+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2102.00529v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2102.00529v1_chunk_1",
    "chunk_text": "Finally, we show that successful contrastive losses used in the self-supervised learning literature do not yield similar performance gains when used in multimodal transformers",
    "original_url": "http://arxiv.org/pdf/2102.00529v1",
    "original_title": "Decoupling the Role of Data, Attention, and Losses in Multimodal Transformers",
    "source": "arxiv",
    "authors": [
      "Lisa Anne Hendricks",
      "John Mellor",
      "Rosalia Schneider",
      "Jean-Baptiste Alayrac",
      "Aida Nematzadeh"
    ],
    "published": "2021-01-31T20:36:41+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2102.00529v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1911.09483v1_chunk_0",
    "chunk_text": "MUSE: Parallel Multi-Scale Attention for Sequence to Sequence Learning\n\nIn sequence to sequence learning, the self-attention mechanism proves to be highly effective, and achieves significant improvements in many tasks. However, the self-attention mechanism is not without its own flaws. Although self-attention can model extremely long dependencies, the attention in deep layers tends to overconcentrate on a single token, leading to insufficient use of local information and difficultly in representing long sequences. In this work, we explore parallel multi-scale representation learning on sequence data, striving to capture both long-range and short-range language structures. To this end, we propose the Parallel MUlti-Scale attEntion (MUSE) and MUSE-simple.",
    "original_url": "http://arxiv.org/pdf/1911.09483v1",
    "original_title": "MUSE: Parallel Multi-Scale Attention for Sequence to Sequence Learning",
    "source": "arxiv",
    "authors": [
      "Guangxiang Zhao",
      "Xu Sun",
      "Jingjing Xu",
      "Zhiyuan Zhang",
      "Liangchen Luo"
    ],
    "published": "2019-11-17T09:36:07+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1911.09483v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1911.09483v1_chunk_1",
    "chunk_text": "To this end, we propose the Parallel MUlti-Scale attEntion (MUSE) and MUSE-simple. MUSE-simple contains the basic idea of parallel multi-scale sequence representation learning, and it encodes the sequence in parallel, in terms of different scales with the help from self-attention, and pointwise transformation. MUSE builds on MUSE-simple and explores combining convolution and self-attention for learning sequence representations from more different scales. We focus on machine translation and the proposed approach achieves substantial performance improvements over Transformer, especially on long sequences. More importantly, we find that although conceptually simple, its success in practice requires intricate considerations, and the multi-scale attention must build on unified semantic space.",
    "original_url": "http://arxiv.org/pdf/1911.09483v1",
    "original_title": "MUSE: Parallel Multi-Scale Attention for Sequence to Sequence Learning",
    "source": "arxiv",
    "authors": [
      "Guangxiang Zhao",
      "Xu Sun",
      "Jingjing Xu",
      "Zhiyuan Zhang",
      "Liangchen Luo"
    ],
    "published": "2019-11-17T09:36:07+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1911.09483v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1911.09483v1_chunk_2",
    "chunk_text": "More importantly, we find that although conceptually simple, its success in practice requires intricate considerations, and the multi-scale attention must build on unified semantic space. Under common setting, the proposed model achieves substantial performance and outperforms all previous models on three main machine translation tasks. In addition, MUSE has potential for accelerating inference due to its parallelism. Code will be available at https://github.com/lancopku/MUSE",
    "original_url": "http://arxiv.org/pdf/1911.09483v1",
    "original_title": "MUSE: Parallel Multi-Scale Attention for Sequence to Sequence Learning",
    "source": "arxiv",
    "authors": [
      "Guangxiang Zhao",
      "Xu Sun",
      "Jingjing Xu",
      "Zhiyuan Zhang",
      "Liangchen Luo"
    ],
    "published": "2019-11-17T09:36:07+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1911.09483v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2209.00606v1_chunk_0",
    "chunk_text": "Sparse Attention Acceleration with Synergistic In-Memory Pruning and On-Chip Recomputation\n\nAs its core computation, a self-attention mechanism gauges pairwise correlations across the entire input sequence. Despite favorable performance, calculating pairwise correlations is prohibitively costly. While recent work has shown the benefits of runtime pruning of elements with low attention scores, the quadratic complexity of self-attention mechanisms and their on-chip memory capacity demands are overlooked. This work addresses these constraints by architecting an accelerator, called SPRINT, which leverages the inherent parallelism of ReRAM crossbar arrays to compute attention scores in an approximate manner. Our design prunes the low attention scores using a lightweight analog thresholding circuitry within ReRAM, enabling SPRINT to fetch only a small subset of relevant data to on-chip memory.",
    "original_url": "http://arxiv.org/pdf/2209.00606v1",
    "original_title": "Sparse Attention Acceleration with Synergistic In-Memory Pruning and On-Chip Recomputation",
    "source": "arxiv",
    "authors": [
      "Amir Yazdanbakhsh",
      "Ashkan Moradifirouzabadi",
      "Zheng Li",
      "Mingu Kang"
    ],
    "published": "2022-09-01T17:18:19+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2209.00606v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2209.00606v1_chunk_1",
    "chunk_text": "Our design prunes the low attention scores using a lightweight analog thresholding circuitry within ReRAM, enabling SPRINT to fetch only a small subset of relevant data to on-chip memory. To mitigate potential negative repercussions for model accuracy, SPRINT re-computes the attention scores for the few fetched data in digital. The combined in-memory pruning and on-chip recompute of the relevant attention scores enables SPRINT to transform quadratic complexity to a merely linear one. In addition, we identify and leverage a dynamic spatial locality between the adjacent attention operations even after pruning, which eliminates costly yet redundant data fetches. We evaluate our proposed technique on a wide range of state-of-the-art transformer models.",
    "original_url": "http://arxiv.org/pdf/2209.00606v1",
    "original_title": "Sparse Attention Acceleration with Synergistic In-Memory Pruning and On-Chip Recomputation",
    "source": "arxiv",
    "authors": [
      "Amir Yazdanbakhsh",
      "Ashkan Moradifirouzabadi",
      "Zheng Li",
      "Mingu Kang"
    ],
    "published": "2022-09-01T17:18:19+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2209.00606v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2209.00606v1_chunk_2",
    "chunk_text": "We evaluate our proposed technique on a wide range of state-of-the-art transformer models. On average, SPRINT yields 7.5x speedup and 19.6x energy reduction when total 16KB on-chip memory is used, while virtually on par with iso-accuracy of the baseline models (on average 0.36% degradation).",
    "original_url": "http://arxiv.org/pdf/2209.00606v1",
    "original_title": "Sparse Attention Acceleration with Synergistic In-Memory Pruning and On-Chip Recomputation",
    "source": "arxiv",
    "authors": [
      "Amir Yazdanbakhsh",
      "Ashkan Moradifirouzabadi",
      "Zheng Li",
      "Mingu Kang"
    ],
    "published": "2022-09-01T17:18:19+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2209.00606v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2304.07235v4_chunk_0",
    "chunk_text": "Mapping of attention mechanisms to a generalized Potts model\n\nTransformers are neural networks that revolutionized natural language processing and machine learning. They process sequences of inputs, like words, using a mechanism called self-attention, which is trained via masked language modeling (MLM). In MLM, a word is randomly masked in an input sequence, and the network is trained to predict the missing word. Despite the practical success of transformers, it remains unclear what type of data distribution self-attention can learn efficiently. Here, we show analytically that if one decouples the treatment of word positions and embeddings, a single layer of self-attention learns the conditionals of a generalized Potts model with interactions between sites and Potts colors.",
    "original_url": "http://arxiv.org/pdf/2304.07235v4",
    "original_title": "Mapping of attention mechanisms to a generalized Potts model",
    "source": "arxiv",
    "authors": [
      "Riccardo Rende",
      "Federica Gerace",
      "Alessandro Laio",
      "Sebastian Goldt"
    ],
    "published": "2023-04-14T16:32:56+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2304.07235v4"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2304.07235v4_chunk_1",
    "chunk_text": "Here, we show analytically that if one decouples the treatment of word positions and embeddings, a single layer of self-attention learns the conditionals of a generalized Potts model with interactions between sites and Potts colors. Moreover, we show that training this neural network is exactly equivalent to solving the inverse Potts problem by the so-called pseudo-likelihood method, well known in statistical physics. Using this mapping, we compute the generalization error of self-attention in a model scenario analytically using the replica method.",
    "original_url": "http://arxiv.org/pdf/2304.07235v4",
    "original_title": "Mapping of attention mechanisms to a generalized Potts model",
    "source": "arxiv",
    "authors": [
      "Riccardo Rende",
      "Federica Gerace",
      "Alessandro Laio",
      "Sebastian Goldt"
    ],
    "published": "2023-04-14T16:32:56+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2304.07235v4"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2411.01542v1_chunk_0",
    "chunk_text": "FactorizePhys: Matrix Factorization for Multidimensional Attention in Remote Physiological Sensing\n\nRemote photoplethysmography (rPPG) enables non-invasive extraction of blood volume pulse signals through imaging, transforming spatial-temporal data into time series signals. Advances in end-to-end rPPG approaches have focused on this transformation where attention mechanisms are crucial for feature extraction. However, existing methods compute attention disjointly across spatial, temporal, and channel dimensions. Here, we propose the Factorized Self-Attention Module (FSAM), which jointly computes multidimensional attention from voxel embeddings using nonnegative matrix factorization. To demonstrate FSAM's effectiveness, we developed FactorizePhys, an end-to-end 3D-CNN architecture for estimating blood volume pulse signals from raw video frames.",
    "original_url": "http://arxiv.org/pdf/2411.01542v1",
    "original_title": "FactorizePhys: Matrix Factorization for Multidimensional Attention in Remote Physiological Sensing",
    "source": "arxiv",
    "authors": [
      "Jitesh Joshi",
      "Sos S. Agaian",
      "Youngjun Cho"
    ],
    "published": "2024-11-03T12:22:58+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2411.01542v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2411.01542v1_chunk_1",
    "chunk_text": "To demonstrate FSAM's effectiveness, we developed FactorizePhys, an end-to-end 3D-CNN architecture for estimating blood volume pulse signals from raw video frames. Our approach adeptly factorizes voxel embeddings to achieve comprehensive spatial, temporal, and channel attention, enhancing performance of generic signal extraction tasks. Furthermore, we deploy FSAM within an existing 2D-CNN-based rPPG architecture to illustrate its versatility. FSAM and FactorizePhys are thoroughly evaluated against state-of-the-art rPPG methods, each representing different types of architecture and attention mechanism. We perform ablation studies to investigate the architectural decisions and hyperparameters of FSAM.",
    "original_url": "http://arxiv.org/pdf/2411.01542v1",
    "original_title": "FactorizePhys: Matrix Factorization for Multidimensional Attention in Remote Physiological Sensing",
    "source": "arxiv",
    "authors": [
      "Jitesh Joshi",
      "Sos S. Agaian",
      "Youngjun Cho"
    ],
    "published": "2024-11-03T12:22:58+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2411.01542v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2411.01542v1_chunk_2",
    "chunk_text": "We perform ablation studies to investigate the architectural decisions and hyperparameters of FSAM. Experiments on four publicly available datasets and intuitive visualization of learned spatial-temporal features substantiate the effectiveness of FSAM and enhanced cross-dataset generalization in estimating rPPG signals, suggesting its broader potential as a multidimensional attention mechanism. The code is accessible at https://github.com/PhysiologicAILab/FactorizePhys.",
    "original_url": "http://arxiv.org/pdf/2411.01542v1",
    "original_title": "FactorizePhys: Matrix Factorization for Multidimensional Attention in Remote Physiological Sensing",
    "source": "arxiv",
    "authors": [
      "Jitesh Joshi",
      "Sos S. Agaian",
      "Youngjun Cho"
    ],
    "published": "2024-11-03T12:22:58+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2411.01542v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2412.16112v1_chunk_0",
    "chunk_text": "CLEAR: Conv-Like Linearization Revs Pre-Trained Diffusion Transformers Up\n\nDiffusion Transformers (DiT) have become a leading architecture in image generation. However, the quadratic complexity of attention mechanisms, which are responsible for modeling token-wise relationships, results in significant latency when generating high-resolution images. To address this issue, we aim at a linear attention mechanism in this paper that reduces the complexity of pre-trained DiTs to linear. We begin our exploration with a comprehensive summary of existing efficient attention mechanisms and identify four key factors crucial for successful linearization of pre-trained DiTs: locality, formulation consistency, high-rank attention maps, and feature integrity. Based on these insights, we introduce a convolution-like local attention strategy termed CLEAR, which limits feature interactions to a local window around each query token, and thus achieves linear complexity.",
    "original_url": "http://arxiv.org/pdf/2412.16112v1",
    "original_title": "CLEAR: Conv-Like Linearization Revs Pre-Trained Diffusion Transformers Up",
    "source": "arxiv",
    "authors": [
      "Songhua Liu",
      "Zhenxiong Tan",
      "Xinchao Wang"
    ],
    "published": "2024-12-20T17:57:09+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2412.16112v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2412.16112v1_chunk_1",
    "chunk_text": "Based on these insights, we introduce a convolution-like local attention strategy termed CLEAR, which limits feature interactions to a local window around each query token, and thus achieves linear complexity. Our experiments indicate that, by fine-tuning the attention layer on merely 10K self-generated samples for 10K iterations, we can effectively transfer knowledge from a pre-trained DiT to a student model with linear complexity, yielding results comparable to the teacher model. Simultaneously, it reduces attention computations by 99.5% and accelerates generation by 6.3 times for generating 8K-resolution images. Furthermore, we investigate favorable properties in the distilled attention layers, such as zero-shot generalization cross various models and plugins, and improved support for multi-GPU parallel inference. Models and codes are available here: https://github.com/Huage001/CLEAR.",
    "original_url": "http://arxiv.org/pdf/2412.16112v1",
    "original_title": "CLEAR: Conv-Like Linearization Revs Pre-Trained Diffusion Transformers Up",
    "source": "arxiv",
    "authors": [
      "Songhua Liu",
      "Zhenxiong Tan",
      "Xinchao Wang"
    ],
    "published": "2024-12-20T17:57:09+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2412.16112v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2412.16112v1_chunk_2",
    "chunk_text": "Models and codes are available here: https://github.com/Huage001/CLEAR.",
    "original_url": "http://arxiv.org/pdf/2412.16112v1",
    "original_title": "CLEAR: Conv-Like Linearization Revs Pre-Trained Diffusion Transformers Up",
    "source": "arxiv",
    "authors": [
      "Songhua Liu",
      "Zhenxiong Tan",
      "Xinchao Wang"
    ],
    "published": "2024-12-20T17:57:09+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2412.16112v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2405.12328v1_chunk_0",
    "chunk_text": "Multi-dimension Transformer with Attention-based Filtering for Medical Image Segmentation\n\nThe accurate segmentation of medical images is crucial for diagnosing and treating diseases. Recent studies demonstrate that vision transformer-based methods have significantly improved performance in medical image segmentation, primarily due to their superior ability to establish global relationships among features and adaptability to various inputs. However, these methods struggle with the low signal-to-noise ratio inherent to medical images. Additionally, the effective utilization of channel and spatial information, which are essential for medical image segmentation, is limited by the representation capacity of self-attention. To address these challenges, we propose a multi-dimension transformer with attention-based filtering (MDT-AF), which redesigns the patch embedding and self-attention mechanism for medical image segmentation.",
    "original_url": "http://arxiv.org/pdf/2405.12328v1",
    "original_title": "Multi-dimension Transformer with Attention-based Filtering for Medical Image Segmentation",
    "source": "arxiv",
    "authors": [
      "Wentao Wang",
      "Xi Xiao",
      "Mingjie Liu",
      "Qing Tian",
      "Xuanyao Huang",
      "Qizhen Lan",
      "Swalpa Kumar Roy",
      "Tianyang Wang"
    ],
    "published": "2024-05-20T18:52:41+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2405.12328v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2405.12328v1_chunk_1",
    "chunk_text": "To address these challenges, we propose a multi-dimension transformer with attention-based filtering (MDT-AF), which redesigns the patch embedding and self-attention mechanism for medical image segmentation. MDT-AF incorporates an attention-based feature filtering mechanism into the patch embedding blocks and employs a coarse-to-fine process to mitigate the impact of low signal-to-noise ratio. To better capture complex structures in medical images, MDT-AF extends the self-attention mechanism to incorporate spatial and channel dimensions, enriching feature representation. Moreover, we introduce an interaction mechanism to improve the feature aggregation between spatial and channel dimensions. Experimental results on three public medical image segmentation benchmarks show that MDT-AF achieves state-of-the-art (SOTA) performance.",
    "original_url": "http://arxiv.org/pdf/2405.12328v1",
    "original_title": "Multi-dimension Transformer with Attention-based Filtering for Medical Image Segmentation",
    "source": "arxiv",
    "authors": [
      "Wentao Wang",
      "Xi Xiao",
      "Mingjie Liu",
      "Qing Tian",
      "Xuanyao Huang",
      "Qizhen Lan",
      "Swalpa Kumar Roy",
      "Tianyang Wang"
    ],
    "published": "2024-05-20T18:52:41+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2405.12328v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2405.12328v1_chunk_2",
    "chunk_text": "Experimental results on three public medical image segmentation benchmarks show that MDT-AF achieves state-of-the-art (SOTA) performance.",
    "original_url": "http://arxiv.org/pdf/2405.12328v1",
    "original_title": "Multi-dimension Transformer with Attention-based Filtering for Medical Image Segmentation",
    "source": "arxiv",
    "authors": [
      "Wentao Wang",
      "Xi Xiao",
      "Mingjie Liu",
      "Qing Tian",
      "Xuanyao Huang",
      "Qizhen Lan",
      "Swalpa Kumar Roy",
      "Tianyang Wang"
    ],
    "published": "2024-05-20T18:52:41+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2405.12328v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2412.11245v1_chunk_0",
    "chunk_text": "Transformer-Based Bearing Fault Detection using Temporal Decomposition Attention Mechanism\n\nBearing fault detection is a critical task in predictive maintenance, where accurate and timely fault identification can prevent costly downtime and equipment damage. Traditional attention mechanisms in Transformer neural networks often struggle to capture the complex temporal patterns in bearing vibration data, leading to suboptimal performance. To address this limitation, we propose a novel attention mechanism, Temporal Decomposition Attention (TDA), which combines temporal bias encoding with seasonal-trend decomposition to capture both long-term dependencies and periodic fluctuations in time series data. Additionally, we incorporate the Hull Exponential Moving Average (HEMA) for feature extraction, enabling the model to effectively capture meaningful characteristics from the data while reducing noise. Our approach integrates TDA into the Transformer architecture, allowing the model to focus separately on the trend and seasonal components of the data.",
    "original_url": "http://arxiv.org/pdf/2412.11245v1",
    "original_title": "Transformer-Based Bearing Fault Detection using Temporal Decomposition Attention Mechanism",
    "source": "arxiv",
    "authors": [
      "Marzieh Mirzaeibonehkhater",
      "Mohammad Ali Labbaf-Khaniki",
      "Mohammad Manthouri"
    ],
    "published": "2024-12-15T16:51:31+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2412.11245v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2412.11245v1_chunk_1",
    "chunk_text": "Our approach integrates TDA into the Transformer architecture, allowing the model to focus separately on the trend and seasonal components of the data. Experimental results on the Case Western Reserve University (CWRU) bearing fault detection dataset demonstrate that our approach outperforms traditional attention mechanisms and achieves state-of-the-art performance in terms of accuracy and interpretability. The HEMA-Transformer-TDA model achieves an accuracy of 98.1%, with exceptional precision, recall, and F1-scores, demonstrating its effectiveness in bearing fault detection and its potential for application in other time series tasks with seasonal patterns or trends.",
    "original_url": "http://arxiv.org/pdf/2412.11245v1",
    "original_title": "Transformer-Based Bearing Fault Detection using Temporal Decomposition Attention Mechanism",
    "source": "arxiv",
    "authors": [
      "Marzieh Mirzaeibonehkhater",
      "Mohammad Ali Labbaf-Khaniki",
      "Mohammad Manthouri"
    ],
    "published": "2024-12-15T16:51:31+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2412.11245v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2310.11960v3_chunk_0",
    "chunk_text": "Fast Multipole Attention: A Divide-and-Conquer Attention Mechanism for Long Sequences\n\nTransformer-based models have achieved state-of-the-art performance in many areas. However, the quadratic complexity of self-attention with respect to the input length hinders the applicability of Transformer-based models to long sequences. To address this, we present Fast Multipole Attention, a new attention mechanism that uses a divide-and-conquer strategy to reduce the time and memory complexity of attention for sequences of length $n$ from $\\mathcal{O}(n^2)$ to $\\mathcal{O}(n \\log n)$ or $O(n)$, while retaining a global receptive field. The hierarchical approach groups queries, keys, and values into $\\mathcal{O}( \\log n)$ levels of resolution, where groups at greater distances are increasingly larger in size and the weights to compute group quantities are learned. As such, the interaction between tokens far from each other is considered in lower resolution in an efficient hierarchical manner.",
    "original_url": "http://arxiv.org/pdf/2310.11960v3",
    "original_title": "Fast Multipole Attention: A Divide-and-Conquer Attention Mechanism for Long Sequences",
    "source": "arxiv",
    "authors": [
      "Yanming Kang",
      "Giang Tran",
      "Hans De Sterck"
    ],
    "published": "2023-10-18T13:40:41+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2310.11960v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2310.11960v3_chunk_1",
    "chunk_text": "As such, the interaction between tokens far from each other is considered in lower resolution in an efficient hierarchical manner. The overall complexity of Fast Multipole Attention is $\\mathcal{O}(n)$ or $\\mathcal{O}(n \\log n)$, depending on whether the queries are down-sampled or not. This multi-level divide-and-conquer strategy is inspired by fast summation methods from $n$-body physics and the Fast Multipole Method. We perform evaluation on autoregressive and bidirectional language modeling tasks and compare our Fast Multipole Attention model with other efficient attention variants on medium-size datasets. We find empirically that the Fast Multipole Transformer performs much better than other efficient transformers in terms of memory size and accuracy.",
    "original_url": "http://arxiv.org/pdf/2310.11960v3",
    "original_title": "Fast Multipole Attention: A Divide-and-Conquer Attention Mechanism for Long Sequences",
    "source": "arxiv",
    "authors": [
      "Yanming Kang",
      "Giang Tran",
      "Hans De Sterck"
    ],
    "published": "2023-10-18T13:40:41+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2310.11960v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2310.11960v3_chunk_2",
    "chunk_text": "We find empirically that the Fast Multipole Transformer performs much better than other efficient transformers in terms of memory size and accuracy. The Fast Multipole Attention mechanism has the potential to empower large language models with much greater sequence lengths, taking the full context into account in an efficient, naturally hierarchical manner during training and when generating long sequences.",
    "original_url": "http://arxiv.org/pdf/2310.11960v3",
    "original_title": "Fast Multipole Attention: A Divide-and-Conquer Attention Mechanism for Long Sequences",
    "source": "arxiv",
    "authors": [
      "Yanming Kang",
      "Giang Tran",
      "Hans De Sterck"
    ],
    "published": "2023-10-18T13:40:41+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2310.11960v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2304.03481v1_chunk_0",
    "chunk_text": "PSLT: A Light-weight Vision Transformer with Ladder Self-Attention and Progressive Shift\n\nVision Transformer (ViT) has shown great potential for various visual tasks due to its ability to model long-range dependency. However, ViT requires a large amount of computing resource to compute the global self-attention. In this work, we propose a ladder self-attention block with multiple branches and a progressive shift mechanism to develop a light-weight transformer backbone that requires less computing resources (e.g. a relatively small number of parameters and FLOPs), termed Progressive Shift Ladder Transformer (PSLT). First, the ladder self-attention block reduces the computational cost by modelling local self-attention in each branch.",
    "original_url": "http://arxiv.org/pdf/2304.03481v1",
    "original_title": "PSLT: A Light-weight Vision Transformer with Ladder Self-Attention and Progressive Shift",
    "source": "arxiv",
    "authors": [
      "Gaojie Wu",
      "Wei-Shi Zheng",
      "Yutong Lu",
      "Qi Tian"
    ],
    "published": "2023-04-07T05:21:37+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2304.03481v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2304.03481v1_chunk_1",
    "chunk_text": "First, the ladder self-attention block reduces the computational cost by modelling local self-attention in each branch. In the meanwhile, the progressive shift mechanism is proposed to enlarge the receptive field in the ladder self-attention block by modelling diverse local self-attention for each branch and interacting among these branches. Second, the input feature of the ladder self-attention block is split equally along the channel dimension for each branch, which considerably reduces the computational cost in the ladder self-attention block (with nearly 1/3 the amount of parameters and FLOPs), and the outputs of these branches are then collaborated by a pixel-adaptive fusion. Therefore, the ladder self-attention block with a relatively small number of parameters and FLOPs is capable of modelling long-range interactions. Based on the ladder self-attention block, PSLT performs well on several vision tasks, including image classification, objection detection and person re-identification.",
    "original_url": "http://arxiv.org/pdf/2304.03481v1",
    "original_title": "PSLT: A Light-weight Vision Transformer with Ladder Self-Attention and Progressive Shift",
    "source": "arxiv",
    "authors": [
      "Gaojie Wu",
      "Wei-Shi Zheng",
      "Yutong Lu",
      "Qi Tian"
    ],
    "published": "2023-04-07T05:21:37+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2304.03481v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2304.03481v1_chunk_2",
    "chunk_text": "Based on the ladder self-attention block, PSLT performs well on several vision tasks, including image classification, objection detection and person re-identification. On the ImageNet-1k dataset, PSLT achieves a top-1 accuracy of 79.9% with 9.2M parameters and 1.9G FLOPs, which is comparable to several existing models with more than 20M parameters and 4G FLOPs. Code is available at https://isee-ai.cn/wugaojie/PSLT.html.",
    "original_url": "http://arxiv.org/pdf/2304.03481v1",
    "original_title": "PSLT: A Light-weight Vision Transformer with Ladder Self-Attention and Progressive Shift",
    "source": "arxiv",
    "authors": [
      "Gaojie Wu",
      "Wei-Shi Zheng",
      "Yutong Lu",
      "Qi Tian"
    ],
    "published": "2023-04-07T05:21:37+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2304.03481v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2106.05968v2_chunk_0",
    "chunk_text": "Space-time Mixing Attention for Video Transformer\n\nThis paper is on video recognition using Transformers. Very recent attempts in this area have demonstrated promising results in terms of recognition accuracy, yet they have been also shown to induce, in many cases, significant computational overheads due to the additional modelling of the temporal information. In this work, we propose a Video Transformer model the complexity of which scales linearly with the number of frames in the video sequence and hence induces no overhead compared to an image-based Transformer model. To achieve this, our model makes two approximations to the full space-time attention used in Video Transformers: (a) It restricts time attention to a local temporal window and capitalizes on the Transformer's depth to obtain full temporal coverage of the video sequence. (b) It uses efficient space-time mixing to attend jointly spatial and temporal locations without inducing any additional cost on top of a spatial-only attention model.",
    "original_url": "http://arxiv.org/pdf/2106.05968v2",
    "original_title": "Space-time Mixing Attention for Video Transformer",
    "source": "arxiv",
    "authors": [
      "Adrian Bulat",
      "Juan-Manuel Perez-Rua",
      "Swathikiran Sudhakaran",
      "Brais Martinez",
      "Georgios Tzimiropoulos"
    ],
    "published": "2021-06-10T17:59:14+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2106.05968v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2106.05968v2_chunk_1",
    "chunk_text": "(b) It uses efficient space-time mixing to attend jointly spatial and temporal locations without inducing any additional cost on top of a spatial-only attention model. We also show how to integrate 2 very lightweight mechanisms for global temporal-only attention which provide additional accuracy improvements at minimal computational cost. We demonstrate that our model produces very high recognition accuracy on the most popular video recognition datasets while at the same time being significantly more efficient than other Video Transformer models. Code will be made available.",
    "original_url": "http://arxiv.org/pdf/2106.05968v2",
    "original_title": "Space-time Mixing Attention for Video Transformer",
    "source": "arxiv",
    "authors": [
      "Adrian Bulat",
      "Juan-Manuel Perez-Rua",
      "Swathikiran Sudhakaran",
      "Brais Martinez",
      "Georgios Tzimiropoulos"
    ],
    "published": "2021-06-10T17:59:14+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2106.05968v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2109.08963v1_chunk_0",
    "chunk_text": "SDTP: Semantic-aware Decoupled Transformer Pyramid for Dense Image Prediction\n\nAlthough transformer has achieved great progress on computer vision tasks, the scale variation in dense image prediction is still the key challenge. Few effective multi-scale techniques are applied in transformer and there are two main limitations in the current methods. On one hand, self-attention module in vanilla transformer fails to sufficiently exploit the diversity of semantic information because of its rigid mechanism. On the other hand, it is hard to build attention and interaction among different levels due to the heavy computational burden. To alleviate this problem, we first revisit multi-scale problem in dense prediction, verifying the significance of diverse semantic representation and multi-scale interaction, and exploring the adaptation of transformer to pyramidal structure.",
    "original_url": "http://arxiv.org/pdf/2109.08963v1",
    "original_title": "SDTP: Semantic-aware Decoupled Transformer Pyramid for Dense Image Prediction",
    "source": "arxiv",
    "authors": [
      "Zekun Li",
      "Yufan Liu",
      "Bing Li",
      "Weiming Hu",
      "Kebin Wu",
      "Pei Wang"
    ],
    "published": "2021-09-18T16:29:14+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2109.08963v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2109.08963v1_chunk_1",
    "chunk_text": "To alleviate this problem, we first revisit multi-scale problem in dense prediction, verifying the significance of diverse semantic representation and multi-scale interaction, and exploring the adaptation of transformer to pyramidal structure. Inspired by these findings, we propose a novel Semantic-aware Decoupled Transformer Pyramid (SDTP) for dense image prediction, consisting of Intra-level Semantic Promotion (ISP), Cross-level Decoupled Interaction (CDI) and Attention Refinement Function (ARF). ISP explores the semantic diversity in different receptive space. CDI builds the global attention and interaction among different levels in decoupled space which also solves the problem of heavy computation. Besides, ARF is further added to refine the attention in transformer.",
    "original_url": "http://arxiv.org/pdf/2109.08963v1",
    "original_title": "SDTP: Semantic-aware Decoupled Transformer Pyramid for Dense Image Prediction",
    "source": "arxiv",
    "authors": [
      "Zekun Li",
      "Yufan Liu",
      "Bing Li",
      "Weiming Hu",
      "Kebin Wu",
      "Pei Wang"
    ],
    "published": "2021-09-18T16:29:14+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2109.08963v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2109.08963v1_chunk_2",
    "chunk_text": "Besides, ARF is further added to refine the attention in transformer. Experimental results demonstrate the validity and generality of the proposed method, which outperforms the state-of-the-art by a significant margin in dense image prediction tasks. Furthermore, the proposed components are all plug-and-play, which can be embedded in other methods.",
    "original_url": "http://arxiv.org/pdf/2109.08963v1",
    "original_title": "SDTP: Semantic-aware Decoupled Transformer Pyramid for Dense Image Prediction",
    "source": "arxiv",
    "authors": [
      "Zekun Li",
      "Yufan Liu",
      "Bing Li",
      "Weiming Hu",
      "Kebin Wu",
      "Pei Wang"
    ],
    "published": "2021-09-18T16:29:14+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2109.08963v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2504.08801v1_chunk_0",
    "chunk_text": "Learnable Multi-Scale Wavelet Transformer: A Novel Alternative to Self-Attention\n\nTransformer architectures, underpinned by the self-attention mechanism, have achieved state-of-the-art results across numerous natural language processing (NLP) tasks by effectively modeling long-range dependencies. However, the computational complexity of self-attention, scaling quadratically with input sequence length, presents significant challenges for processing very long sequences or operating under resource constraints. This paper introduces the Learnable Multi-Scale Wavelet Transformer (LMWT), a novel architecture that replaces the standard dot-product self-attention with a learnable multi-scale Haar wavelet transform module. Leveraging the intrinsic multi-resolution properties of wavelets, the LMWT efficiently captures both local details and global context. Crucially, the parameters of the wavelet transform, including scale-specific coefficients, are learned end-to-end during training, allowing the model to adapt its decomposition strategy to the data and task.",
    "original_url": "http://arxiv.org/pdf/2504.08801v1",
    "original_title": "Learnable Multi-Scale Wavelet Transformer: A Novel Alternative to Self-Attention",
    "source": "arxiv",
    "authors": [
      "Andrew Kiruluta",
      "Priscilla Burity",
      "Samantha Williams"
    ],
    "published": "2025-04-08T22:16:54+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2504.08801v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2504.08801v1_chunk_1",
    "chunk_text": "Crucially, the parameters of the wavelet transform, including scale-specific coefficients, are learned end-to-end during training, allowing the model to adapt its decomposition strategy to the data and task. We present the detailed mathematical formulation of the learnable Haar wavelet module and its integration into the transformer framework, supplemented by an architectural diagram. We conduct a comprehensive experimental evaluation on a standard machine translation benchmark (WMT16 En-De), comparing the LMWT against a baseline self-attention transformer using metrics like BLEU score, perplexity, and token accuracy. Furthermore, we analyze the computational complexity, highlighting the linear scaling of our approach, discuss its novelty in the context of related work, and explore the interpretability offered by visualizing the learned Haar coefficients. Our results indicate that the LMWT achieves competitive performance while offering substantial computational advantages, positioning it as a promising and novel alternative for efficient sequence modeling.",
    "original_url": "http://arxiv.org/pdf/2504.08801v1",
    "original_title": "Learnable Multi-Scale Wavelet Transformer: A Novel Alternative to Self-Attention",
    "source": "arxiv",
    "authors": [
      "Andrew Kiruluta",
      "Priscilla Burity",
      "Samantha Williams"
    ],
    "published": "2025-04-08T22:16:54+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2504.08801v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2504.08801v1_chunk_2",
    "chunk_text": "Our results indicate that the LMWT achieves competitive performance while offering substantial computational advantages, positioning it as a promising and novel alternative for efficient sequence modeling.",
    "original_url": "http://arxiv.org/pdf/2504.08801v1",
    "original_title": "Learnable Multi-Scale Wavelet Transformer: A Novel Alternative to Self-Attention",
    "source": "arxiv",
    "authors": [
      "Andrew Kiruluta",
      "Priscilla Burity",
      "Samantha Williams"
    ],
    "published": "2025-04-08T22:16:54+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2504.08801v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2410.05076v1_chunk_0",
    "chunk_text": "TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention\n\nLarge language models (LLMs) have driven significant advancements across diverse NLP tasks, with long-context models gaining prominence for handling extended inputs. However, the expanding key-value (KV) cache size required by Transformer architectures intensifies the memory constraints, particularly during the decoding phase, creating a significant bottleneck. Existing sparse attention mechanisms designed to address this bottleneck have two limitations: (1) they often fail to reliably identify the most relevant tokens for attention, and (2) they overlook the spatial coherence of token selection across consecutive Transformer layers, which can lead to performance degradation and substantial overhead in token selection. This paper introduces TidalDecode, a simple yet effective algorithm and system for fast and accurate LLM decoding through position persistent sparse attention. TidalDecode leverages the spatial coherence of tokens selected by existing sparse attention methods and introduces a few token selection layers that perform full attention to identify the tokens with the highest attention scores, while all other layers perform sparse attention with the pre-selected tokens.",
    "original_url": "http://arxiv.org/pdf/2410.05076v1",
    "original_title": "TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention",
    "source": "arxiv",
    "authors": [
      "Lijie Yang",
      "Zhihao Zhang",
      "Zhuofu Chen",
      "Zikun Li",
      "Zhihao Jia"
    ],
    "published": "2024-10-07T14:30:27+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2410.05076v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2410.05076v1_chunk_1",
    "chunk_text": "TidalDecode leverages the spatial coherence of tokens selected by existing sparse attention methods and introduces a few token selection layers that perform full attention to identify the tokens with the highest attention scores, while all other layers perform sparse attention with the pre-selected tokens. This design enables TidalDecode to substantially reduce the overhead of token selection for sparse attention without sacrificing the quality of the generated results. Evaluation on a diverse set of LLMs and tasks shows that TidalDecode closely matches the generative performance of full attention methods while reducing the LLM decoding latency by up to 2.1x.",
    "original_url": "http://arxiv.org/pdf/2410.05076v1",
    "original_title": "TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention",
    "source": "arxiv",
    "authors": [
      "Lijie Yang",
      "Zhihao Zhang",
      "Zhuofu Chen",
      "Zikun Li",
      "Zhihao Jia"
    ],
    "published": "2024-10-07T14:30:27+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2410.05076v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2408.04532v1_chunk_0",
    "chunk_text": "How Transformers Utilize Multi-Head Attention in In-Context Learning? A Case Study on Sparse Linear Regression\n\nDespite the remarkable success of transformer-based models in various real-world tasks, their underlying mechanisms remain poorly understood. Recent studies have suggested that transformers can implement gradient descent as an in-context learner for linear regression problems and have developed various theoretical analyses accordingly. However, these works mostly focus on the expressive power of transformers by designing specific parameter constructions, lacking a comprehensive understanding of their inherent working mechanisms post-training. In this study, we consider a sparse linear regression problem and investigate how a trained multi-head transformer performs in-context learning.",
    "original_url": "http://arxiv.org/pdf/2408.04532v1",
    "original_title": "How Transformers Utilize Multi-Head Attention in In-Context Learning? A Case Study on Sparse Linear Regression",
    "source": "arxiv",
    "authors": [
      "Xingwu Chen",
      "Lei Zhao",
      "Difan Zou"
    ],
    "published": "2024-08-08T15:33:02+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2408.04532v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2408.04532v1_chunk_1",
    "chunk_text": "In this study, we consider a sparse linear regression problem and investigate how a trained multi-head transformer performs in-context learning. We experimentally discover that the utilization of multi-heads exhibits different patterns across layers: multiple heads are utilized and essential in the first layer, while usually only a single head is sufficient for subsequent layers. We provide a theoretical explanation for this observation: the first layer preprocesses the context data, and the following layers execute simple optimization steps based on the preprocessed context. Moreover, we demonstrate that such a preprocess-then-optimize algorithm can significantly outperform naive gradient descent and ridge regression algorithms. Further experimental results support our explanations.",
    "original_url": "http://arxiv.org/pdf/2408.04532v1",
    "original_title": "How Transformers Utilize Multi-Head Attention in In-Context Learning? A Case Study on Sparse Linear Regression",
    "source": "arxiv",
    "authors": [
      "Xingwu Chen",
      "Lei Zhao",
      "Difan Zou"
    ],
    "published": "2024-08-08T15:33:02+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2408.04532v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2408.04532v1_chunk_2",
    "chunk_text": "Further experimental results support our explanations. Our findings offer insights into the benefits of multi-head attention and contribute to understanding the more intricate mechanisms hidden within trained transformers.",
    "original_url": "http://arxiv.org/pdf/2408.04532v1",
    "original_title": "How Transformers Utilize Multi-Head Attention in In-Context Learning? A Case Study on Sparse Linear Regression",
    "source": "arxiv",
    "authors": [
      "Xingwu Chen",
      "Lei Zhao",
      "Difan Zou"
    ],
    "published": "2024-08-08T15:33:02+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2408.04532v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2103.12091v2_chunk_0",
    "chunk_text": "Transformer-Based Attention Networks for Continuous Pixel-Wise Prediction\n\nWhile convolutional neural networks have shown a tremendous impact on various computer vision tasks, they generally demonstrate limitations in explicitly modeling long-range dependencies due to the intrinsic locality of the convolution operation. Initially designed for natural language processing tasks, Transformers have emerged as alternative architectures with innate global self-attention mechanisms to capture long-range dependencies. In this paper, we propose TransDepth, an architecture that benefits from both convolutional neural networks and transformers. To avoid the network losing its ability to capture local-level details due to the adoption of transformers, we propose a novel decoder that employs attention mechanisms based on gates. Notably, this is the first paper that applies transformers to pixel-wise prediction problems involving continuous labels (i.e., monocular depth prediction and surface normal estimation).",
    "original_url": "http://arxiv.org/pdf/2103.12091v2",
    "original_title": "Transformer-Based Attention Networks for Continuous Pixel-Wise Prediction",
    "source": "arxiv",
    "authors": [
      "Guanglei Yang",
      "Hao Tang",
      "Mingli Ding",
      "Nicu Sebe",
      "Elisa Ricci"
    ],
    "published": "2021-03-22T18:00:13+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2103.12091v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2103.12091v2_chunk_1",
    "chunk_text": "Notably, this is the first paper that applies transformers to pixel-wise prediction problems involving continuous labels (i.e., monocular depth prediction and surface normal estimation). Extensive experiments demonstrate that the proposed TransDepth achieves state-of-the-art performance on three challenging datasets. Our code is available at: https://github.com/ygjwd12345/TransDepth.",
    "original_url": "http://arxiv.org/pdf/2103.12091v2",
    "original_title": "Transformer-Based Attention Networks for Continuous Pixel-Wise Prediction",
    "source": "arxiv",
    "authors": [
      "Guanglei Yang",
      "Hao Tang",
      "Mingli Ding",
      "Nicu Sebe",
      "Elisa Ricci"
    ],
    "published": "2021-03-22T18:00:13+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2103.12091v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2206.00771v2_chunk_0",
    "chunk_text": "Dynamic Linear Transformer for 3D Biomedical Image Segmentation\n\nTransformer-based neural networks have surpassed promising performance on many biomedical image segmentation tasks due to a better global information modeling from the self-attention mechanism. However, most methods are still designed for 2D medical images while ignoring the essential 3D volume information. The main challenge for 3D transformer-based segmentation methods is the quadratic complexity introduced by the self-attention mechanism \\cite{vaswani2017attention}. In this paper, we propose a novel transformer architecture for 3D medical image segmentation using an encoder-decoder style architecture with linear complexity. Furthermore, we newly introduce a dynamic token concept to further reduce the token numbers for self-attention calculation.",
    "original_url": "http://arxiv.org/pdf/2206.00771v2",
    "original_title": "Dynamic Linear Transformer for 3D Biomedical Image Segmentation",
    "source": "arxiv",
    "authors": [
      "Zheyuan Zhang",
      "Ulas Bagci"
    ],
    "published": "2022-06-01T21:15:01+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2206.00771v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2206.00771v2_chunk_1",
    "chunk_text": "Furthermore, we newly introduce a dynamic token concept to further reduce the token numbers for self-attention calculation. Taking advantage of the global information modeling, we provide uncertainty maps from different hierarchy stages. We evaluate this method on multiple challenging CT pancreas segmentation datasets. Our promising results show that our novel 3D Transformer-based segmentor could provide promising highly feasible segmentation performance and accurate uncertainty quantification using single annotation. Code is available https://github.com/freshman97/LinTransUNet.",
    "original_url": "http://arxiv.org/pdf/2206.00771v2",
    "original_title": "Dynamic Linear Transformer for 3D Biomedical Image Segmentation",
    "source": "arxiv",
    "authors": [
      "Zheyuan Zhang",
      "Ulas Bagci"
    ],
    "published": "2022-06-01T21:15:01+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2206.00771v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2206.00771v2_chunk_2",
    "chunk_text": "Code is available https://github.com/freshman97/LinTransUNet.",
    "original_url": "http://arxiv.org/pdf/2206.00771v2",
    "original_title": "Dynamic Linear Transformer for 3D Biomedical Image Segmentation",
    "source": "arxiv",
    "authors": [
      "Zheyuan Zhang",
      "Ulas Bagci"
    ],
    "published": "2022-06-01T21:15:01+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2206.00771v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2207.00883v1_chunk_0",
    "chunk_text": "Improving Transformer-based Conversational ASR by Inter-Sentential Attention Mechanism\n\nTransformer-based models have demonstrated their effectiveness in automatic speech recognition (ASR) tasks and even shown superior performance over the conventional hybrid framework. The main idea of Transformers is to capture the long-range global context within an utterance by self-attention layers. However, for scenarios like conversational speech, such utterance-level modeling will neglect contextual dependencies that span across utterances. In this paper, we propose to explicitly model the inter-sentential information in a Transformer based end-to-end architecture for conversational speech recognition. Specifically, for the encoder network, we capture the contexts of previous speech and incorporate such historic information into current input by a context-aware residual attention mechanism.",
    "original_url": "http://arxiv.org/pdf/2207.00883v1",
    "original_title": "Improving Transformer-based Conversational ASR by Inter-Sentential Attention Mechanism",
    "source": "arxiv",
    "authors": [
      "Kun Wei",
      "Pengcheng Guo",
      "Ning Jiang"
    ],
    "published": "2022-07-02T17:17:47+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2207.00883v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2207.00883v1_chunk_1",
    "chunk_text": "Specifically, for the encoder network, we capture the contexts of previous speech and incorporate such historic information into current input by a context-aware residual attention mechanism. For the decoder, the prediction of current utterance is also conditioned on the historic linguistic information through a conditional decoder framework. We show the effectiveness of our proposed method on several open-source dialogue corpora and the proposed method consistently improved the performance from the utterance-level Transformer-based ASR models.",
    "original_url": "http://arxiv.org/pdf/2207.00883v1",
    "original_title": "Improving Transformer-based Conversational ASR by Inter-Sentential Attention Mechanism",
    "source": "arxiv",
    "authors": [
      "Kun Wei",
      "Pengcheng Guo",
      "Ning Jiang"
    ],
    "published": "2022-07-02T17:17:47+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2207.00883v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2411.12118v4_chunk_0",
    "chunk_text": "Mechanism and Emergence of Stacked Attention Heads in Multi-Layer Transformers\n\nIn this paper, I introduce the retrieval problem, a simple yet common reasoning task that can be solved only by transformers with a minimum number of layers, which grows logarithmically with the input size. I empirically show that large language models can solve the task under different prompting formulations without any fine-tuning. To understand how transformers solve the retrieval problem, I train several transformers on a minimal formulation. Successful learning occurs only under the presence of an implicit curriculum. I uncover the learned mechanisms by studying the attention maps in the trained transformers.",
    "original_url": "http://arxiv.org/pdf/2411.12118v4",
    "original_title": "Mechanism and Emergence of Stacked Attention Heads in Multi-Layer Transformers",
    "source": "arxiv",
    "authors": [
      "Tiberiu Musat"
    ],
    "published": "2024-11-18T23:12:13+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2411.12118v4"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2411.12118v4_chunk_1",
    "chunk_text": "I uncover the learned mechanisms by studying the attention maps in the trained transformers. I also study the training process, uncovering that attention heads always emerge in a specific sequence guided by the implicit curriculum.",
    "original_url": "http://arxiv.org/pdf/2411.12118v4",
    "original_title": "Mechanism and Emergence of Stacked Attention Heads in Multi-Layer Transformers",
    "source": "arxiv",
    "authors": [
      "Tiberiu Musat"
    ],
    "published": "2024-11-18T23:12:13+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2411.12118v4"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2203.10435v1_chunk_0",
    "chunk_text": "Vision Transformer with Convolutions Architecture Search\n\nTransformers exhibit great advantages in handling computer vision tasks. They model image classification tasks by utilizing a multi-head attention mechanism to process a series of patches consisting of split images. However, for complex tasks, Transformer in computer vision not only requires inheriting a bit of dynamic attention and global context, but also needs to introduce features concerning noise reduction, shifting, and scaling invariance of objects. Therefore, here we take a step forward to study the structural characteristics of Transformer and convolution and propose an architecture search method-Vision Transformer with Convolutions Architecture Search (VTCAS). The high-performance backbone network searched by VTCAS introduces the desirable features of convolutional neural networks into the Transformer architecture while maintaining the benefits of the multi-head attention mechanism.",
    "original_url": "http://arxiv.org/pdf/2203.10435v1",
    "original_title": "Vision Transformer with Convolutions Architecture Search",
    "source": "arxiv",
    "authors": [
      "Haichao Zhang",
      "Kuangrong Hao",
      "Witold Pedrycz",
      "Lei Gao",
      "Xuesong Tang",
      "Bing Wei"
    ],
    "published": "2022-03-20T02:59:51+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2203.10435v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2203.10435v1_chunk_1",
    "chunk_text": "The high-performance backbone network searched by VTCAS introduces the desirable features of convolutional neural networks into the Transformer architecture while maintaining the benefits of the multi-head attention mechanism. The searched block-based backbone network can extract feature maps at different scales. These features are compatible with a wider range of visual tasks, such as image classification (32 M parameters, 82.0% Top-1 accuracy on ImageNet-1K) and object detection (50.4% mAP on COCO2017). The proposed topology based on the multi-head attention mechanism and CNN adaptively associates relational features of pixels with multi-scale features of objects. It enhances the robustness of the neural network for object recognition, especially in the low illumination indoor scene.",
    "original_url": "http://arxiv.org/pdf/2203.10435v1",
    "original_title": "Vision Transformer with Convolutions Architecture Search",
    "source": "arxiv",
    "authors": [
      "Haichao Zhang",
      "Kuangrong Hao",
      "Witold Pedrycz",
      "Lei Gao",
      "Xuesong Tang",
      "Bing Wei"
    ],
    "published": "2022-03-20T02:59:51+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2203.10435v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2203.10435v1_chunk_2",
    "chunk_text": "It enhances the robustness of the neural network for object recognition, especially in the low illumination indoor scene.",
    "original_url": "http://arxiv.org/pdf/2203.10435v1",
    "original_title": "Vision Transformer with Convolutions Architecture Search",
    "source": "arxiv",
    "authors": [
      "Haichao Zhang",
      "Kuangrong Hao",
      "Witold Pedrycz",
      "Lei Gao",
      "Xuesong Tang",
      "Bing Wei"
    ],
    "published": "2022-03-20T02:59:51+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2203.10435v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2406.13762v2_chunk_0",
    "chunk_text": "Unveiling the Hidden Structure of Self-Attention via Kernel Principal Component Analysis\n\nThe remarkable success of transformers in sequence modeling tasks, spanning various applications in natural language processing and computer vision, is attributed to the critical role of self-attention. Similar to the development of most deep learning models, the construction of these attention mechanisms relies on heuristics and experience. In our work, we derive self-attention from kernel principal component analysis (kernel PCA) and show that self-attention projects its query vectors onto the principal component axes of its key matrix in a feature space. We then formulate the exact formula for the value matrix in self-attention, theoretically and empirically demonstrating that this value matrix captures the eigenvectors of the Gram matrix of the key vectors in self-attention. Leveraging our kernel PCA framework, we propose Attention with Robust Principal Components (RPC-Attention), a novel class of robust attention that is resilient to data contamination.",
    "original_url": "http://arxiv.org/pdf/2406.13762v2",
    "original_title": "Unveiling the Hidden Structure of Self-Attention via Kernel Principal Component Analysis",
    "source": "arxiv",
    "authors": [
      "Rachel S. Y. Teo",
      "Tan M. Nguyen"
    ],
    "published": "2024-06-19T18:22:32+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2406.13762v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2406.13762v2_chunk_1",
    "chunk_text": "Leveraging our kernel PCA framework, we propose Attention with Robust Principal Components (RPC-Attention), a novel class of robust attention that is resilient to data contamination. We empirically demonstrate the advantages of RPC-Attention over softmax attention on the ImageNet-1K object classification, WikiText-103 language modeling, and ADE20K image segmentation task.",
    "original_url": "http://arxiv.org/pdf/2406.13762v2",
    "original_title": "Unveiling the Hidden Structure of Self-Attention via Kernel Principal Component Analysis",
    "source": "arxiv",
    "authors": [
      "Rachel S. Y. Teo",
      "Tan M. Nguyen"
    ],
    "published": "2024-06-19T18:22:32+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2406.13762v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2406.05678v1_chunk_0",
    "chunk_text": "SinkLoRA: Enhanced Efficiency and Chat Capabilities for Long-Context Large Language Models\n\nExtending the functionality of the Transformer model to accommodate longer sequence lengths has become a critical challenge. This extension is crucial not only for improving tasks such as language translation and long-context processing but also for enabling novel applications like chatbots, code generation, and multimedia content creation. The primary obstacle is the self-attention mechanism, which scales quadratically with sequence length in terms of computation time and memory requirements. LongLoRA proposed shifted sparse attention (S\\(^2\\)-Attn), effectively enabling context extension and leading to non-trivial computation savings with similar performance to fine-tuning with vanilla attention. However, LongLoRA is still not as efficient as vanilla attention, reaching only 39\\% of the perplexity improvement compared to full attention.",
    "original_url": "http://arxiv.org/pdf/2406.05678v1",
    "original_title": "SinkLoRA: Enhanced Efficiency and Chat Capabilities for Long-Context Large Language Models",
    "source": "arxiv",
    "authors": [
      "Hengyu Zhang"
    ],
    "published": "2024-06-09T07:23:34+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2406.05678v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2406.05678v1_chunk_1",
    "chunk_text": "However, LongLoRA is still not as efficient as vanilla attention, reaching only 39\\% of the perplexity improvement compared to full attention. This inefficiency is due to the cyclic shift applied within different attention head patterns, causing either chaos in the attention head structure or unnecessary information exchange between token groups. To address these issues, We propose \\textbf{SinkLoRA}, which features better work partitioning. Specifically, (1) we developed SF-Attn with a segmentation and reassembly algorithm to proportionally return cyclically shifted groups of attention heads to their un-shifted state together with global attention of \"sink attention tokens\", achieving 92\\% of the perplexity improvement compared to full attention after fine tuning, and (2) applied a SOTA KV cache compression algorithm H$_2$O to accelerate inference. Furthermore, We conducted supervised fine-tuning with SinkLoRA using a self collected LongAlpaca-plus dataset.",
    "original_url": "http://arxiv.org/pdf/2406.05678v1",
    "original_title": "SinkLoRA: Enhanced Efficiency and Chat Capabilities for Long-Context Large Language Models",
    "source": "arxiv",
    "authors": [
      "Hengyu Zhang"
    ],
    "published": "2024-06-09T07:23:34+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2406.05678v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2406.05678v1_chunk_2",
    "chunk_text": "Furthermore, We conducted supervised fine-tuning with SinkLoRA using a self collected LongAlpaca-plus dataset. All our code, models, datasets, and demos are available at \\url{https://github.com/Dexter-GT-86/SinkLoRA}.",
    "original_url": "http://arxiv.org/pdf/2406.05678v1",
    "original_title": "SinkLoRA: Enhanced Efficiency and Chat Capabilities for Long-Context Large Language Models",
    "source": "arxiv",
    "authors": [
      "Hengyu Zhang"
    ],
    "published": "2024-06-09T07:23:34+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2406.05678v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1811.04716v1_chunk_0",
    "chunk_text": "Input Combination Strategies for Multi-Source Transformer Decoder\n\nIn multi-source sequence-to-sequence tasks, the attention mechanism can be modeled in several ways. This topic has been thoroughly studied on recurrent architectures. In this paper, we extend the previous work to the encoder-decoder attention in the Transformer architecture. We propose four different input combination strategies for the encoder-decoder attention: serial, parallel, flat, and hierarchical. We evaluate our methods on tasks of multimodal translation and translation with multiple source languages.",
    "original_url": "http://arxiv.org/pdf/1811.04716v1",
    "original_title": "Input Combination Strategies for Multi-Source Transformer Decoder",
    "source": "arxiv",
    "authors": [
      "Jindřich Libovický",
      "Jindřich Helcl",
      "David Mareček"
    ],
    "published": "2018-11-12T13:33:35+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1811.04716v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1811.04716v1_chunk_1",
    "chunk_text": "We evaluate our methods on tasks of multimodal translation and translation with multiple source languages. The experiments show that the models are able to use multiple sources and improve over single source baselines.",
    "original_url": "http://arxiv.org/pdf/1811.04716v1",
    "original_title": "Input Combination Strategies for Multi-Source Transformer Decoder",
    "source": "arxiv",
    "authors": [
      "Jindřich Libovický",
      "Jindřich Helcl",
      "David Mareček"
    ],
    "published": "2018-11-12T13:33:35+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1811.04716v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2308.14850v1_chunk_0",
    "chunk_text": "Attention Visualizer Package: Revealing Word Importance for Deeper Insight into Encoder-Only Transformer Models\n\nThis report introduces the Attention Visualizer package, which is crafted to visually illustrate the significance of individual words in encoder-only transformer-based models. In contrast to other methods that center on tokens and self-attention scores, our approach will examine the words and their impact on the final embedding representation. Libraries like this play a crucial role in enhancing the interpretability and explainability of neural networks. They offer the opportunity to illuminate their internal mechanisms, providing a better understanding of how they operate and can be enhanced. You can access the code and review examples on the following GitHub repository: https://github.com/AlaFalaki/AttentionVisualizer.",
    "original_url": "http://arxiv.org/pdf/2308.14850v1",
    "original_title": "Attention Visualizer Package: Revealing Word Importance for Deeper Insight into Encoder-Only Transformer Models",
    "source": "arxiv",
    "authors": [
      "Ala Alam Falaki",
      "Robin Gras"
    ],
    "published": "2023-08-28T19:11:52+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2308.14850v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2308.14850v1_chunk_1",
    "chunk_text": "You can access the code and review examples on the following GitHub repository: https://github.com/AlaFalaki/AttentionVisualizer.",
    "original_url": "http://arxiv.org/pdf/2308.14850v1",
    "original_title": "Attention Visualizer Package: Revealing Word Importance for Deeper Insight into Encoder-Only Transformer Models",
    "source": "arxiv",
    "authors": [
      "Ala Alam Falaki",
      "Robin Gras"
    ],
    "published": "2023-08-28T19:11:52+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2308.14850v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1905.07799v2_chunk_0",
    "chunk_text": "Adaptive Attention Span in Transformers\n\nWe propose a novel self-attention mechanism that can learn its optimal attention span. This allows us to extend significantly the maximum context size used in Transformer, while maintaining control over their memory footprint and computational time. We show the effectiveness of our approach on the task of character level language modeling, where we achieve state-of-the-art performances on text8 and enwiki8 by using a maximum context of 8k characters.",
    "original_url": "http://arxiv.org/pdf/1905.07799v2",
    "original_title": "Adaptive Attention Span in Transformers",
    "source": "arxiv",
    "authors": [
      "Sainbayar Sukhbaatar",
      "Edouard Grave",
      "Piotr Bojanowski",
      "Armand Joulin"
    ],
    "published": "2019-05-19T19:43:14+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1905.07799v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2109.09233v1_chunk_0",
    "chunk_text": "Unified and Multilingual Author Profiling for Detecting Haters\n\nThis paper presents a unified user profiling framework to identify hate speech spreaders by processing their tweets regardless of the language. The framework encodes the tweets with sentence transformers and applies an attention mechanism to select important tweets for learning user profiles. Furthermore, the attention layer helps to explain why a user is a hate speech spreader by producing attention weights at both token and post level. Our proposed model outperformed the state-of-the-art multilingual transformer models.",
    "original_url": "http://arxiv.org/pdf/2109.09233v1",
    "original_title": "Unified and Multilingual Author Profiling for Detecting Haters",
    "source": "arxiv",
    "authors": [
      "Ipek Baris Schlicht",
      "Angel Felipe Magnossão de Paula"
    ],
    "published": "2021-09-19T21:53:23+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2109.09233v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2206.14318v1_chunk_0",
    "chunk_text": "Bottleneck Low-rank Transformers for Low-resource Spoken Language Understanding\n\nEnd-to-end spoken language understanding (SLU) systems benefit from pretraining on large corpora, followed by fine-tuning on application-specific data. The resulting models are too large for on-edge applications. For instance, BERT-based systems contain over 110M parameters. Observing the model is overparameterized, we propose lean transformer structure where the dimension of the attention mechanism is automatically reduced using group sparsity. We propose a variant where the learned attention subspace is transferred to an attention bottleneck layer.",
    "original_url": "http://arxiv.org/pdf/2206.14318v1",
    "original_title": "Bottleneck Low-rank Transformers for Low-resource Spoken Language Understanding",
    "source": "arxiv",
    "authors": [
      "Pu Wang",
      "Hugo Van hamme"
    ],
    "published": "2022-06-28T23:08:32+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2206.14318v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2206.14318v1_chunk_1",
    "chunk_text": "We propose a variant where the learned attention subspace is transferred to an attention bottleneck layer. In a low-resource setting and without pre-training, the resulting compact SLU model achieves accuracies competitive with pre-trained large models.",
    "original_url": "http://arxiv.org/pdf/2206.14318v1",
    "original_title": "Bottleneck Low-rank Transformers for Low-resource Spoken Language Understanding",
    "source": "arxiv",
    "authors": [
      "Pu Wang",
      "Hugo Van hamme"
    ],
    "published": "2022-06-28T23:08:32+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2206.14318v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2307.08247v1_chunk_0",
    "chunk_text": "PAT: Parallel Attention Transformer for Visual Question Answering in Vietnamese\n\nWe present in this paper a novel scheme for multimodal learning named the Parallel Attention mechanism. In addition, to take into account the advantages of grammar and context in Vietnamese, we propose the Hierarchical Linguistic Features Extractor instead of using an LSTM network to extract linguistic features. Based on these two novel modules, we introduce the Parallel Attention Transformer (PAT), achieving the best accuracy compared to all baselines on the benchmark ViVQA dataset and other SOTA methods including SAAA and MCAN.",
    "original_url": "http://arxiv.org/pdf/2307.08247v1",
    "original_title": "PAT: Parallel Attention Transformer for Visual Question Answering in Vietnamese",
    "source": "arxiv",
    "authors": [
      "Nghia Hieu Nguyen",
      "Kiet Van Nguyen"
    ],
    "published": "2023-07-17T05:05:15+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2307.08247v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2406.19391v3_chunk_0",
    "chunk_text": "Fibottention: Inceptive Visual Representation Learning with Diverse Attention Across Heads\n\nTransformer architectures such as Vision Transformers (ViT) have proven effective for solving visual perception tasks. However, they suffer from two major limitations; first, the quadratic complexity of self-attention limits the number of tokens that can be processed, and second, Transformers often require large amounts of training data to attain state-of-the-art performance. In this paper, we propose a new multi-head self-attention (MHSA) variant named Fibottention, which can replace MHSA in Transformer architectures. Fibottention is data-efficient and computationally more suitable for processing large numbers of tokens than the standard MHSA. It employs structured sparse attention based on dilated Fibonacci sequences, which, uniquely, differ across attention heads, resulting in inception-like diverse features across heads.",
    "original_url": "http://arxiv.org/pdf/2406.19391v3",
    "original_title": "Fibottention: Inceptive Visual Representation Learning with Diverse Attention Across Heads",
    "source": "arxiv",
    "authors": [
      "Ali Khaleghi Rahimian",
      "Manish Kumar Govind",
      "Subhajit Maity",
      "Dominick Reilly",
      "Christian Kümmerle",
      "Srijan Das",
      "Aritra Dutta"
    ],
    "published": "2024-06-27T17:59:40+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2406.19391v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2406.19391v3_chunk_1",
    "chunk_text": "It employs structured sparse attention based on dilated Fibonacci sequences, which, uniquely, differ across attention heads, resulting in inception-like diverse features across heads. The spacing of the Fibonacci sequences follows the Wythoff array, which minimizes the redundancy of token interactions aggregated across different attention heads, while still capturing sufficient complementary information through token pair interactions. These sparse attention patterns are unique among the existing sparse attention and lead to an $O(N \\log N)$ complexity, where $N$ is the number of tokens. Leveraging only 2-6% of the elements in the self-attention heads, Fibottention embedded into popular, state-of-the-art Transformer architectures can achieve significantly improved predictive performance for domains with limited data such as image classification, video understanding, and robot learning tasks, and render reduced computational complexity. We further validated the improved diversity of feature representations resulting from different self-attention heads, and our model design against other sparse attention mechanisms.",
    "original_url": "http://arxiv.org/pdf/2406.19391v3",
    "original_title": "Fibottention: Inceptive Visual Representation Learning with Diverse Attention Across Heads",
    "source": "arxiv",
    "authors": [
      "Ali Khaleghi Rahimian",
      "Manish Kumar Govind",
      "Subhajit Maity",
      "Dominick Reilly",
      "Christian Kümmerle",
      "Srijan Das",
      "Aritra Dutta"
    ],
    "published": "2024-06-27T17:59:40+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2406.19391v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2406.19391v3_chunk_2",
    "chunk_text": "We further validated the improved diversity of feature representations resulting from different self-attention heads, and our model design against other sparse attention mechanisms.",
    "original_url": "http://arxiv.org/pdf/2406.19391v3",
    "original_title": "Fibottention: Inceptive Visual Representation Learning with Diverse Attention Across Heads",
    "source": "arxiv",
    "authors": [
      "Ali Khaleghi Rahimian",
      "Manish Kumar Govind",
      "Subhajit Maity",
      "Dominick Reilly",
      "Christian Kümmerle",
      "Srijan Das",
      "Aritra Dutta"
    ],
    "published": "2024-06-27T17:59:40+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2406.19391v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2408.10483v1_chunk_0",
    "chunk_text": "PRformer: Pyramidal Recurrent Transformer for Multivariate Time Series Forecasting\n\nThe self-attention mechanism in Transformer architecture, invariant to sequence order, necessitates positional embeddings to encode temporal order in time series prediction. We argue that this reliance on positional embeddings restricts the Transformer's ability to effectively represent temporal sequences, particularly when employing longer lookback windows. To address this, we introduce an innovative approach that combines Pyramid RNN embeddings(PRE) for univariate time series with the Transformer's capability to model multivariate dependencies. PRE, utilizing pyramidal one-dimensional convolutional layers, constructs multiscale convolutional features that preserve temporal order. Additionally, RNNs, layered atop these features, learn multiscale time series representations sensitive to sequence order.",
    "original_url": "http://arxiv.org/pdf/2408.10483v1",
    "original_title": "PRformer: Pyramidal Recurrent Transformer for Multivariate Time Series Forecasting",
    "source": "arxiv",
    "authors": [
      "Yongbo Yu",
      "Weizhong Yu",
      "Feiping Nie",
      "Xuelong Li"
    ],
    "published": "2024-08-20T01:56:07+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2408.10483v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2408.10483v1_chunk_1",
    "chunk_text": "Additionally, RNNs, layered atop these features, learn multiscale time series representations sensitive to sequence order. This integration into Transformer models with attention mechanisms results in significant performance enhancements. We present the PRformer, a model integrating PRE with a standard Transformer encoder, demonstrating state-of-the-art performance on various real-world datasets. This performance highlights the effectiveness of our approach in leveraging longer lookback windows and underscores the critical role of robust temporal representations in maximizing Transformer's potential for prediction tasks. Code is available at this repository: \\url{https://github.com/usualheart/PRformer}.",
    "original_url": "http://arxiv.org/pdf/2408.10483v1",
    "original_title": "PRformer: Pyramidal Recurrent Transformer for Multivariate Time Series Forecasting",
    "source": "arxiv",
    "authors": [
      "Yongbo Yu",
      "Weizhong Yu",
      "Feiping Nie",
      "Xuelong Li"
    ],
    "published": "2024-08-20T01:56:07+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2408.10483v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2408.10483v1_chunk_2",
    "chunk_text": "Code is available at this repository: \\url{https://github.com/usualheart/PRformer}.",
    "original_url": "http://arxiv.org/pdf/2408.10483v1",
    "original_title": "PRformer: Pyramidal Recurrent Transformer for Multivariate Time Series Forecasting",
    "source": "arxiv",
    "authors": [
      "Yongbo Yu",
      "Weizhong Yu",
      "Feiping Nie",
      "Xuelong Li"
    ],
    "published": "2024-08-20T01:56:07+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2408.10483v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2301.07407v1_chunk_0",
    "chunk_text": "TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks\n\nThe apparent ``black box'' nature of neural networks is a barrier to adoption in applications where explainability is essential. This paper presents TAME (Trainable Attention Mechanism for Explanations), a method for generating explanation maps with a multi-branch hierarchical attention mechanism. TAME combines a target model's feature maps from multiple layers using an attention mechanism, transforming them into an explanation map. TAME can easily be applied to any convolutional neural network (CNN) by streamlining the optimization of the attention mechanism's training method and the selection of target model's feature maps. After training, explanation maps can be computed in a single forward pass.",
    "original_url": "http://arxiv.org/pdf/2301.07407v1",
    "original_title": "TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks",
    "source": "arxiv",
    "authors": [
      "Mariano Ntrougkas",
      "Nikolaos Gkalelis",
      "Vasileios Mezaris"
    ],
    "published": "2023-01-18T10:05:28+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2301.07407v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2301.07407v1_chunk_1",
    "chunk_text": "After training, explanation maps can be computed in a single forward pass. We apply TAME to two widely used models, i.e. VGG-16 and ResNet-50, trained on ImageNet and show improvements over previous top-performing methods. We also provide a comprehensive ablation study comparing the performance of different variations of TAME's architecture. TAME source code is made publicly available at https://github.com/bmezaris/TAME",
    "original_url": "http://arxiv.org/pdf/2301.07407v1",
    "original_title": "TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks",
    "source": "arxiv",
    "authors": [
      "Mariano Ntrougkas",
      "Nikolaos Gkalelis",
      "Vasileios Mezaris"
    ],
    "published": "2023-01-18T10:05:28+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2301.07407v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2301.07407v1_chunk_2",
    "chunk_text": "TAME source code is made publicly available at https://github.com/bmezaris/TAME",
    "original_url": "http://arxiv.org/pdf/2301.07407v1",
    "original_title": "TAME: Attention Mechanism Based Feature Fusion for Generating Explanation Maps of Convolutional Neural Networks",
    "source": "arxiv",
    "authors": [
      "Mariano Ntrougkas",
      "Nikolaos Gkalelis",
      "Vasileios Mezaris"
    ],
    "published": "2023-01-18T10:05:28+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2301.07407v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2110.13094v1_chunk_0",
    "chunk_text": "Gophormer: Ego-Graph Transformer for Node Classification\n\nTransformers have achieved remarkable performance in a myriad of fields including natural language processing and computer vision. However, when it comes to the graph mining area, where graph neural network (GNN) has been the dominant paradigm, transformers haven't achieved competitive performance, especially on the node classification task. Existing graph transformer models typically adopt fully-connected attention mechanism on the whole input graph and thus suffer from severe scalability issues and are intractable to train in data insufficient cases. To alleviate these issues, we propose a novel Gophormer model which applies transformers on ego-graphs instead of full-graphs. Specifically, Node2Seq module is proposed to sample ego-graphs as the input of transformers, which alleviates the challenge of scalability and serves as an effective data augmentation technique to boost model performance.",
    "original_url": "http://arxiv.org/pdf/2110.13094v1",
    "original_title": "Gophormer: Ego-Graph Transformer for Node Classification",
    "source": "arxiv",
    "authors": [
      "Jianan Zhao",
      "Chaozhuo Li",
      "Qianlong Wen",
      "Yiqi Wang",
      "Yuming Liu",
      "Hao Sun",
      "Xing Xie",
      "Yanfang Ye"
    ],
    "published": "2021-10-25T16:43:32+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2110.13094v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2110.13094v1_chunk_1",
    "chunk_text": "Specifically, Node2Seq module is proposed to sample ego-graphs as the input of transformers, which alleviates the challenge of scalability and serves as an effective data augmentation technique to boost model performance. Moreover, different from the feature-based attention strategy in vanilla transformers, we propose a proximity-enhanced attention mechanism to capture the fine-grained structural bias. In order to handle the uncertainty introduced by the ego-graph sampling, we further propose a consistency regularization and a multi-sample inference strategy for stabilized training and testing, respectively. Extensive experiments on six benchmark datasets are conducted to demonstrate the superiority of Gophormer over existing graph transformers and popular GNNs, revealing the promising future of graph transformers.",
    "original_url": "http://arxiv.org/pdf/2110.13094v1",
    "original_title": "Gophormer: Ego-Graph Transformer for Node Classification",
    "source": "arxiv",
    "authors": [
      "Jianan Zhao",
      "Chaozhuo Li",
      "Qianlong Wen",
      "Yiqi Wang",
      "Yuming Liu",
      "Hao Sun",
      "Xing Xie",
      "Yanfang Ye"
    ],
    "published": "2021-10-25T16:43:32+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2110.13094v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2308.05051v1_chunk_0",
    "chunk_text": "PAT: Position-Aware Transformer for Dense Multi-Label Action Detection\n\nWe present PAT, a transformer-based network that learns complex temporal co-occurrence action dependencies in a video by exploiting multi-scale temporal features. In existing methods, the self-attention mechanism in transformers loses the temporal positional information, which is essential for robust action detection. To address this issue, we (i) embed relative positional encoding in the self-attention mechanism and (ii) exploit multi-scale temporal relationships by designing a novel non hierarchical network, in contrast to the recent transformer-based approaches that use a hierarchical structure. We argue that joining the self-attention mechanism with multiple sub-sampling processes in the hierarchical approaches results in increased loss of positional information. We evaluate the performance of our proposed approach on two challenging dense multi-label benchmark datasets, and show that PAT improves the current state-of-the-art result by 1.1% and 0.6% mAP on the Charades and MultiTHUMOS datasets, respectively, thereby achieving the new state-of-the-art mAP at 26.5% and 44.6%, respectively.",
    "original_url": "http://arxiv.org/pdf/2308.05051v1",
    "original_title": "PAT: Position-Aware Transformer for Dense Multi-Label Action Detection",
    "source": "arxiv",
    "authors": [
      "Faegheh Sardari",
      "Armin Mustafa",
      "Philip J. B. Jackson",
      "Adrian Hilton"
    ],
    "published": "2023-08-09T16:29:31+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2308.05051v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2308.05051v1_chunk_1",
    "chunk_text": "We evaluate the performance of our proposed approach on two challenging dense multi-label benchmark datasets, and show that PAT improves the current state-of-the-art result by 1.1% and 0.6% mAP on the Charades and MultiTHUMOS datasets, respectively, thereby achieving the new state-of-the-art mAP at 26.5% and 44.6%, respectively. We also perform extensive ablation studies to examine the impact of the different components of our proposed network.",
    "original_url": "http://arxiv.org/pdf/2308.05051v1",
    "original_title": "PAT: Position-Aware Transformer for Dense Multi-Label Action Detection",
    "source": "arxiv",
    "authors": [
      "Faegheh Sardari",
      "Armin Mustafa",
      "Philip J. B. Jackson",
      "Adrian Hilton"
    ],
    "published": "2023-08-09T16:29:31+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2308.05051v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2402.05969v2_chunk_0",
    "chunk_text": "Breaking Symmetry When Training Transformers\n\nAs we show in this paper, the prediction for output token $n+1$ of Transformer architectures without one of the mechanisms of positional encodings and causal attention is invariant to permutations of input tokens $1, 2, ..., n-1$. Usually, both mechanisms are employed and the symmetry with respect to the input tokens is broken. Recently, it has been shown that one can train Transformers without positional encodings. This must be enabled by the causal attention mechanism. In this paper, we elaborate on the argument that the causal connection mechanism must be responsible for the fact that Transformers are able to model input sequences where the order is important.",
    "original_url": "http://arxiv.org/pdf/2402.05969v2",
    "original_title": "Breaking Symmetry When Training Transformers",
    "source": "arxiv",
    "authors": [
      "Chunsheng Zuo",
      "Michael Guerzhoy"
    ],
    "published": "2024-02-06T00:32:28+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2402.05969v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2402.05969v2_chunk_1",
    "chunk_text": "In this paper, we elaborate on the argument that the causal connection mechanism must be responsible for the fact that Transformers are able to model input sequences where the order is important. Vertical \"slices\" of Transformers are all encouraged to represent the same location $k$ in the input sequence. We hypothesize that residual connections contribute to this phenomenon, and demonstrate evidence for this.",
    "original_url": "http://arxiv.org/pdf/2402.05969v2",
    "original_title": "Breaking Symmetry When Training Transformers",
    "source": "arxiv",
    "authors": [
      "Chunsheng Zuo",
      "Michael Guerzhoy"
    ],
    "published": "2024-02-06T00:32:28+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2402.05969v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2101.08114v1_chunk_0",
    "chunk_text": "Classifying Scientific Publications with BERT -- Is Self-Attention a Feature Selection Method? We investigate the self-attention mechanism of BERT in a fine-tuning scenario for the classification of scientific articles over a taxonomy of research disciplines. We observe how self-attention focuses on words that are highly related to the domain of the article. Particularly, a small subset of vocabulary words tends to receive most of the attention. We compare and evaluate the subset of the most attended words with feature selection methods normally used for text classification in order to characterize self-attention as a possible feature selection approach.",
    "original_url": "http://arxiv.org/pdf/2101.08114v1",
    "original_title": "Classifying Scientific Publications with BERT -- Is Self-Attention a Feature Selection Method?",
    "source": "arxiv",
    "authors": [
      "Andres Garcia-Silva",
      "Jose Manuel Gomez-Perez"
    ],
    "published": "2021-01-20T13:22:26+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2101.08114v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2101.08114v1_chunk_1",
    "chunk_text": "We compare and evaluate the subset of the most attended words with feature selection methods normally used for text classification in order to characterize self-attention as a possible feature selection approach. Using ConceptNet as ground truth, we also find that attended words are more related to the research fields of the articles. However, conventional feature selection methods are still a better option to learn classifiers from scratch. This result suggests that, while self-attention identifies domain-relevant terms, the discriminatory information in BERT is encoded in the contextualized outputs and the classification layer. It also raises the question whether injecting feature selection methods in the self-attention mechanism could further optimize single sequence classification using transformers.",
    "original_url": "http://arxiv.org/pdf/2101.08114v1",
    "original_title": "Classifying Scientific Publications with BERT -- Is Self-Attention a Feature Selection Method?",
    "source": "arxiv",
    "authors": [
      "Andres Garcia-Silva",
      "Jose Manuel Gomez-Perez"
    ],
    "published": "2021-01-20T13:22:26+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2101.08114v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2101.08114v1_chunk_2",
    "chunk_text": "It also raises the question whether injecting feature selection methods in the self-attention mechanism could further optimize single sequence classification using transformers.",
    "original_url": "http://arxiv.org/pdf/2101.08114v1",
    "original_title": "Classifying Scientific Publications with BERT -- Is Self-Attention a Feature Selection Method?",
    "source": "arxiv",
    "authors": [
      "Andres Garcia-Silva",
      "Jose Manuel Gomez-Perez"
    ],
    "published": "2021-01-20T13:22:26+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2101.08114v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2205.08681v3_chunk_0",
    "chunk_text": "U-Former: Improving Monaural Speech Enhancement with Multi-head Self and Cross Attention\n\nFor supervised speech enhancement, contextual information is important for accurate spectral mapping. However, commonly used deep neural networks (DNNs) are limited in capturing temporal contexts. To leverage long-term contexts for tracking a target speaker, this paper treats the speech enhancement as sequence-to-sequence mapping, and propose a novel monaural speech enhancement U-net structure based on Transformer, dubbed U-Former. The key idea is to model long-term correlations and dependencies, which are crucial for accurate noisy speech modeling, through the multi-head attention mechanisms. For this purpose, U-Former incorporates multi-head attention mechanisms at two levels: 1) a multi-head self-attention module which calculate the attention map along both time- and frequency-axis to generate time and frequency sub-attention maps for leveraging global interactions between encoder features, while 2) multi-head cross-attention module which are inserted in the skip connections allows a fine recovery in the decoder by filtering out uncorrelated features.",
    "original_url": "http://arxiv.org/pdf/2205.08681v3",
    "original_title": "U-Former: Improving Monaural Speech Enhancement with Multi-head Self and Cross Attention",
    "source": "arxiv",
    "authors": [
      "Xinmeng Xu",
      "Jianjun Hao"
    ],
    "published": "2022-05-18T01:33:10+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2205.08681v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2205.08681v3_chunk_1",
    "chunk_text": "For this purpose, U-Former incorporates multi-head attention mechanisms at two levels: 1) a multi-head self-attention module which calculate the attention map along both time- and frequency-axis to generate time and frequency sub-attention maps for leveraging global interactions between encoder features, while 2) multi-head cross-attention module which are inserted in the skip connections allows a fine recovery in the decoder by filtering out uncorrelated features. Experimental results illustrate that the U-Former obtains consistently better performance than recent models of PESQ, STOI, and SSNR scores.",
    "original_url": "http://arxiv.org/pdf/2205.08681v3",
    "original_title": "U-Former: Improving Monaural Speech Enhancement with Multi-head Self and Cross Attention",
    "source": "arxiv",
    "authors": [
      "Xinmeng Xu",
      "Jianjun Hao"
    ],
    "published": "2022-05-18T01:33:10+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2205.08681v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2402.15055v2_chunk_0",
    "chunk_text": "Interpreting Context Look-ups in Transformers: Investigating Attention-MLP Interactions\n\nUnderstanding the inner workings of large language models (LLMs) is crucial for advancing their theoretical foundations and real-world applications. While the attention mechanism and multi-layer perceptrons (MLPs) have been studied independently, their interactions remain largely unexplored. This study investigates how attention heads and next-token neurons interact in LLMs to predict new words. We propose a methodology to identify next-token neurons, find prompts that highly activate them, and determine the upstream attention heads responsible. We then generate and evaluate explanations for the activity of these attention heads in an automated manner.",
    "original_url": "http://arxiv.org/pdf/2402.15055v2",
    "original_title": "Interpreting Context Look-ups in Transformers: Investigating Attention-MLP Interactions",
    "source": "arxiv",
    "authors": [
      "Clement Neo",
      "Shay B. Cohen",
      "Fazl Barez"
    ],
    "published": "2024-02-23T02:15:47+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2402.15055v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2402.15055v2_chunk_1",
    "chunk_text": "We then generate and evaluate explanations for the activity of these attention heads in an automated manner. Our findings reveal that some attention heads recognize specific contexts relevant to predicting a token and activate a downstream token-predicting neuron accordingly. This mechanism provides a deeper understanding of how attention heads work with MLP neurons to perform next-token prediction. Our approach offers a foundation for further research into the intricate workings of LLMs and their impact on text generation and understanding.",
    "original_url": "http://arxiv.org/pdf/2402.15055v2",
    "original_title": "Interpreting Context Look-ups in Transformers: Investigating Attention-MLP Interactions",
    "source": "arxiv",
    "authors": [
      "Clement Neo",
      "Shay B. Cohen",
      "Fazl Barez"
    ],
    "published": "2024-02-23T02:15:47+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2402.15055v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2405.20867v1_chunk_0",
    "chunk_text": "Automatic Channel Pruning for Multi-Head Attention\n\nDespite the strong performance of Transformers, their quadratic computation complexity presents challenges in applying them to vision tasks. Automatic pruning is one of effective methods for reducing computation complexity without heuristic approaches. However, directly applying it to multi-head attention is not straightforward due to channel misalignment. In this paper, we propose an automatic channel pruning method to take into account the multi-head attention mechanism. First, we incorporate channel similarity-based weights into the pruning indicator to preserve more informative channels in each head.",
    "original_url": "http://arxiv.org/pdf/2405.20867v1",
    "original_title": "Automatic Channel Pruning for Multi-Head Attention",
    "source": "arxiv",
    "authors": [
      "Eunho Lee",
      "Youngbae Hwang"
    ],
    "published": "2024-05-31T14:47:20+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2405.20867v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2405.20867v1_chunk_1",
    "chunk_text": "First, we incorporate channel similarity-based weights into the pruning indicator to preserve more informative channels in each head. Then, we adjust pruning indicator to enforce removal of channels in equal proportions across all heads, preventing the channel misalignment. We also add a reweight module to compensate for information loss resulting from channel removal, and an effective initialization step for pruning indicator based on difference of attention between original structure and each channel. Our proposed method can be used to not only original attention, but also linear attention, which is more efficient as linear complexity with respect to the number of tokens. On ImageNet-1K, applying our pruning method to the FLattenTransformer, which includes both attention mechanisms, shows outperformed accuracy for several MACs compared with previous state-of-the-art efficient models and pruned methods.",
    "original_url": "http://arxiv.org/pdf/2405.20867v1",
    "original_title": "Automatic Channel Pruning for Multi-Head Attention",
    "source": "arxiv",
    "authors": [
      "Eunho Lee",
      "Youngbae Hwang"
    ],
    "published": "2024-05-31T14:47:20+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2405.20867v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2405.20867v1_chunk_2",
    "chunk_text": "On ImageNet-1K, applying our pruning method to the FLattenTransformer, which includes both attention mechanisms, shows outperformed accuracy for several MACs compared with previous state-of-the-art efficient models and pruned methods. Code will be available soon.",
    "original_url": "http://arxiv.org/pdf/2405.20867v1",
    "original_title": "Automatic Channel Pruning for Multi-Head Attention",
    "source": "arxiv",
    "authors": [
      "Eunho Lee",
      "Youngbae Hwang"
    ],
    "published": "2024-05-31T14:47:20+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2405.20867v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2412.16545v1_chunk_0",
    "chunk_text": "Attention Entropy is a Key Factor: An Analysis of Parallel Context Encoding with Full-attention-based Pre-trained Language Models\n\nLarge language models have shown remarkable performance across a wide range of language tasks, owing to their exceptional capabilities in context modeling. The most commonly used method of context modeling is full self-attention, as seen in standard decoder-only Transformers. Although powerful, this method can be inefficient for long sequences and may overlook inherent input structures. To address these problems, an alternative approach is parallel context encoding, which splits the context into sub-pieces and encodes them parallelly. Because parallel patterns are not encountered during training, naively applying parallel encoding leads to performance degradation.",
    "original_url": "http://arxiv.org/pdf/2412.16545v1",
    "original_title": "Attention Entropy is a Key Factor: An Analysis of Parallel Context Encoding with Full-attention-based Pre-trained Language Models",
    "source": "arxiv",
    "authors": [
      "Zhisong Zhang",
      "Yan Wang",
      "Xinting Huang",
      "Tianqing Fang",
      "Hongming Zhang",
      "Chenlong Deng",
      "Shuaiyi Li",
      "Dong Yu"
    ],
    "published": "2024-12-21T09:04:51+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2412.16545v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2412.16545v1_chunk_1",
    "chunk_text": "Because parallel patterns are not encountered during training, naively applying parallel encoding leads to performance degradation. However, the underlying reasons and potential mitigations are unclear. In this work, we provide a detailed analysis of this issue and identify that unusually high attention entropy can be a key factor. Furthermore, we adopt two straightforward methods to reduce attention entropy by incorporating attention sinks and selective mechanisms. Experiments on various tasks reveal that these methods effectively lower irregular attention entropy and narrow performance gaps.",
    "original_url": "http://arxiv.org/pdf/2412.16545v1",
    "original_title": "Attention Entropy is a Key Factor: An Analysis of Parallel Context Encoding with Full-attention-based Pre-trained Language Models",
    "source": "arxiv",
    "authors": [
      "Zhisong Zhang",
      "Yan Wang",
      "Xinting Huang",
      "Tianqing Fang",
      "Hongming Zhang",
      "Chenlong Deng",
      "Shuaiyi Li",
      "Dong Yu"
    ],
    "published": "2024-12-21T09:04:51+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2412.16545v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2412.16545v1_chunk_2",
    "chunk_text": "Experiments on various tasks reveal that these methods effectively lower irregular attention entropy and narrow performance gaps. We hope this study can illuminate ways to enhance context modeling mechanisms.",
    "original_url": "http://arxiv.org/pdf/2412.16545v1",
    "original_title": "Attention Entropy is a Key Factor: An Analysis of Parallel Context Encoding with Full-attention-based Pre-trained Language Models",
    "source": "arxiv",
    "authors": [
      "Zhisong Zhang",
      "Yan Wang",
      "Xinting Huang",
      "Tianqing Fang",
      "Hongming Zhang",
      "Chenlong Deng",
      "Shuaiyi Li",
      "Dong Yu"
    ],
    "published": "2024-12-21T09:04:51+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2412.16545v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2211.08543v1_chunk_0",
    "chunk_text": "Demystify Self-Attention in Vision Transformers from a Semantic Perspective: Analysis and Application\n\nSelf-attention mechanisms, especially multi-head self-attention (MSA), have achieved great success in many fields such as computer vision and natural language processing. However, many existing vision transformer (ViT) works simply inherent transformer designs from NLP to adapt vision tasks, while ignoring the fundamental difference between ``how MSA works in image and language settings''. Language naturally contains highly semantic structures that are directly interpretable by humans. Its basic unit (word) is discrete without redundant information, which readily supports interpretable studies on MSA mechanisms of language transformer. In contrast, visual data exhibits a fundamentally different structure: Its basic unit (pixel) is a natural low-level representation with significant redundancies in the neighbourhood, which poses obvious challenges to the interpretability of MSA mechanism in ViT.",
    "original_url": "http://arxiv.org/pdf/2211.08543v1",
    "original_title": "Demystify Self-Attention in Vision Transformers from a Semantic Perspective: Analysis and Application",
    "source": "arxiv",
    "authors": [
      "Leijie Wu",
      "Song Guo",
      "Yaohong Ding",
      "Junxiao Wang",
      "Wenchao Xu",
      "Richard Yida Xu",
      "Jie Zhang"
    ],
    "published": "2022-11-13T15:18:31+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2211.08543v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2211.08543v1_chunk_1",
    "chunk_text": "In contrast, visual data exhibits a fundamentally different structure: Its basic unit (pixel) is a natural low-level representation with significant redundancies in the neighbourhood, which poses obvious challenges to the interpretability of MSA mechanism in ViT. In this paper, we introduce a typical image processing technique, i.e., scale-invariant feature transforms (SIFTs), which maps low-level representations into mid-level spaces, and annotates extensive discrete keypoints with semantically rich information. Next, we construct a weighted patch interrelation analysis based on SIFT keypoints to capture the attention patterns hidden in patches with different semantic concentrations Interestingly, we find this quantitative analysis is not only an effective complement to the interpretability of MSA mechanisms in ViT, but can also be applied to 1) spurious correlation discovery and ``prompting'' during model inference, 2) and guided model pre-training acceleration. Experimental results on both applications show significant advantages over baselines, demonstrating the efficacy of our method.",
    "original_url": "http://arxiv.org/pdf/2211.08543v1",
    "original_title": "Demystify Self-Attention in Vision Transformers from a Semantic Perspective: Analysis and Application",
    "source": "arxiv",
    "authors": [
      "Leijie Wu",
      "Song Guo",
      "Yaohong Ding",
      "Junxiao Wang",
      "Wenchao Xu",
      "Richard Yida Xu",
      "Jie Zhang"
    ],
    "published": "2022-11-13T15:18:31+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2211.08543v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2402.08211v1_chunk_0",
    "chunk_text": "Transformer Mechanisms Mimic Frontostriatal Gating Operations When Trained on Human Working Memory Tasks\n\nModels based on the Transformer neural network architecture have seen success on a wide variety of tasks that appear to require complex \"cognitive branching\" -- or the ability to maintain pursuit of one goal while accomplishing others. In cognitive neuroscience, success on such tasks is thought to rely on sophisticated frontostriatal mechanisms for selective \\textit{gating}, which enable role-addressable updating -- and later readout -- of information to and from distinct \"addresses\" of memory, in the form of clusters of neurons. However, Transformer models have no such mechanisms intentionally built-in. It is thus an open question how Transformers solve such tasks, and whether the mechanisms that emerge to help them to do so bear any resemblance to the gating mechanisms in the human brain. In this work, we analyze the mechanisms that emerge within a vanilla attention-only Transformer trained on a simple sequence modeling task inspired by a task explicitly designed to study working memory gating in computational cognitive neuroscience.",
    "original_url": "http://arxiv.org/pdf/2402.08211v1",
    "original_title": "Transformer Mechanisms Mimic Frontostriatal Gating Operations When Trained on Human Working Memory Tasks",
    "source": "arxiv",
    "authors": [
      "Aaron Traylor",
      "Jack Merullo",
      "Michael J. Frank",
      "Ellie Pavlick"
    ],
    "published": "2024-02-13T04:28:43+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2402.08211v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2402.08211v1_chunk_1",
    "chunk_text": "In this work, we analyze the mechanisms that emerge within a vanilla attention-only Transformer trained on a simple sequence modeling task inspired by a task explicitly designed to study working memory gating in computational cognitive neuroscience. We find that, as a result of training, the self-attention mechanism within the Transformer specializes in a way that mirrors the input and output gating mechanisms which were explicitly incorporated into earlier, more biologically-inspired architectures. These results suggest opportunities for future research on computational similarities between modern AI architectures and models of the human brain.",
    "original_url": "http://arxiv.org/pdf/2402.08211v1",
    "original_title": "Transformer Mechanisms Mimic Frontostriatal Gating Operations When Trained on Human Working Memory Tasks",
    "source": "arxiv",
    "authors": [
      "Aaron Traylor",
      "Jack Merullo",
      "Michael J. Frank",
      "Ellie Pavlick"
    ],
    "published": "2024-02-13T04:28:43+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2402.08211v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2107.02192v3_chunk_0",
    "chunk_text": "Long-Short Transformer: Efficient Transformers for Language and Vision\n\nTransformers have achieved success in both language and vision domains. However, it is prohibitively expensive to scale them to long sequences such as long documents or high-resolution images, because self-attention mechanism has quadratic time and memory complexities with respect to the input sequence length. In this paper, we propose Long-Short Transformer (Transformer-LS), an efficient self-attention mechanism for modeling long sequences with linear complexity for both language and vision tasks. It aggregates a novel long-range attention with dynamic projection to model distant correlations and a short-term attention to capture fine-grained local correlations. We propose a dual normalization strategy to account for the scale mismatch between the two attention mechanisms.",
    "original_url": "http://arxiv.org/pdf/2107.02192v3",
    "original_title": "Long-Short Transformer: Efficient Transformers for Language and Vision",
    "source": "arxiv",
    "authors": [
      "Chen Zhu",
      "Wei Ping",
      "Chaowei Xiao",
      "Mohammad Shoeybi",
      "Tom Goldstein",
      "Anima Anandkumar",
      "Bryan Catanzaro"
    ],
    "published": "2021-07-05T18:00:14+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2107.02192v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2107.02192v3_chunk_1",
    "chunk_text": "We propose a dual normalization strategy to account for the scale mismatch between the two attention mechanisms. Transformer-LS can be applied to both autoregressive and bidirectional models without additional complexity. Our method outperforms the state-of-the-art models on multiple tasks in language and vision domains, including the Long Range Arena benchmark, autoregressive language modeling, and ImageNet classification. For instance, Transformer-LS achieves 0.97 test BPC on enwik8 using half the number of parameters than previous method, while being faster and is able to handle 3x as long sequences compared to its full-attention version on the same hardware. On ImageNet, it can obtain the state-of-the-art results (e.g., a moderate size of 55.8M model solely trained on 224x224 ImageNet-1K can obtain Top-1 accuracy 84.1%), while being more scalable on high-resolution images.",
    "original_url": "http://arxiv.org/pdf/2107.02192v3",
    "original_title": "Long-Short Transformer: Efficient Transformers for Language and Vision",
    "source": "arxiv",
    "authors": [
      "Chen Zhu",
      "Wei Ping",
      "Chaowei Xiao",
      "Mohammad Shoeybi",
      "Tom Goldstein",
      "Anima Anandkumar",
      "Bryan Catanzaro"
    ],
    "published": "2021-07-05T18:00:14+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2107.02192v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2107.02192v3_chunk_2",
    "chunk_text": "On ImageNet, it can obtain the state-of-the-art results (e.g., a moderate size of 55.8M model solely trained on 224x224 ImageNet-1K can obtain Top-1 accuracy 84.1%), while being more scalable on high-resolution images. The source code and models are released at https://github.com/NVIDIA/transformer-ls .",
    "original_url": "http://arxiv.org/pdf/2107.02192v3",
    "original_title": "Long-Short Transformer: Efficient Transformers for Language and Vision",
    "source": "arxiv",
    "authors": [
      "Chen Zhu",
      "Wei Ping",
      "Chaowei Xiao",
      "Mohammad Shoeybi",
      "Tom Goldstein",
      "Anima Anandkumar",
      "Bryan Catanzaro"
    ],
    "published": "2021-07-05T18:00:14+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2107.02192v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1909.09595v1_chunk_0",
    "chunk_text": "SANVis: Visual Analytics for Understanding Self-Attention Networks\n\nAttention networks, a deep neural network architecture inspired by humans' attention mechanism, have seen significant success in image captioning, machine translation, and many other applications. Recently, they have been further evolved into an advanced approach called multi-head self-attention networks, which can encode a set of input vectors, e.g., word vectors in a sentence, into another set of vectors. Such encoding aims at simultaneously capturing diverse syntactic and semantic features within a set, each of which corresponds to a particular attention head, forming altogether multi-head attention. Meanwhile, the increased model complexity prevents users from easily understanding and manipulating the inner workings of models. To tackle the challenges, we present a visual analytics system called SANVis, which helps users understand the behaviors and the characteristics of multi-head self-attention networks.",
    "original_url": "http://arxiv.org/pdf/1909.09595v1",
    "original_title": "SANVis: Visual Analytics for Understanding Self-Attention Networks",
    "source": "arxiv",
    "authors": [
      "Cheonbok Park",
      "Inyoup Na",
      "Yongjang Jo",
      "Sungbok Shin",
      "Jaehyo Yoo",
      "Bum Chul Kwon",
      "Jian Zhao",
      "Hyungjong Noh",
      "Yeonsoo Lee",
      "Jaegul Choo"
    ],
    "published": "2019-09-13T05:59:40+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1909.09595v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1909.09595v1_chunk_1",
    "chunk_text": "To tackle the challenges, we present a visual analytics system called SANVis, which helps users understand the behaviors and the characteristics of multi-head self-attention networks. Using a state-of-the-art self-attention model called Transformer, we demonstrate usage scenarios of SANVis in machine translation tasks. Our system is available at http://short.sanvis.org",
    "original_url": "http://arxiv.org/pdf/1909.09595v1",
    "original_title": "SANVis: Visual Analytics for Understanding Self-Attention Networks",
    "source": "arxiv",
    "authors": [
      "Cheonbok Park",
      "Inyoup Na",
      "Yongjang Jo",
      "Sungbok Shin",
      "Jaehyo Yoo",
      "Bum Chul Kwon",
      "Jian Zhao",
      "Hyungjong Noh",
      "Yeonsoo Lee",
      "Jaegul Choo"
    ],
    "published": "2019-09-13T05:59:40+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1909.09595v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2106.09003v2_chunk_0",
    "chunk_text": "Invertible Attention\n\nAttention has been proved to be an efficient mechanism to capture long-range dependencies. However, so far it has not been deployed in invertible networks. This is due to the fact that in order to make a network invertible, every component within the network needs to be a bijective transformation, but a normal attention block is not. In this paper, we propose invertible attention that can be plugged into existing invertible models. We mathematically and experimentally prove that the invertibility of an attention model can be achieved by carefully constraining its Lipschitz constant.",
    "original_url": "http://arxiv.org/pdf/2106.09003v2",
    "original_title": "Invertible Attention",
    "source": "arxiv",
    "authors": [
      "Jiajun Zha",
      "Yiran Zhong",
      "Jing Zhang",
      "Richard Hartley",
      "Liang Zheng"
    ],
    "published": "2021-06-16T17:55:02+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2106.09003v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2106.09003v2_chunk_1",
    "chunk_text": "We mathematically and experimentally prove that the invertibility of an attention model can be achieved by carefully constraining its Lipschitz constant. We validate the invertibility of our invertible attention on image reconstruction task with 3 popular datasets: CIFAR-10, SVHN, and CelebA. We also show that our invertible attention achieves similar performance in comparison with normal non-invertible attention on dense prediction tasks. The code is available at https://github.com/Schwartz-Zha/InvertibleAttention",
    "original_url": "http://arxiv.org/pdf/2106.09003v2",
    "original_title": "Invertible Attention",
    "source": "arxiv",
    "authors": [
      "Jiajun Zha",
      "Yiran Zhong",
      "Jing Zhang",
      "Richard Hartley",
      "Liang Zheng"
    ],
    "published": "2021-06-16T17:55:02+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2106.09003v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2207.01844v1_chunk_0",
    "chunk_text": "Efficient Representation Learning via Adaptive Context Pooling\n\nSelf-attention mechanisms model long-range context by using pairwise attention between all input tokens. In doing so, they assume a fixed attention granularity defined by the individual tokens (e.g., text characters or image pixels), which may not be optimal for modeling complex dependencies at higher levels. In this paper, we propose ContextPool to address this problem by adapting the attention granularity for each token. Inspired by the success of ConvNets that are combined with pooling to capture long-range dependencies, we learn to pool neighboring features for each token before computing attention in a given attention layer. The pooling weights and support size are adaptively determined, allowing the pooled features to encode meaningful context with varying scale.",
    "original_url": "http://arxiv.org/pdf/2207.01844v1",
    "original_title": "Efficient Representation Learning via Adaptive Context Pooling",
    "source": "arxiv",
    "authors": [
      "Chen Huang",
      "Walter Talbott",
      "Navdeep Jaitly",
      "Josh Susskind"
    ],
    "published": "2022-07-05T07:10:31+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2207.01844v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2207.01844v1_chunk_1",
    "chunk_text": "The pooling weights and support size are adaptively determined, allowing the pooled features to encode meaningful context with varying scale. We show that ContextPool makes attention models more expressive, achieving strong performance often with fewer layers and thus significantly reduced cost. Experiments validate that our ContextPool module, when plugged into transformer models, matches or surpasses state-of-the-art performance using less compute on several language and image benchmarks, outperforms recent works with learned context sizes or sparse attention patterns, and is also applicable to ConvNets for efficient feature learning.",
    "original_url": "http://arxiv.org/pdf/2207.01844v1",
    "original_title": "Efficient Representation Learning via Adaptive Context Pooling",
    "source": "arxiv",
    "authors": [
      "Chen Huang",
      "Walter Talbott",
      "Navdeep Jaitly",
      "Josh Susskind"
    ],
    "published": "2022-07-05T07:10:31+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2207.01844v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2204.10962v3_chunk_0",
    "chunk_text": "Visual Attention Emerges from Recurrent Sparse Reconstruction\n\nVisual attention helps achieve robust perception under noise, corruption, and distribution shifts in human vision, which are areas where modern neural networks still fall short. We present VARS, Visual Attention from Recurrent Sparse reconstruction, a new attention formulation built on two prominent features of the human visual attention mechanism: recurrency and sparsity. Related features are grouped together via recurrent connections between neurons, with salient objects emerging via sparse regularization. VARS adopts an attractor network with recurrent connections that converges toward a stable pattern over time. Network layers are represented as ordinary differential equations (ODEs), formulating attention as a recurrent attractor network that equivalently optimizes the sparse reconstruction of input using a dictionary of \"templates\" encoding underlying patterns of data.",
    "original_url": "http://arxiv.org/pdf/2204.10962v3",
    "original_title": "Visual Attention Emerges from Recurrent Sparse Reconstruction",
    "source": "arxiv",
    "authors": [
      "Baifeng Shi",
      "Yale Song",
      "Neel Joshi",
      "Trevor Darrell",
      "Xin Wang"
    ],
    "published": "2022-04-23T00:35:02+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2204.10962v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2204.10962v3_chunk_1",
    "chunk_text": "Network layers are represented as ordinary differential equations (ODEs), formulating attention as a recurrent attractor network that equivalently optimizes the sparse reconstruction of input using a dictionary of \"templates\" encoding underlying patterns of data. We show that self-attention is a special case of VARS with a single-step optimization and no sparsity constraint. VARS can be readily used as a replacement for self-attention in popular vision transformers, consistently improving their robustness across various benchmarks. Code is released on GitHub (https://github.com/bfshi/VARS).",
    "original_url": "http://arxiv.org/pdf/2204.10962v3",
    "original_title": "Visual Attention Emerges from Recurrent Sparse Reconstruction",
    "source": "arxiv",
    "authors": [
      "Baifeng Shi",
      "Yale Song",
      "Neel Joshi",
      "Trevor Darrell",
      "Xin Wang"
    ],
    "published": "2022-04-23T00:35:02+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2204.10962v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2205.13730v1_chunk_0",
    "chunk_text": "Understanding Long Programming Languages with Structure-Aware Sparse Attention\n\nProgramming-based Pre-trained Language Models (PPLMs) such as CodeBERT have achieved great success in many downstream code-related tasks. Since the memory and computational complexity of self-attention in the Transformer grow quadratically with the sequence length, PPLMs typically limit the code length to 512. However, codes in real-world applications are generally long, such as code searches, which cannot be processed efficiently by existing PPLMs. To solve this problem, in this paper, we present SASA, a Structure-Aware Sparse Attention mechanism, which reduces the complexity and improves performance for long code understanding tasks. The key components in SASA are top-$k$ sparse attention and Abstract Syntax Tree (AST)-based structure-aware attention.",
    "original_url": "http://arxiv.org/pdf/2205.13730v1",
    "original_title": "Understanding Long Programming Languages with Structure-Aware Sparse Attention",
    "source": "arxiv",
    "authors": [
      "Tingting Liu",
      "Chengyu Wang",
      "Cen Chen",
      "Ming Gao",
      "Aoying Zhou"
    ],
    "published": "2022-05-27T02:50:57+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2205.13730v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2205.13730v1_chunk_1",
    "chunk_text": "The key components in SASA are top-$k$ sparse attention and Abstract Syntax Tree (AST)-based structure-aware attention. With top-$k$ sparse attention, the most crucial attention relation can be obtained with a lower computational cost. As the code structure represents the logic of the code statements, which is a complement to the code sequence characteristics, we further introduce AST structures into attention. Extensive experiments on CodeXGLUE tasks show that SASA achieves better performance than the competing baselines.",
    "original_url": "http://arxiv.org/pdf/2205.13730v1",
    "original_title": "Understanding Long Programming Languages with Structure-Aware Sparse Attention",
    "source": "arxiv",
    "authors": [
      "Tingting Liu",
      "Chengyu Wang",
      "Cen Chen",
      "Ming Gao",
      "Aoying Zhou"
    ],
    "published": "2022-05-27T02:50:57+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2205.13730v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2209.06096v1_chunk_0",
    "chunk_text": "Analysis of Self-Attention Head Diversity for Conformer-based Automatic Speech Recognition\n\nAttention layers are an integral part of modern end-to-end automatic speech recognition systems, for instance as part of the Transformer or Conformer architecture. Attention is typically multi-headed, where each head has an independent set of learned parameters and operates on the same input feature sequence. The output of multi-headed attention is a fusion of the outputs from the individual heads. We empirically analyze the diversity between representations produced by the different attention heads and demonstrate that the heads become highly correlated during the course of training. We investigate a few approaches to increasing attention head diversity, including using different attention mechanisms for each head and auxiliary training loss functions to promote head diversity.",
    "original_url": "http://arxiv.org/pdf/2209.06096v1",
    "original_title": "Analysis of Self-Attention Head Diversity for Conformer-based Automatic Speech Recognition",
    "source": "arxiv",
    "authors": [
      "Kartik Audhkhasi",
      "Yinghui Huang",
      "Bhuvana Ramabhadran",
      "Pedro J. Moreno"
    ],
    "published": "2022-09-13T15:50:03+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2209.06096v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2209.06096v1_chunk_1",
    "chunk_text": "We investigate a few approaches to increasing attention head diversity, including using different attention mechanisms for each head and auxiliary training loss functions to promote head diversity. We show that introducing diversity-promoting auxiliary loss functions during training is a more effective approach, and obtain WER improvements of up to 6% relative on the Librispeech corpus. Finally, we draw a connection between the diversity of attention heads and the similarity of the gradients of head parameters.",
    "original_url": "http://arxiv.org/pdf/2209.06096v1",
    "original_title": "Analysis of Self-Attention Head Diversity for Conformer-based Automatic Speech Recognition",
    "source": "arxiv",
    "authors": [
      "Kartik Audhkhasi",
      "Yinghui Huang",
      "Bhuvana Ramabhadran",
      "Pedro J. Moreno"
    ],
    "published": "2022-09-13T15:50:03+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2209.06096v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2210.05144v1_chunk_0",
    "chunk_text": "Mixture of Attention Heads: Selecting Attention Heads Per Token\n\nMixture-of-Experts (MoE) networks have been proposed as an efficient way to scale up model capacity and implement conditional computing. However, the study of MoE components mostly focused on the feedforward layer in Transformer architecture. This paper proposes the Mixture of Attention Heads (MoA), a new architecture that combines multi-head attention with the MoE mechanism. MoA includes a set of attention heads that each has its own set of parameters. Given an input, a router dynamically selects a subset of $k$ attention heads per token.",
    "original_url": "http://arxiv.org/pdf/2210.05144v1",
    "original_title": "Mixture of Attention Heads: Selecting Attention Heads Per Token",
    "source": "arxiv",
    "authors": [
      "Xiaofeng Zhang",
      "Yikang Shen",
      "Zeyu Huang",
      "Jie Zhou",
      "Wenge Rong",
      "Zhang Xiong"
    ],
    "published": "2022-10-11T04:54:05+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2210.05144v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2210.05144v1_chunk_1",
    "chunk_text": "Given an input, a router dynamically selects a subset of $k$ attention heads per token. This conditional computation schema allows MoA to achieve stronger performance than the standard multi-head attention layer. Furthermore, the sparsely gated MoA can easily scale up the number of attention heads and the number of parameters while preserving computational efficiency. In addition to the performance improvements, MoA also automatically differentiates heads' utilities, providing a new perspective to discuss the model's interpretability. We conducted experiments on several important tasks, including Machine Translation and Masked Language Modeling.",
    "original_url": "http://arxiv.org/pdf/2210.05144v1",
    "original_title": "Mixture of Attention Heads: Selecting Attention Heads Per Token",
    "source": "arxiv",
    "authors": [
      "Xiaofeng Zhang",
      "Yikang Shen",
      "Zeyu Huang",
      "Jie Zhou",
      "Wenge Rong",
      "Zhang Xiong"
    ],
    "published": "2022-10-11T04:54:05+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2210.05144v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2210.05144v1_chunk_2",
    "chunk_text": "We conducted experiments on several important tasks, including Machine Translation and Masked Language Modeling. Experiments have shown promising results on several tasks against strong baselines that involve large and very deep models.",
    "original_url": "http://arxiv.org/pdf/2210.05144v1",
    "original_title": "Mixture of Attention Heads: Selecting Attention Heads Per Token",
    "source": "arxiv",
    "authors": [
      "Xiaofeng Zhang",
      "Yikang Shen",
      "Zeyu Huang",
      "Jie Zhou",
      "Wenge Rong",
      "Zhang Xiong"
    ],
    "published": "2022-10-11T04:54:05+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2210.05144v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2212.11185v1_chunk_0",
    "chunk_text": "Entropy- and Distance-Based Predictors From GPT-2 Attention Patterns Predict Reading Times Over and Above GPT-2 Surprisal\n\nTransformer-based large language models are trained to make predictions about the next word by aggregating representations of previous tokens through their self-attention mechanism. In the field of cognitive modeling, such attention patterns have recently been interpreted as embodying the process of cue-based retrieval, in which attention over multiple targets is taken to generate interference and latency during retrieval. Under this framework, this work first defines an entropy-based predictor that quantifies the diffuseness of self-attention, as well as distance-based predictors that capture the incremental change in attention patterns across timesteps. Moreover, following recent studies that question the informativeness of attention weights, we also experiment with alternative methods for incorporating vector norms into attention weights. Regression experiments using predictors calculated from the GPT-2 language model show that these predictors deliver a substantially better fit to held-out self-paced reading and eye-tracking data over a rigorous baseline including GPT-2 surprisal.",
    "original_url": "http://arxiv.org/pdf/2212.11185v1",
    "original_title": "Entropy- and Distance-Based Predictors From GPT-2 Attention Patterns Predict Reading Times Over and Above GPT-2 Surprisal",
    "source": "arxiv",
    "authors": [
      "Byung-Doh Oh",
      "William Schuler"
    ],
    "published": "2022-12-21T16:56:07+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2212.11185v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2212.11185v1_chunk_1",
    "chunk_text": "Regression experiments using predictors calculated from the GPT-2 language model show that these predictors deliver a substantially better fit to held-out self-paced reading and eye-tracking data over a rigorous baseline including GPT-2 surprisal. Additionally, the distance-based predictors generally demonstrated higher predictive power, with effect sizes of up to 6.59 ms per standard deviation on self-paced reading times (compared to 2.82 ms for surprisal) and 1.05 ms per standard deviation on eye-gaze durations (compared to 3.81 ms for surprisal).",
    "original_url": "http://arxiv.org/pdf/2212.11185v1",
    "original_title": "Entropy- and Distance-Based Predictors From GPT-2 Attention Patterns Predict Reading Times Over and Above GPT-2 Surprisal",
    "source": "arxiv",
    "authors": [
      "Byung-Doh Oh",
      "William Schuler"
    ],
    "published": "2022-12-21T16:56:07+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2212.11185v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2501.15061v2_chunk_0",
    "chunk_text": "PolaFormer: Polarity-aware Linear Attention for Vision Transformers\n\nLinear attention has emerged as a promising alternative to softmax-based attention, leveraging kernelized feature maps to reduce complexity from quadratic to linear in sequence length. However, the non-negative constraint on feature maps and the relaxed exponential function used in approximation lead to significant information loss compared to the original query-key dot products, resulting in less discriminative attention maps with higher entropy. To address the missing interactions driven by negative values in query-key pairs, we propose a polarity-aware linear attention mechanism that explicitly models both same-signed and opposite-signed query-key interactions, ensuring comprehensive coverage of relational information. Furthermore, to restore the spiky properties of attention maps, we provide a theoretical analysis proving the existence of a class of element-wise functions (with positive first and second derivatives) that can reduce entropy in the attention distribution. For simplicity, and recognizing the distinct contributions of each dimension, we employ a learnable power function for rescaling, allowing strong and weak attention signals to be effectively separated.",
    "original_url": "http://arxiv.org/pdf/2501.15061v2",
    "original_title": "PolaFormer: Polarity-aware Linear Attention for Vision Transformers",
    "source": "arxiv",
    "authors": [
      "Weikang Meng",
      "Yadan Luo",
      "Xin Li",
      "Dongmei Jiang",
      "Zheng Zhang"
    ],
    "published": "2025-01-25T03:46:35+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2501.15061v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2501.15061v2_chunk_1",
    "chunk_text": "For simplicity, and recognizing the distinct contributions of each dimension, we employ a learnable power function for rescaling, allowing strong and weak attention signals to be effectively separated. Extensive experiments demonstrate that the proposed PolaFormer improves performance on various vision tasks, enhancing both expressiveness and efficiency by up to 4.6%.",
    "original_url": "http://arxiv.org/pdf/2501.15061v2",
    "original_title": "PolaFormer: Polarity-aware Linear Attention for Vision Transformers",
    "source": "arxiv",
    "authors": [
      "Weikang Meng",
      "Yadan Luo",
      "Xin Li",
      "Dongmei Jiang",
      "Zheng Zhang"
    ],
    "published": "2025-01-25T03:46:35+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2501.15061v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2305.00581v1_chunk_0",
    "chunk_text": "Multimodal Graph Transformer for Multimodal Question Answering\n\nDespite the success of Transformer models in vision and language tasks, they often learn knowledge from enormous data implicitly and cannot utilize structured input data directly. On the other hand, structured learning approaches such as graph neural networks (GNNs) that integrate prior information can barely compete with Transformer models. In this work, we aim to benefit from both worlds and propose a novel Multimodal Graph Transformer for question answering tasks that requires performing reasoning across multiple modalities. We introduce a graph-involved plug-and-play quasi-attention mechanism to incorporate multimodal graph information, acquired from text and visual data, to the vanilla self-attention as effective prior. In particular, we construct the text graph, dense region graph, and semantic graph to generate adjacency matrices, and then compose them with input vision and language features to perform downstream reasoning.",
    "original_url": "http://arxiv.org/pdf/2305.00581v1",
    "original_title": "Multimodal Graph Transformer for Multimodal Question Answering",
    "source": "arxiv",
    "authors": [
      "Xuehai He",
      "Xin Eric Wang"
    ],
    "published": "2023-04-30T21:22:35+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2305.00581v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2305.00581v1_chunk_1",
    "chunk_text": "In particular, we construct the text graph, dense region graph, and semantic graph to generate adjacency matrices, and then compose them with input vision and language features to perform downstream reasoning. Such a way of regularizing self-attention with graph information significantly improves the inferring ability and helps align features from different modalities. We validate the effectiveness of Multimodal Graph Transformer over its Transformer baselines on GQA, VQAv2, and MultiModalQA datasets.",
    "original_url": "http://arxiv.org/pdf/2305.00581v1",
    "original_title": "Multimodal Graph Transformer for Multimodal Question Answering",
    "source": "arxiv",
    "authors": [
      "Xuehai He",
      "Xin Eric Wang"
    ],
    "published": "2023-04-30T21:22:35+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2305.00581v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2305.11424v3_chunk_0",
    "chunk_text": "Graph Propagation Transformer for Graph Representation Learning\n\nThis paper presents a novel transformer architecture for graph representation learning. The core insight of our method is to fully consider the information propagation among nodes and edges in a graph when building the attention module in the transformer blocks. Specifically, we propose a new attention mechanism called Graph Propagation Attention (GPA). It explicitly passes the information among nodes and edges in three ways, i.e. node-to-node, node-to-edge, and edge-to-node, which is essential for learning graph-structured data.",
    "original_url": "http://arxiv.org/pdf/2305.11424v3",
    "original_title": "Graph Propagation Transformer for Graph Representation Learning",
    "source": "arxiv",
    "authors": [
      "Zhe Chen",
      "Hao Tan",
      "Tao Wang",
      "Tianrun Shen",
      "Tong Lu",
      "Qiuying Peng",
      "Cheng Cheng",
      "Yue Qi"
    ],
    "published": "2023-05-19T04:42:58+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2305.11424v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2305.11424v3_chunk_1",
    "chunk_text": "node-to-node, node-to-edge, and edge-to-node, which is essential for learning graph-structured data. On this basis, we design an effective transformer architecture named Graph Propagation Transformer (GPTrans) to further help learn graph data. We verify the performance of GPTrans in a wide range of graph learning experiments on several benchmark datasets. These results show that our method outperforms many state-of-the-art transformer-based graph models with better performance. The code will be released at https://github.com/czczup/GPTrans.",
    "original_url": "http://arxiv.org/pdf/2305.11424v3",
    "original_title": "Graph Propagation Transformer for Graph Representation Learning",
    "source": "arxiv",
    "authors": [
      "Zhe Chen",
      "Hao Tan",
      "Tao Wang",
      "Tianrun Shen",
      "Tong Lu",
      "Qiuying Peng",
      "Cheng Cheng",
      "Yue Qi"
    ],
    "published": "2023-05-19T04:42:58+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2305.11424v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2305.11424v3_chunk_2",
    "chunk_text": "The code will be released at https://github.com/czczup/GPTrans.",
    "original_url": "http://arxiv.org/pdf/2305.11424v3",
    "original_title": "Graph Propagation Transformer for Graph Representation Learning",
    "source": "arxiv",
    "authors": [
      "Zhe Chen",
      "Hao Tan",
      "Tao Wang",
      "Tianrun Shen",
      "Tong Lu",
      "Qiuying Peng",
      "Cheng Cheng",
      "Yue Qi"
    ],
    "published": "2023-05-19T04:42:58+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2305.11424v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2308.03364v2_chunk_0",
    "chunk_text": "Dual Aggregation Transformer for Image Super-Resolution\n\nTransformer has recently gained considerable popularity in low-level vision tasks, including image super-resolution (SR). These networks utilize self-attention along different dimensions, spatial or channel, and achieve impressive performance. This inspires us to combine the two dimensions in Transformer for a more powerful representation capability. Based on the above idea, we propose a novel Transformer model, Dual Aggregation Transformer (DAT), for image SR. Our DAT aggregates features across spatial and channel dimensions, in the inter-block and intra-block dual manner. Specifically, we alternately apply spatial and channel self-attention in consecutive Transformer blocks.",
    "original_url": "http://arxiv.org/pdf/2308.03364v2",
    "original_title": "Dual Aggregation Transformer for Image Super-Resolution",
    "source": "arxiv",
    "authors": [
      "Zheng Chen",
      "Yulun Zhang",
      "Jinjin Gu",
      "Linghe Kong",
      "Xiaokang Yang",
      "Fisher Yu"
    ],
    "published": "2023-08-07T07:39:39+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2308.03364v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2308.03364v2_chunk_1",
    "chunk_text": "Specifically, we alternately apply spatial and channel self-attention in consecutive Transformer blocks. The alternate strategy enables DAT to capture the global context and realize inter-block feature aggregation. Furthermore, we propose the adaptive interaction module (AIM) and the spatial-gate feed-forward network (SGFN) to achieve intra-block feature aggregation. AIM complements two self-attention mechanisms from corresponding dimensions. Meanwhile, SGFN introduces additional non-linear spatial information in the feed-forward network.",
    "original_url": "http://arxiv.org/pdf/2308.03364v2",
    "original_title": "Dual Aggregation Transformer for Image Super-Resolution",
    "source": "arxiv",
    "authors": [
      "Zheng Chen",
      "Yulun Zhang",
      "Jinjin Gu",
      "Linghe Kong",
      "Xiaokang Yang",
      "Fisher Yu"
    ],
    "published": "2023-08-07T07:39:39+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2308.03364v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2308.03364v2_chunk_2",
    "chunk_text": "Meanwhile, SGFN introduces additional non-linear spatial information in the feed-forward network. Extensive experiments show that our DAT surpasses current methods. Code and models are obtainable at https://github.com/zhengchen1999/DAT.",
    "original_url": "http://arxiv.org/pdf/2308.03364v2",
    "original_title": "Dual Aggregation Transformer for Image Super-Resolution",
    "source": "arxiv",
    "authors": [
      "Zheng Chen",
      "Yulun Zhang",
      "Jinjin Gu",
      "Linghe Kong",
      "Xiaokang Yang",
      "Fisher Yu"
    ],
    "published": "2023-08-07T07:39:39+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2308.03364v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2312.00662v1_chunk_0",
    "chunk_text": "Nonparametric Variational Regularisation of Pretrained Transformers\n\nThe current paradigm of large-scale pre-training and fine-tuning Transformer large language models has lead to significant improvements across the board in natural language processing. However, such large models are susceptible to overfitting to their training data, and as a result the models perform poorly when the domain changes. Also, due to the model's scale, the cost of fine-tuning the model to the new domain is large. Nonparametric Variational Information Bottleneck (NVIB) has been proposed as a regulariser for training cross-attention in Transformers, potentially addressing the overfitting problem. We extend the NVIB framework to replace all types of attention functions in Transformers, and show that existing pretrained Transformers can be reinterpreted as Nonparametric Variational (NV) models using a proposed identity initialisation.",
    "original_url": "http://arxiv.org/pdf/2312.00662v1",
    "original_title": "Nonparametric Variational Regularisation of Pretrained Transformers",
    "source": "arxiv",
    "authors": [
      "Fabio Fehr",
      "James Henderson"
    ],
    "published": "2023-12-01T15:40:30+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2312.00662v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2312.00662v1_chunk_1",
    "chunk_text": "We extend the NVIB framework to replace all types of attention functions in Transformers, and show that existing pretrained Transformers can be reinterpreted as Nonparametric Variational (NV) models using a proposed identity initialisation. We then show that changing the initialisation introduces a novel, information-theoretic post-training regularisation in the attention mechanism, which improves out-of-domain generalisation without any training. This success supports the hypothesis that pretrained Transformers are implicitly NV Bayesian models.",
    "original_url": "http://arxiv.org/pdf/2312.00662v1",
    "original_title": "Nonparametric Variational Regularisation of Pretrained Transformers",
    "source": "arxiv",
    "authors": [
      "Fabio Fehr",
      "James Henderson"
    ],
    "published": "2023-12-01T15:40:30+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2312.00662v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2410.08711v1_chunk_0",
    "chunk_text": "On-Chip Learning via Transformer In-Context Learning\n\nAutoregressive decoder-only transformers have become key components for scalable sequence processing and generation models. However, the transformer's self-attention mechanism requires transferring prior token projections from the main memory at each time step (token), thus severely limiting their performance on conventional processors. Self-attention can be viewed as a dynamic feed-forward layer, whose matrix is input sequence-dependent similarly to the result of local synaptic plasticity. Using this insight, we present a neuromorphic decoder-only transformer model that utilizes an on-chip plasticity processor to compute self-attention. Interestingly, the training of transformers enables them to ``learn'' the input context during inference.",
    "original_url": "http://arxiv.org/pdf/2410.08711v1",
    "original_title": "On-Chip Learning via Transformer In-Context Learning",
    "source": "arxiv",
    "authors": [
      "Jan Finkbeiner",
      "Emre Neftci"
    ],
    "published": "2024-10-11T10:54:09+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2410.08711v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2410.08711v1_chunk_1",
    "chunk_text": "Interestingly, the training of transformers enables them to ``learn'' the input context during inference. We demonstrate this in-context learning ability of transformers on the Loihi 2 processor by solving a few-shot classification problem. With this we emphasize the importance of pretrained models especially their ability to find simple, local, backpropagation free, learning rules enabling on-chip learning and adaptation in a hardware friendly manner.",
    "original_url": "http://arxiv.org/pdf/2410.08711v1",
    "original_title": "On-Chip Learning via Transformer In-Context Learning",
    "source": "arxiv",
    "authors": [
      "Jan Finkbeiner",
      "Emre Neftci"
    ],
    "published": "2024-10-11T10:54:09+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2410.08711v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2412.17571v1_chunk_0",
    "chunk_text": "HPCNeuroNet: A Neuromorphic Approach Merging SNN Temporal Dynamics with Transformer Attention for FPGA-based Particle Physics\n\nThis paper presents the innovative HPCNeuroNet model, a pioneering fusion of Spiking Neural Networks (SNNs), Transformers, and high-performance computing tailored for particle physics, particularly in particle identification from detector responses. Our approach leverages SNNs' intrinsic temporal dynamics and Transformers' robust attention mechanisms to enhance performance when discerning intricate particle interactions. At the heart of HPCNeuroNet lies the integration of the sequential dynamism inherent in SNNs with the context-aware attention capabilities of Transformers, enabling the model to precisely decode and interpret complex detector data. HPCNeuroNet is realized through the HLS4ML framework and optimized for deployment in FPGA environments. The model accuracy and scalability are also enhanced by this architectural choice.",
    "original_url": "http://arxiv.org/pdf/2412.17571v1",
    "original_title": "HPCNeuroNet: A Neuromorphic Approach Merging SNN Temporal Dynamics with Transformer Attention for FPGA-based Particle Physics",
    "source": "arxiv",
    "authors": [
      "Murat Isik",
      "Hiruna Vishwamith",
      "Jonathan Naoukin",
      "I. Can Dikmen"
    ],
    "published": "2024-12-23T13:44:29+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2412.17571v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2412.17571v1_chunk_1",
    "chunk_text": "The model accuracy and scalability are also enhanced by this architectural choice. Benchmarked against machine learning models, HPCNeuroNet showcases better performance metrics, underlining its transformative potential in high-energy physics. We demonstrate that the combination of SNNs, Transformers, and FPGA-based high-performance computing in particle physics signifies a significant step forward and provides a strong foundation for future research.",
    "original_url": "http://arxiv.org/pdf/2412.17571v1",
    "original_title": "HPCNeuroNet: A Neuromorphic Approach Merging SNN Temporal Dynamics with Transformer Attention for FPGA-based Particle Physics",
    "source": "arxiv",
    "authors": [
      "Murat Isik",
      "Hiruna Vishwamith",
      "Jonathan Naoukin",
      "I. Can Dikmen"
    ],
    "published": "2024-12-23T13:44:29+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2412.17571v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2303.11126v3_chunk_0",
    "chunk_text": "Robustifying Token Attention for Vision Transformers\n\nDespite the success of vision transformers (ViTs), they still suffer from significant drops in accuracy in the presence of common corruptions, such as noise or blur. Interestingly, we observe that the attention mechanism of ViTs tends to rely on few important tokens, a phenomenon we call token overfocusing. More critically, these tokens are not robust to corruptions, often leading to highly diverging attention patterns. In this paper, we intend to alleviate this overfocusing issue and make attention more stable through two general techniques: First, our Token-aware Average Pooling (TAP) module encourages the local neighborhood of each token to take part in the attention mechanism. Specifically, TAP learns average pooling schemes for each token such that the information of potentially important tokens in the neighborhood can adaptively be taken into account.",
    "original_url": "http://arxiv.org/pdf/2303.11126v3",
    "original_title": "Robustifying Token Attention for Vision Transformers",
    "source": "arxiv",
    "authors": [
      "Yong Guo",
      "David Stutz",
      "Bernt Schiele"
    ],
    "published": "2023-03-20T14:04:40+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2303.11126v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2303.11126v3_chunk_1",
    "chunk_text": "Specifically, TAP learns average pooling schemes for each token such that the information of potentially important tokens in the neighborhood can adaptively be taken into account. Second, we force the output tokens to aggregate information from a diverse set of input tokens rather than focusing on just a few by using our Attention Diversification Loss (ADL). We achieve this by penalizing high cosine similarity between the attention vectors of different tokens. In experiments, we apply our methods to a wide range of transformer architectures and improve robustness significantly. For example, we improve corruption robustness on ImageNet-C by 2.4% while improving accuracy by 0.4% based on state-of-the-art robust architecture FAN.",
    "original_url": "http://arxiv.org/pdf/2303.11126v3",
    "original_title": "Robustifying Token Attention for Vision Transformers",
    "source": "arxiv",
    "authors": [
      "Yong Guo",
      "David Stutz",
      "Bernt Schiele"
    ],
    "published": "2023-03-20T14:04:40+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2303.11126v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2303.11126v3_chunk_2",
    "chunk_text": "For example, we improve corruption robustness on ImageNet-C by 2.4% while improving accuracy by 0.4% based on state-of-the-art robust architecture FAN. Also, when fine-tuning on semantic segmentation tasks, we improve robustness on CityScapes-C by 2.4% and ACDC by 3.0%. Our code is available at https://github.com/guoyongcs/TAPADL.",
    "original_url": "http://arxiv.org/pdf/2303.11126v3",
    "original_title": "Robustifying Token Attention for Vision Transformers",
    "source": "arxiv",
    "authors": [
      "Yong Guo",
      "David Stutz",
      "Bernt Schiele"
    ],
    "published": "2023-03-20T14:04:40+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2303.11126v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2111.09714v1_chunk_0",
    "chunk_text": "You Only Sample (Almost) Once: Linear Cost Self-Attention Via Bernoulli Sampling\n\nTransformer-based models are widely used in natural language processing (NLP). Central to the transformer model is the self-attention mechanism, which captures the interactions of token pairs in the input sequences and depends quadratically on the sequence length. Training such models on longer sequences is expensive. In this paper, we show that a Bernoulli sampling attention mechanism based on Locality Sensitive Hashing (LSH), decreases the quadratic complexity of such models to linear. We bypass the quadratic cost by considering self-attention as a sum of individual tokens associated with Bernoulli random variables that can, in principle, be sampled at once by a single hash (although in practice, this number may be a small constant).",
    "original_url": "http://arxiv.org/pdf/2111.09714v1",
    "original_title": "You Only Sample (Almost) Once: Linear Cost Self-Attention Via Bernoulli Sampling",
    "source": "arxiv",
    "authors": [
      "Zhanpeng Zeng",
      "Yunyang Xiong",
      "Sathya N. Ravi",
      "Shailesh Acharya",
      "Glenn Fung",
      "Vikas Singh"
    ],
    "published": "2021-11-18T14:24:34+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2111.09714v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2111.09714v1_chunk_1",
    "chunk_text": "We bypass the quadratic cost by considering self-attention as a sum of individual tokens associated with Bernoulli random variables that can, in principle, be sampled at once by a single hash (although in practice, this number may be a small constant). This leads to an efficient sampling scheme to estimate self-attention which relies on specific modifications of LSH (to enable deployment on GPU architectures). We evaluate our algorithm on the GLUE benchmark with standard 512 sequence length where we see favorable performance relative to a standard pretrained Transformer. On the Long Range Arena (LRA) benchmark, for evaluating performance on long sequences, our method achieves results consistent with softmax self-attention but with sizable speed-ups and memory savings and often outperforms other efficient self-attention methods. Our code is available at https://github.com/mlpen/YOSO",
    "original_url": "http://arxiv.org/pdf/2111.09714v1",
    "original_title": "You Only Sample (Almost) Once: Linear Cost Self-Attention Via Bernoulli Sampling",
    "source": "arxiv",
    "authors": [
      "Zhanpeng Zeng",
      "Yunyang Xiong",
      "Sathya N. Ravi",
      "Shailesh Acharya",
      "Glenn Fung",
      "Vikas Singh"
    ],
    "published": "2021-11-18T14:24:34+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2111.09714v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2111.09714v1_chunk_2",
    "chunk_text": "Our code is available at https://github.com/mlpen/YOSO",
    "original_url": "http://arxiv.org/pdf/2111.09714v1",
    "original_title": "You Only Sample (Almost) Once: Linear Cost Self-Attention Via Bernoulli Sampling",
    "source": "arxiv",
    "authors": [
      "Zhanpeng Zeng",
      "Yunyang Xiong",
      "Sathya N. Ravi",
      "Shailesh Acharya",
      "Glenn Fung",
      "Vikas Singh"
    ],
    "published": "2021-11-18T14:24:34+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2111.09714v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2402.12788v3_chunk_0",
    "chunk_text": "RhythmFormer: Extracting Patterned rPPG Signals based on Periodic Sparse Attention\n\nRemote photoplethysmography (rPPG) is a non-contact method for detecting physiological signals based on facial videos, holding high potential in various applications. Due to the periodicity nature of rPPG signals, the long-range dependency capturing capacity of the transformer was assumed to be advantageous for such signals. However, existing methods have not conclusively demonstrated the superior performance of transformers over traditional convolutional neural networks. This may be attributed to the quadratic scaling exhibited by transformer with sequence length, resulting in coarse-grained feature extraction, which in turn affects robustness and generalization. To address that, this paper proposes a periodic sparse attention mechanism based on temporal attention sparsity induced by periodicity.",
    "original_url": "http://arxiv.org/pdf/2402.12788v3",
    "original_title": "RhythmFormer: Extracting Patterned rPPG Signals based on Periodic Sparse Attention",
    "source": "arxiv",
    "authors": [
      "Bochao Zou",
      "Zizheng Guo",
      "Jiansheng Chen",
      "Junbao Zhuo",
      "Weiran Huang",
      "Huimin Ma"
    ],
    "published": "2024-02-20T07:56:02+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2402.12788v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2402.12788v3_chunk_1",
    "chunk_text": "To address that, this paper proposes a periodic sparse attention mechanism based on temporal attention sparsity induced by periodicity. A pre-attention stage is introduced before the conventional attention mechanism. This stage learns periodic patterns to filter out a large number of irrelevant attention computations, thus enabling fine-grained feature extraction. Moreover, to address the issue of fine-grained features being more susceptible to noise interference, a fusion stem is proposed to effectively guide self-attention towards rPPG features. It can be easily integrated into existing methods to enhance their performance.",
    "original_url": "http://arxiv.org/pdf/2402.12788v3",
    "original_title": "RhythmFormer: Extracting Patterned rPPG Signals based on Periodic Sparse Attention",
    "source": "arxiv",
    "authors": [
      "Bochao Zou",
      "Zizheng Guo",
      "Jiansheng Chen",
      "Junbao Zhuo",
      "Weiran Huang",
      "Huimin Ma"
    ],
    "published": "2024-02-20T07:56:02+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2402.12788v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2402.12788v3_chunk_2",
    "chunk_text": "It can be easily integrated into existing methods to enhance their performance. Extensive experiments show that the proposed method achieves state-of-the-art performance in both intra-dataset and cross-dataset evaluations. The codes are available at https://github.com/zizheng-guo/RhythmFormer.",
    "original_url": "http://arxiv.org/pdf/2402.12788v3",
    "original_title": "RhythmFormer: Extracting Patterned rPPG Signals based on Periodic Sparse Attention",
    "source": "arxiv",
    "authors": [
      "Bochao Zou",
      "Zizheng Guo",
      "Jiansheng Chen",
      "Junbao Zhuo",
      "Weiran Huang",
      "Huimin Ma"
    ],
    "published": "2024-02-20T07:56:02+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2402.12788v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2411.12502v3_chunk_0",
    "chunk_text": "Transformer Neural Processes - Kernel Regression\n\nNeural Processes (NPs) are a rapidly evolving class of models designed to directly model the posterior predictive distribution of stochastic processes. Originally developed as a scalable alternative to Gaussian Processes (GPs), which are limited by $O(n^3)$ runtime complexity, the most accurate modern NPs can often rival GPs but still suffer from an $O(n^2)$ bottleneck due to their attention mechanism. We introduce the Transformer Neural Process - Kernel Regression (TNP-KR), a scalable NP featuring: (1) a Kernel Regression Block (KRBlock), a simple, extensible, and parameter efficient transformer block with complexity $O(n_c^2 + n_c n_t)$, where $n_c$ and $n_t$ are the number of context and test points, respectively; (2) a kernel-based attention bias; and (3) two novel attention mechanisms: scan attention (SA), a memory-efficient scan-based attention that when paired with a kernel-based bias can make TNP-KR translation invariant, and deep kernel attention (DKA), a Performer-style attention that implicitly incoporates a distance bias and further reduces complexity to $O(n_c)$. These enhancements enable both TNP-KR variants to perform inference with 100K context points on over 1M test points in under a minute on a single 24GB GPU. On benchmarks spanning meta regression, Bayesian optimization, image completion, and epidemiology, TNP-KR with DKA outperforms its Performer counterpart on nearly every benchmark, while TNP-KR with SA achieves state-of-the-art results.",
    "original_url": "http://arxiv.org/pdf/2411.12502v3",
    "original_title": "Transformer Neural Processes - Kernel Regression",
    "source": "arxiv",
    "authors": [
      "Daniel Jenson",
      "Jhonathan Navott",
      "Mengyan Zhang",
      "Makkunda Sharma",
      "Elizaveta Semenova",
      "Seth Flaxman"
    ],
    "published": "2024-11-19T13:40:49+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2411.12502v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2411.12502v3_chunk_1",
    "chunk_text": "On benchmarks spanning meta regression, Bayesian optimization, image completion, and epidemiology, TNP-KR with DKA outperforms its Performer counterpart on nearly every benchmark, while TNP-KR with SA achieves state-of-the-art results.",
    "original_url": "http://arxiv.org/pdf/2411.12502v3",
    "original_title": "Transformer Neural Processes - Kernel Regression",
    "source": "arxiv",
    "authors": [
      "Daniel Jenson",
      "Jhonathan Navott",
      "Mengyan Zhang",
      "Makkunda Sharma",
      "Elizaveta Semenova",
      "Seth Flaxman"
    ],
    "published": "2024-11-19T13:40:49+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2411.12502v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2412.18778v1_chunk_0",
    "chunk_text": "Unified Local and Global Attention Interaction Modeling for Vision Transformers\n\nWe present a novel method that extends the self-attention mechanism of a vision transformer (ViT) for more accurate object detection across diverse datasets. ViTs show strong capability for image understanding tasks such as object detection, segmentation, and classification. This is due in part to their ability to leverage global information from interactions among visual tokens. However, the self-attention mechanism in ViTs are limited because they do not allow visual tokens to exchange local or global information with neighboring features before computing global attention. This is problematic because tokens are treated in isolation when attending (matching) to other tokens, and valuable spatial relationships are overlooked.",
    "original_url": "http://arxiv.org/pdf/2412.18778v1",
    "original_title": "Unified Local and Global Attention Interaction Modeling for Vision Transformers",
    "source": "arxiv",
    "authors": [
      "Tan Nguyen",
      "Coy D. Heldermon",
      "Corey Toler-Franklin"
    ],
    "published": "2024-12-25T04:53:19+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2412.18778v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2412.18778v1_chunk_1",
    "chunk_text": "This is problematic because tokens are treated in isolation when attending (matching) to other tokens, and valuable spatial relationships are overlooked. This isolation is further compounded by dot-product similarity operations that make tokens from different semantic classes appear visually similar. To address these limitations, we introduce two modifications to the traditional self-attention framework; a novel aggressive convolution pooling strategy for local feature mixing, and a new conceptual attention transformation to facilitate interaction and feature exchange between semantic concepts. Experimental results demonstrate that local and global information exchange among visual features before self-attention significantly improves performance on challenging object detection tasks and generalizes across multiple benchmark datasets and challenging medical datasets. We publish source code and a novel dataset of cancerous tumors (chimeric cell clusters).",
    "original_url": "http://arxiv.org/pdf/2412.18778v1",
    "original_title": "Unified Local and Global Attention Interaction Modeling for Vision Transformers",
    "source": "arxiv",
    "authors": [
      "Tan Nguyen",
      "Coy D. Heldermon",
      "Corey Toler-Franklin"
    ],
    "published": "2024-12-25T04:53:19+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2412.18778v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2412.18778v1_chunk_2",
    "chunk_text": "We publish source code and a novel dataset of cancerous tumors (chimeric cell clusters).",
    "original_url": "http://arxiv.org/pdf/2412.18778v1",
    "original_title": "Unified Local and Global Attention Interaction Modeling for Vision Transformers",
    "source": "arxiv",
    "authors": [
      "Tan Nguyen",
      "Coy D. Heldermon",
      "Corey Toler-Franklin"
    ],
    "published": "2024-12-25T04:53:19+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2412.18778v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2503.13479v1_chunk_0",
    "chunk_text": "EAGLE: Contextual Point Cloud Generation via Adaptive Continuous Normalizing Flow with Self-Attention\n\nAs 3D point clouds become the prevailing shape representation in computer vision, how to generate high-resolution point clouds has become a pressing issue. Flow-based generative models can effectively perform point cloud generation tasks. However, traditional CNN-based flow architectures rely only on local information to extract features, making it difficult to capture global contextual information. Inspired by the wide adoption of Transformers, we explored the complementary roles of self-attention mechanisms in Transformers, CNN, and continuous normalizing flows. To this end, we propose a probabilistic model via adaptive normalizing flows and self-attention.",
    "original_url": "http://arxiv.org/pdf/2503.13479v1",
    "original_title": "EAGLE: Contextual Point Cloud Generation via Adaptive Continuous Normalizing Flow with Self-Attention",
    "source": "arxiv",
    "authors": [
      "Linhao Wang",
      "Qichang Zhang",
      "Yifan Yang",
      "Hao Wang"
    ],
    "published": "2025-03-05T03:02:14+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2503.13479v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2503.13479v1_chunk_1",
    "chunk_text": "To this end, we propose a probabilistic model via adaptive normalizing flows and self-attention. Our idea leverages self-attention mechanisms to capture global contextual information. We also propose adaptive continuous normalizing flows by introducing adaptive bias correction mechanism. Combined with normalization, the mechanism dynamically handles different input contexts and mitigates potential bias-shift issues from standard initialization. Experimental results demonstrate that EAGLE achieves competitive performance in point cloud generation.",
    "original_url": "http://arxiv.org/pdf/2503.13479v1",
    "original_title": "EAGLE: Contextual Point Cloud Generation via Adaptive Continuous Normalizing Flow with Self-Attention",
    "source": "arxiv",
    "authors": [
      "Linhao Wang",
      "Qichang Zhang",
      "Yifan Yang",
      "Hao Wang"
    ],
    "published": "2025-03-05T03:02:14+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2503.13479v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2503.13479v1_chunk_2",
    "chunk_text": "Experimental results demonstrate that EAGLE achieves competitive performance in point cloud generation.",
    "original_url": "http://arxiv.org/pdf/2503.13479v1",
    "original_title": "EAGLE: Contextual Point Cloud Generation via Adaptive Continuous Normalizing Flow with Self-Attention",
    "source": "arxiv",
    "authors": [
      "Linhao Wang",
      "Qichang Zhang",
      "Yifan Yang",
      "Hao Wang"
    ],
    "published": "2025-03-05T03:02:14+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2503.13479v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2504.19901v1_chunk_0",
    "chunk_text": "Attention Mechanism, Max-Affine Partition, and Universal Approximation\n\nWe establish the universal approximation capability of single-layer, single-head self- and cross-attention mechanisms with minimal attached structures. Our key insight is to interpret single-head attention as an input domain-partition mechanism that assigns distinct values to subregions. This allows us to engineer the attention weights such that this assignment imitates the target function. Building on this, we prove that a single self-attention layer, preceded by sum-of-linear transformations, is capable of approximating any continuous function on a compact domain under the $L_\\infty$-norm. Furthermore, we extend this construction to approximate any Lebesgue integrable function under $L_p$-norm for $1\\leq p <\\infty$.",
    "original_url": "http://arxiv.org/pdf/2504.19901v1",
    "original_title": "Attention Mechanism, Max-Affine Partition, and Universal Approximation",
    "source": "arxiv",
    "authors": [
      "Hude Liu",
      "Jerry Yao-Chieh Hu",
      "Zhao Song",
      "Han Liu"
    ],
    "published": "2025-04-28T15:31:45+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2504.19901v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2504.19901v1_chunk_1",
    "chunk_text": "Furthermore, we extend this construction to approximate any Lebesgue integrable function under $L_p$-norm for $1\\leq p <\\infty$. Lastly, we also extend our techniques and show that, for the first time, single-head cross-attention achieves the same universal approximation guarantees.",
    "original_url": "http://arxiv.org/pdf/2504.19901v1",
    "original_title": "Attention Mechanism, Max-Affine Partition, and Universal Approximation",
    "source": "arxiv",
    "authors": [
      "Hude Liu",
      "Jerry Yao-Chieh Hu",
      "Zhao Song",
      "Han Liu"
    ],
    "published": "2025-04-28T15:31:45+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2504.19901v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2002.05556v2_chunk_0",
    "chunk_text": "Sparse and Structured Visual Attention\n\nVisual attention mechanisms are widely used in multimodal tasks, as visual question answering (VQA). One drawback of softmax-based attention mechanisms is that they assign some probability mass to all image regions, regardless of their adjacency structure and of their relevance to the text. In this paper, to better link the image structure with the text, we replace the traditional softmax attention mechanism with two alternative sparsity-promoting transformations: sparsemax, which is able to select only the relevant regions (assigning zero weight to the rest), and a newly proposed Total-Variation Sparse Attention (TVmax), which further encourages the joint selection of adjacent spatial locations. Experiments in VQA show gains in accuracy as well as higher similarity to human attention, which suggests better interpretability.",
    "original_url": "http://arxiv.org/pdf/2002.05556v2",
    "original_title": "Sparse and Structured Visual Attention",
    "source": "arxiv",
    "authors": [
      "Pedro Henrique Martins",
      "Vlad Niculae",
      "Zita Marinho",
      "André Martins"
    ],
    "published": "2020-02-13T15:08:12+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2002.05556v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2206.14550v1_chunk_0",
    "chunk_text": "SALO: An Efficient Spatial Accelerator Enabling Hybrid Sparse Attention Mechanisms for Long Sequences\n\nThe attention mechanisms of transformers effectively extract pertinent information from the input sequence. However, the quadratic complexity of self-attention w.r.t the sequence length incurs heavy computational and memory burdens, especially for tasks with long sequences. Existing accelerators face performance degradation in these tasks. To this end, we propose SALO to enable hybrid sparse attention mechanisms for long sequences. SALO contains a data scheduler to map hybrid sparse attention patterns onto hardware and a spatial accelerator to perform the efficient attention computation.",
    "original_url": "http://arxiv.org/pdf/2206.14550v1",
    "original_title": "SALO: An Efficient Spatial Accelerator Enabling Hybrid Sparse Attention Mechanisms for Long Sequences",
    "source": "arxiv",
    "authors": [
      "Guan Shen",
      "Jieru Zhao",
      "Quan Chen",
      "Jingwen Leng",
      "Chao Li",
      "Minyi Guo"
    ],
    "published": "2022-06-29T12:01:19+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2206.14550v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2206.14550v1_chunk_1",
    "chunk_text": "SALO contains a data scheduler to map hybrid sparse attention patterns onto hardware and a spatial accelerator to perform the efficient attention computation. We show that SALO achieves 17.66x and 89.33x speedup on average compared to GPU and CPU implementations, respectively, on typical workloads, i.e., Longformer and ViL.",
    "original_url": "http://arxiv.org/pdf/2206.14550v1",
    "original_title": "SALO: An Efficient Spatial Accelerator Enabling Hybrid Sparse Attention Mechanisms for Long Sequences",
    "source": "arxiv",
    "authors": [
      "Guan Shen",
      "Jieru Zhao",
      "Quan Chen",
      "Jingwen Leng",
      "Chao Li",
      "Minyi Guo"
    ],
    "published": "2022-06-29T12:01:19+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2206.14550v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2503.23353v1_chunk_0",
    "chunk_text": "Object Isolated Attention for Consistent Story Visualization\n\nOpen-ended story visualization is a challenging task that involves generating coherent image sequences from a given storyline. One of the main difficulties is maintaining character consistency while creating natural and contextually fitting scenes--an area where many existing methods struggle. In this paper, we propose an enhanced Transformer module that uses separate self attention and cross attention mechanisms, leveraging prior knowledge from pre-trained diffusion models to ensure logical scene creation. The isolated self attention mechanism improves character consistency by refining attention maps to reduce focus on irrelevant areas and highlight key features of the same character. Meanwhile, the isolated cross attention mechanism independently processes each character's features, avoiding feature fusion and further strengthening consistency.",
    "original_url": "http://arxiv.org/pdf/2503.23353v1",
    "original_title": "Object Isolated Attention for Consistent Story Visualization",
    "source": "arxiv",
    "authors": [
      "Xiangyang Luo",
      "Junhao Cheng",
      "Yifan Xie",
      "Xin Zhang",
      "Tao Feng",
      "Zhou Liu",
      "Fei Ma",
      "Fei Yu"
    ],
    "published": "2025-03-30T08:16:52+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2503.23353v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2503.23353v1_chunk_1",
    "chunk_text": "Meanwhile, the isolated cross attention mechanism independently processes each character's features, avoiding feature fusion and further strengthening consistency. Notably, our method is training-free, allowing the continuous generation of new characters and storylines without re-tuning. Both qualitative and quantitative evaluations show that our approach outperforms current methods, demonstrating its effectiveness.",
    "original_url": "http://arxiv.org/pdf/2503.23353v1",
    "original_title": "Object Isolated Attention for Consistent Story Visualization",
    "source": "arxiv",
    "authors": [
      "Xiangyang Luo",
      "Junhao Cheng",
      "Yifan Xie",
      "Xin Zhang",
      "Tao Feng",
      "Zhou Liu",
      "Fei Ma",
      "Fei Yu"
    ],
    "published": "2025-03-30T08:16:52+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2503.23353v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2305.19798v2_chunk_0",
    "chunk_text": "Primal-Attention: Self-attention through Asymmetric Kernel SVD in Primal Representation\n\nRecently, a new line of works has emerged to understand and improve self-attention in Transformers by treating it as a kernel machine. However, existing works apply the methods for symmetric kernels to the asymmetric self-attention, resulting in a nontrivial gap between the analytical understanding and numerical implementation. In this paper, we provide a new perspective to represent and optimize self-attention through asymmetric Kernel Singular Value Decomposition (KSVD), which is also motivated by the low-rank property of self-attention normally observed in deep layers. Through asymmetric KSVD, $i$) a primal-dual representation of self-attention is formulated, where the optimization objective is cast to maximize the projection variances in the attention outputs; $ii$) a novel attention mechanism, i.e., Primal-Attention, is proposed via the primal representation of KSVD, avoiding explicit computation of the kernel matrix in the dual; $iii$) with KKT conditions, we prove that the stationary solution to the KSVD optimization in Primal-Attention yields a zero-value objective. In this manner, KSVD optimization can be implemented by simply minimizing a regularization loss, so that low-rank property is promoted without extra decomposition.",
    "original_url": "http://arxiv.org/pdf/2305.19798v2",
    "original_title": "Primal-Attention: Self-attention through Asymmetric Kernel SVD in Primal Representation",
    "source": "arxiv",
    "authors": [
      "Yingyi Chen",
      "Qinghua Tao",
      "Francesco Tonin",
      "Johan A. K. Suykens"
    ],
    "published": "2023-05-31T12:38:24+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2305.19798v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2305.19798v2_chunk_1",
    "chunk_text": "In this manner, KSVD optimization can be implemented by simply minimizing a regularization loss, so that low-rank property is promoted without extra decomposition. Numerical experiments show state-of-the-art performance of our Primal-Attention with improved efficiency. Moreover, we demonstrate that the deployed KSVD optimization regularizes Primal-Attention with a sharper singular value decay than that of the canonical self-attention, further verifying the great potential of our method. To the best of our knowledge, this is the first work that provides a primal-dual representation for the asymmetric kernel in self-attention and successfully applies it to modeling and optimization.",
    "original_url": "http://arxiv.org/pdf/2305.19798v2",
    "original_title": "Primal-Attention: Self-attention through Asymmetric Kernel SVD in Primal Representation",
    "source": "arxiv",
    "authors": [
      "Yingyi Chen",
      "Qinghua Tao",
      "Francesco Tonin",
      "Johan A. K. Suykens"
    ],
    "published": "2023-05-31T12:38:24+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2305.19798v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2108.03428v1_chunk_0",
    "chunk_text": "PSViT: Better Vision Transformer via Token Pooling and Attention Sharing\n\nIn this paper, we observe two levels of redundancies when applying vision transformers (ViT) for image recognition. First, fixing the number of tokens through the whole network produces redundant features at the spatial level. Second, the attention maps among different transformer layers are redundant. Based on the observations above, we propose a PSViT: a ViT with token Pooling and attention Sharing to reduce the redundancy, effectively enhancing the feature representation ability, and achieving a better speed-accuracy trade-off. Specifically, in our PSViT, token pooling can be defined as the operation that decreases the number of tokens at the spatial level.",
    "original_url": "http://arxiv.org/pdf/2108.03428v1",
    "original_title": "PSViT: Better Vision Transformer via Token Pooling and Attention Sharing",
    "source": "arxiv",
    "authors": [
      "Boyu Chen",
      "Peixia Li",
      "Baopu Li",
      "Chuming Li",
      "Lei Bai",
      "Chen Lin",
      "Ming Sun",
      "Junjie Yan",
      "Wanli Ouyang"
    ],
    "published": "2021-08-07T11:30:54+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2108.03428v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2108.03428v1_chunk_1",
    "chunk_text": "Specifically, in our PSViT, token pooling can be defined as the operation that decreases the number of tokens at the spatial level. Besides, attention sharing will be built between the neighboring transformer layers for reusing the attention maps having a strong correlation among adjacent layers. Then, a compact set of the possible combinations for different token pooling and attention sharing mechanisms are constructed. Based on the proposed compact set, the number of tokens in each layer and the choices of layers sharing attention can be treated as hyper-parameters that are learned from data automatically. Experimental results show that the proposed scheme can achieve up to 6.6% accuracy improvement in ImageNet classification compared with the DeiT.",
    "original_url": "http://arxiv.org/pdf/2108.03428v1",
    "original_title": "PSViT: Better Vision Transformer via Token Pooling and Attention Sharing",
    "source": "arxiv",
    "authors": [
      "Boyu Chen",
      "Peixia Li",
      "Baopu Li",
      "Chuming Li",
      "Lei Bai",
      "Chen Lin",
      "Ming Sun",
      "Junjie Yan",
      "Wanli Ouyang"
    ],
    "published": "2021-08-07T11:30:54+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2108.03428v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2108.03428v1_chunk_2",
    "chunk_text": "Experimental results show that the proposed scheme can achieve up to 6.6% accuracy improvement in ImageNet classification compared with the DeiT.",
    "original_url": "http://arxiv.org/pdf/2108.03428v1",
    "original_title": "PSViT: Better Vision Transformer via Token Pooling and Attention Sharing",
    "source": "arxiv",
    "authors": [
      "Boyu Chen",
      "Peixia Li",
      "Baopu Li",
      "Chuming Li",
      "Lei Bai",
      "Chen Lin",
      "Ming Sun",
      "Junjie Yan",
      "Wanli Ouyang"
    ],
    "published": "2021-08-07T11:30:54+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2108.03428v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2206.05495v1_chunk_0",
    "chunk_text": "DRAformer: Differentially Reconstructed Attention Transformer for Time-Series Forecasting\n\nTime-series forecasting plays an important role in many real-world scenarios, such as equipment life cycle forecasting, weather forecasting, and traffic flow forecasting. It can be observed from recent research that a variety of transformer-based models have shown remarkable results in time-series forecasting. However, there are still some issues that limit the ability of transformer-based models on time-series forecasting tasks: (i) learning directly on raw data is susceptible to noise due to its complex and unstable feature representation; (ii) the self-attention mechanisms pay insufficient attention to changing features and temporal dependencies. In order to solve these two problems, we propose a transformer-based differentially reconstructed attention model DRAformer. Specifically, DRAformer has the following innovations: (i) learning against differenced sequences, which preserves clear and stable sequence features by differencing and highlights the changing properties of sequences; (ii) the reconstructed attention: integrated distance attention exhibits sequential distance through a learnable Gaussian kernel, distributed difference attention calculates distribution difference by mapping the difference sequence to the adaptive feature space, and the combination of the two effectively focuses on the sequences with prominent associations; (iii) the reconstructed decoder input, which extracts sequence features by integrating variation information and temporal correlations, thereby obtaining a more comprehensive sequence representation.",
    "original_url": "http://arxiv.org/pdf/2206.05495v1",
    "original_title": "DRAformer: Differentially Reconstructed Attention Transformer for Time-Series Forecasting",
    "source": "arxiv",
    "authors": [
      "Benhan Li",
      "Shengdong Du",
      "Tianrui Li",
      "Jie Hu",
      "Zhen Jia"
    ],
    "published": "2022-06-11T10:34:29+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2206.05495v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2206.05495v1_chunk_1",
    "chunk_text": "Specifically, DRAformer has the following innovations: (i) learning against differenced sequences, which preserves clear and stable sequence features by differencing and highlights the changing properties of sequences; (ii) the reconstructed attention: integrated distance attention exhibits sequential distance through a learnable Gaussian kernel, distributed difference attention calculates distribution difference by mapping the difference sequence to the adaptive feature space, and the combination of the two effectively focuses on the sequences with prominent associations; (iii) the reconstructed decoder input, which extracts sequence features by integrating variation information and temporal correlations, thereby obtaining a more comprehensive sequence representation. Extensive experiments on four large-scale datasets demonstrate that DRAformer outperforms state-of-the-art baselines.",
    "original_url": "http://arxiv.org/pdf/2206.05495v1",
    "original_title": "DRAformer: Differentially Reconstructed Attention Transformer for Time-Series Forecasting",
    "source": "arxiv",
    "authors": [
      "Benhan Li",
      "Shengdong Du",
      "Tianrui Li",
      "Jie Hu",
      "Zhen Jia"
    ],
    "published": "2022-06-11T10:34:29+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2206.05495v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2303.16450v1_chunk_0",
    "chunk_text": "Self-positioning Point-based Transformer for Point Cloud Understanding\n\nTransformers have shown superior performance on various computer vision tasks with their capabilities to capture long-range dependencies. Despite the success, it is challenging to directly apply Transformers on point clouds due to their quadratic cost in the number of points. In this paper, we present a Self-Positioning point-based Transformer (SPoTr), which is designed to capture both local and global shape contexts with reduced complexity. Specifically, this architecture consists of local self-attention and self-positioning point-based global cross-attention. The self-positioning points, adaptively located based on the input shape, consider both spatial and semantic information with disentangled attention to improve expressive power.",
    "original_url": "http://arxiv.org/pdf/2303.16450v1",
    "original_title": "Self-positioning Point-based Transformer for Point Cloud Understanding",
    "source": "arxiv",
    "authors": [
      "Jinyoung Park",
      "Sanghyeok Lee",
      "Sihyeon Kim",
      "Yunyang Xiong",
      "Hyunwoo J. Kim"
    ],
    "published": "2023-03-29T04:27:11+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2303.16450v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2303.16450v1_chunk_1",
    "chunk_text": "The self-positioning points, adaptively located based on the input shape, consider both spatial and semantic information with disentangled attention to improve expressive power. With the self-positioning points, we propose a novel global cross-attention mechanism for point clouds, which improves the scalability of global self-attention by allowing the attention module to compute attention weights with only a small set of self-positioning points. Experiments show the effectiveness of SPoTr on three point cloud tasks such as shape classification, part segmentation, and scene segmentation. In particular, our proposed model achieves an accuracy gain of 2.6% over the previous best models on shape classification with ScanObjectNN. We also provide qualitative analyses to demonstrate the interpretability of self-positioning points.",
    "original_url": "http://arxiv.org/pdf/2303.16450v1",
    "original_title": "Self-positioning Point-based Transformer for Point Cloud Understanding",
    "source": "arxiv",
    "authors": [
      "Jinyoung Park",
      "Sanghyeok Lee",
      "Sihyeon Kim",
      "Yunyang Xiong",
      "Hyunwoo J. Kim"
    ],
    "published": "2023-03-29T04:27:11+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2303.16450v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2303.16450v1_chunk_2",
    "chunk_text": "We also provide qualitative analyses to demonstrate the interpretability of self-positioning points. The code of SPoTr is available at https://github.com/mlvlab/SPoTr.",
    "original_url": "http://arxiv.org/pdf/2303.16450v1",
    "original_title": "Self-positioning Point-based Transformer for Point Cloud Understanding",
    "source": "arxiv",
    "authors": [
      "Jinyoung Park",
      "Sanghyeok Lee",
      "Sihyeon Kim",
      "Yunyang Xiong",
      "Hyunwoo J. Kim"
    ],
    "published": "2023-03-29T04:27:11+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2303.16450v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2305.02582v2_chunk_0",
    "chunk_text": "On the Expressivity Role of LayerNorm in Transformers' Attention\n\nLayer Normalization (LayerNorm) is an inherent component in all Transformer-based models. In this paper, we show that LayerNorm is crucial to the expressivity of the multi-head attention layer that follows it. This is in contrast to the common belief that LayerNorm's only role is to normalize the activations during the forward pass, and their gradients during the backward pass. We consider a geometric interpretation of LayerNorm and show that it consists of two components: (a) projection of the input vectors to a $d-1$ space that is orthogonal to the $\\left[1,1,...,1\\right]$ vector, and (b) scaling of all vectors to the same norm of $\\sqrt{d}$. We show that each of these components is important for the attention layer that follows it in Transformers: (a) projection allows the attention mechanism to create an attention query that attends to all keys equally, offloading the need to learn this operation by the attention; and (b) scaling allows each key to potentially receive the highest attention, and prevents keys from being \"un-select-able\".",
    "original_url": "http://arxiv.org/pdf/2305.02582v2",
    "original_title": "On the Expressivity Role of LayerNorm in Transformers' Attention",
    "source": "arxiv",
    "authors": [
      "Shaked Brody",
      "Uri Alon",
      "Eran Yahav"
    ],
    "published": "2023-05-04T06:32:05+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2305.02582v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2305.02582v2_chunk_1",
    "chunk_text": "We show that each of these components is important for the attention layer that follows it in Transformers: (a) projection allows the attention mechanism to create an attention query that attends to all keys equally, offloading the need to learn this operation by the attention; and (b) scaling allows each key to potentially receive the highest attention, and prevents keys from being \"un-select-able\". We show empirically that Transformers do indeed benefit from these properties of LayeNorm in general language modeling and even in computing simple functions such as \"majority\". Our code is available at https://github.com/tech-srl/layer_norm_expressivity_role .",
    "original_url": "http://arxiv.org/pdf/2305.02582v2",
    "original_title": "On the Expressivity Role of LayerNorm in Transformers' Attention",
    "source": "arxiv",
    "authors": [
      "Shaked Brody",
      "Uri Alon",
      "Eran Yahav"
    ],
    "published": "2023-05-04T06:32:05+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2305.02582v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1909.00188v1_chunk_0",
    "chunk_text": "Improving Multi-Head Attention with Capsule Networks\n\nMulti-head attention advances neural machine translation by working out multiple versions of attention in different subspaces, but the neglect of semantic overlapping between subspaces increases the difficulty of translation and consequently hinders the further improvement of translation performance. In this paper, we employ capsule networks to comb the information from the multiple heads of the attention so that similar information can be clustered and unique information can be reserved. To this end, we adopt two routing mechanisms of Dynamic Routing and EM Routing, to fulfill the clustering and separating. We conducted experiments on Chinese-to-English and English-to-German translation tasks and got consistent improvements over the strong Transformer baseline.",
    "original_url": "http://arxiv.org/pdf/1909.00188v1",
    "original_title": "Improving Multi-Head Attention with Capsule Networks",
    "source": "arxiv",
    "authors": [
      "Shuhao Gu",
      "Yang Feng"
    ],
    "published": "2019-08-31T10:43:06+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1909.00188v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2403.10842v4_chunk_0",
    "chunk_text": "Twin Transformer using Gated Dynamic Learnable Attention mechanism for Fault Detection and Diagnosis in the Tennessee Eastman Process\n\nFault detection and diagnosis (FDD) is a crucial task for ensuring the safety and efficiency of industrial processes. We propose a novel FDD methodology for the Tennessee Eastman Process (TEP), a widely used benchmark for chemical process control. The model employs two separate Transformer branches, enabling independent processing of input data and potential extraction of diverse information. A novel attention mechanism, Gated Dynamic Learnable Attention (GDLAttention), is introduced which integrates a gating mechanism and dynamic learning capabilities. The gating mechanism modulates the attention weights, allowing the model to focus on the most relevant parts of the input.",
    "original_url": "http://arxiv.org/pdf/2403.10842v4",
    "original_title": "Twin Transformer using Gated Dynamic Learnable Attention mechanism for Fault Detection and Diagnosis in the Tennessee Eastman Process",
    "source": "arxiv",
    "authors": [
      "Mohammad Ali Labbaf-Khaniki",
      "Mohammad Manthouri",
      "Hanieh Ajami"
    ],
    "published": "2024-03-16T07:40:23+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2403.10842v4"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2403.10842v4_chunk_1",
    "chunk_text": "The gating mechanism modulates the attention weights, allowing the model to focus on the most relevant parts of the input. The dynamic learning approach adapts the attention strategy during training, potentially leading to improved performance. The attention mechanism uses a bilinear similarity function, providing greater flexibility in capturing complex relationships between query and key vectors. In order to assess the effectiveness of our approach, we tested it against 21 and 18 distinct fault scenarios in TEP, and compared its performance with several established FDD techniques. The outcomes indicate that the method outperforms others in terms of accuracy, false alarm rate, and misclassification rate.",
    "original_url": "http://arxiv.org/pdf/2403.10842v4",
    "original_title": "Twin Transformer using Gated Dynamic Learnable Attention mechanism for Fault Detection and Diagnosis in the Tennessee Eastman Process",
    "source": "arxiv",
    "authors": [
      "Mohammad Ali Labbaf-Khaniki",
      "Mohammad Manthouri",
      "Hanieh Ajami"
    ],
    "published": "2024-03-16T07:40:23+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2403.10842v4"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2403.10842v4_chunk_2",
    "chunk_text": "The outcomes indicate that the method outperforms others in terms of accuracy, false alarm rate, and misclassification rate. This underscores the robustness and efficacy of the approach for FDD in intricate industrial processes.",
    "original_url": "http://arxiv.org/pdf/2403.10842v4",
    "original_title": "Twin Transformer using Gated Dynamic Learnable Attention mechanism for Fault Detection and Diagnosis in the Tennessee Eastman Process",
    "source": "arxiv",
    "authors": [
      "Mohammad Ali Labbaf-Khaniki",
      "Mohammad Manthouri",
      "Hanieh Ajami"
    ],
    "published": "2024-03-16T07:40:23+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2403.10842v4"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2204.12406v1_chunk_0",
    "chunk_text": "A survey on attention mechanisms for medical applications: are we moving towards better algorithms? The increasing popularity of attention mechanisms in deep learning algorithms for computer vision and natural language processing made these models attractive to other research domains. In healthcare, there is a strong need for tools that may improve the routines of the clinicians and the patients. Naturally, the use of attention-based algorithms for medical applications occurred smoothly. However, being healthcare a domain that depends on high-stake decisions, the scientific community must ponder if these high-performing algorithms fit the needs of medical applications.",
    "original_url": "http://arxiv.org/pdf/2204.12406v1",
    "original_title": "A survey on attention mechanisms for medical applications: are we moving towards better algorithms?",
    "source": "arxiv",
    "authors": [
      "Tiago Gonçalves",
      "Isabel Rio-Torto",
      "Luís F. Teixeira",
      "Jaime S. Cardoso"
    ],
    "published": "2022-04-26T16:04:19+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2204.12406v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2204.12406v1_chunk_1",
    "chunk_text": "However, being healthcare a domain that depends on high-stake decisions, the scientific community must ponder if these high-performing algorithms fit the needs of medical applications. With this motto, this paper extensively reviews the use of attention mechanisms in machine learning (including Transformers) for several medical applications. This work distinguishes itself from its predecessors by proposing a critical analysis of the claims and potentialities of attention mechanisms presented in the literature through an experimental case study on medical image classification with three different use cases. These experiments focus on the integrating process of attention mechanisms into established deep learning architectures, the analysis of their predictive power, and a visual assessment of their saliency maps generated by post-hoc explanation methods. This paper concludes with a critical analysis of the claims and potentialities presented in the literature about attention mechanisms and proposes future research lines in medical applications that may benefit from these frameworks.",
    "original_url": "http://arxiv.org/pdf/2204.12406v1",
    "original_title": "A survey on attention mechanisms for medical applications: are we moving towards better algorithms?",
    "source": "arxiv",
    "authors": [
      "Tiago Gonçalves",
      "Isabel Rio-Torto",
      "Luís F. Teixeira",
      "Jaime S. Cardoso"
    ],
    "published": "2022-04-26T16:04:19+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2204.12406v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2204.12406v1_chunk_2",
    "chunk_text": "This paper concludes with a critical analysis of the claims and potentialities presented in the literature about attention mechanisms and proposes future research lines in medical applications that may benefit from these frameworks.",
    "original_url": "http://arxiv.org/pdf/2204.12406v1",
    "original_title": "A survey on attention mechanisms for medical applications: are we moving towards better algorithms?",
    "source": "arxiv",
    "authors": [
      "Tiago Gonçalves",
      "Isabel Rio-Torto",
      "Luís F. Teixeira",
      "Jaime S. Cardoso"
    ],
    "published": "2022-04-26T16:04:19+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2204.12406v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2005.00743v3_chunk_0",
    "chunk_text": "Synthesizer: Rethinking Self-Attention in Transformer Models\n\nThe dot product self-attention is known to be central and indispensable to state-of-the-art Transformer models. But is it really required? This paper investigates the true importance and contribution of the dot product-based self-attention mechanism on the performance of Transformer models. Via extensive experiments, we find that (1) random alignment matrices surprisingly perform quite competitively and (2) learning attention weights from token-token (query-key) interactions is useful but not that important after all. To this end, we propose \\textsc{Synthesizer}, a model that learns synthetic attention weights without token-token interactions.",
    "original_url": "http://arxiv.org/pdf/2005.00743v3",
    "original_title": "Synthesizer: Rethinking Self-Attention in Transformer Models",
    "source": "arxiv",
    "authors": [
      "Yi Tay",
      "Dara Bahri",
      "Donald Metzler",
      "Da-Cheng Juan",
      "Zhe Zhao",
      "Che Zheng"
    ],
    "published": "2020-05-02T08:16:19+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2005.00743v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2005.00743v3_chunk_1",
    "chunk_text": "To this end, we propose \\textsc{Synthesizer}, a model that learns synthetic attention weights without token-token interactions. In our experiments, we first show that simple Synthesizers achieve highly competitive performance when compared against vanilla Transformer models across a range of tasks, including machine translation, language modeling, text generation and GLUE/SuperGLUE benchmarks. When composed with dot product attention, we find that Synthesizers consistently outperform Transformers. Moreover, we conduct additional comparisons of Synthesizers against Dynamic Convolutions, showing that simple Random Synthesizer is not only $60\\%$ faster but also improves perplexity by a relative $3.5\\%$. Finally, we show that simple factorized Synthesizers can outperform Linformers on encoding only tasks.",
    "original_url": "http://arxiv.org/pdf/2005.00743v3",
    "original_title": "Synthesizer: Rethinking Self-Attention in Transformer Models",
    "source": "arxiv",
    "authors": [
      "Yi Tay",
      "Dara Bahri",
      "Donald Metzler",
      "Da-Cheng Juan",
      "Zhe Zhao",
      "Che Zheng"
    ],
    "published": "2020-05-02T08:16:19+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2005.00743v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2005.00743v3_chunk_2",
    "chunk_text": "Finally, we show that simple factorized Synthesizers can outperform Linformers on encoding only tasks.",
    "original_url": "http://arxiv.org/pdf/2005.00743v3",
    "original_title": "Synthesizer: Rethinking Self-Attention in Transformer Models",
    "source": "arxiv",
    "authors": [
      "Yi Tay",
      "Dara Bahri",
      "Donald Metzler",
      "Da-Cheng Juan",
      "Zhe Zhao",
      "Che Zheng"
    ],
    "published": "2020-05-02T08:16:19+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2005.00743v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2401.04956v2_chunk_0",
    "chunk_text": "EmMixformer: Mix transformer for eye movement recognition\n\nEye movement (EM) is a new highly secure biometric behavioral modality that has received increasing attention in recent years. Although deep neural networks, such as convolutional neural network (CNN), have recently achieved promising performance, current solutions fail to capture local and global temporal dependencies within eye movement data. To overcome this problem, we propose in this paper a mixed transformer termed EmMixformer to extract time and frequency domain information for eye movement recognition. To this end, we propose a mixed block consisting of three modules, transformer, attention Long short-term memory (attention LSTM), and Fourier transformer. We are the first to attempt leveraging transformer to learn long temporal dependencies within eye movement.",
    "original_url": "http://arxiv.org/pdf/2401.04956v2",
    "original_title": "EmMixformer: Mix transformer for eye movement recognition",
    "source": "arxiv",
    "authors": [
      "Huafeng Qin",
      "Hongyu Zhu",
      "Xin Jin",
      "Qun Song",
      "Mounim A. El-Yacoubi",
      "Xinbo Gao"
    ],
    "published": "2024-01-10T06:45:37+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2401.04956v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2401.04956v2_chunk_1",
    "chunk_text": "We are the first to attempt leveraging transformer to learn long temporal dependencies within eye movement. Second, we incorporate the attention mechanism into LSTM to propose attention LSTM with the aim to learn short temporal dependencies. Third, we perform self attention in the frequency domain to learn global features. As the three modules provide complementary feature representations in terms of local and global dependencies, the proposed EmMixformer is capable of improving recognition accuracy. The experimental results on our eye movement dataset and two public eye movement datasets show that the proposed EmMixformer outperforms the state of the art by achieving the lowest verification error.",
    "original_url": "http://arxiv.org/pdf/2401.04956v2",
    "original_title": "EmMixformer: Mix transformer for eye movement recognition",
    "source": "arxiv",
    "authors": [
      "Huafeng Qin",
      "Hongyu Zhu",
      "Xin Jin",
      "Qun Song",
      "Mounim A. El-Yacoubi",
      "Xinbo Gao"
    ],
    "published": "2024-01-10T06:45:37+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2401.04956v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2401.04956v2_chunk_2",
    "chunk_text": "The experimental results on our eye movement dataset and two public eye movement datasets show that the proposed EmMixformer outperforms the state of the art by achieving the lowest verification error.",
    "original_url": "http://arxiv.org/pdf/2401.04956v2",
    "original_title": "EmMixformer: Mix transformer for eye movement recognition",
    "source": "arxiv",
    "authors": [
      "Huafeng Qin",
      "Hongyu Zhu",
      "Xin Jin",
      "Qun Song",
      "Mounim A. El-Yacoubi",
      "Xinbo Gao"
    ],
    "published": "2024-01-10T06:45:37+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2401.04956v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2409.14090v6_chunk_0",
    "chunk_text": "Window-based Channel Attention for Wavelet-enhanced Learned Image Compression\n\nLearned Image Compression (LIC) models have achieved superior rate-distortion performance than traditional codecs. Existing LIC models use CNN, Transformer, or Mixed CNN-Transformer as basic blocks. However, limited by the shifted window attention, Swin-Transformer-based LIC exhibits a restricted growth of receptive fields, affecting the ability to model large objects for image compression. To address this issue and improve the performance, we incorporate window partition into channel attention for the first time to obtain large receptive fields and capture more global information. Since channel attention hinders local information learning, it is important to extend existing attention mechanisms in Transformer codecs to the space-channel attention to establish multiple receptive fields, being able to capture global correlations with large receptive fields while maintaining detailed characterization of local correlations with small receptive fields.",
    "original_url": "http://arxiv.org/pdf/2409.14090v6",
    "original_title": "Window-based Channel Attention for Wavelet-enhanced Learned Image Compression",
    "source": "arxiv",
    "authors": [
      "Heng Xu",
      "Bowen Hai",
      "Yushun Tang",
      "Zhihai He"
    ],
    "published": "2024-09-21T10:08:52+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2409.14090v6"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2409.14090v6_chunk_1",
    "chunk_text": "Since channel attention hinders local information learning, it is important to extend existing attention mechanisms in Transformer codecs to the space-channel attention to establish multiple receptive fields, being able to capture global correlations with large receptive fields while maintaining detailed characterization of local correlations with small receptive fields. We also incorporate the discrete wavelet transform into our Spatial-Channel Hybrid (SCH) framework for efficient frequency-dependent down-sampling and further enlarging receptive fields. Experiment results demonstrate that our method achieves state-of-the-art performances, reducing BD-rate by 18.54%, 23.98%, 22.33%, and 24.71% on four standard datasets compared to VTM-23.1.",
    "original_url": "http://arxiv.org/pdf/2409.14090v6",
    "original_title": "Window-based Channel Attention for Wavelet-enhanced Learned Image Compression",
    "source": "arxiv",
    "authors": [
      "Heng Xu",
      "Bowen Hai",
      "Yushun Tang",
      "Zhihai He"
    ],
    "published": "2024-09-21T10:08:52+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2409.14090v6"
  },
  {
    "chunk_id": "https://distill.pub/_chunk_0",
    "chunk_text": "Ameya Daigavane, Balaraman Ravindran, and Gaurav Aggarwal\nUnderstanding the building blocks and design choices of graph neural networks. Benjamin Sanchez-Lengeling, Emily Reif, Adam Pearce, and Alexander B. Wiltschko\nWhat components are needed for building learning algorithms that leverage the structure and properties of graphs? Editorial Team\nAfter five years, Distill will be taking a break. Gabriel Goh, Nick Cammarata †, Chelsea Voss †, Shan Carter, Michael Petrov, Ludwig Schubert, Alec Radford, and Chris Olah\nWe report the existence of multimodal neurons in artificial neural networks, similar to those found in the human brain. Jacob Hilton, Nick Cammarata, Shan Carter, Gabriel Goh, and Chris Olah\nWith diverse environments, we can analyze, diagnose and edit deep reinforcement learning models using attribution.",
    "original_url": "https://distill.pub/",
    "original_title": "Distill — Latest articles about machine learning",
    "source": "html",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://distill.pub/_chunk_1",
    "chunk_text": "Jacob Hilton, Nick Cammarata, Shan Carter, Gabriel Goh, and Chris Olah\nWith diverse environments, we can analyze, diagnose and edit deep reinforcement learning models using attribution. Fred Hohman, Matthew Conlen, Jeffrey Heer, and Duen Horng (Polo) Chau\nExamining the design of interactive articles by synthesizing theory from disciplines such as education, journalism, and visualization. Alexander Mordvintsev, Ettore Randazzo, Eyvind Niklasson, Michael Levin, and Sam Greydanus\nA collection of articles and comments with the goal of understanding how to design robust and general purpose self-organizing systems. Apoorv Agnihotri and Nipun Batra\nHow to tune hyperparameters for your machine learning model using Bayesian optimization. Mingwei Li, Zhenge Zhao, and Carlos Scheidegger\nBy focusing on linear dimensionality reduction, we show how to visualize many dynamic phenomena in neural networks.",
    "original_url": "https://distill.pub/",
    "original_title": "Distill — Latest articles about machine learning",
    "source": "html",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://distill.pub/_chunk_2",
    "chunk_text": "Mingwei Li, Zhenge Zhao, and Carlos Scheidegger\nBy focusing on linear dimensionality reduction, we show how to visualize many dynamic phenomena in neural networks. Nick Cammarata, Shan Carter, Gabriel Goh, Chris Olah, Michael Petrov, Ludwig Schubert, Chelsea Voss, Ben Egan, and Swee Kiat Lim\nWhat can we learn if we invest heavily in reverse engineering a single neural network? Pascal Sturmfels, Scott Lundberg, and Su-In Lee\nExploring the baseline input hyperparameter, and how it impacts interpretations of neural network behavior. André Araujo, Wade Norris, and Jack Sim\nDetailed derivations and open-source code to analyze the receptive fields of convnets. Sam Greydanus and Chris Olah\nA closer look at how Temporal Difference Learning merges paths of experience for greater statistical efficiency\nLogan Engstrom, Justin Gilmer, Gabriel Goh, Dan Hendrycks, Andrew Ilyas, Aleksander Madry, Reiichiro Nakano, Preetum Nakkiran, Shibani Santurkar, Brandon Tran, Dimitris Tsipras, and Eric Wallace\nSix comments from the community and responses from the original authors\nAugustus Odena\nWhat we’d like to find out about GANs that we don’t know yet.",
    "original_url": "https://distill.pub/",
    "original_title": "Distill — Latest articles about machine learning",
    "source": "html",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://distill.pub/_chunk_3",
    "chunk_text": "Sam Greydanus and Chris Olah\nA closer look at how Temporal Difference Learning merges paths of experience for greater statistical efficiency\nLogan Engstrom, Justin Gilmer, Gabriel Goh, Dan Hendrycks, Andrew Ilyas, Aleksander Madry, Reiichiro Nakano, Preetum Nakkiran, Shibani Santurkar, Brandon Tran, Dimitris Tsipras, and Eric Wallace\nSix comments from the community and responses from the original authors\nAugustus Odena\nWhat we’d like to find out about GANs that we don’t know yet. Jochen Görtler, Rebecca Kehlbeck, and Oliver Deussen\nHow to turn a collection of small building blocks into a versatile tool for solving regression problems. Andreas Madsen\nInspecting gradient magnitudes in context can be a powerful tool to see when recurrent units use short-term or long-term contextual understanding. Shan Carter, Zan Armstrong, Ludwig Schubert, Ian Johnson, and Chris Olah\nBy using feature inversion to visualize millions of activations from an image classification network, we create an explorable activation atlas of features the network has learned and what concepts it typically represents. Geoffrey Irving and Amanda Askell\nIf we want to train AI to do what humans want, we need to study humans.",
    "original_url": "https://distill.pub/",
    "original_title": "Distill — Latest articles about machine learning",
    "source": "html",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://distill.pub/_chunk_4",
    "chunk_text": "Geoffrey Irving and Amanda Askell\nIf we want to train AI to do what humans want, we need to study humans. Distill Editors\nAn Update from the Editorial Team\nAlexander Mordvintsev, Nicola Pezzotti, Ludwig Schubert, and Chris Olah\nA powerful, under-explored tool for neural network visualizations and art. Vincent Dumoulin, Ethan Perez, Nathan Schucher, Florian Strub, Harm de Vries, Aaron Courville, and Yoshua Bengio\nA simple and surprisingly effective family of conditioning mechanisms. Chris Olah, Arvind Satyanarayan, Ian Johnson, Shan Carter, Ludwig Schubert, Katherine Ye, and Alexander Mordvintsev\nInterpretability techniques are normally studied in isolation. We explore the powerful interfaces that arise when you combine them — and the rich structure of this combinatorial space.",
    "original_url": "https://distill.pub/",
    "original_title": "Distill — Latest articles about machine learning",
    "source": "html",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://distill.pub/_chunk_5",
    "chunk_text": "We explore the powerful interfaces that arise when you combine them — and the rich structure of this combinatorial space. Shan Carter and Michael Nielsen\nBy creating user interfaces which let us work with the representations inside machine learning models, we can give people new tools for reasoning. Awni Hannun\nA visual guide to Connectionist Temporal Classification, an algorithm used to train deep neural networks in speech recognition, handwriting recognition and other sequence problems. Chris Olah, Alexander Mordvintsev, and Ludwig Schubert\nHow neural networks build up their understanding of images\nGabriel Goh\nWe often think of optimization with momentum as a ball rolling down a hill. This isn’t wrong, but there is much more to the story.",
    "original_url": "https://distill.pub/",
    "original_title": "Distill — Latest articles about machine learning",
    "source": "html",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://distill.pub/_chunk_6",
    "chunk_text": "This isn’t wrong, but there is much more to the story. Chris Olah and Shan Carter\nScience is a human activity. When we fail to distill and explain research, we accumulate a kind of debt...\nShan Carter, David Ha, Ian Johnson, and Chris Olah\nSeveral interactive visualizations of a generative model of handwriting. Some are fun, some are serious. Augustus Odena, Vincent Dumoulin, and Chris Olah\nWhen we look very closely at images generated by neural networks, we often see a strange checkerboard pattern of artifacts.",
    "original_url": "https://distill.pub/",
    "original_title": "Distill — Latest articles about machine learning",
    "source": "html",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://distill.pub/_chunk_7",
    "chunk_text": "Augustus Odena, Vincent Dumoulin, and Chris Olah\nWhen we look very closely at images generated by neural networks, we often see a strange checkerboard pattern of artifacts. Martin Wattenberg, Fernanda Viégas, and Ian Johnson\nAlthough extremely useful for visualizing high-dimensional data, t-SNE plots can sometimes be mysterious or misleading. A visual overview of neural attention, and the powerful extensions of neural networks being built on top of it.",
    "original_url": "https://distill.pub/",
    "original_title": "Distill — Latest articles about machine learning",
    "source": "html",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  }
]