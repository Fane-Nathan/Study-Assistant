[
  {
    "chunk_id": "http://arxiv.org/pdf/1406.5306v1_chunk_0",
    "chunk_text": "Unraveling simplicity in elementary cellular automata. We show that a large number of elementary cellular automata are computationally simple. This work is the first systematic classification of elementary cellular automata based on a formal notion of computational complexity. Thanks to the generality of communication complexity, the perspectives of our method include its application to other natural systems such as neural networks and gene regulatory networks.",
    "original_url": "http://arxiv.org/pdf/1406.5306v1",
    "original_title": "Unraveling simplicity in elementary cellular automata",
    "source": "arxiv",
    "authors": [
      "Pierre-Étienne Meunier"
    ],
    "published": "2014-06-20T08:10:02+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1406.5306v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/cond-mat/0410563v2_chunk_0",
    "chunk_text": "Eventually Number-Conserving Cellular Automata. We present a preliminary study of a new class of two-input cellular automata called eventually number-conserving cellular automata characterized by the property of evolving after a finite number of time steps to states whose number of active sites remains constant. Eventually number-conserving cellular automata are models of open systems of interacting particles, that is, system of particles interacting with the external world, The particle aspect of eventually number-conserving cellular automata can be emphasized by the motion representation of the cellular automaton evolution rule. This new class of cellular automata contains, as strict subclasses, number-conserving cellular automata, monotone cellular automata, and cellular automata emulating number-conserving ones. Our main objective is to show that they are not what one might naively think they are.",
    "original_url": "http://arxiv.org/pdf/cond-mat/0410563v2",
    "original_title": "Eventually Number-Conserving Cellular Automata",
    "source": "arxiv",
    "authors": [
      "Nino Boccara"
    ],
    "published": "2004-10-21T21:36:24+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/cond-mat/0410563v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/cond-mat/0410563v2_chunk_1",
    "chunk_text": "Our main objective is to show that they are not what one might naively think they are.",
    "original_url": "http://arxiv.org/pdf/cond-mat/0410563v2",
    "original_title": "Eventually Number-Conserving Cellular Automata",
    "source": "arxiv",
    "authors": [
      "Nino Boccara"
    ],
    "published": "2004-10-21T21:36:24+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/cond-mat/0410563v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2410.02651v2_chunk_0",
    "chunk_text": "CAX: Cellular Automata Accelerated in JAX. Cellular automata have become a cornerstone for investigating emergence and self-organization across diverse scientific disciplines. However, the absence of a hardware-accelerated cellular automata library limits the exploration of new research directions, hinders collaboration, and impedes reproducibility. In this work, we introduce CAX (Cellular Automata Accelerated in JAX), a high-performance and flexible open-source library designed to accelerate cellular automata research. CAX delivers cutting-edge performance through hardware acceleration while maintaining flexibility through its modular architecture, intuitive API, and support for both discrete and continuous cellular automata in arbitrary dimensions.",
    "original_url": "http://arxiv.org/pdf/2410.02651v2",
    "original_title": "CAX: Cellular Automata Accelerated in JAX",
    "source": "arxiv",
    "authors": [
      "Maxence Faldor",
      "Antoine Cully"
    ],
    "published": "2024-10-03T16:36:05+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2410.02651v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2410.02651v2_chunk_1",
    "chunk_text": "CAX delivers cutting-edge performance through hardware acceleration while maintaining flexibility through its modular architecture, intuitive API, and support for both discrete and continuous cellular automata in arbitrary dimensions. We demonstrate CAX's performance and flexibility through a wide range of benchmarks and applications. From classic models like elementary cellular automata and Conway's Game of Life to advanced applications such as growing neural cellular automata and self-classifying MNIST digits, CAX speeds up simulations up to 2,000 times faster. Furthermore, we demonstrate CAX's potential to accelerate research by presenting a collection of three novel cellular automata experiments, each implemented in just a few lines of code thanks to the library's modular architecture. Notably, we show that a simple one-dimensional cellular automaton can outperform GPT-4 on the 1D-ARC challenge.",
    "original_url": "http://arxiv.org/pdf/2410.02651v2",
    "original_title": "CAX: Cellular Automata Accelerated in JAX",
    "source": "arxiv",
    "authors": [
      "Maxence Faldor",
      "Antoine Cully"
    ],
    "published": "2024-10-03T16:36:05+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2410.02651v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2410.02651v2_chunk_2",
    "chunk_text": "Notably, we show that a simple one-dimensional cellular automaton can outperform GPT-4 on the 1D-ARC challenge.",
    "original_url": "http://arxiv.org/pdf/2410.02651v2",
    "original_title": "CAX: Cellular Automata Accelerated in JAX",
    "source": "arxiv",
    "authors": [
      "Maxence Faldor",
      "Antoine Cully"
    ],
    "published": "2024-10-03T16:36:05+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2410.02651v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2501.04761v1_chunk_0",
    "chunk_text": "Evolution of Spots and Stripes in Cellular Automata. Cellular automata are computers, similar to Turing machines. The main difference is that Turing machines use a one-dimensional tape, whereas cellular automata use a two-dimensional grid. The best-known cellular automaton is the Game of Life, which is a universal computer. It belongs to a family of cellular automata with 262,144 members.",
    "original_url": "http://arxiv.org/pdf/2501.04761v1",
    "original_title": "Evolution of Spots and Stripes in Cellular Automata",
    "source": "arxiv",
    "authors": [
      "Peter Turney"
    ],
    "published": "2025-01-08T17:46:38+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2501.04761v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2501.04761v1_chunk_1",
    "chunk_text": "It belongs to a family of cellular automata with 262,144 members. Playing the Game of Life generally involves engineering; that is, assembling a device composed of various parts that are combined to achieve a specific intended result. Instead of engineering cellular automata, we propose evolving cellular automata. Evolution applies mutation and selection to a population of organisms. If a mutation increases the fitness of an organism, it may have many descendants, displacing the less fit organisms.",
    "original_url": "http://arxiv.org/pdf/2501.04761v1",
    "original_title": "Evolution of Spots and Stripes in Cellular Automata",
    "source": "arxiv",
    "authors": [
      "Peter Turney"
    ],
    "published": "2025-01-08T17:46:38+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2501.04761v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2501.04761v1_chunk_2",
    "chunk_text": "If a mutation increases the fitness of an organism, it may have many descendants, displacing the less fit organisms. Unlike engineering, evolution does not work towards an imagined goal. Evolution works towards increasing fitness, with no expectations about the specific form of the final result. Mutation, selection, and fitness yield structures that appear to be more organic and life-like than engineered structures. In our experiments, the patterns resulting from evolving cellular automata look much like the spots on leopards and the stripes on tigers.",
    "original_url": "http://arxiv.org/pdf/2501.04761v1",
    "original_title": "Evolution of Spots and Stripes in Cellular Automata",
    "source": "arxiv",
    "authors": [
      "Peter Turney"
    ],
    "published": "2025-01-08T17:46:38+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2501.04761v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2501.04761v1_chunk_3",
    "chunk_text": "In our experiments, the patterns resulting from evolving cellular automata look much like the spots on leopards and the stripes on tigers.",
    "original_url": "http://arxiv.org/pdf/2501.04761v1",
    "original_title": "Evolution of Spots and Stripes in Cellular Automata",
    "source": "arxiv",
    "authors": [
      "Peter Turney"
    ],
    "published": "2025-01-08T17:46:38+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2501.04761v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2403.15525v1_chunk_0",
    "chunk_text": "Latent Neural Cellular Automata for Resource-Efficient Image Restoration. Neural cellular automata represent an evolution of the traditional cellular automata model, enhanced by the integration of a deep learning-based transition function. This shift from a manual to a data-driven approach significantly increases the adaptability of these models, enabling their application in diverse domains, including content generation and artificial life. However, their widespread application has been hampered by significant computational requirements. In this work, we introduce the Latent Neural Cellular Automata (LNCA) model, a novel architecture designed to address the resource limitations of neural cellular automata.",
    "original_url": "http://arxiv.org/pdf/2403.15525v1",
    "original_title": "Latent Neural Cellular Automata for Resource-Efficient Image Restoration",
    "source": "arxiv",
    "authors": [
      "Andrea Menta",
      "Alberto Archetti",
      "Matteo Matteucci"
    ],
    "published": "2024-03-22T14:15:28+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2403.15525v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2403.15525v1_chunk_1",
    "chunk_text": "In this work, we introduce the Latent Neural Cellular Automata (LNCA) model, a novel architecture designed to address the resource limitations of neural cellular automata. Our approach shifts the computation from the conventional input space to a specially designed latent space, relying on a pre-trained autoencoder. We apply our model in the context of image restoration, which aims to reconstruct high-quality images from their degraded versions. This modification not only reduces the model's resource consumption but also maintains a flexible framework suitable for various applications. Our model achieves a significant reduction in computational requirements while maintaining high reconstruction fidelity.",
    "original_url": "http://arxiv.org/pdf/2403.15525v1",
    "original_title": "Latent Neural Cellular Automata for Resource-Efficient Image Restoration",
    "source": "arxiv",
    "authors": [
      "Andrea Menta",
      "Alberto Archetti",
      "Matteo Matteucci"
    ],
    "published": "2024-03-22T14:15:28+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2403.15525v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2403.15525v1_chunk_2",
    "chunk_text": "Our model achieves a significant reduction in computational requirements while maintaining high reconstruction fidelity. This increase in efficiency allows for inputs up to 16 times larger than current state-of-the-art neural cellular automata models, using the same resources.",
    "original_url": "http://arxiv.org/pdf/2403.15525v1",
    "original_title": "Latent Neural Cellular Automata for Resource-Efficient Image Restoration",
    "source": "arxiv",
    "authors": [
      "Andrea Menta",
      "Alberto Archetti",
      "Matteo Matteucci"
    ],
    "published": "2024-03-22T14:15:28+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2403.15525v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/cond-mat/0407622v1_chunk_0",
    "chunk_text": "Cellular automata with majority rule on evolving network. The cellular automata discrete dynamical system is considered as the two-stage process: the majority rule for the change in the automata state and the rule for the change in topological relations between automata. The influence of changing topology to the cooperative phenomena, namely zero-temperature ferromagnetic phase transition, is observed.",
    "original_url": "http://arxiv.org/pdf/cond-mat/0407622v1",
    "original_title": "Cellular automata with majority rule on evolving network",
    "source": "arxiv",
    "authors": [
      "Danuta Makowiec"
    ],
    "published": "2004-07-23T09:38:48+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/cond-mat/0407622v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2501.03573v1_chunk_0",
    "chunk_text": "Neural Cellular Automata and Deep Equilibrium Models. This essay discusses the connections and differences between two emerging paradigms in deep learning, namely Neural Cellular Automata and Deep Equilibrium Models, and train a simple Deep Equilibrium Convolutional model to demonstrate the inherent similarity of NCA and DEQ based methods. Finally, this essay speculates about ways to combine theoretical and practical aspects of both approaches for future research.",
    "original_url": "http://arxiv.org/pdf/2501.03573v1",
    "original_title": "Neural Cellular Automata and Deep Equilibrium Models",
    "source": "arxiv",
    "authors": [
      "Zhibai Jia"
    ],
    "published": "2025-01-07T06:54:10+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2501.03573v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2305.13043v1_chunk_0",
    "chunk_text": "Self-Replication, Spontaneous Mutations, and Exponential Genetic Drift in Neural Cellular Automata. This paper reports on patterns exhibiting self-replication with spontaneous, inheritable mutations and exponential genetic drift in Neural Cellular Automata. Despite the models not being explicitly trained for mutation or inheritability, the descendant patterns exponentially drift away from ancestral patterns, even when the automaton is deterministic. While this is far from being the first instance of evolutionary dynamics in a cellular automaton, it is the first to do so by exploiting the power and convenience of Neural Cellular Automata, arguably increasing the space of variations and the opportunity for Open Ended Evolution.",
    "original_url": "http://arxiv.org/pdf/2305.13043v1",
    "original_title": "Self-Replication, Spontaneous Mutations, and Exponential Genetic Drift in Neural Cellular Automata",
    "source": "arxiv",
    "authors": [
      "Lana Sinapayen"
    ],
    "published": "2023-05-22T13:48:46+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2305.13043v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2108.00415v1_chunk_0",
    "chunk_text": "Computational Hierarchy of Elementary Cellular Automata. The complexity of cellular automata is traditionally measured by their computational capacity. However, it is difficult to choose a challenging set of computational tasks suitable for the parallel nature of such systems. We study the ability of automata to emulate one another, and we use this notion to define such a set of naturally emerging tasks. We present the results for elementary cellular automata, although the core ideas can be extended to other computational systems.",
    "original_url": "http://arxiv.org/pdf/2108.00415v1",
    "original_title": "Computational Hierarchy of Elementary Cellular Automata",
    "source": "arxiv",
    "authors": [
      "Barbora Hudcová",
      "Tomáš Mikolov"
    ],
    "published": "2021-08-01T10:00:54+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2108.00415v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2108.00415v1_chunk_1",
    "chunk_text": "We present the results for elementary cellular automata, although the core ideas can be extended to other computational systems. We compute a graph showing which elementary cellular automata can be emulated by which and show that certain chaotic automata are the only ones that cannot emulate any automata non-trivially. Finally, we use the emulation notion to suggest a novel definition of chaos that we believe is suitable for discrete computational systems. We believe our work can help design parallel computational systems that are Turing-complete and also computationally efficient.",
    "original_url": "http://arxiv.org/pdf/2108.00415v1",
    "original_title": "Computational Hierarchy of Elementary Cellular Automata",
    "source": "arxiv",
    "authors": [
      "Barbora Hudcová",
      "Tomáš Mikolov"
    ],
    "published": "2021-08-01T10:00:54+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2108.00415v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2309.00241v1_chunk_0",
    "chunk_text": "Spiking based Cellular Learning Automata (SCLA) algorithm for mobile robot motion formulation. In this paper a new method called SCLA which stands for Spiking based Cellular Learning Automata is proposed for a mobile robot to get to the target from any random initial point. The proposed method is a result of the integration of both cellular automata and spiking neural networks. The environment consists of multiple squares of the same size and the robot only observes the neighboring squares of its current square. It should be stated that the robot only moves either up and down or right and left.",
    "original_url": "http://arxiv.org/pdf/2309.00241v1",
    "original_title": "Spiking based Cellular Learning Automata (SCLA) algorithm for mobile robot motion formulation",
    "source": "arxiv",
    "authors": [
      "Vahid Pashaei Rad",
      "Vahid Azimi Rad",
      "Saleh Valizadeh Sotubadi"
    ],
    "published": "2023-09-01T04:16:23+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2309.00241v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2309.00241v1_chunk_1",
    "chunk_text": "It should be stated that the robot only moves either up and down or right and left. The environment returns feedback to the learning automata to optimize its decision making in the next steps resulting in cellular automata training. Simultaneously a spiking neural network is trained to implement long term improvements and reductions on the paths. The results show that the integration of both cellular automata and spiking neural network ends up in reinforcing the proper paths and training time reduction at the same time.",
    "original_url": "http://arxiv.org/pdf/2309.00241v1",
    "original_title": "Spiking based Cellular Learning Automata (SCLA) algorithm for mobile robot motion formulation",
    "source": "arxiv",
    "authors": [
      "Vahid Pashaei Rad",
      "Vahid Azimi Rad",
      "Saleh Valizadeh Sotubadi"
    ],
    "published": "2023-09-01T04:16:23+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2309.00241v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2304.02354v2_chunk_0",
    "chunk_text": "Neural Cellular Automata for Solidification Microstructure Modelling. We propose Neural Cellular Automata (NCA) to simulate the microstructure development during the solidification process in metals. Based on convolutional neural networks, NCA can learn essential solidification features, such as preferred growth direction and competitive grain growth, and are up to six orders of magnitude faster than the conventional Cellular Automata (CA). Notably, NCA delivers reliable predictions also outside their training range, which indicates that they learn the physics of the solidification process. While in this study we employ data produced by CA for training, NCA can be trained based on any microstructural simulation data, e.g.",
    "original_url": "http://arxiv.org/pdf/2304.02354v2",
    "original_title": "Neural Cellular Automata for Solidification Microstructure Modelling",
    "source": "arxiv",
    "authors": [
      "Jian Tang",
      "Siddhant Kumar",
      "Laura De Lorenzis",
      "Ehsan Hosseini"
    ],
    "published": "2023-04-05T10:42:28+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2304.02354v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2304.02354v2_chunk_1",
    "chunk_text": "While in this study we employ data produced by CA for training, NCA can be trained based on any microstructural simulation data, e.g. from phase-field models.",
    "original_url": "http://arxiv.org/pdf/2304.02354v2",
    "original_title": "Neural Cellular Automata for Solidification Microstructure Modelling",
    "source": "arxiv",
    "authors": [
      "Jian Tang",
      "Siddhant Kumar",
      "Laura De Lorenzis",
      "Ehsan Hosseini"
    ],
    "published": "2023-04-05T10:42:28+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2304.02354v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/math/9801152v1_chunk_0",
    "chunk_text": "On the classifiability of cellular automata. Based on computer simulations Wolfram presented in several papers conjectured classifications of cellular automata into 4 types. He distinguishes the 4 classes of cellular automata by the evolution of the pattern generated by applying a cellular automaton to a finite input. Wolfram's qualitative classification is based on the examination of a large number of simulations. In addition to this classification based on the rate of growth, he conjectured a similar classification according to the eventual pattern.",
    "original_url": "http://arxiv.org/pdf/math/9801152v1",
    "original_title": "On the classifiability of cellular automata",
    "source": "arxiv",
    "authors": [
      "John T. Baldwin",
      "Saharon Shelah"
    ],
    "published": "1998-01-15T00:00:00+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/math/9801152v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/math/9801152v1_chunk_1",
    "chunk_text": "In addition to this classification based on the rate of growth, he conjectured a similar classification according to the eventual pattern. We consider here one formalization of his rate of growth suggestion. After completing our major results (based only on Wolfram's work), we investigated other contributions to the area and we report the relation of some of them to our discoveries.",
    "original_url": "http://arxiv.org/pdf/math/9801152v1",
    "original_title": "On the classifiability of cellular automata",
    "source": "arxiv",
    "authors": [
      "John T. Baldwin",
      "Saharon Shelah"
    ],
    "published": "1998-01-15T00:00:00+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/math/9801152v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1105.5335v1_chunk_0",
    "chunk_text": "Cellular automata on a $G$-set. We extend the usual definition of cellular automaton on a group in order to deal with a new kind of cellular automata, like cellular automata in the hyperbolic plane and we explore some properties of these cellular automata. This definition also allows to deal with maps, intuitively considered as cellular automata, even if they did not match the usual definition, like the Margolus billiard-ball. One of the main results is an extension of Hedlund's theorem for these cellular automata.",
    "original_url": "http://arxiv.org/pdf/1105.5335v1",
    "original_title": "Cellular automata on a $G$-set",
    "source": "arxiv",
    "authors": [
      "Sébastien Moriceau"
    ],
    "published": "2011-05-26T16:14:21+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1105.5335v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2106.15240v2_chunk_0",
    "chunk_text": "Towards self-organized control: Using neural cellular automata to robustly control a cart-pole agent. Neural cellular automata (Neural CA) are a recent framework used to model biological phenomena emerging from multicellular organisms. In these systems, artificial neural networks are used as update rules for cellular automata. Neural CA are end-to-end differentiable systems where the parameters of the neural network can be learned to achieve a particular task. In this work, we used neural CA to control a cart-pole agent.",
    "original_url": "http://arxiv.org/pdf/2106.15240v2",
    "original_title": "Towards self-organized control: Using neural cellular automata to robustly control a cart-pole agent",
    "source": "arxiv",
    "authors": [
      "Alexandre Variengien",
      "Stefano Nichele",
      "Tom Glover",
      "Sidney Pontes-Filho"
    ],
    "published": "2021-06-29T10:49:42+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2106.15240v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2106.15240v2_chunk_1",
    "chunk_text": "In this work, we used neural CA to control a cart-pole agent. The observations of the environment are transmitted in input cells, while the values of output cells are used as a readout of the system. We trained the model using deep-Q learning, where the states of the output cells were used as the Q-value estimates to be optimized. We found that the computing abilities of the cellular automata were maintained over several hundreds of thousands of iterations, producing an emergent stable behavior in the environment it controls for thousands of steps. Moreover, the system demonstrated life-like phenomena such as a developmental phase, regeneration after damage, stability despite a noisy environment, and robustness to unseen disruption such as input deletion.",
    "original_url": "http://arxiv.org/pdf/2106.15240v2",
    "original_title": "Towards self-organized control: Using neural cellular automata to robustly control a cart-pole agent",
    "source": "arxiv",
    "authors": [
      "Alexandre Variengien",
      "Stefano Nichele",
      "Tom Glover",
      "Sidney Pontes-Filho"
    ],
    "published": "2021-06-29T10:49:42+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2106.15240v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2106.15240v2_chunk_2",
    "chunk_text": "Moreover, the system demonstrated life-like phenomena such as a developmental phase, regeneration after damage, stability despite a noisy environment, and robustness to unseen disruption such as input deletion.",
    "original_url": "http://arxiv.org/pdf/2106.15240v2",
    "original_title": "Towards self-organized control: Using neural cellular automata to robustly control a cart-pole agent",
    "source": "arxiv",
    "authors": [
      "Alexandre Variengien",
      "Stefano Nichele",
      "Tom Glover",
      "Sidney Pontes-Filho"
    ],
    "published": "2021-06-29T10:49:42+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2106.15240v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2010.04949v2_chunk_0",
    "chunk_text": "Image Generation With Neural Cellular Automatas. In this paper, we propose a novel approach to generate images (or other artworks) by using neural cellular automatas (NCAs). Rather than training NCAs based on single images one by one, we combined the idea with variational autoencoders (VAEs), and hence explored some applications, such as image restoration and style fusion. The code for model implementation is available online.",
    "original_url": "http://arxiv.org/pdf/2010.04949v2",
    "original_title": "Image Generation With Neural Cellular Automatas",
    "source": "arxiv",
    "authors": [
      "Mingxiang Chen",
      "Zhecheng Wang"
    ],
    "published": "2020-10-10T08:52:52+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2010.04949v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1003.1983v1_chunk_0",
    "chunk_text": "Cellular Automata, PDEs, and Pattern Formation. State-of-the-art review of cellular automata, cellular automata for partial differential equations, differential equations for cellular automata and pattern formation in biology and engineering.",
    "original_url": "http://arxiv.org/pdf/1003.1983v1",
    "original_title": "Cellular Automata, PDEs, and Pattern Formation",
    "source": "arxiv",
    "authors": [
      "Xin-She Yang",
      "Y. Young"
    ],
    "published": "2010-03-09T21:20:56+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1003.1983v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2306.07067v1_chunk_0",
    "chunk_text": "Locally adaptive cellular automata for goal-oriented self-organization. The essential ingredient for studying the phenomena of emergence is the ability to generate and manipulate emergent systems that span large scales. Cellular automata are the model class particularly known for their effective scalability but are also typically constrained by fixed local rules. In this paper, we propose a new model class of adaptive cellular automata that allows for the generation of scalable and expressive models. We show how to implement computation-effective adaptation by coupling the update rule of the cellular automaton with itself and the system state in a localized way.",
    "original_url": "http://arxiv.org/pdf/2306.07067v1",
    "original_title": "Locally adaptive cellular automata for goal-oriented self-organization",
    "source": "arxiv",
    "authors": [
      "Sina Khajehabdollahi",
      "Emmanouil Giannakakis",
      "Victor Buendia",
      "Georg Martius",
      "Anna Levina"
    ],
    "published": "2023-06-12T12:32:23+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2306.07067v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2306.07067v1_chunk_1",
    "chunk_text": "We show how to implement computation-effective adaptation by coupling the update rule of the cellular automaton with itself and the system state in a localized way. To demonstrate the applications of this approach, we implement two different emergent models: a self-organizing Ising model and two types of plastic neural networks, a rate and spiking model. With the Ising model, we show how coupling local/global temperatures to local/global measurements can tune the model to stay in the vicinity of the critical temperature. With the neural models, we reproduce a classical balanced state in large recurrent neuronal networks with excitatory and inhibitory neurons and various plasticity mechanisms. Our study opens multiple directions for studying collective behavior and emergence.",
    "original_url": "http://arxiv.org/pdf/2306.07067v1",
    "original_title": "Locally adaptive cellular automata for goal-oriented self-organization",
    "source": "arxiv",
    "authors": [
      "Sina Khajehabdollahi",
      "Emmanouil Giannakakis",
      "Victor Buendia",
      "Georg Martius",
      "Anna Levina"
    ],
    "published": "2023-06-12T12:32:23+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2306.07067v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2306.07067v1_chunk_2",
    "chunk_text": "Our study opens multiple directions for studying collective behavior and emergence.",
    "original_url": "http://arxiv.org/pdf/2306.07067v1",
    "original_title": "Locally adaptive cellular automata for goal-oriented self-organization",
    "source": "arxiv",
    "authors": [
      "Sina Khajehabdollahi",
      "Emmanouil Giannakakis",
      "Victor Buendia",
      "Georg Martius",
      "Anna Levina"
    ],
    "published": "2023-06-12T12:32:23+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2306.07067v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2103.02076v1_chunk_0",
    "chunk_text": "A continuous-state cellular automata algorithm for global optimization. Cellular automata are capable of developing complex behaviors based on simple local interactions between their elements. Some of these characteristics have been used to propose and improve meta-heuristics for global optimization; however, the properties offered by the evolution rules in cellular automata have not yet been used directly in optimization tasks. Inspired by the complexity that various evolution rules of cellular automata can offer, the continuous-state cellular automata algorithm (CCAA) is proposed. In this way, the CCAA takes advantage of different evolution rules to maintain a balance that maximizes the exploration and exploitation properties in each iteration.",
    "original_url": "http://arxiv.org/pdf/2103.02076v1",
    "original_title": "A continuous-state cellular automata algorithm for global optimization",
    "source": "arxiv",
    "authors": [
      "Juan Carlos Seck-Tuoh-Mora",
      "Norberto Hernandez-Romero",
      "Pedro Lagos-Eulogio",
      "Joselito Medina-Marin",
      "Nadia Samantha Zuñiga-Peña"
    ],
    "published": "2021-03-02T23:01:58+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2103.02076v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2103.02076v1_chunk_1",
    "chunk_text": "In this way, the CCAA takes advantage of different evolution rules to maintain a balance that maximizes the exploration and exploitation properties in each iteration. The efficiency of the CCAA is proven with 33 test problems widely used in the literature, 4 engineering applications that were also used in recent literature, and the design of adaptive infinite-impulse response (IIR) filters, testing 10 full-order IIR reference functions. The numerical results prove its competitiveness in comparison with state-of-the-art algorithms. The source codes of the CCAA are publicly available at https://github.com/juanseck/CCAA.git",
    "original_url": "http://arxiv.org/pdf/2103.02076v1",
    "original_title": "A continuous-state cellular automata algorithm for global optimization",
    "source": "arxiv",
    "authors": [
      "Juan Carlos Seck-Tuoh-Mora",
      "Norberto Hernandez-Romero",
      "Pedro Lagos-Eulogio",
      "Joselito Medina-Marin",
      "Nadia Samantha Zuñiga-Peña"
    ],
    "published": "2021-03-02T23:01:58+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2103.02076v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2108.04328v1_chunk_0",
    "chunk_text": "Generative Adversarial Neural Cellular Automata. Motivated by the interaction between cells, the recently introduced concept of Neural Cellular Automata shows promising results in a variety of tasks. So far, this concept was mostly used to generate images for a single scenario. As each scenario requires a new model, this type of generation seems contradictory to the adaptability of cells in nature. To address this contradiction, we introduce a concept using different initial environments as input while using a single Neural Cellular Automata to produce several outputs.",
    "original_url": "http://arxiv.org/pdf/2108.04328v1",
    "original_title": "Generative Adversarial Neural Cellular Automata",
    "source": "arxiv",
    "authors": [
      "Maximilian Otte",
      "Quentin Delfosse",
      "Johannes Czech",
      "Kristian Kersting"
    ],
    "published": "2021-07-19T06:23:11+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2108.04328v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2108.04328v1_chunk_1",
    "chunk_text": "To address this contradiction, we introduce a concept using different initial environments as input while using a single Neural Cellular Automata to produce several outputs. Additionally, we introduce GANCA, a novel algorithm that combines Neural Cellular Automata with Generative Adversarial Networks, allowing for more generalization through adversarial training. The experiments show that a single model is capable of learning several images when presented with different inputs, and that the adversarially trained model improves drastically on out-of-distribution data compared to a supervised trained model.",
    "original_url": "http://arxiv.org/pdf/2108.04328v1",
    "original_title": "Generative Adversarial Neural Cellular Automata",
    "source": "arxiv",
    "authors": [
      "Maximilian Otte",
      "Quentin Delfosse",
      "Johannes Czech",
      "Kristian Kersting"
    ],
    "published": "2021-07-19T06:23:11+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2108.04328v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/nlin/0511061v1_chunk_0",
    "chunk_text": "Self-synchronization of Cellular Automata: an attempt to control patterns. The searching for the stable patterns in the evolution of cellular automata is implemented using stochastic synchronization between the present structures of the system and its precedent configurations. For most of the known evolution rules with complex behavior a dynamic competition among all the possible stable patterns is established and no stationary regime is reached. For the particular rule coded by the decimal number 18, a self-synchronization phenomenon can be obtained, even when strong modifications to the synchronization method are applied.",
    "original_url": "http://arxiv.org/pdf/nlin/0511061v1",
    "original_title": "Self-synchronization of Cellular Automata: an attempt to control patterns",
    "source": "arxiv",
    "authors": [
      "J. R. Sanchez",
      "R. Lopez-Ruiz"
    ],
    "published": "2005-11-28T13:27:10+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/nlin/0511061v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/0803.4240v1_chunk_0",
    "chunk_text": "Neutral Fitness Landscape in the Cellular Automata Majority Problem. We study in detail the fitness landscape of a difficult cellular automata computational task: the majority problem. Our results show why this problem landscape is so hard to search, and we quantify the large degree of neutrality found in various ways. We show that a particular subspace of the solution space, called the \"Olympus\", is where good solutions concentrate, and give measures to quantitatively characterize this subspace.",
    "original_url": "http://arxiv.org/pdf/0803.4240v1",
    "original_title": "Neutral Fitness Landscape in the Cellular Automata Majority Problem",
    "source": "arxiv",
    "authors": [
      "Sébastien Verel",
      "Philippe Collard",
      "Marco Tomassini",
      "Leonardo Vanneschi"
    ],
    "published": "2008-03-29T07:50:24+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/0803.4240v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1203.3939v1_chunk_0",
    "chunk_text": "Mathematical Physics of Cellular Automata. A universal map is derived for all deterministic 1D cellular automata (CA) containing no freely adjustable parameters. The map can be extended to an arbitrary number of dimensions and topologies and its invariances allow to classify all CA rules into equivalence classes. Complexity in 1D systems is then shown to emerge from the weak symmetry breaking of the addition modulo an integer number p. The latter symmetry is possessed by certain rules that produce Pascal simplices in their time evolution. These results elucidate Wolfram's classification of CA dynamics.",
    "original_url": "http://arxiv.org/pdf/1203.3939v1",
    "original_title": "Mathematical Physics of Cellular Automata",
    "source": "arxiv",
    "authors": [
      "Vladimir Garcia-Morales"
    ],
    "published": "2012-03-18T11:25:55+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1203.3939v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1203.3939v1_chunk_1",
    "chunk_text": "These results elucidate Wolfram's classification of CA dynamics.",
    "original_url": "http://arxiv.org/pdf/1203.3939v1",
    "original_title": "Mathematical Physics of Cellular Automata",
    "source": "arxiv",
    "authors": [
      "Vladimir Garcia-Morales"
    ],
    "published": "2012-03-18T11:25:55+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1203.3939v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1508.05752v1_chunk_0",
    "chunk_text": "An evolutionary approach to the identification of Cellular Automata based on partial observations. In this paper we consider the identification problem of Cellular Automata (CAs). The problem is defined and solved in the context of partial observations with time gaps of unknown length, i.e. pre-recorded, partial configurations of the system at certain, unknown time steps. A solution method based on a modified variant of a Genetic Algorithm (GA) is proposed and illustrated with brief experimental results.",
    "original_url": "http://arxiv.org/pdf/1508.05752v1",
    "original_title": "An evolutionary approach to the identification of Cellular Automata based on partial observations",
    "source": "arxiv",
    "authors": [
      "Witold Bołt",
      "Jan M. Baetens",
      "Bernard De Baets"
    ],
    "published": "2015-08-24T10:47:47+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1508.05752v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1508.05752v1_chunk_1",
    "chunk_text": "A solution method based on a modified variant of a Genetic Algorithm (GA) is proposed and illustrated with brief experimental results.",
    "original_url": "http://arxiv.org/pdf/1508.05752v1",
    "original_title": "An evolutionary approach to the identification of Cellular Automata based on partial observations",
    "source": "arxiv",
    "authors": [
      "Witold Bołt",
      "Jan M. Baetens",
      "Bernard De Baets"
    ],
    "published": "2015-08-24T10:47:47+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1508.05752v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2005.06142v1_chunk_0",
    "chunk_text": "Using Genetic Algorithm To Evolve Cellular Automata In Performing Edge Detection. Cellular automata are discrete and computational models thatcan be shown as general models of complexity. They are used in varied applications to derive the generalized behavior of the presented model. In this paper we have took one such application. We have made an effort to perform edge detection on an image using genetic algorithm.",
    "original_url": "http://arxiv.org/pdf/2005.06142v1",
    "original_title": "Using Genetic Algorithm To Evolve Cellular Automata In Performing Edge Detection",
    "source": "arxiv",
    "authors": [
      "Karan Nayak"
    ],
    "published": "2020-05-13T04:07:43+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2005.06142v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2005.06142v1_chunk_1",
    "chunk_text": "We have made an effort to perform edge detection on an image using genetic algorithm. The purpose and the intention here is to analyze the capability and performance of the suggested genetic algorithm. Genetic algorithms are used to depict or obtain a general solution of given problem. Using this feature of GA we have tried to evolve the cellular automata and shown that how with time it converges to the desired results.",
    "original_url": "http://arxiv.org/pdf/2005.06142v1",
    "original_title": "Using Genetic Algorithm To Evolve Cellular Automata In Performing Edge Detection",
    "source": "arxiv",
    "authors": [
      "Karan Nayak"
    ],
    "published": "2020-05-13T04:07:43+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2005.06142v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2201.01557v1_chunk_0",
    "chunk_text": "Asynchronism and nonequilibrium phase transitions in $(1+1)$D quantum cellular automata. Probabilistic cellular automata provide a simple framework for the exploration of classical nonequilibrium processes. Recently, quantum cellular automata have been proposed that rely on the propagation of a one-dimensional quantum state along a fictitious discrete time dimension via the sequential application of quantum gates. The resulting $(1+1)$-dimensional space-time structure makes these automata special cases of feed-forward quantum neural networks. Here we show how asynchronism -- introduced via non-commuting gates -- impacts on the collective nonequilibrium behavior of quantum cellular automata.",
    "original_url": "http://arxiv.org/pdf/2201.01557v1",
    "original_title": "Asynchronism and nonequilibrium phase transitions in $(1+1)$D quantum cellular automata",
    "source": "arxiv",
    "authors": [
      "Edward Gillman",
      "Federico Carollo",
      "Igor Lesanovsky"
    ],
    "published": "2022-01-05T11:59:07+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2201.01557v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2201.01557v1_chunk_1",
    "chunk_text": "Here we show how asynchronism -- introduced via non-commuting gates -- impacts on the collective nonequilibrium behavior of quantum cellular automata. We illustrate this through a simple model, whose synchronous version implements a contact process and features a nonequilibrium phase transition in the directed percolation universality class. Non-commuting quantum gates lead to an \"asynchronism transition\", i.e. a sudden qualitative change in the phase transition behavior once a certain degree of asynchronicity is surpassed. Our results show how quantum effects may lead to abrupt changes of non-equilibrium dynamics, which may be relevant for understanding the role of quantum correlations in neural networks.",
    "original_url": "http://arxiv.org/pdf/2201.01557v1",
    "original_title": "Asynchronism and nonequilibrium phase transitions in $(1+1)$D quantum cellular automata",
    "source": "arxiv",
    "authors": [
      "Edward Gillman",
      "Federico Carollo",
      "Igor Lesanovsky"
    ],
    "published": "2022-01-05T11:59:07+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2201.01557v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2201.01557v1_chunk_2",
    "chunk_text": "Our results show how quantum effects may lead to abrupt changes of non-equilibrium dynamics, which may be relevant for understanding the role of quantum correlations in neural networks.",
    "original_url": "http://arxiv.org/pdf/2201.01557v1",
    "original_title": "Asynchronism and nonequilibrium phase transitions in $(1+1)$D quantum cellular automata",
    "source": "arxiv",
    "authors": [
      "Edward Gillman",
      "Federico Carollo",
      "Igor Lesanovsky"
    ],
    "published": "2022-01-05T11:59:07+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2201.01557v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2311.16123v1_chunk_0",
    "chunk_text": "Exploring Multiple Neighborhood Neural Cellular Automata (MNNCA) for Enhanced Texture Learning. Cellular Automata (CA) have long been foundational in simulating dynamical systems computationally. With recent innovations, this model class has been brought into the realm of deep learning by parameterizing the CA's update rule using an artificial neural network, termed Neural Cellular Automata (NCA). This allows NCAs to be trained via gradient descent, enabling them to evolve into specific shapes, generate textures, and mimic behaviors such as swarming. However, a limitation of traditional NCAs is their inability to exhibit sufficiently complex behaviors, restricting their potential in creative and modeling tasks.",
    "original_url": "http://arxiv.org/pdf/2311.16123v1",
    "original_title": "Exploring Multiple Neighborhood Neural Cellular Automata (MNNCA) for Enhanced Texture Learning",
    "source": "arxiv",
    "authors": [
      "Magnus Petersen"
    ],
    "published": "2023-10-27T15:16:19+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2311.16123v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2311.16123v1_chunk_1",
    "chunk_text": "However, a limitation of traditional NCAs is their inability to exhibit sufficiently complex behaviors, restricting their potential in creative and modeling tasks. Our research explores enhancing the NCA framework by incorporating multiple neighborhoods and introducing structured noise for seed states. This approach is inspired by techniques that have historically amplified the expressiveness of classical continuous CA. All code and example videos are publicly available on https://github.com/MagnusPetersen/MNNCA.",
    "original_url": "http://arxiv.org/pdf/2311.16123v1",
    "original_title": "Exploring Multiple Neighborhood Neural Cellular Automata (MNNCA) for Enhanced Texture Learning",
    "source": "arxiv",
    "authors": [
      "Magnus Petersen"
    ],
    "published": "2023-10-27T15:16:19+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2311.16123v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/nlin/0702046v1_chunk_0",
    "chunk_text": "Randomized Cellular Automata. We define and study a few properties of a class of random automata networks. While regular finite one-dimensional cellular automata are defined on periodic lattices, these automata networks, called randomized cellular automata, are defined on random directed graphs with constant out-degrees and evolve according to cellular automaton rules. For some families of rules, a few typical a priori unexpected results are presented.",
    "original_url": "http://arxiv.org/pdf/nlin/0702046v1",
    "original_title": "Randomized Cellular Automata",
    "source": "arxiv",
    "authors": [
      "Nino Boccara"
    ],
    "published": "2007-02-23T14:33:56+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/nlin/0702046v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1004.1830v1_chunk_0",
    "chunk_text": "About the embedding of one dimensional cellular automata into hyperbolic cellular automata. In this paper, we look at two ways to implement determinisitic one dimensional cellular automata into hyperbolic cellular automata in three contexts: the pentagrid, the heptagrid and the dodecagrid, these tilings being classically denoted by $\\{5,4\\}$, $\\{7,3\\}$ and $\\{5,3,4\\}$ respectively.",
    "original_url": "http://arxiv.org/pdf/1004.1830v1",
    "original_title": "About the embedding of one dimensional cellular automata into hyperbolic cellular automata",
    "source": "arxiv",
    "authors": [
      "Maurice Margenstern"
    ],
    "published": "2010-04-11T18:35:11+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1004.1830v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/nlin/0502061v1_chunk_0",
    "chunk_text": "Two Cellular Automata for the 3x+1 Map. Two simple Cellular Automata, which mimic the Collatz-Ulam iterated map (3x+1 map), are introduced. These Cellular Automata allow to test efficiently the Collatz conjecture for very large numbers.",
    "original_url": "http://arxiv.org/pdf/nlin/0502061v1",
    "original_title": "Two Cellular Automata for the 3x+1 Map",
    "source": "arxiv",
    "authors": [
      "M. Bruschi"
    ],
    "published": "2005-02-26T18:37:46+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/nlin/0502061v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2205.06806v1_chunk_0",
    "chunk_text": "Goal-Guided Neural Cellular Automata: Learning to Control Self-Organising Systems. Inspired by cellular growth and self-organization, Neural Cellular Automata (NCAs) have been capable of \"growing\" artificial cells into images, 3D structures, and even functional machines. NCAs are flexible and robust computational systems but -- similarly to many other self-organizing systems -- inherently uncontrollable during and after their growth process. We present an approach to control these type of systems called Goal-Guided Neural Cellular Automata (GoalNCA), which leverages goal encodings to control cell behavior dynamically at every step of cellular growth. This approach enables the NCA to continually change behavior, and in some cases, generalize its behavior to unseen scenarios.",
    "original_url": "http://arxiv.org/pdf/2205.06806v1",
    "original_title": "Goal-Guided Neural Cellular Automata: Learning to Control Self-Organising Systems",
    "source": "arxiv",
    "authors": [
      "Shyam Sudhakaran",
      "Elias Najarro",
      "Sebastian Risi"
    ],
    "published": "2022-04-25T23:11:51+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2205.06806v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2205.06806v1_chunk_1",
    "chunk_text": "This approach enables the NCA to continually change behavior, and in some cases, generalize its behavior to unseen scenarios. We also demonstrate the robustness of the NCA with its ability to preserve task performance, even when only a portion of cells receive goal information.",
    "original_url": "http://arxiv.org/pdf/2205.06806v1",
    "original_title": "Goal-Guided Neural Cellular Automata: Learning to Control Self-Organising Systems",
    "source": "arxiv",
    "authors": [
      "Shyam Sudhakaran",
      "Elias Najarro",
      "Sebastian Risi"
    ],
    "published": "2022-04-25T23:11:51+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2205.06806v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/nlin/0404003v2_chunk_0",
    "chunk_text": "Cellular Automata: Wolfram's Metaphors for Complex Systems. A brief introduction to Wolfram's work on cellular automata.",
    "original_url": "http://arxiv.org/pdf/nlin/0404003v2",
    "original_title": "Cellular Automata: Wolfram's Metaphors for Complex Systems",
    "source": "arxiv",
    "authors": [
      "Pratip Bhattacharyya"
    ],
    "published": "2004-04-01T10:40:52+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/nlin/0404003v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/nlin/0505033v1_chunk_0",
    "chunk_text": "New Cellular Automata associated with the Schroedinger Discrete Spectral Problem. New Cellular Automata associated with the Schroedinger discrete spectral problem are derived. These Cellular Automata possess an infinite (countable) set of constants of motion.",
    "original_url": "http://arxiv.org/pdf/nlin/0505033v1",
    "original_title": "New Cellular Automata associated with the Schroedinger Discrete Spectral Problem",
    "source": "arxiv",
    "authors": [
      "M. Bruschi"
    ],
    "published": "2005-05-12T16:51:00+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/nlin/0505033v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2204.11674v1_chunk_0",
    "chunk_text": "HyperNCA: Growing Developmental Networks with Neural Cellular Automata. In contrast to deep reinforcement learning agents, biological neural networks are grown through a self-organized developmental process. Here we propose a new hypernetwork approach to grow artificial neural networks based on neural cellular automata (NCA). Inspired by self-organising systems and information-theoretic approaches to developmental biology, we show that our HyperNCA method can grow neural networks capable of solving common reinforcement learning tasks. Finally, we explore how the same approach can be used to build developmental metamorphosis networks capable of transforming their weights to solve variations of the initial RL task.",
    "original_url": "http://arxiv.org/pdf/2204.11674v1",
    "original_title": "HyperNCA: Growing Developmental Networks with Neural Cellular Automata",
    "source": "arxiv",
    "authors": [
      "Elias Najarro",
      "Shyam Sudhakaran",
      "Claire Glanois",
      "Sebastian Risi"
    ],
    "published": "2022-04-25T14:08:50+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2204.11674v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2204.11674v1_chunk_1",
    "chunk_text": "Finally, we explore how the same approach can be used to build developmental metamorphosis networks capable of transforming their weights to solve variations of the initial RL task.",
    "original_url": "http://arxiv.org/pdf/2204.11674v1",
    "original_title": "HyperNCA: Growing Developmental Networks with Neural Cellular Automata",
    "source": "arxiv",
    "authors": [
      "Elias Najarro",
      "Shyam Sudhakaran",
      "Claire Glanois",
      "Sebastian Risi"
    ],
    "published": "2022-04-25T14:08:50+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2204.11674v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2103.14886v2_chunk_0",
    "chunk_text": "Generalization over different cellular automata rules learned by a deep feed-forward neural network. To test generalization ability of a class of deep neural networks, we randomly generate a large number of different rule sets for 2-D cellular automata (CA), based on John Conway's Game of Life. Using these rules, we compute several trajectories for each CA instance. A deep convolutional encoder-decoder network with short and long range skip connections is trained on various generated CA trajectories to predict the next CA state given its previous states. Results show that the network is able to learn the rules of various, complex cellular automata and generalize to unseen configurations.",
    "original_url": "http://arxiv.org/pdf/2103.14886v2",
    "original_title": "Generalization over different cellular automata rules learned by a deep feed-forward neural network",
    "source": "arxiv",
    "authors": [
      "Marcel Aach",
      "Jens Henrik Goebbert",
      "Jenia Jitsev"
    ],
    "published": "2021-03-27T12:12:07+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2103.14886v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2103.14886v2_chunk_1",
    "chunk_text": "Results show that the network is able to learn the rules of various, complex cellular automata and generalize to unseen configurations. To some extent, the network shows generalization to rule sets and neighborhood sizes that were not seen during the training at all. Code to reproduce the experiments is publicly available at: https://github.com/SLAMPAI/generalization-cellular-automata",
    "original_url": "http://arxiv.org/pdf/2103.14886v2",
    "original_title": "Generalization over different cellular automata rules learned by a deep feed-forward neural network",
    "source": "arxiv",
    "authors": [
      "Marcel Aach",
      "Jens Henrik Goebbert",
      "Jenia Jitsev"
    ],
    "published": "2021-03-27T12:12:07+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2103.14886v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2409.02722v1_chunk_0",
    "chunk_text": "Efficient Simulation of Non-uniform Cellular Automata with a Convolutional Neural Network. Cellular automata (CAs) and convolutional neural networks (CNNs) are closely related due to the local nature of information processing. The connection between these topics is beneficial to both related fields, for conceptual as well as practical reasons. Our contribution solidifies this connection in the case of non-uniform CAs (nuCAs), simulating a global update in the architecture of the Python package TensorFlow. Additionally, we demonstrate how the highly optimised out-of-the-box multiprocessing in TensorFlow offers interesting computational benefits, especially when simulating large numbers of nuCAs with many cells.",
    "original_url": "http://arxiv.org/pdf/2409.02722v1",
    "original_title": "Efficient Simulation of Non-uniform Cellular Automata with a Convolutional Neural Network",
    "source": "arxiv",
    "authors": [
      "Michiel Rollier",
      "Aisling J. Daly",
      "Odemir M. Bruno",
      "Jan M. Baetens"
    ],
    "published": "2024-09-04T13:58:51+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2409.02722v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2409.02722v1_chunk_1",
    "chunk_text": "Additionally, we demonstrate how the highly optimised out-of-the-box multiprocessing in TensorFlow offers interesting computational benefits, especially when simulating large numbers of nuCAs with many cells.",
    "original_url": "http://arxiv.org/pdf/2409.02722v1",
    "original_title": "Efficient Simulation of Non-uniform Cellular Automata with a Convolutional Neural Network",
    "source": "arxiv",
    "authors": [
      "Michiel Rollier",
      "Aisling J. Daly",
      "Odemir M. Bruno",
      "Jan M. Baetens"
    ],
    "published": "2024-09-04T13:58:51+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2409.02722v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/nlin/0504058v1_chunk_0",
    "chunk_text": "On Reversible Cellular Automata with Triplet Local Rules. Bijections between sets may be seen as discrete (or crisp) unitary transformations used in quantum computations. So discrete quantum cellular automata are cellular automata with reversible transition functions. This note studies on 1d reversible cellular automata with triplet local rules.",
    "original_url": "http://arxiv.org/pdf/nlin/0504058v1",
    "original_title": "On Reversible Cellular Automata with Triplet Local Rules",
    "source": "arxiv",
    "authors": [
      "Shuichi Inokuchi",
      "Kazumasa Honda",
      "Hyen Yeal Lee",
      "Tatsuro Sato",
      "Yoshihiro Mizoguchi",
      "Yasuo Kawahara"
    ],
    "published": "2005-04-28T02:06:41+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/nlin/0504058v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2107.07980v1_chunk_0",
    "chunk_text": "p-adic Cellular Neural Networks. In this article we introduce the p-adic cellular neural networks which are mathematical generalizations of the classical cellular neural networks (CNNs) introduced by Chua and Yang. The new networks have infinitely many cells which are organized hierarchically in rooted trees, and also they have infinitely many hidden layers. Intuitively, the p-adic CNNs occur as limits of large hierarchical discrete CNNs. More precisely, the new networks can be very well approximated by hierarchical discrete CNNs.",
    "original_url": "http://arxiv.org/pdf/2107.07980v1",
    "original_title": "p-adic Cellular Neural Networks",
    "source": "arxiv",
    "authors": [
      "B. A. Zambrano-Luna",
      "W. A. Zúñiga-Galindo"
    ],
    "published": "2021-07-16T15:51:06+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2107.07980v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2107.07980v1_chunk_1",
    "chunk_text": "More precisely, the new networks can be very well approximated by hierarchical discrete CNNs. Mathematically speaking, each of the new networks is modeled by one integro-differential equation depending on several p-adic spatial variables and the time. We study the Cauchy problem associated to these integro-differential equations and also provide numerical methods for solving them.",
    "original_url": "http://arxiv.org/pdf/2107.07980v1",
    "original_title": "p-adic Cellular Neural Networks",
    "source": "arxiv",
    "authors": [
      "B. A. Zambrano-Luna",
      "W. A. Zúñiga-Galindo"
    ],
    "published": "2021-07-16T15:51:06+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2107.07980v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1102.1691v2_chunk_0",
    "chunk_text": "Schema Redescription in Cellular Automata: Revisiting Emergence in Complex Systems. We present a method to eliminate redundancy in the transition tables of Boolean automata: schema redescription with two symbols. One symbol is used to capture redundancy of individual input variables, and another to capture permutability in sets of input variables: fully characterizing the canalization present in Boolean functions. Two-symbol schemata explain aspects of the behaviour of automata networks that the characterization of their emergent patterns does not capture. We use our method to compare two well-known cellular automata for the density classification task: the human engineered CA GKL, and another obtained via genetic programming (GP).",
    "original_url": "http://arxiv.org/pdf/1102.1691v2",
    "original_title": "Schema Redescription in Cellular Automata: Revisiting Emergence in Complex Systems",
    "source": "arxiv",
    "authors": [
      "Manuel Marques-Pita",
      "Luis M. Rocha"
    ],
    "published": "2011-02-08T19:13:53+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1102.1691v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1102.1691v2_chunk_1",
    "chunk_text": "We use our method to compare two well-known cellular automata for the density classification task: the human engineered CA GKL, and another obtained via genetic programming (GP). We show that despite having very different collective behaviour, these rules are very similar. Indeed, GKL is a special case of GP. Therefore, we demonstrate that it is more feasible to compare cellular automata via schema redescriptions of their rules, than by looking at their emergent behaviour, leading us to question the tendency in complexity research to pay much more attention to emergent patterns than to local interactions.",
    "original_url": "http://arxiv.org/pdf/1102.1691v2",
    "original_title": "Schema Redescription in Cellular Automata: Revisiting Emergence in Complex Systems",
    "source": "arxiv",
    "authors": [
      "Manuel Marques-Pita",
      "Luis M. Rocha"
    ],
    "published": "2011-02-08T19:13:53+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1102.1691v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2010.08431v2_chunk_0",
    "chunk_text": "Measuring Behavioural Similarity of Cellular Automata. Conway's Game of Life is the best-known cellular automaton. It is a classic model of emergence and self-organization, it is Turing-complete, and it can simulate a universal constructor. The Game of Life belongs to the set of semi-totalistic cellular automata, a family with 262,144 members. Many of these automata may deserve as much attention as the Game of Life, if not more.",
    "original_url": "http://arxiv.org/pdf/2010.08431v2",
    "original_title": "Measuring Behavioural Similarity of Cellular Automata",
    "source": "arxiv",
    "authors": [
      "Peter D. Turney"
    ],
    "published": "2020-10-16T14:53:33+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2010.08431v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2010.08431v2_chunk_1",
    "chunk_text": "Many of these automata may deserve as much attention as the Game of Life, if not more. The challenge we address here is to provide a structure for organizing this large family, to make it easier to find interesting automata, and to understand the relations between automata. Packard and Wolfram (1985) divided the family into four classes, based on the observed behaviours of the rules. Eppstein (2010) proposed an alternative four-class system, based on the forms of the rules. Instead of a class-based organization, we propose a continuous high-dimensional vector space, where each automaton is represented by a point in the space.",
    "original_url": "http://arxiv.org/pdf/2010.08431v2",
    "original_title": "Measuring Behavioural Similarity of Cellular Automata",
    "source": "arxiv",
    "authors": [
      "Peter D. Turney"
    ],
    "published": "2020-10-16T14:53:33+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2010.08431v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2010.08431v2_chunk_2",
    "chunk_text": "Instead of a class-based organization, we propose a continuous high-dimensional vector space, where each automaton is represented by a point in the space. The distance between two automata in this space corresponds to the differences in their behavioural characteristics. Nearest neighbours in the space have similar behaviours. This space should make it easier for researchers to see the structure of the family of semi-totalistic rules and to find the hidden gems in the family.",
    "original_url": "http://arxiv.org/pdf/2010.08431v2",
    "original_title": "Measuring Behavioural Similarity of Cellular Automata",
    "source": "arxiv",
    "authors": [
      "Peter D. Turney"
    ],
    "published": "2020-10-16T14:53:33+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2010.08431v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/nlin/0512017v1_chunk_0",
    "chunk_text": "Symmetry pattern transition in cellular automata with complex behavior. A transition from asymmetric to symmetric patterns in time-dependent extended systems is described. It is found that one dimensional cellular automata, started from fully random initial conditions, can be forced to evolve into complex symmetrical patterns by stochastically coupling a proportion $p$ of pairs of sites located at equal distance from the center of the lattice. A nontrivial critical value of $p$ must be surpassed in order to obtain symmetrical patterns during the evolution. This strategy is able to classify the cellular automata rules -with complex behavior- between those that support time-dependent symmetric patterns and those which do not support such kind of patterns.",
    "original_url": "http://arxiv.org/pdf/nlin/0512017v1",
    "original_title": "Symmetry pattern transition in cellular automata with complex behavior",
    "source": "arxiv",
    "authors": [
      "J. R. Sanchez",
      "R. Lopez-Ruiz"
    ],
    "published": "2005-12-08T11:22:56+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/nlin/0512017v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/nlin/0512017v1_chunk_1",
    "chunk_text": "This strategy is able to classify the cellular automata rules -with complex behavior- between those that support time-dependent symmetric patterns and those which do not support such kind of patterns.",
    "original_url": "http://arxiv.org/pdf/nlin/0512017v1",
    "original_title": "Symmetry pattern transition in cellular automata with complex behavior",
    "source": "arxiv",
    "authors": [
      "J. R. Sanchez",
      "R. Lopez-Ruiz"
    ],
    "published": "2005-12-08T11:22:56+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/nlin/0512017v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1405.3166v1_chunk_0",
    "chunk_text": "Clonal-Based Cellular Automata in Bioinformatics. This paper aims at providing a survey on the problems that can be easily addressed by clonalbased cellular automata in bioinformatics. Researchers try to address the problems in bioinformatics independent of each problem. None of the researchers has tried to relate the major problems in bioinformatics and find a solution using common frame work. We tried to find various problems in bioinformatics which can be addressed easily by clonal based cellular automata.",
    "original_url": "http://arxiv.org/pdf/1405.3166v1",
    "original_title": "Clonal-Based Cellular Automata in Bioinformatics",
    "source": "arxiv",
    "authors": [
      "Pokkuluri Kiran Sree",
      "Inampudi Ramesh Babu"
    ],
    "published": "2014-05-13T14:35:18+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1405.3166v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1405.3166v1_chunk_1",
    "chunk_text": "We tried to find various problems in bioinformatics which can be addressed easily by clonal based cellular automata. Extensive literature survey is conducted. We have considered some papers in various journals and conferences for conduct of our research. This paper provides intuition towards relating various problems in bioinformatics logically and tries to attain a common frame work with respect to clonal based cellular automata classifier for addressing the same.",
    "original_url": "http://arxiv.org/pdf/1405.3166v1",
    "original_title": "Clonal-Based Cellular Automata in Bioinformatics",
    "source": "arxiv",
    "authors": [
      "Pokkuluri Kiran Sree",
      "Inampudi Ramesh Babu"
    ],
    "published": "2014-05-13T14:35:18+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1405.3166v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2110.00585v2_chunk_0",
    "chunk_text": "Absolutely stable time crystals at finite temperature. We show that locally-interacting, periodically-driven (Floquet) Hamiltonian dynamics coupled to a Langevin bath support finite-temperature discrete time crystals (DTC) with an infinite auto-correlation time. By contrast to both prethermal and many-body localized DTCs, the time crystalline order we uncover is stable to arbitrary perturbations, including those that break the time translation symmetry of the underlying drive. Our approach utilizes a general mapping from probabilistic cellular automata (PCA) to open classical Floquet systems undergoing continuous-time Langevin dynamics. Applying this mapping to a variant of the Toom cellular automata, which we dub the ''$\\pi$-Toom time crystal'', leads to a 2D Floquet Hamiltonian with a finite-temperature DTC phase transition.",
    "original_url": "http://arxiv.org/pdf/2110.00585v2",
    "original_title": "Absolutely stable time crystals at finite temperature",
    "source": "arxiv",
    "authors": [
      "Francisco Machado",
      "Quntao Zhuang",
      "Norman Y. Yao",
      "Michael P. Zaletel"
    ],
    "published": "2021-10-01T18:00:00+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2110.00585v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2110.00585v2_chunk_1",
    "chunk_text": "Applying this mapping to a variant of the Toom cellular automata, which we dub the ''$\\pi$-Toom time crystal'', leads to a 2D Floquet Hamiltonian with a finite-temperature DTC phase transition. We provide numerical evidence for the existence of this transition, and analyze the statistics of the finite temperature fluctuations. Finally, we discuss how general results from the field of probabilistic cellular automata imply the existence of discrete time crystals (with an infinite auto-correlation time) in all dimensions, $d \\geq 1$.",
    "original_url": "http://arxiv.org/pdf/2110.00585v2",
    "original_title": "Absolutely stable time crystals at finite temperature",
    "source": "arxiv",
    "authors": [
      "Francisco Machado",
      "Quntao Zhuang",
      "Norman Y. Yao",
      "Michael P. Zaletel"
    ],
    "published": "2021-10-01T18:00:00+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2110.00585v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2402.03961v2_chunk_0",
    "chunk_text": "Self-Reproduction and Evolution in Cellular Automata: 25 Years after Evoloops. The year of 2024 marks the 25th anniversary of the publication of evoloops, an evolutionary variant of Chris Langton's self-reproducing loops which proved constructively that Darwinian evolution of self-reproducing organisms by variation and natural selection is possible within deterministic cellular automata. Over the last few decades, this line of Artificial Life research has since undergone several important developments. Although it experienced a relative dormancy of activities for a while, the recent rise of interest in open-ended evolution and the success of continuous cellular automata models have brought researchers' attention back to how to make spatio-temporal patterns self-reproduce and evolve within spatially distributed computational media. This article provides a review of the relevant literature on this topic over the past 25 years and highlights the major accomplishments made so far, the challenges being faced, and promising future research directions.",
    "original_url": "http://arxiv.org/pdf/2402.03961v2",
    "original_title": "Self-Reproduction and Evolution in Cellular Automata: 25 Years after Evoloops",
    "source": "arxiv",
    "authors": [
      "Hiroki Sayama",
      "Chrystopher L. Nehaniv"
    ],
    "published": "2024-02-06T12:41:39+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2402.03961v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2402.03961v2_chunk_1",
    "chunk_text": "This article provides a review of the relevant literature on this topic over the past 25 years and highlights the major accomplishments made so far, the challenges being faced, and promising future research directions.",
    "original_url": "http://arxiv.org/pdf/2402.03961v2",
    "original_title": "Self-Reproduction and Evolution in Cellular Automata: 25 Years after Evoloops",
    "source": "arxiv",
    "authors": [
      "Hiroki Sayama",
      "Chrystopher L. Nehaniv"
    ],
    "published": "2024-02-06T12:41:39+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2402.03961v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2308.11522v1_chunk_0",
    "chunk_text": "ReLiCADA -- Reservoir Computing using Linear Cellular Automata Design Algorithm. In this paper, we present a novel algorithm to optimize the design of Reservoir Computing using Cellular Automata models for time series applications. Besides selecting the models' hyperparameters, the proposed algorithm particularly solves the open problem of linear Cellular Automaton rule selection. The selection method pre-selects only a few promising candidate rules out of an exponentially growing rule space. When applied to relevant benchmark datasets, the selected rules achieve low errors, with the best rules being among the top 5% of the overall rule space.",
    "original_url": "http://arxiv.org/pdf/2308.11522v1",
    "original_title": "ReLiCADA -- Reservoir Computing using Linear Cellular Automata Design Algorithm",
    "source": "arxiv",
    "authors": [
      "Jonas Kantic",
      "Fabian C. Legl",
      "Walter Stechele",
      "Jakob Hermann"
    ],
    "published": "2023-08-22T15:52:37+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2308.11522v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2308.11522v1_chunk_1",
    "chunk_text": "When applied to relevant benchmark datasets, the selected rules achieve low errors, with the best rules being among the top 5% of the overall rule space. The algorithm was developed based on mathematical analysis of linear Cellular Automaton properties and is backed by almost one million experiments, adding up to a computational runtime of nearly one year. Comparisons to other state-of-the-art time series models show that the proposed Reservoir Computing using Cellular Automata models have lower computational complexity, at the same time, achieve lower errors. Hence, our approach reduces the time needed for training and hyperparameter optimization by up to several orders of magnitude.",
    "original_url": "http://arxiv.org/pdf/2308.11522v1",
    "original_title": "ReLiCADA -- Reservoir Computing using Linear Cellular Automata Design Algorithm",
    "source": "arxiv",
    "authors": [
      "Jonas Kantic",
      "Fabian C. Legl",
      "Walter Stechele",
      "Jakob Hermann"
    ],
    "published": "2023-08-22T15:52:37+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2308.11522v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1107.5228v1_chunk_0",
    "chunk_text": "Non-Uniform Cellular Automata: classes, dynamics, and decidability. The dynamical behavior of non-uniform cellular automata is compared with the one of classical cellular automata. Several differences and similarities are pointed out by a series of examples. Decidability of basic properties like surjectivity and injectivity is also established. The final part studies a strong form of equicontinuity property specially suited for non-uniform cellular automata.",
    "original_url": "http://arxiv.org/pdf/1107.5228v1",
    "original_title": "Non-Uniform Cellular Automata: classes, dynamics, and decidability",
    "source": "arxiv",
    "authors": [
      "Alberto Dennunzio",
      "Enrico Formenti",
      "Julien Provillard"
    ],
    "published": "2011-07-26T14:47:15+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1107.5228v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1107.5228v1_chunk_1",
    "chunk_text": "The final part studies a strong form of equicontinuity property specially suited for non-uniform cellular automata.",
    "original_url": "http://arxiv.org/pdf/1107.5228v1",
    "original_title": "Non-Uniform Cellular Automata: classes, dynamics, and decidability",
    "source": "arxiv",
    "authors": [
      "Alberto Dennunzio",
      "Enrico Formenti",
      "Julien Provillard"
    ],
    "published": "2011-07-26T14:47:15+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1107.5228v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2301.00897v1_chunk_0",
    "chunk_text": "Game of Intelligent Life. Cellular automata (CA) captivate researchers due to teh emergent, complex individualized behavior that simple global rules of interaction enact. Recent advances in the field have combined CA with convolutional neural networks to achieve self-regenerating images. This new branch of CA is called neural cellular automata [1]. The goal of this project is to use the idea of idea of neural cellular automata to grow prediction machines.",
    "original_url": "http://arxiv.org/pdf/2301.00897v1",
    "original_title": "Game of Intelligent Life",
    "source": "arxiv",
    "authors": [
      "Marlene Grieskamp",
      "Chaytan Inman",
      "Shaun Lee"
    ],
    "published": "2023-01-02T23:06:26+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2301.00897v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2301.00897v1_chunk_1",
    "chunk_text": "The goal of this project is to use the idea of idea of neural cellular automata to grow prediction machines. We place many different convolutional neural networks in a grid. Each conv net cell outputs a prediction of what the next state will be, and minimizes predictive error. Cells received their neighbors' colors and fitnesses as input. Each cell's fitness score described how accurate its predictions were.",
    "original_url": "http://arxiv.org/pdf/2301.00897v1",
    "original_title": "Game of Intelligent Life",
    "source": "arxiv",
    "authors": [
      "Marlene Grieskamp",
      "Chaytan Inman",
      "Shaun Lee"
    ],
    "published": "2023-01-02T23:06:26+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2301.00897v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2301.00897v1_chunk_2",
    "chunk_text": "Each cell's fitness score described how accurate its predictions were. Cells could also move to explore their environment and some stochasticity was applied to movement.",
    "original_url": "http://arxiv.org/pdf/2301.00897v1",
    "original_title": "Game of Intelligent Life",
    "source": "arxiv",
    "authors": [
      "Marlene Grieskamp",
      "Chaytan Inman",
      "Shaun Lee"
    ],
    "published": "2023-01-02T23:06:26+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2301.00897v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1610.00338v1_chunk_0",
    "chunk_text": "A Linear Acceleration Theorem for 2D Cellular Automata on all Complete Neighborhoods. Linear acceleration theorems are known for most computational models. Although such results have been proved for two-dimensional cellular automata working on specific neighborhoods, no general construction was known. We present here a technique of linear acceleration for all two-dimensional languages recognized by cellular automata working on complete neighborhoods.",
    "original_url": "http://arxiv.org/pdf/1610.00338v1",
    "original_title": "A Linear Acceleration Theorem for 2D Cellular Automata on all Complete Neighborhoods",
    "source": "arxiv",
    "authors": [
      "Anaël Grandjean",
      "Victor Poupet"
    ],
    "published": "2016-10-02T19:11:29+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1610.00338v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1208.2765v1_chunk_0",
    "chunk_text": "Phase Space Invertible Asynchronous Cellular Automata. While for synchronous deterministic cellular automata there is an accepted definition of reversibility, the situation is less clear for asynchronous cellular automata. We first discuss a few possibilities and then investigate what we call phase space invertible asynchronous cellular automata in more detail. We will show that for each Turing machine there is such a cellular automaton simulating it, and that it is decidable whether an asynchronous cellular automaton has this property or not, even in higher dimensions.",
    "original_url": "http://arxiv.org/pdf/1208.2765v1",
    "original_title": "Phase Space Invertible Asynchronous Cellular Automata",
    "source": "arxiv",
    "authors": [
      "Simon Wacker",
      "Thomas Worsch"
    ],
    "published": "2012-08-14T01:55:52+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1208.2765v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1705.09425v1_chunk_0",
    "chunk_text": "Hierarchical Cellular Automata for Visual Saliency. Saliency detection, finding the most important parts of an image, has become increasingly popular in computer vision. In this paper, we introduce Hierarchical Cellular Automata (HCA) -- a temporally evolving model to intelligently detect salient objects. HCA consists of two main components: Single-layer Cellular Automata (SCA) and Cuboid Cellular Automata (CCA). As an unsupervised propagation mechanism, Single-layer Cellular Automata can exploit the intrinsic relevance of similar regions through interactions with neighbors.",
    "original_url": "http://arxiv.org/pdf/1705.09425v1",
    "original_title": "Hierarchical Cellular Automata for Visual Saliency",
    "source": "arxiv",
    "authors": [
      "Yao Qin",
      "Mengyang Feng",
      "Huchuan Lu",
      "Garrison W. Cottrell"
    ],
    "published": "2017-05-26T03:43:16+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1705.09425v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1705.09425v1_chunk_1",
    "chunk_text": "As an unsupervised propagation mechanism, Single-layer Cellular Automata can exploit the intrinsic relevance of similar regions through interactions with neighbors. Low-level image features as well as high-level semantic information extracted from deep neural networks are incorporated into the SCA to measure the correlation between different image patches. With these hierarchical deep features, an impact factor matrix and a coherence matrix are constructed to balance the influences on each cell's next state. The saliency values of all cells are iteratively updated according to a well-defined update rule. Furthermore, we propose CCA to integrate multiple saliency maps generated by SCA at different scales in a Bayesian framework.",
    "original_url": "http://arxiv.org/pdf/1705.09425v1",
    "original_title": "Hierarchical Cellular Automata for Visual Saliency",
    "source": "arxiv",
    "authors": [
      "Yao Qin",
      "Mengyang Feng",
      "Huchuan Lu",
      "Garrison W. Cottrell"
    ],
    "published": "2017-05-26T03:43:16+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1705.09425v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1705.09425v1_chunk_2",
    "chunk_text": "Furthermore, we propose CCA to integrate multiple saliency maps generated by SCA at different scales in a Bayesian framework. Therefore, single-layer propagation and multi-layer integration are jointly modeled in our unified HCA. Surprisingly, we find that the SCA can improve all existing methods that we applied it to, resulting in a similar precision level regardless of the original results. The CCA can act as an efficient pixel-wise aggregation algorithm that can integrate state-of-the-art methods, resulting in even better results. Extensive experiments on four challenging datasets demonstrate that the proposed algorithm outperforms state-of-the-art conventional methods and is competitive with deep learning based approaches.",
    "original_url": "http://arxiv.org/pdf/1705.09425v1",
    "original_title": "Hierarchical Cellular Automata for Visual Saliency",
    "source": "arxiv",
    "authors": [
      "Yao Qin",
      "Mengyang Feng",
      "Huchuan Lu",
      "Garrison W. Cottrell"
    ],
    "published": "2017-05-26T03:43:16+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1705.09425v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1705.09425v1_chunk_3",
    "chunk_text": "Extensive experiments on four challenging datasets demonstrate that the proposed algorithm outperforms state-of-the-art conventional methods and is competitive with deep learning based approaches.",
    "original_url": "http://arxiv.org/pdf/1705.09425v1",
    "original_title": "Hierarchical Cellular Automata for Visual Saliency",
    "source": "arxiv",
    "authors": [
      "Yao Qin",
      "Mengyang Feng",
      "Huchuan Lu",
      "Garrison W. Cottrell"
    ],
    "published": "2017-05-26T03:43:16+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1705.09425v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2310.15894v1_chunk_0",
    "chunk_text": "Dynamical Phase Transitions in Graph Cellular Automata. Discrete dynamical systems can exhibit complex behaviour from the iterative application of straightforward local rules. A famous example are cellular automata whose global dynamics are notoriously challenging to analyze. To address this, we relax the regular connectivity grid of cellular automata to a random graph, which gives the class of graph cellular automata. Using the dynamical cavity method (DCM) and its backtracking version (BDCM), we show that this relaxation allows us to derive asymptotically exact analytical results on the global dynamics of these systems on sparse random graphs.",
    "original_url": "http://arxiv.org/pdf/2310.15894v1",
    "original_title": "Dynamical Phase Transitions in Graph Cellular Automata",
    "source": "arxiv",
    "authors": [
      "Freya Behrens",
      "Barbora Hudcová",
      "Lenka Zdeborová"
    ],
    "published": "2023-10-24T14:50:02+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2310.15894v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2310.15894v1_chunk_1",
    "chunk_text": "Using the dynamical cavity method (DCM) and its backtracking version (BDCM), we show that this relaxation allows us to derive asymptotically exact analytical results on the global dynamics of these systems on sparse random graphs. Concretely, we showcase the results on a specific subclass of graph cellular automata with ``conforming non-conformist'' update rules, which exhibit dynamics akin to opinion formation. Such rules update a node's state according to the majority within their own neighbourhood. In cases where the majority leads only by a small margin over the minority, nodes may exhibit non-conformist behaviour. Instead of following the majority, they either maintain their own state, switch it, or follow the minority.",
    "original_url": "http://arxiv.org/pdf/2310.15894v1",
    "original_title": "Dynamical Phase Transitions in Graph Cellular Automata",
    "source": "arxiv",
    "authors": [
      "Freya Behrens",
      "Barbora Hudcová",
      "Lenka Zdeborová"
    ],
    "published": "2023-10-24T14:50:02+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2310.15894v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2310.15894v1_chunk_2",
    "chunk_text": "Instead of following the majority, they either maintain their own state, switch it, or follow the minority. For configurations with different initial biases towards one state we identify sharp dynamical phase transitions in terms of the convergence speed and attractor types. From the perspective of opinion dynamics this answers when consensus will emerge and when two opinions coexist almost indefinitely.",
    "original_url": "http://arxiv.org/pdf/2310.15894v1",
    "original_title": "Dynamical Phase Transitions in Graph Cellular Automata",
    "source": "arxiv",
    "authors": [
      "Freya Behrens",
      "Barbora Hudcová",
      "Lenka Zdeborová"
    ],
    "published": "2023-10-24T14:50:02+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2310.15894v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2008.12020v1_chunk_0",
    "chunk_text": "A Data-driven Understanding of COVID-19 Dynamics Using Sequential Genetic Algorithm Based Probabilistic Cellular Automata. COVID-19 pandemic is severely impacting the lives of billions across the globe. Even after taking massive protective measures like nation-wide lockdowns, discontinuation of international flight services, rigorous testing etc., the infection spreading is still growing steadily, causing thousands of deaths and serious socio-economic crisis. Thus, the identification of the major factors of this infection spreading dynamics is becoming crucial to minimize impact and lifetime of COVID-19 and any future pandemic. In this work, a probabilistic cellular automata based method has been employed to model the infection dynamics for a significant number of different countries.",
    "original_url": "http://arxiv.org/pdf/2008.12020v1",
    "original_title": "A Data-driven Understanding of COVID-19 Dynamics Using Sequential Genetic Algorithm Based Probabilistic Cellular Automata",
    "source": "arxiv",
    "authors": [
      "Sayantari Ghosh",
      "Saumik Bhattacharya"
    ],
    "published": "2020-08-27T09:53:21+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2008.12020v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2008.12020v1_chunk_1",
    "chunk_text": "In this work, a probabilistic cellular automata based method has been employed to model the infection dynamics for a significant number of different countries. This study proposes that for an accurate data-driven modeling of this infection spread, cellular automata provides an excellent platform, with a sequential genetic algorithm for efficiently estimating the parameters of the dynamics. To the best of our knowledge, this is the first attempt to understand and interpret COVID-19 data using optimized cellular automata, through genetic algorithm. It has been demonstrated that the proposed methodology can be flexible and robust at the same time, and can be used to model the daily active cases, total number of infected people and total death cases through systematic parameter estimation. Elaborate analyses for COVID-19 statistics of forty countries from different continents have been performed, with markedly divergent time evolution of the infection spreading because of demographic and socioeconomic factors.",
    "original_url": "http://arxiv.org/pdf/2008.12020v1",
    "original_title": "A Data-driven Understanding of COVID-19 Dynamics Using Sequential Genetic Algorithm Based Probabilistic Cellular Automata",
    "source": "arxiv",
    "authors": [
      "Sayantari Ghosh",
      "Saumik Bhattacharya"
    ],
    "published": "2020-08-27T09:53:21+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2008.12020v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2008.12020v1_chunk_2",
    "chunk_text": "Elaborate analyses for COVID-19 statistics of forty countries from different continents have been performed, with markedly divergent time evolution of the infection spreading because of demographic and socioeconomic factors. The substantial predictive power of this model has been established with conclusions on the key players in this pandemic dynamics.",
    "original_url": "http://arxiv.org/pdf/2008.12020v1",
    "original_title": "A Data-driven Understanding of COVID-19 Dynamics Using Sequential Genetic Algorithm Based Probabilistic Cellular Automata",
    "source": "arxiv",
    "authors": [
      "Sayantari Ghosh",
      "Saumik Bhattacharya"
    ],
    "published": "2020-08-27T09:53:21+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2008.12020v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1907.01856v1_chunk_0",
    "chunk_text": "A general representation of dynamical systems for reservoir computing. Dynamical systems are capable of performing computation in a reservoir computing paradigm. This paper presents a general representation of these systems as an artificial neural network (ANN). Initially, we implement the simplest dynamical system, a cellular automaton. The mathematical fundamentals behind an ANN are maintained, but the weights of the connections and the activation function are adjusted to work as an update rule in the context of cellular automata.",
    "original_url": "http://arxiv.org/pdf/1907.01856v1",
    "original_title": "A general representation of dynamical systems for reservoir computing",
    "source": "arxiv",
    "authors": [
      "Sidney Pontes-Filho",
      "Anis Yazidi",
      "Jianhua Zhang",
      "Hugo Hammer",
      "Gustavo B. M. Mello",
      "Ioanna Sandvig",
      "Gunnar Tufte",
      "Stefano Nichele"
    ],
    "published": "2019-07-03T11:12:42+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1907.01856v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1907.01856v1_chunk_1",
    "chunk_text": "The mathematical fundamentals behind an ANN are maintained, but the weights of the connections and the activation function are adjusted to work as an update rule in the context of cellular automata. The advantages of such implementation are its usage on specialized and optimized deep learning libraries, the capabilities to generalize it to other types of networks and the possibility to evolve cellular automata and other dynamical systems in terms of connectivity, update and learning rules. Our implementation of cellular automata constitutes an initial step towards a general framework for dynamical systems. It aims to evolve such systems to optimize their usage in reservoir computing and to model physical computing substrates.",
    "original_url": "http://arxiv.org/pdf/1907.01856v1",
    "original_title": "A general representation of dynamical systems for reservoir computing",
    "source": "arxiv",
    "authors": [
      "Sidney Pontes-Filho",
      "Anis Yazidi",
      "Jianhua Zhang",
      "Hugo Hammer",
      "Gustavo B. M. Mello",
      "Ioanna Sandvig",
      "Gunnar Tufte",
      "Stefano Nichele"
    ],
    "published": "2019-07-03T11:12:42+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1907.01856v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2211.01233v1_chunk_0",
    "chunk_text": "Attention-based Neural Cellular Automata. Recent extensions of Cellular Automata (CA) have incorporated key ideas from modern deep learning, dramatically extending their capabilities and catalyzing a new family of Neural Cellular Automata (NCA) techniques. Inspired by Transformer-based architectures, our work presents a new class of $\\textit{attention-based}$ NCAs formed using a spatially localized$\\unicode{x2014}$yet globally organized$\\unicode{x2014}$self-attention scheme. We introduce an instance of this class named $\\textit{Vision Transformer Cellular Automata}$ (ViTCA). We present quantitative and qualitative results on denoising autoencoding across six benchmark datasets, comparing ViTCA to a U-Net, a U-Net-based CA baseline (UNetCA), and a Vision Transformer (ViT).",
    "original_url": "http://arxiv.org/pdf/2211.01233v1",
    "original_title": "Attention-based Neural Cellular Automata",
    "source": "arxiv",
    "authors": [
      "Mattie Tesfaldet",
      "Derek Nowrouzezahrai",
      "Christopher Pal"
    ],
    "published": "2022-11-02T16:14:41+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2211.01233v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2211.01233v1_chunk_1",
    "chunk_text": "We present quantitative and qualitative results on denoising autoencoding across six benchmark datasets, comparing ViTCA to a U-Net, a U-Net-based CA baseline (UNetCA), and a Vision Transformer (ViT). When comparing across architectures configured to similar parameter complexity, ViTCA architectures yield superior performance across all benchmarks and for nearly every evaluation metric. We present an ablation study on various architectural configurations of ViTCA, an analysis of its effect on cell states, and an investigation on its inductive biases. Finally, we examine its learned representations via linear probes on its converged cell state hidden representations, yielding, on average, superior results when compared to our U-Net, ViT, and UNetCA baselines.",
    "original_url": "http://arxiv.org/pdf/2211.01233v1",
    "original_title": "Attention-based Neural Cellular Automata",
    "source": "arxiv",
    "authors": [
      "Mattie Tesfaldet",
      "Derek Nowrouzezahrai",
      "Christopher Pal"
    ],
    "published": "2022-11-02T16:14:41+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2211.01233v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1503.03318v2_chunk_0",
    "chunk_text": "On the decomposition of stochastic cellular automata. In this paper we present two interesting properties of stochastic cellular automata that can be helpful in analyzing the dynamical behavior of such automata. The first property allows for calculating cell-wise probability distributions over the state set of a stochastic cellular automaton, i.e. images that show the average state of each cell during the evolution of the stochastic cellular automaton. The second property shows that stochastic cellular automata are equivalent to so-called stochastic mixtures of deterministic cellular automata.",
    "original_url": "http://arxiv.org/pdf/1503.03318v2",
    "original_title": "On the decomposition of stochastic cellular automata",
    "source": "arxiv",
    "authors": [
      "Witold Bołt",
      "Jan M. Baetens",
      "Bernard DeBaets"
    ],
    "published": "2015-03-11T13:18:58+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1503.03318v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1503.03318v2_chunk_1",
    "chunk_text": "The second property shows that stochastic cellular automata are equivalent to so-called stochastic mixtures of deterministic cellular automata. Based on this property, any stochastic cellular automaton can be decomposed into a set of deterministic cellular automata, each of which contributes to the behavior of the stochastic cellular automaton.",
    "original_url": "http://arxiv.org/pdf/1503.03318v2",
    "original_title": "On the decomposition of stochastic cellular automata",
    "source": "arxiv",
    "authors": [
      "Witold Bołt",
      "Jan M. Baetens",
      "Bernard DeBaets"
    ],
    "published": "2015-03-11T13:18:58+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1503.03318v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1212.6304v1_chunk_0",
    "chunk_text": "Scaling behaviour in probabilistic neuronal cellular automata. We study a neural network model of interacting stochastic discrete two--state cellular automata on a regular lattice. The system is externally tuned to a critical point which varies with the degree of stochasticity (or the effective temperature). There are avalanches of neuronal activity, namely spatially and temporally contiguous sites of activity; a detailed numerical study of these activity avalanches is presented, and single, joint and marginal probability distributions are computed. At the critical point, we find that the scaling exponents for the variables are in good agreement with a mean--field theory.",
    "original_url": "http://arxiv.org/pdf/1212.6304v1",
    "original_title": "Scaling behaviour in probabilistic neuronal cellular automata",
    "source": "arxiv",
    "authors": [
      "Kaustubh Manchanda",
      "Avinash Chand Yadav",
      "Ramakrishna Ramaswamy"
    ],
    "published": "2012-12-27T05:21:19+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1212.6304v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1212.6304v1_chunk_1",
    "chunk_text": "At the critical point, we find that the scaling exponents for the variables are in good agreement with a mean--field theory.",
    "original_url": "http://arxiv.org/pdf/1212.6304v1",
    "original_title": "Scaling behaviour in probabilistic neuronal cellular automata",
    "source": "arxiv",
    "authors": [
      "Kaustubh Manchanda",
      "Avinash Chand Yadav",
      "Ramakrishna Ramaswamy"
    ],
    "published": "2012-12-27T05:21:19+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1212.6304v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1901.00881v1_chunk_0",
    "chunk_text": "Trainable Associative Memory Neural Networks in a Quantum-Dot Cellular Automata. Quantum-dot cellular automata (QCAs) offer a diffusive computing paradigm with picosecond transmission speed, making them an ideal candidate for moving diffusive computing to real-world applications. By implementing a trainable associative memory neural network into this substrate, we demonstrate that high-speed, high-density associative memory is feasible through QCAs. The presented design occupies $415\\text{nm}^2$ per neuron, which translates to circa $240 \\text{ billion neurons/cm}^2$, or $28\\text{GB/cm}^2$ of memory storage, offering a real possibility for large-scale associative memory circuits. Results are presented from simulation, demonstrating correct working behaviour of the associative memory in single neurons, two-neuron and four-neuron arrays.",
    "original_url": "http://arxiv.org/pdf/1901.00881v1",
    "original_title": "Trainable Associative Memory Neural Networks in a Quantum-Dot Cellular Automata",
    "source": "arxiv",
    "authors": [
      "James Stovold"
    ],
    "published": "2019-01-03T19:10:24+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1901.00881v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1901.00881v1_chunk_1",
    "chunk_text": "Results are presented from simulation, demonstrating correct working behaviour of the associative memory in single neurons, two-neuron and four-neuron arrays.",
    "original_url": "http://arxiv.org/pdf/1901.00881v1",
    "original_title": "Trainable Associative Memory Neural Networks in a Quantum-Dot Cellular Automata",
    "source": "arxiv",
    "authors": [
      "James Stovold"
    ],
    "published": "2019-01-03T19:10:24+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1901.00881v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/0708.2244v2_chunk_0",
    "chunk_text": "Unbiased Random Threshold Networks Are Chaotic or Critical. This paper has been withdrawn.",
    "original_url": "http://arxiv.org/pdf/0708.2244v2",
    "original_title": "Unbiased Random Threshold Networks Are Chaotic or Critical",
    "source": "arxiv",
    "authors": [
      "Volkan Sevim",
      "Per Arne Rikvold"
    ],
    "published": "2007-08-16T17:15:52+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/0708.2244v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/cond-mat/0606632v1_chunk_0",
    "chunk_text": "Deducing Local Rules for Solving Global Tasks with Random Boolean Networks. It has been shown that uniform as well as non-uniform cellular automata (CA) can be evolved to perform certain computational tasks. Random Boolean networks are a generalization of two-state cellular automata, where the interconnection topology and the cell's rules are specified at random. Here we present a novel analytical approach to find the local rules of random Boolean networks (RBNs) to solve the global density classification and the synchronization task from any initial configuration. We quantitatively and qualitatively compare our results with previously published work on cellular automata and show that randomly interconnected automata are computationally more efficient in solving these two global tasks.",
    "original_url": "http://arxiv.org/pdf/cond-mat/0606632v1",
    "original_title": "Deducing Local Rules for Solving Global Tasks with Random Boolean Networks",
    "source": "arxiv",
    "authors": [
      "Bertrand Mesot",
      "Christof Teuscher"
    ],
    "published": "2006-06-24T21:02:20+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/cond-mat/0606632v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/cond-mat/0606632v1_chunk_1",
    "chunk_text": "We quantitatively and qualitatively compare our results with previously published work on cellular automata and show that randomly interconnected automata are computationally more efficient in solving these two global tasks. Our approach also provides convergence and quality estimates and allows the networks to be randomly rewired during operation, without affecting the global performance. Finally, we show that RBNs outperform small-world topologies on the density classification task and that they perform equally well on the synchronization task. Our novel approach and the results may have applications in designing robust complex networks and locally interacting distributed computing systems for solving global tasks.",
    "original_url": "http://arxiv.org/pdf/cond-mat/0606632v1",
    "original_title": "Deducing Local Rules for Solving Global Tasks with Random Boolean Networks",
    "source": "arxiv",
    "authors": [
      "Bertrand Mesot",
      "Christof Teuscher"
    ],
    "published": "2006-06-24T21:02:20+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/cond-mat/0606632v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1512.07450v3_chunk_0",
    "chunk_text": "Interacting Behavior and Emerging Complexity. Can we quantify the change of complexity throughout evolutionary processes? We attempt to address this question through an empirical approach. In very general terms, we simulate two simple organisms on a computer that compete over limited available resources. We implement Global Rules that determine the interaction between two Elementary Cellular Automata on the same grid.",
    "original_url": "http://arxiv.org/pdf/1512.07450v3",
    "original_title": "Interacting Behavior and Emerging Complexity",
    "source": "arxiv",
    "authors": [
      "Alyssa Adams",
      "Hector Zenil",
      "Eduardo Hermo Reyes",
      "Joost Joosten"
    ],
    "published": "2015-12-23T12:29:01+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1512.07450v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1512.07450v3_chunk_1",
    "chunk_text": "We implement Global Rules that determine the interaction between two Elementary Cellular Automata on the same grid. Global Rules change the complexity of the state evolution output which suggests that some complexity is intrinsic to the interaction rules themselves. The largest increases in complexity occurred when the interacting elementary rules had very little complexity, suggesting that they are able to accept complexity through interaction only. We also found that some Class 3 or 4 CA rules are more fragile than others to Global Rules, while others are more robust, hence suggesting some intrinsic properties of the rules independent of the Global Rule choice. We provide statistical mappings of Elementary Cellular Automata exposed to Global Rules and different initial conditions onto different complexity classes.",
    "original_url": "http://arxiv.org/pdf/1512.07450v3",
    "original_title": "Interacting Behavior and Emerging Complexity",
    "source": "arxiv",
    "authors": [
      "Alyssa Adams",
      "Hector Zenil",
      "Eduardo Hermo Reyes",
      "Joost Joosten"
    ],
    "published": "2015-12-23T12:29:01+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1512.07450v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1512.07450v3_chunk_2",
    "chunk_text": "We provide statistical mappings of Elementary Cellular Automata exposed to Global Rules and different initial conditions onto different complexity classes.",
    "original_url": "http://arxiv.org/pdf/1512.07450v3",
    "original_title": "Interacting Behavior and Emerging Complexity",
    "source": "arxiv",
    "authors": [
      "Alyssa Adams",
      "Hector Zenil",
      "Eduardo Hermo Reyes",
      "Joost Joosten"
    ],
    "published": "2015-12-23T12:29:01+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1512.07450v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1212.2846v2_chunk_0",
    "chunk_text": "Expressiveness of Elementary Cellular Automata. We investigate expressiveness, a parameter of one-dimensional cellular automata, in the context of simulated biological systems. The development of elementary cellular automata is interpreted in terms of biological systems, and biologically inspired parameters for biodiversity are applied to the configurations of cellular automata. This article contains a survey of the Elementary Cellular Automata in terms of their expressiveness and an evaluation whether expressiveness is a meaningful term in the context of simulated biology.",
    "original_url": "http://arxiv.org/pdf/1212.2846v2",
    "original_title": "Expressiveness of Elementary Cellular Automata",
    "source": "arxiv",
    "authors": [
      "Markus Redeker",
      "Andrew Adamatzky",
      "Genaro J. Martínez"
    ],
    "published": "2012-12-12T15:41:29+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1212.2846v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1306.5577v2_chunk_0",
    "chunk_text": "A Note on Elementary Cellular Automata Classification. We overview and compare classifications of elementary cellular automata, including Wolfram's, Wuensche's, Li and Packard, communication complexity, power spectral, topological, surface, compression, lattices, and morphological diversity classifications. This paper summarises several classifications of elementary cellular automata (ECA) and compares them with a newly proposed one, that induced by endowing rules with memory.",
    "original_url": "http://arxiv.org/pdf/1306.5577v2",
    "original_title": "A Note on Elementary Cellular Automata Classification",
    "source": "arxiv",
    "authors": [
      "Genaro J. Martinez"
    ],
    "published": "2013-06-24T11:15:17+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1306.5577v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2004.03656v2_chunk_0",
    "chunk_text": "Gauge-invariance in cellular automata. Gauge-invariance is a fundamental concept in Physics -- known to provide mathematical justification for the fundamental forces. In this paper, we provide discrete counterparts to the main gauge theoretical concepts directly in terms of Cellular Automata. More precisely, the notions of gauge-invariance and gauge-equivalence in Cellular Automata are formalized. A step-by-step gauging procedure to enforce this symmetry upon a given Cellular Automaton is developed, and three examples of gauge-invariant Cellular Automata are examined.",
    "original_url": "http://arxiv.org/pdf/2004.03656v2",
    "original_title": "Gauge-invariance in cellular automata",
    "source": "arxiv",
    "authors": [
      "Pablo Arrighi",
      "Giuseppe Di Molfetta",
      "Nathanaël Eon"
    ],
    "published": "2020-04-07T19:20:26+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2004.03656v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2004.03656v2_chunk_1",
    "chunk_text": "A step-by-step gauging procedure to enforce this symmetry upon a given Cellular Automaton is developed, and three examples of gauge-invariant Cellular Automata are examined.",
    "original_url": "http://arxiv.org/pdf/2004.03656v2",
    "original_title": "Gauge-invariance in cellular automata",
    "source": "arxiv",
    "authors": [
      "Pablo Arrighi",
      "Giuseppe Di Molfetta",
      "Nathanaël Eon"
    ],
    "published": "2020-04-07T19:20:26+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2004.03656v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2101.02309v1_chunk_0",
    "chunk_text": "Cellular Automata on Quantum Annealing Systems. We present herein an introduction to implementing 2-color cellular automata on quantum annealing systems, such as the D-Wave quantum computer. We show that implementing nearest-neighbor cellular automata is possible. We present an implementation of Wolfram's cellular automata Rule 110, which has previously been shown to be a universal Turing machine, as a QUBO suitable for use on quantum annealing systems. We demonstrate back-propagation of cellular automata rule sets to determine initial cell states for a desired later system state.",
    "original_url": "http://arxiv.org/pdf/2101.02309v1",
    "original_title": "Cellular Automata on Quantum Annealing Systems",
    "source": "arxiv",
    "authors": [
      "Robert A. Dunn"
    ],
    "published": "2021-01-07T00:23:48+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2101.02309v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2101.02309v1_chunk_1",
    "chunk_text": "We demonstrate back-propagation of cellular automata rule sets to determine initial cell states for a desired later system state. We show 2-D 2-color cellular automata, such as Conway's Game of Life, can be expressed for quantum annealing systems.",
    "original_url": "http://arxiv.org/pdf/2101.02309v1",
    "original_title": "Cellular Automata on Quantum Annealing Systems",
    "source": "arxiv",
    "authors": [
      "Robert A. Dunn"
    ],
    "published": "2021-01-07T00:23:48+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2101.02309v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2110.14237v1_chunk_0",
    "chunk_text": "Learning Graph Cellular Automata. Cellular automata (CA) are a class of computational models that exhibit rich dynamics emerging from the local interaction of cells arranged in a regular lattice. In this work we focus on a generalised version of typical CA, called graph cellular automata (GCA), in which the lattice structure is replaced by an arbitrary graph. In particular, we extend previous work that used convolutional neural networks to learn the transition rule of conventional CA and we use graph neural networks to learn a variety of transition rules for GCA. First, we present a general-purpose architecture for learning GCA, and we show that it can represent any arbitrary GCA with finite and discrete state space.",
    "original_url": "http://arxiv.org/pdf/2110.14237v1",
    "original_title": "Learning Graph Cellular Automata",
    "source": "arxiv",
    "authors": [
      "Daniele Grattarola",
      "Lorenzo Livi",
      "Cesare Alippi"
    ],
    "published": "2021-10-27T07:42:48+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2110.14237v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2110.14237v1_chunk_1",
    "chunk_text": "First, we present a general-purpose architecture for learning GCA, and we show that it can represent any arbitrary GCA with finite and discrete state space. Then, we test our approach on three different tasks: 1) learning the transition rule of a GCA on a Voronoi tessellation; 2) imitating the behaviour of a group of flocking agents; 3) learning a rule that converges to a desired target state.",
    "original_url": "http://arxiv.org/pdf/2110.14237v1",
    "original_title": "Learning Graph Cellular Automata",
    "source": "arxiv",
    "authors": [
      "Daniele Grattarola",
      "Lorenzo Livi",
      "Cesare Alippi"
    ],
    "published": "2021-10-27T07:42:48+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2110.14237v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1102.2315v1_chunk_0",
    "chunk_text": "Cellular Automata and Discrete Geometry. In this paper, we look at the possibility to implement the algorithm to construct a discrete line devised by the first author in cellular automata. It turns out that such an implementation is feasible.",
    "original_url": "http://arxiv.org/pdf/1102.2315v1",
    "original_title": "Cellular Automata and Discrete Geometry",
    "source": "arxiv",
    "authors": [
      "Isabelle Debled-Rennesson",
      "Maurice Margenstern"
    ],
    "published": "2011-02-11T10:16:12+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1102.2315v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2501.06648v1_chunk_0",
    "chunk_text": "Quantum Circuits for Elementary Cellular Automata. In this paper we identify full list of Elementary Cellular Automata rules which can be simulated using a quantum circuit (there are 22 such rules). For every such rule we present quantum circuit implementing it with $O(N)$ gates.",
    "original_url": "http://arxiv.org/pdf/2501.06648v1",
    "original_title": "Quantum Circuits for Elementary Cellular Automata",
    "source": "arxiv",
    "authors": [
      "Dmytro Fedoriaka"
    ],
    "published": "2025-01-11T22:05:46+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2501.06648v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2203.05551v3_chunk_0",
    "chunk_text": "Cellular automata can classify data by inducing trajectory phase coexistence. We show that cellular automata can classify data by inducing a form of dynamical phase coexistence. We use Monte Carlo methods to search for general two-dimensional deterministic automata that classify images on the basis of activity, the number of state changes that occur in a trajectory initiated from the image. When the number of timesteps of the automaton is a trainable parameter, the search scheme identifies automata that generate a population of dynamical trajectories displaying high or low activity, depending on initial conditions. Automata of this nature behave as nonlinear activation functions with an output that is effectively binary, resembling an emergent version of a spiking neuron.",
    "original_url": "http://arxiv.org/pdf/2203.05551v3",
    "original_title": "Cellular automata can classify data by inducing trajectory phase coexistence",
    "source": "arxiv",
    "authors": [
      "Stephen Whitelam",
      "Isaac Tamblyn"
    ],
    "published": "2022-03-10T18:57:27+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2203.05551v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2203.05551v3_chunk_1",
    "chunk_text": "Automata of this nature behave as nonlinear activation functions with an output that is effectively binary, resembling an emergent version of a spiking neuron.",
    "original_url": "http://arxiv.org/pdf/2203.05551v3",
    "original_title": "Cellular automata can classify data by inducing trajectory phase coexistence",
    "source": "arxiv",
    "authors": [
      "Stephen Whitelam",
      "Isaac Tamblyn"
    ],
    "published": "2022-03-10T18:57:27+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2203.05551v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1806.04932v2_chunk_0",
    "chunk_text": "Reservoir Computing Hardware with Cellular Automata. Elementary cellular automata (ECA) is a widely studied one-dimensional processing methodology where the successive iteration of the automaton may lead to the recreation of a rich pattern dynamic. Recently, cellular automata have been proposed as a feasible way to implement Reservoir Computing (RC) systems in which the automata rule is fixed and the training is performed using a linear regression. In this work we perform an exhaustive study of the performance of the different ECA rules when applied to pattern recognition of time-independent input signals using a RC scheme. Once the different ECA rules have been tested, the most accurate one (rule 90) is selected to implement a digital circuit.",
    "original_url": "http://arxiv.org/pdf/1806.04932v2",
    "original_title": "Reservoir Computing Hardware with Cellular Automata",
    "source": "arxiv",
    "authors": [
      "Alejandro Morán",
      "Christiam F. Frasser",
      "Josep L. Rosselló"
    ],
    "published": "2018-06-13T10:28:44+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1806.04932v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1806.04932v2_chunk_1",
    "chunk_text": "Once the different ECA rules have been tested, the most accurate one (rule 90) is selected to implement a digital circuit. Rule 90 is easily reproduced using a reduced set of XOR gates and shift-registers, thus representing a high-performance alternative for RC hardware implementation in terms of processing time, circuit area, power dissipation and system accuracy. The model (both in software and its hardware implementation) has been tested using a pattern recognition task of handwritten numbers (the MNIST database) for which we obtained competitive results in terms of accuracy, speed and power dissipation. The proposed model can be considered to be a low-cost method to implement fast pattern recognition digital circuits.",
    "original_url": "http://arxiv.org/pdf/1806.04932v2",
    "original_title": "Reservoir Computing Hardware with Cellular Automata",
    "source": "arxiv",
    "authors": [
      "Alejandro Morán",
      "Christiam F. Frasser",
      "Josep L. Rosselló"
    ],
    "published": "2018-06-13T10:28:44+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1806.04932v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/quant-ph/0307176v1_chunk_0",
    "chunk_text": "Quantum Cellular Automata from Lattice Field Theories. We apply the methods of lattice field theories to the quantization of cellular automata. We discuss the quantization of five main categories of cellular automata: bosonic, fermionic, supersymmetric, spin and quantum dot using path integral and operator formalisms of lattice field theories. We show that the quantization of supersymmetric cellular automata is related to recently discussed string bit models of Thorn and Bergman and represents a link of cellular automata theory to fundamental physics. We discuss spin and quantum dot cellular automata for their importance in experimental realizations and their use in quantum computation.",
    "original_url": "http://arxiv.org/pdf/quant-ph/0307176v1",
    "original_title": "Quantum Cellular Automata from Lattice Field Theories",
    "source": "arxiv",
    "authors": [
      "Michael McGuigan"
    ],
    "published": "2003-07-24T20:09:01+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/quant-ph/0307176v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/quant-ph/0307176v1_chunk_1",
    "chunk_text": "We discuss spin and quantum dot cellular automata for their importance in experimental realizations and their use in quantum computation. Previous studies of quantum cellular automata utilize the wave function values as cell contents and the discretized linear Dirac equation as an update equation. We show that our approach to the quantization of fermionic cellular automata includes this utilization as a field equation, and in addition allows for nonlinearity through lattice field interactions.",
    "original_url": "http://arxiv.org/pdf/quant-ph/0307176v1",
    "original_title": "Quantum Cellular Automata from Lattice Field Theories",
    "source": "arxiv",
    "authors": [
      "Michael McGuigan"
    ],
    "published": "2003-07-24T20:09:01+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/quant-ph/0307176v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/0711.1349v1_chunk_0",
    "chunk_text": "Enumeration of number-conserving cellular automata rules with two inputs. We show that there exists a one-to-one correspondence between the set of number-conserving cellular automata (CA) with $q$ inputs and the set of balanced sequences with $q$ terms. This allows to enumerate number-conserving CA. We also show that number-conserving rules are becoming increasingly rare as the number of states increases.",
    "original_url": "http://arxiv.org/pdf/0711.1349v1",
    "original_title": "Enumeration of number-conserving cellular automata rules with two inputs",
    "source": "arxiv",
    "authors": [
      "Henryk Fuks",
      "Kate Sullivan"
    ],
    "published": "2007-11-08T20:22:28+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/0711.1349v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/0811.1513v1_chunk_0",
    "chunk_text": "Issues on drawing the State Transition Diagram for arbitrary Cellular Automata. This paper proposes several algorithms and their Cellular Automata Machine (CAM) for drawing the State Transition Diagram (STD) of an arbitrary Cellular Automata (CA) Rule (any neighborhood, uniform/ hybrid and null/ periodic boundary) and length of the CA n. It also discusses the novelty, hardware cost and the complexities of these algorithms.",
    "original_url": "http://arxiv.org/pdf/0811.1513v1",
    "original_title": "Issues on drawing the State Transition Diagram for arbitrary Cellular Automata",
    "source": "arxiv",
    "authors": [
      "Sudhakar Sahoo",
      "Pabitra Pal Choudhury"
    ],
    "published": "2008-11-10T16:38:25+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/0811.1513v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1406.0792v2_chunk_0",
    "chunk_text": "A guided tour of asynchronous cellular automata. Research on asynchronous cellular automata has received a great amount of attention these last years and has turned to a thriving field. We survey the recent research that has been carried out on this topic and present a wide state of the art where computing and modelling issues are both represented.",
    "original_url": "http://arxiv.org/pdf/1406.0792v2",
    "original_title": "A guided tour of asynchronous cellular automata",
    "source": "arxiv",
    "authors": [
      "Nazim Fatès"
    ],
    "published": "2014-06-03T17:23:04+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1406.0792v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1807.05067v1_chunk_0",
    "chunk_text": "Regional Control of Probabilistic Cellular Automata. Probabilistic Cellular Automata are extended stochastic systems, widely used for modelling phenomena in many disciplines. The possibility of controlling their behaviour is therefore an important topic. We shall present here an approach to the problem of controlling such systems by acting only on the boundary of a target region.",
    "original_url": "http://arxiv.org/pdf/1807.05067v1",
    "original_title": "Regional Control of Probabilistic Cellular Automata",
    "source": "arxiv",
    "authors": [
      "Franco Bagnoli",
      "Sara Dridi",
      "Samira El Yacoubi",
      "Raul Rechtman"
    ],
    "published": "2018-07-11T05:54:07+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1807.05067v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2201.12360v2_chunk_0",
    "chunk_text": "Variational Neural Cellular Automata. In nature, the process of cellular growth and differentiation has lead to an amazing diversity of organisms -- algae, starfish, giant sequoia, tardigrades, and orcas are all created by the same generative process. Inspired by the incredible diversity of this biological generative process, we propose a generative model, the Variational Neural Cellular Automata (VNCA), which is loosely inspired by the biological processes of cellular growth and differentiation. Unlike previous related works, the VNCA is a proper probabilistic generative model, and we evaluate it according to best practices. We find that the VNCA learns to reconstruct samples well and that despite its relatively few parameters and simple local-only communication, the VNCA can learn to generate a large variety of output from information encoded in a common vector format.",
    "original_url": "http://arxiv.org/pdf/2201.12360v2",
    "original_title": "Variational Neural Cellular Automata",
    "source": "arxiv",
    "authors": [
      "Rasmus Berg Palm",
      "Miguel González-Duque",
      "Shyam Sudhakaran",
      "Sebastian Risi"
    ],
    "published": "2022-01-28T11:41:53+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2201.12360v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2201.12360v2_chunk_1",
    "chunk_text": "We find that the VNCA learns to reconstruct samples well and that despite its relatively few parameters and simple local-only communication, the VNCA can learn to generate a large variety of output from information encoded in a common vector format. While there is a significant gap to the current state-of-the-art in terms of generative modeling performance, we show that the VNCA can learn a purely self-organizing generative process of data. Additionally, we show that the VNCA can learn a distribution of stable attractors that can recover from significant damage.",
    "original_url": "http://arxiv.org/pdf/2201.12360v2",
    "original_title": "Variational Neural Cellular Automata",
    "source": "arxiv",
    "authors": [
      "Rasmus Berg Palm",
      "Miguel González-Duque",
      "Shyam Sudhakaran",
      "Sebastian Risi"
    ],
    "published": "2022-01-28T11:41:53+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2201.12360v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/cond-mat/0005038v1_chunk_0",
    "chunk_text": "Quantum Cellular Neural Networks. We have previously proposed a way of using coupled quantum dots to construct digital computing elements - quantum-dot cellular automata (QCA). Here we consider a different approach to using coupled quantum-dot cells in an architecture which, rather that reproducing Boolean logic, uses a physical near-neighbor connectivity to construct an analog Cellular Neural Network (CNN).",
    "original_url": "http://arxiv.org/pdf/cond-mat/0005038v1",
    "original_title": "Quantum Cellular Neural Networks",
    "source": "arxiv",
    "authors": [
      "Geza Toth",
      "Craig S. Lent",
      "P. Douglas Tougaw",
      "Yuriy Brazhnik",
      "Weiwen Weng",
      "Wolfgang Porod",
      "Ruey-Wen Liu",
      "Yih-Fang Huang"
    ],
    "published": "2000-05-02T05:35:35+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/cond-mat/0005038v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1205.6785v1_chunk_0",
    "chunk_text": "Cellular automata on regular rooted trees. We study cellular automata on regular rooted trees. This includes the characterization of sofic tree shifts in terms of unrestricted Rabin automata and the decidability of the surjectivity problem for cellular automata between sofic tree shifts.",
    "original_url": "http://arxiv.org/pdf/1205.6785v1",
    "original_title": "Cellular automata on regular rooted trees",
    "source": "arxiv",
    "authors": [
      "Tullio Ceccherini-Silberstein",
      "Michel Coornaert",
      "Francesca Fiorenzi",
      "Zoran Sunic"
    ],
    "published": "2012-05-30T19:34:26+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1205.6785v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2202.13809v1_chunk_0",
    "chunk_text": "A natural class of cellular automata containing fractional multiplication automata, Rule 30, and others. We define the class of rapidly left expansive cellular automata, which contains fractional multiplication automata, Wolfram's Rule 30, and many others. The definition has been shaped by a proposition of Jen on aperiodicity of columns in space-time diagrams of certain cellular automata, which generalizes to this new class. We also present results that originate from the theory of distribution modulo 1.",
    "original_url": "http://arxiv.org/pdf/2202.13809v1",
    "original_title": "A natural class of cellular automata containing fractional multiplication automata, Rule 30, and others",
    "source": "arxiv",
    "authors": [
      "Johan Kopra"
    ],
    "published": "2022-02-28T14:15:44+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2202.13809v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/nlin/0411063v1_chunk_0",
    "chunk_text": "Detecting synchronization in spatially extended discrete systems by complexity measurements. The synchronization of two stochastically coupled one-dimensional cellular automata (CA) is analyzed. It is shown that the transition to synchronization is characterized by a dramatic increase of the statistical complexity of the patterns generated by the difference automaton. This singular behavior is verified to be present in several CA rules displaying complex behavior.",
    "original_url": "http://arxiv.org/pdf/nlin/0411063v1",
    "original_title": "Detecting synchronization in spatially extended discrete systems by complexity measurements",
    "source": "arxiv",
    "authors": [
      "Juan R. Sánchez",
      "Ricardo López-Ruiz"
    ],
    "published": "2004-11-30T12:29:12+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/nlin/0411063v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1909.03560v1_chunk_0",
    "chunk_text": "Evolving Order and Chaos: Comparing Particle Swarm Optimization and Genetic Algorithms for Global Coordination of Cellular Automata. We apply two evolutionary search algorithms: Particle Swarm Optimization (PSO) and Genetic Algorithms (GAs) to the design of Cellular Automata (CA) that can perform computational tasks requiring global coordination. In particular, we compare search efficiency for PSO and GAs applied to both the density classification problem and to the novel generation of 'chaotic' CA. Our work furthermore introduces a new variant of PSO, the Binary Global-Local PSO (BGL-PSO).",
    "original_url": "http://arxiv.org/pdf/1909.03560v1",
    "original_title": "Evolving Order and Chaos: Comparing Particle Swarm Optimization and Genetic Algorithms for Global Coordination of Cellular Automata",
    "source": "arxiv",
    "authors": [
      "Anthony D. Rhodes"
    ],
    "published": "2019-09-08T22:57:10+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1909.03560v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2210.06052v1_chunk_0",
    "chunk_text": "System theoretic approach of information processing in nested cellular automata. The subject of this paper is the evolution of the concept of information processing in regular structures based on multi-level processing in nested cellular automata. The essence of the proposed model is a discrete space-time containing nested orthogonal space-times at its points. The factorization of the function describing the global behavior of a system is the key element of the mathematical description. Factorization describes the relations of physical connections, signal propagation times and signal processing to global behavior.",
    "original_url": "http://arxiv.org/pdf/2210.06052v1",
    "original_title": "System theoretic approach of information processing in nested cellular automata",
    "source": "arxiv",
    "authors": [
      "Jerzy Szynka"
    ],
    "published": "2022-10-12T09:45:45+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2210.06052v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2210.06052v1_chunk_1",
    "chunk_text": "Factorization describes the relations of physical connections, signal propagation times and signal processing to global behavior. In the model appear expressions similar to expressions used in the Special Relativity Theory.",
    "original_url": "http://arxiv.org/pdf/2210.06052v1",
    "original_title": "System theoretic approach of information processing in nested cellular automata",
    "source": "arxiv",
    "authors": [
      "Jerzy Szynka"
    ],
    "published": "2022-10-12T09:45:45+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2210.06052v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2407.02175v1_chunk_0",
    "chunk_text": "Interaction spaces: towards a universal mathematical theory of complex systems. We present the first steps of interaction spaces theory, a universal mathematical theory of complex systems which is able to embed cellular automata, agent based models, master equation based models, stochastic or deterministic, continuous or discrete dynamical systems, networked dynamical models, artificial neural networks and genetic algorithms in a single notion. Therefore, interaction spaces represent a common mathematical language that can be used to describe several complex systems modeling frameworks. This is the first step to start a mathematical theory of complex systems. Every notion is introduced both using an intuitive description by listing lots of examples, and using a modern mathematical language.",
    "original_url": "http://arxiv.org/pdf/2407.02175v1",
    "original_title": "Interaction spaces: towards a universal mathematical theory of complex systems",
    "source": "arxiv",
    "authors": [
      "Paolo Giordano"
    ],
    "published": "2024-07-02T11:30:41+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2407.02175v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2407.02175v1_chunk_1",
    "chunk_text": "Every notion is introduced both using an intuitive description by listing lots of examples, and using a modern mathematical language.",
    "original_url": "http://arxiv.org/pdf/2407.02175v1",
    "original_title": "Interaction spaces: towards a universal mathematical theory of complex systems",
    "source": "arxiv",
    "authors": [
      "Paolo Giordano"
    ],
    "published": "2024-07-02T11:30:41+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2407.02175v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2404.06406v3_chunk_0",
    "chunk_text": "Emergent Dynamics in Neural Cellular Automata. Neural Cellular Automata (NCA) models are trainable variations of traditional Cellular Automata (CA). Emergent motion in the patterns created by NCA has been successfully applied to synthesize dynamic textures. However, the conditions required for an NCA to display dynamic patterns remain unexplored. Here, we investigate the relationship between the NCA architecture and the emergent dynamics of the trained models.",
    "original_url": "http://arxiv.org/pdf/2404.06406v3",
    "original_title": "Emergent Dynamics in Neural Cellular Automata",
    "source": "arxiv",
    "authors": [
      "Yitao Xu",
      "Ehsan Pajouheshgar",
      "Sabine Süsstrunk"
    ],
    "published": "2024-04-09T15:54:03+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2404.06406v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2404.06406v3_chunk_1",
    "chunk_text": "Here, we investigate the relationship between the NCA architecture and the emergent dynamics of the trained models. Specifically, we vary the number of channels in the cell state and the number of hidden neurons in the MultiLayer Perceptron (MLP), and draw a relationship between the combination of these two variables and the motion strength between successive frames. Our analysis reveals that the disparity and proportionality between these two variables have a strong correlation with the emergent dynamics in the NCA output. We thus propose a design principle for creating dynamic NCA.",
    "original_url": "http://arxiv.org/pdf/2404.06406v3",
    "original_title": "Emergent Dynamics in Neural Cellular Automata",
    "source": "arxiv",
    "authors": [
      "Yitao Xu",
      "Ehsan Pajouheshgar",
      "Sabine Süsstrunk"
    ],
    "published": "2024-04-09T15:54:03+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2404.06406v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2304.11209v1_chunk_0",
    "chunk_text": "Dissipative quantum many-body dynamics in (1+1)D quantum cellular automata and quantum neural networks. Classical artificial neural networks, built from perceptrons as their elementary units, possess enormous expressive power. Here we investigate a quantum neural network architecture, which follows a similar paradigm. It is structurally equivalent to so-called (1+1)D quantum cellular automata, which are two-dimensional quantum lattice systems on which dynamics takes place in discrete time. Information transfer between consecutive time slices -- or adjacent network layers -- is governed by local quantum gates, which can be regarded as the quantum counterpart of the classical perceptrons.",
    "original_url": "http://arxiv.org/pdf/2304.11209v1",
    "original_title": "Dissipative quantum many-body dynamics in (1+1)D quantum cellular automata and quantum neural networks",
    "source": "arxiv",
    "authors": [
      "Mario Boneberg",
      "Federico Carollo",
      "Igor Lesanovsky"
    ],
    "published": "2023-04-21T18:47:07+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2304.11209v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2304.11209v1_chunk_1",
    "chunk_text": "Information transfer between consecutive time slices -- or adjacent network layers -- is governed by local quantum gates, which can be regarded as the quantum counterpart of the classical perceptrons. Along the time-direction an effective dissipative evolution emerges on the level of the reduced state, and the nature of this dynamics is dictated by the structure of the elementary gates. We show how to construct the local unitary gates to yield a desired many-body dynamics, which in certain parameter regimes is governed by a Lindblad master equation. We study this for small system sizes through numerical simulations and demonstrate how collective effects within the quantum cellular automaton can be controlled parametrically. Our study constitutes a step towards the utilisation of large-scale emergent phenomena in large quantum neural networks for machine learning purposes.",
    "original_url": "http://arxiv.org/pdf/2304.11209v1",
    "original_title": "Dissipative quantum many-body dynamics in (1+1)D quantum cellular automata and quantum neural networks",
    "source": "arxiv",
    "authors": [
      "Mario Boneberg",
      "Federico Carollo",
      "Igor Lesanovsky"
    ],
    "published": "2023-04-21T18:47:07+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2304.11209v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2304.11209v1_chunk_2",
    "chunk_text": "Our study constitutes a step towards the utilisation of large-scale emergent phenomena in large quantum neural networks for machine learning purposes.",
    "original_url": "http://arxiv.org/pdf/2304.11209v1",
    "original_title": "Dissipative quantum many-body dynamics in (1+1)D quantum cellular automata and quantum neural networks",
    "source": "arxiv",
    "authors": [
      "Mario Boneberg",
      "Federico Carollo",
      "Igor Lesanovsky"
    ],
    "published": "2023-04-21T18:47:07+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2304.11209v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1001.5471v2_chunk_0",
    "chunk_text": "Bulking II: Classifications of Cellular Automata. This paper is the second part of a series of two papers dealing with bulking: a way to define quasi-order on cellular automata by comparing space-time diagrams up to rescaling. In the present paper, we introduce three notions of simulation between cellular automata and study the quasi-order structures induced by these simulation relations on the whole set of cellular automata. Various aspects of these quasi-orders are considered (induced equivalence relations, maximum elements, induced orders, etc) providing several formal tools allowing to classify cellular automata.",
    "original_url": "http://arxiv.org/pdf/1001.5471v2",
    "original_title": "Bulking II: Classifications of Cellular Automata",
    "source": "arxiv",
    "authors": [
      "Marianne Delorme",
      "Jacques Mazoyer",
      "Nicolas Ollinger",
      "Guillaume Theyssier"
    ],
    "published": "2010-01-29T20:15:23+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1001.5471v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2407.06175v2_chunk_0",
    "chunk_text": "Classification of Cellular Automata based on the Hamming distance. Elementary cellular automata are the simplest form of cellular automata, studied extensively by Wolfram in the 1980s. He discovered complex behavior in some of these automata and developed a classification for all cellular automata based on their phenomenology. In this paper, we present an algorithm to classify them more effectively by measuring difference patterns using the Hamming distance. Our classification aligns with Wolfram's and further categorizes them into additional subclasses.",
    "original_url": "http://arxiv.org/pdf/2407.06175v2",
    "original_title": "Classification of Cellular Automata based on the Hamming distance",
    "source": "arxiv",
    "authors": [
      "Gaspar Alfaro",
      "Miguel A. F. Sanjuán"
    ],
    "published": "2024-07-08T17:49:56+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2407.06175v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2407.06175v2_chunk_1",
    "chunk_text": "Our classification aligns with Wolfram's and further categorizes them into additional subclasses.",
    "original_url": "http://arxiv.org/pdf/2407.06175v2",
    "original_title": "Classification of Cellular Automata based on the Hamming distance",
    "source": "arxiv",
    "authors": [
      "Gaspar Alfaro",
      "Miguel A. F. Sanjuán"
    ],
    "published": "2024-07-08T17:49:56+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2407.06175v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1311.4297v2_chunk_0",
    "chunk_text": "On generalization of reversible second-order cellular automata. A cellular automaton with $n$ states may be used for construction of reversible second-order cellular automaton with $n^2$ states. Reversible cellular automata with hidden parameters discussed in this paper are generalization of such construction and may have number of states $N=n m$ with arbitrary $m$. Further modification produces reversible cellular automata with reduced number of states $N' < N = n m$.",
    "original_url": "http://arxiv.org/pdf/1311.4297v2",
    "original_title": "On generalization of reversible second-order cellular automata",
    "source": "arxiv",
    "authors": [
      "Alexander Yu. Vlasov"
    ],
    "published": "2013-11-18T08:52:01+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1311.4297v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2309.15795v6_chunk_0",
    "chunk_text": "Triangular Automata: The 256 Elementary Cellular Automata of the 2D Plane. Triangular Automata (TA) stands for cellular automata in the triangular grid. This work focuses on the simplest type of TA called Elementary Triangular Automata (ETA). They are argued to be the two-dimensional counterpart of Wolfram's Elementary Cellular Automata. Conceptual and computational tools for their study are presented along with an initial analysis.",
    "original_url": "http://arxiv.org/pdf/2309.15795v6",
    "original_title": "Triangular Automata: The 256 Elementary Cellular Automata of the 2D Plane",
    "source": "arxiv",
    "authors": [
      "Paul Cousin"
    ],
    "published": "2023-09-26T10:44:19+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2309.15795v6"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2309.15795v6_chunk_1",
    "chunk_text": "Conceptual and computational tools for their study are presented along with an initial analysis. The paper is accompanied by a website where the results can be explored interactively. The source code is available in the form of a Mathematica package.",
    "original_url": "http://arxiv.org/pdf/2309.15795v6",
    "original_title": "Triangular Automata: The 256 Elementary Cellular Automata of the 2D Plane",
    "source": "arxiv",
    "authors": [
      "Paul Cousin"
    ],
    "published": "2023-09-26T10:44:19+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2309.15795v6"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2502.19372v1_chunk_0",
    "chunk_text": "On Cellular Automata. Cellular automata are a fundamental computational model with applications in mathematics, computer science, and physics. In this work, we explore the study of cellular automata to cases where the universe is a group, introducing the concept of \\( \\phi \\)-cellular automata. We establish new theoretical results, including a generalized Uniform Curtis-Hedlund Theorem and linear \\( \\phi \\)-cellular automata. Additionally, we define the covering map for \\( \\phi \\)-cellular automata and investigate its properties.",
    "original_url": "http://arxiv.org/pdf/2502.19372v1",
    "original_title": "On Cellular Automata",
    "source": "arxiv",
    "authors": [
      "Tawfiq Hamed",
      "Mohammad Saleh"
    ],
    "published": "2025-02-26T18:14:18+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2502.19372v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2502.19372v1_chunk_1",
    "chunk_text": "Additionally, we define the covering map for \\( \\phi \\)-cellular automata and investigate its properties. Specifically, we derive results for quotient covers when the universe of the automaton is a circulant graph. This work contributes to the algebraic and topological understanding of cellular automata, paving the way for future exploration of different types of covers and their applications to broader classes of graphs and dynamical systems.",
    "original_url": "http://arxiv.org/pdf/2502.19372v1",
    "original_title": "On Cellular Automata",
    "source": "arxiv",
    "authors": [
      "Tawfiq Hamed",
      "Mohammad Saleh"
    ],
    "published": "2025-02-26T18:14:18+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2502.19372v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2409.02740v1_chunk_0",
    "chunk_text": "Convolutional Neural Networks for Automated Cellular Automaton Classification. The emergent dynamics in spacetime diagrams of cellular automata (CAs) is often organised by means of a number of behavioural classes. Whilst classification of elementary CAs is feasible and well-studied, non-elementary CAs are generally too diverse and numerous to exhaustively classify manually. In this chapter we treat the spacetime diagram as a digital image, and implement simple computer vision techniques to perform an automated classification of elementary cellular automata into the five Li-Packard classes. In particular, we present a supervised learning task to a convolutional neural network, in such a way that it may be generalised to non-elementary CAs.",
    "original_url": "http://arxiv.org/pdf/2409.02740v1",
    "original_title": "Convolutional Neural Networks for Automated Cellular Automaton Classification",
    "source": "arxiv",
    "authors": [
      "Michiel Rollier",
      "Aisling J. Daly",
      "Jan M. Baetens"
    ],
    "published": "2024-09-04T14:21:00+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2409.02740v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2409.02740v1_chunk_1",
    "chunk_text": "In particular, we present a supervised learning task to a convolutional neural network, in such a way that it may be generalised to non-elementary CAs. If we want to do so, we must divert the algorithm's focus away from the underlying 'microscopic' local updates. We first show that previously developed deep learning approaches have in fact been trained to identify the local update rule, rather than directly focus on the mesoscopic patterns that are associated with the particular behavioural classes. By means of a well-argued neural network design, as well as a number of data augmentation techniques, we then present a convolutional neural network that performs nearly perfectly at identifying the behavioural class, without necessarily first identifying the underlying microscopic dynamics.",
    "original_url": "http://arxiv.org/pdf/2409.02740v1",
    "original_title": "Convolutional Neural Networks for Automated Cellular Automaton Classification",
    "source": "arxiv",
    "authors": [
      "Michiel Rollier",
      "Aisling J. Daly",
      "Jan M. Baetens"
    ],
    "published": "2024-09-04T14:21:00+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2409.02740v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2006.12155v3_chunk_0",
    "chunk_text": "Neural Cellular Automata Manifold. Very recently, the Neural Cellular Automata (NCA) has been proposed to simulate the morphogenesis process with deep networks. NCA learns to grow an image starting from a fixed single pixel. In this work, we show that the neural network (NN) architecture of the NCA can be encapsulated in a larger NN. This allows us to propose a new model that encodes a manifold of NCA, each of them capable of generating a distinct image.",
    "original_url": "http://arxiv.org/pdf/2006.12155v3",
    "original_title": "Neural Cellular Automata Manifold",
    "source": "arxiv",
    "authors": [
      "Alejandro Hernandez Ruiz",
      "Armand Vilalta",
      "Francesc Moreno-Noguer"
    ],
    "published": "2020-06-22T11:41:57+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2006.12155v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2006.12155v3_chunk_1",
    "chunk_text": "This allows us to propose a new model that encodes a manifold of NCA, each of them capable of generating a distinct image. Therefore, we are effectively learning an embedding space of CA, which shows generalization capabilities. We accomplish this by introducing dynamic convolutions inside an Auto-Encoder architecture, for the first time used to join two different sources of information, the encoding and cells environment information. In biological terms, our approach would play the role of the transcription factors, modulating the mapping of genes into specific proteins that drive cellular differentiation, which occurs right before the morphogenesis. We thoroughly evaluate our approach in a dataset of synthetic emojis and also in real images of CIFAR10.",
    "original_url": "http://arxiv.org/pdf/2006.12155v3",
    "original_title": "Neural Cellular Automata Manifold",
    "source": "arxiv",
    "authors": [
      "Alejandro Hernandez Ruiz",
      "Armand Vilalta",
      "Francesc Moreno-Noguer"
    ],
    "published": "2020-06-22T11:41:57+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2006.12155v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2006.12155v3_chunk_2",
    "chunk_text": "We thoroughly evaluate our approach in a dataset of synthetic emojis and also in real images of CIFAR10. Our model introduces a general-purpose network, which can be used in a broad range of problems beyond image generation.",
    "original_url": "http://arxiv.org/pdf/2006.12155v3",
    "original_title": "Neural Cellular Automata Manifold",
    "source": "arxiv",
    "authors": [
      "Alejandro Hernandez Ruiz",
      "Armand Vilalta",
      "Francesc Moreno-Noguer"
    ],
    "published": "2020-06-22T11:41:57+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2006.12155v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2407.05991v2_chunk_0",
    "chunk_text": "Multi-Texture Synthesis through Signal Responsive Neural Cellular Automata. Neural Cellular Automata (NCA) have proven to be effective in a variety of fields, with numerous biologically inspired applications. One of the fields, in which NCAs perform well is the generation of textures, modelling global patterns from local interactions governed by uniform and coherent rules. This paper aims to enhance the usability of NCAs in texture synthesis by addressing a shortcoming of current NCA architectures for texture generation, which requires separately trained NCA for each individual texture. In this work, we train a single NCA for the evolution of multiple textures, based on individual examples.",
    "original_url": "http://arxiv.org/pdf/2407.05991v2",
    "original_title": "Multi-Texture Synthesis through Signal Responsive Neural Cellular Automata",
    "source": "arxiv",
    "authors": [
      "Mirela-Magdalena Catrina",
      "Ioana Cristina Plajer",
      "Alexandra Baicoianu"
    ],
    "published": "2024-07-08T14:36:20+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2407.05991v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2407.05991v2_chunk_1",
    "chunk_text": "In this work, we train a single NCA for the evolution of multiple textures, based on individual examples. Our solution provides texture information in the state of each cell, in the form of an internally coded genomic signal, which enables the NCA to generate the expected texture. Such a neural cellular automaton not only maintains its regenerative capability but also allows for interpolation between learned textures and supports grafting techniques. This demonstrates the ability to edit generated textures and the potential for them to merge and coexist within the same automaton. We also address questions related to the influence of the genomic information and the cost function on the evolution of the NCA.",
    "original_url": "http://arxiv.org/pdf/2407.05991v2",
    "original_title": "Multi-Texture Synthesis through Signal Responsive Neural Cellular Automata",
    "source": "arxiv",
    "authors": [
      "Mirela-Magdalena Catrina",
      "Ioana Cristina Plajer",
      "Alexandra Baicoianu"
    ],
    "published": "2024-07-08T14:36:20+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2407.05991v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2407.05991v2_chunk_2",
    "chunk_text": "We also address questions related to the influence of the genomic information and the cost function on the evolution of the NCA.",
    "original_url": "http://arxiv.org/pdf/2407.05991v2",
    "original_title": "Multi-Texture Synthesis through Signal Responsive Neural Cellular Automata",
    "source": "arxiv",
    "authors": [
      "Mirela-Magdalena Catrina",
      "Ioana Cristina Plajer",
      "Alexandra Baicoianu"
    ],
    "published": "2024-07-08T14:36:20+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2407.05991v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/adap-org/9809001v1_chunk_0",
    "chunk_text": "The Evolutionary Design of Collective Computation in Cellular Automata. We investigate the ability of a genetic algorithm to design cellular automata that perform computations. The computational strategies of the resulting cellular automata can be understood using a framework in which ``particles'' embedded in space-time configurations carry information and interactions between particles effect information processing. This structural analysis can also be used to explain the evolutionary process by which the strategies were designed by the genetic algorithm. More generally, our goals are to understand how machine-learning processes can design complex decentralized systems with sophisticated collective computational abilities and to develop rigorous frameworks for understanding how the resulting dynamical systems perform computation.",
    "original_url": "http://arxiv.org/pdf/adap-org/9809001v1",
    "original_title": "The Evolutionary Design of Collective Computation in Cellular Automata",
    "source": "arxiv",
    "authors": [
      "James P. Crutchfield",
      "Melanie Mitchell",
      "Rajarshi Das"
    ],
    "published": "1998-09-08T21:42:55+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/adap-org/9809001v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/adap-org/9809001v1_chunk_1",
    "chunk_text": "More generally, our goals are to understand how machine-learning processes can design complex decentralized systems with sophisticated collective computational abilities and to develop rigorous frameworks for understanding how the resulting dynamical systems perform computation.",
    "original_url": "http://arxiv.org/pdf/adap-org/9809001v1",
    "original_title": "The Evolutionary Design of Collective Computation in Cellular Automata",
    "source": "arxiv",
    "authors": [
      "James P. Crutchfield",
      "Melanie Mitchell",
      "Rajarshi Das"
    ],
    "published": "1998-09-08T21:42:55+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/adap-org/9809001v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/cond-mat/9812361v1_chunk_0",
    "chunk_text": "Competing Species Dynamics: Qualitative Advantage versus Geography. A simple cellular automata model for a two-group war over the same territory is presented. It is shown that a qualitative advantage is not enough for a minority to win. A spatial organization as well a definite degree of aggressiveness are instrumental to overcome a less fitted majority. The model applies to a large spectrum of competing groups: smoker-non smoker war, epidemic spreading, opinion formation, competition for industrial standards and species evolution.",
    "original_url": "http://arxiv.org/pdf/cond-mat/9812361v1",
    "original_title": "Competing Species Dynamics: Qualitative Advantage versus Geography",
    "source": "arxiv",
    "authors": [
      "Serge Galam",
      "Bastien Chopard",
      "Alexander Masselot",
      "Michel Droz"
    ],
    "published": "1998-12-22T10:30:16+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/cond-mat/9812361v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/cond-mat/9812361v1_chunk_1",
    "chunk_text": "The model applies to a large spectrum of competing groups: smoker-non smoker war, epidemic spreading, opinion formation, competition for industrial standards and species evolution. In the last case, it provides a new explanation for punctuated equilibria.",
    "original_url": "http://arxiv.org/pdf/cond-mat/9812361v1",
    "original_title": "Competing Species Dynamics: Qualitative Advantage versus Geography",
    "source": "arxiv",
    "authors": [
      "Serge Galam",
      "Bastien Chopard",
      "Alexander Masselot",
      "Michel Droz"
    ],
    "published": "1998-12-22T10:30:16+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/cond-mat/9812361v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/nlin/0004012v1_chunk_0",
    "chunk_text": "Deterministic site exchange cellular automata model for the spread of diseases in human settlements. A cellular automata model that describes as limit cases of his parameters the spread of contagious diseases modeled by systems of ordinary or partial differential equations is developed. Periodic features of the behavior of human settlement are considered. The model is built taking into account the range of motion of the elements of population. For small (large) values of this range, the behaviors described by partial (ordinary) differential equation models are reproduced.",
    "original_url": "http://arxiv.org/pdf/nlin/0004012v1",
    "original_title": "Deterministic site exchange cellular automata model for the spread of diseases in human settlements",
    "source": "arxiv",
    "authors": [
      "Ricardo Mansilla",
      "Jose L. Gutierrez"
    ],
    "published": "2000-04-07T17:13:36+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/nlin/0004012v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/nlin/0004012v1_chunk_1",
    "chunk_text": "For small (large) values of this range, the behaviors described by partial (ordinary) differential equation models are reproduced. Emphasis is done in the study of those scenarios in which the above mentioned equations fail to describe. Some interesting results in these cases are reported.",
    "original_url": "http://arxiv.org/pdf/nlin/0004012v1",
    "original_title": "Deterministic site exchange cellular automata model for the spread of diseases in human settlements",
    "source": "arxiv",
    "authors": [
      "Ricardo Mansilla",
      "Jose L. Gutierrez"
    ],
    "published": "2000-04-07T17:13:36+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/nlin/0004012v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1209.2668v1_chunk_0",
    "chunk_text": "On diversity of configurations generated by excitable cellular automata with dynamical excitation intervals. Excitable cellular automata with dynamical excitation interval exhibit a wide range of space-time dynamics based on an interplay between propagating excitation patterns which modify excitability of the automaton cells. Such interactions leads to formation of standing domains of excitation, stationary waves and localised excitations. We analysed morphological and generative diversities of the functions studied and characterised the functions with highest values of the diversities. Amongst other intriguing discoveries we found that upper boundary of excitation interval more significantly affects morphological diversity of configurations generated than lower boundary of the interval does and there is no match between functions which produce configurations of excitation with highest morphological diversity and configurations of interval boundaries with highest morphological diversity.",
    "original_url": "http://arxiv.org/pdf/1209.2668v1",
    "original_title": "On diversity of configurations generated by excitable cellular automata with dynamical excitation intervals",
    "source": "arxiv",
    "authors": [
      "Andrew Adamatzky"
    ],
    "published": "2012-09-12T17:06:51+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1209.2668v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1209.2668v1_chunk_1",
    "chunk_text": "Amongst other intriguing discoveries we found that upper boundary of excitation interval more significantly affects morphological diversity of configurations generated than lower boundary of the interval does and there is no match between functions which produce configurations of excitation with highest morphological diversity and configurations of interval boundaries with highest morphological diversity. Potential directions of future studies of excitable media with dynamically changing excitability may focus on relations of the automaton model with living excitable media, e.g. neural tissue and muscles, novel materials with memristive properties, and networks of conductive polymers.",
    "original_url": "http://arxiv.org/pdf/1209.2668v1",
    "original_title": "On diversity of configurations generated by excitable cellular automata with dynamical excitation intervals",
    "source": "arxiv",
    "authors": [
      "Andrew Adamatzky"
    ],
    "published": "2012-09-12T17:06:51+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1209.2668v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1911.01086v2_chunk_0",
    "chunk_text": "Evolving Structures in Complex Systems. In this paper we propose an approach for measuring growth of complexity of emerging patterns in complex systems such as cellular automata. We discuss several ways how a metric for measuring the complexity growth can be defined. This includes approaches based on compression algorithms and artificial neural networks. We believe such a metric can be useful for designing systems that could exhibit open-ended evolution, which itself might be a prerequisite for development of general artificial intelligence.",
    "original_url": "http://arxiv.org/pdf/1911.01086v2",
    "original_title": "Evolving Structures in Complex Systems",
    "source": "arxiv",
    "authors": [
      "Hugo Cisneros",
      "Josef Sivic",
      "Tomas Mikolov"
    ],
    "published": "2019-11-04T09:35:54+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1911.01086v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1911.01086v2_chunk_1",
    "chunk_text": "We believe such a metric can be useful for designing systems that could exhibit open-ended evolution, which itself might be a prerequisite for development of general artificial intelligence. We conduct experiments on 1D and 2D grid worlds and demonstrate that using the proposed metric we can automatically construct computational models with emerging properties similar to those found in the Conway's Game of Life, as well as many other emergent phenomena. Interestingly, some of the patterns we observe resemble forms of artificial life. Our metric of structural complexity growth can be applied to a wide range of complex systems, as it is not limited to cellular automata.",
    "original_url": "http://arxiv.org/pdf/1911.01086v2",
    "original_title": "Evolving Structures in Complex Systems",
    "source": "arxiv",
    "authors": [
      "Hugo Cisneros",
      "Josef Sivic",
      "Tomas Mikolov"
    ],
    "published": "2019-11-04T09:35:54+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1911.01086v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2108.01573v1_chunk_0",
    "chunk_text": "Classification of Discrete Dynamical Systems Based on Transients. In order to develop systems capable of artificial evolution, we need to identify which systems can produce complex behavior. We present a novel classification method applicable to any class of deterministic discrete space and time dynamical systems. The method is based on classifying the asymptotic behavior of the average computation time in a given system before entering a loop. We were able to identify a critical region of behavior that corresponds to a phase transition from ordered behavior to chaos across various classes of dynamical systems.",
    "original_url": "http://arxiv.org/pdf/2108.01573v1",
    "original_title": "Classification of Discrete Dynamical Systems Based on Transients",
    "source": "arxiv",
    "authors": [
      "Barbora Hudcová",
      "Tomáš Mikolov"
    ],
    "published": "2021-08-03T15:34:01+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2108.01573v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2108.01573v1_chunk_1",
    "chunk_text": "We were able to identify a critical region of behavior that corresponds to a phase transition from ordered behavior to chaos across various classes of dynamical systems. To show that our approach can be applied to many different computational systems, we demonstrate the results of classifying cellular automata, Turing machines, and random Boolean networks. Further, we use this method to classify 2D cellular automata to automatically find those with interesting, complex dynamics. We believe that our work can be used to design systems in which complex structures emerge. Also, it can be used to compare various versions of existing attempts to model open-ended evolution (Ray (1991), Ofria et al.",
    "original_url": "http://arxiv.org/pdf/2108.01573v1",
    "original_title": "Classification of Discrete Dynamical Systems Based on Transients",
    "source": "arxiv",
    "authors": [
      "Barbora Hudcová",
      "Tomáš Mikolov"
    ],
    "published": "2021-08-03T15:34:01+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2108.01573v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2108.01573v1_chunk_2",
    "chunk_text": "Also, it can be used to compare various versions of existing attempts to model open-ended evolution (Ray (1991), Ofria et al. (2004), Channon (2006)).",
    "original_url": "http://arxiv.org/pdf/2108.01573v1",
    "original_title": "Classification of Discrete Dynamical Systems Based on Transients",
    "source": "arxiv",
    "authors": [
      "Barbora Hudcová",
      "Tomáš Mikolov"
    ],
    "published": "2021-08-03T15:34:01+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2108.01573v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2208.00746v1_chunk_0",
    "chunk_text": "Assessing the robustness of critical behavior in stochastic cellular automata. There is evidence that biological systems, such as the brain, work at a critical regime robust to noise, and are therefore able to remain in it under perturbations. In this work, we address the question of robustness of critical systems to noise. In particular, we investigate the robustness of stochastic cellular automata (CAs) at criticality. A stochastic CA is one of the simplest stochastic models showing criticality.",
    "original_url": "http://arxiv.org/pdf/2208.00746v1",
    "original_title": "Assessing the robustness of critical behavior in stochastic cellular automata",
    "source": "arxiv",
    "authors": [
      "Sidney Pontes-Filho",
      "Pedro Lind",
      "Stefano Nichele"
    ],
    "published": "2022-08-01T11:08:50+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2208.00746v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2208.00746v1_chunk_1",
    "chunk_text": "A stochastic CA is one of the simplest stochastic models showing criticality. The transition state of stochastic CA is defined through a set of probabilities. We systematically perturb the probabilities of an optimal stochastic CA known to produce critical behavior, and we report that such a CA is able to remain in a critical regime up to a certain degree of noise. We present the results using error metrics of the resulting power-law fitting, such as Kolmogorov-Smirnov statistic and Kullback-Leibler divergence. We discuss the implication of our results in regards to future realization of brain-inspired artificial intelligence systems.",
    "original_url": "http://arxiv.org/pdf/2208.00746v1",
    "original_title": "Assessing the robustness of critical behavior in stochastic cellular automata",
    "source": "arxiv",
    "authors": [
      "Sidney Pontes-Filho",
      "Pedro Lind",
      "Stefano Nichele"
    ],
    "published": "2022-08-01T11:08:50+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2208.00746v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2208.00746v1_chunk_2",
    "chunk_text": "We discuss the implication of our results in regards to future realization of brain-inspired artificial intelligence systems.",
    "original_url": "http://arxiv.org/pdf/2208.00746v1",
    "original_title": "Assessing the robustness of critical behavior in stochastic cellular automata",
    "source": "arxiv",
    "authors": [
      "Sidney Pontes-Filho",
      "Pedro Lind",
      "Stefano Nichele"
    ],
    "published": "2022-08-01T11:08:50+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2208.00746v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2304.05639v1_chunk_0",
    "chunk_text": "Towards Large-Scale Simulations of Open-Ended Evolution in Continuous Cellular Automata. Inspired by biological and cultural evolution, there have been many attempts to explore and elucidate the necessary conditions for open-endedness in artificial intelligence and artificial life. Using a continuous cellular automata called Lenia as the base system, we built large-scale evolutionary simulations using parallel computing framework JAX, in order to achieve the goal of never-ending evolution of self-organizing patterns. We report a number of system design choices, including (1) implicit implementation of genetic operators, such as reproduction by pattern self-replication, and selection by differential existential success; (2) localization of genetic information; and (3) algorithms for dynamically maintenance of the localized genotypes and translation to phenotypes. Simulation results tend to go through a phase of diversity and creativity, gradually converge to domination by fast expanding patterns, presumably a optimal solution under the current design.",
    "original_url": "http://arxiv.org/pdf/2304.05639v1",
    "original_title": "Towards Large-Scale Simulations of Open-Ended Evolution in Continuous Cellular Automata",
    "source": "arxiv",
    "authors": [
      "Bert Wang-Chak Chan"
    ],
    "published": "2023-04-12T06:40:11+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2304.05639v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2304.05639v1_chunk_1",
    "chunk_text": "Simulation results tend to go through a phase of diversity and creativity, gradually converge to domination by fast expanding patterns, presumably a optimal solution under the current design. Based on our experimentation, we propose several factors that may further facilitate open-ended evolution, such as virtual environment design, mass conservation, and energy constraints.",
    "original_url": "http://arxiv.org/pdf/2304.05639v1",
    "original_title": "Towards Large-Scale Simulations of Open-Ended Evolution in Continuous Cellular Automata",
    "source": "arxiv",
    "authors": [
      "Bert Wang-Chak Chan"
    ],
    "published": "2023-04-12T06:40:11+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2304.05639v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2405.05797v1_chunk_0",
    "chunk_text": "Adaptability and Homeostasis in the Game of Life interacting with the evolved Cellular Automata. In this paper we study the emergence of homeostasis in a two-layer system of the Game of Life, in which the Game of Life in the first layer couples with another system of cellular automata in the second layer. Homeostasis is defined here as a space-time dynamic that regulates the number of cells in state-1 in the Game of Life layer. A genetic algorithm is used to evolve the rules of the second layer to control the pattern of the Game of Life. We discovered that there are two antagonistic attractors that control the numbers of cells in state-1 in the first layer.",
    "original_url": "http://arxiv.org/pdf/2405.05797v1",
    "original_title": "Adaptability and Homeostasis in the Game of Life interacting with the evolved Cellular Automata",
    "source": "arxiv",
    "authors": [
      "Keisuke Suzuki",
      "Takashi Ikegami"
    ],
    "published": "2024-05-09T14:29:23+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2405.05797v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2405.05797v1_chunk_1",
    "chunk_text": "We discovered that there are two antagonistic attractors that control the numbers of cells in state-1 in the first layer. The homeostasis sustained by these attractors are compared with the homeostatic dynamics observed in Daisy World.",
    "original_url": "http://arxiv.org/pdf/2405.05797v1",
    "original_title": "Adaptability and Homeostasis in the Game of Life interacting with the evolved Cellular Automata",
    "source": "arxiv",
    "authors": [
      "Keisuke Suzuki",
      "Takashi Ikegami"
    ],
    "published": "2024-05-09T14:29:23+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2405.05797v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2407.02613v1_chunk_0",
    "chunk_text": "Wildfire Autonomous Response and Prediction Using Cellular Automata (WARP-CA). Wildfires pose a severe challenge to ecosystems and human settlements, exacerbated by climate change and environmental factors. Traditional wildfire modeling, while useful, often fails to adapt to the rapid dynamics of such events. This report introduces the (Wildfire Autonomous Response and Prediction Using Cellular Automata) WARP-CA model, a novel approach that integrates terrain generation using Perlin noise with the dynamism of Cellular Automata (CA) to simulate wildfire spread. We explore the potential of Multi-Agent Reinforcement Learning (MARL) to manage wildfires by simulating autonomous agents, such as UAVs and UGVs, within a collaborative framework.",
    "original_url": "http://arxiv.org/pdf/2407.02613v1",
    "original_title": "Wildfire Autonomous Response and Prediction Using Cellular Automata (WARP-CA)",
    "source": "arxiv",
    "authors": [
      "Abdelrahman Ramadan"
    ],
    "published": "2024-07-02T19:01:59+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2407.02613v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2407.02613v1_chunk_1",
    "chunk_text": "We explore the potential of Multi-Agent Reinforcement Learning (MARL) to manage wildfires by simulating autonomous agents, such as UAVs and UGVs, within a collaborative framework. Our methodology combines world simulation techniques and investigates emergent behaviors in MARL, focusing on efficient wildfire suppression and considering critical environmental factors like wind patterns and terrain features.",
    "original_url": "http://arxiv.org/pdf/2407.02613v1",
    "original_title": "Wildfire Autonomous Response and Prediction Using Cellular Automata (WARP-CA)",
    "source": "arxiv",
    "authors": [
      "Abdelrahman Ramadan"
    ],
    "published": "2024-07-02T19:01:59+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2407.02613v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/physics/0507122v3_chunk_0",
    "chunk_text": "Metastable Congested States in Multisegment Traffic Cellular Automaton. We investigate a simple multisegment cellular automaton model of traffic flow. With the introduction of segment-dependent deceleration probability, metastable congested states in the intermediate density region emerge, and the initial state dependence of the flow is observed. The essential feature of three-phased structure empirically found in real-world traffic flow is reproduced without elaborate assumptions.",
    "original_url": "http://arxiv.org/pdf/physics/0507122v3",
    "original_title": "Metastable Congested States in Multisegment Traffic Cellular Automaton",
    "source": "arxiv",
    "authors": [
      "Yutaka Nishimura",
      "Taksu Cheon",
      "Petr Seba"
    ],
    "published": "2005-07-15T16:28:18+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/physics/0507122v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1905.04232v6_chunk_0",
    "chunk_text": "Automatic Programming of Cellular Automata and Artificial Neural Networks Guided by Philosophy. Many computer models such as cellular automata and artificial neural networks have been developed and successfully applied. However, in some cases, these models might be restrictive on the possible solutions or their solutions might be difficult to interpret. To overcome this problem, we outline a new approach, the so-called allagmatic method, that automatically programs and executes models with as little limitations as possible while maintaining human interpretability. Earlier we described a metamodel and its building blocks according to the philosophical concepts of structure (spatial dimension) and operation (temporal dimension).",
    "original_url": "http://arxiv.org/pdf/1905.04232v6",
    "original_title": "Automatic Programming of Cellular Automata and Artificial Neural Networks Guided by Philosophy",
    "source": "arxiv",
    "authors": [
      "Patrik Christen",
      "Olivier Del Fabbro"
    ],
    "published": "2019-05-10T16:00:09+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1905.04232v6"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1905.04232v6_chunk_1",
    "chunk_text": "Earlier we described a metamodel and its building blocks according to the philosophical concepts of structure (spatial dimension) and operation (temporal dimension). They are entity, milieu, and update function that together abstractly describe cellular automata, artificial neural networks, and possibly any kind of computer model. By automatically combining these building blocks in an evolutionary computation, interpretability might be increased by the relationship to the metamodel, and models might be translated into more interpretable models via the metamodel. We propose generic and object-oriented programming to implement the entities and their milieus as dynamic and generic arrays and the update function as a method. We show two experiments where a simple cellular automaton and an artificial neural network are automatically programmed, compiled, and executed.",
    "original_url": "http://arxiv.org/pdf/1905.04232v6",
    "original_title": "Automatic Programming of Cellular Automata and Artificial Neural Networks Guided by Philosophy",
    "source": "arxiv",
    "authors": [
      "Patrik Christen",
      "Olivier Del Fabbro"
    ],
    "published": "2019-05-10T16:00:09+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1905.04232v6"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1905.04232v6_chunk_2",
    "chunk_text": "We show two experiments where a simple cellular automaton and an artificial neural network are automatically programmed, compiled, and executed. A target state is successfully evolved and learned in the cellular automaton and artificial neural network, respectively. We conclude that the allagmatic method can create and execute cellular automaton and artificial neural network models in an automated manner with the guidance of philosophy.",
    "original_url": "http://arxiv.org/pdf/1905.04232v6",
    "original_title": "Automatic Programming of Cellular Automata and Artificial Neural Networks Guided by Philosophy",
    "source": "arxiv",
    "authors": [
      "Patrik Christen",
      "Olivier Del Fabbro"
    ],
    "published": "2019-05-10T16:00:09+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1905.04232v6"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1703.09030v2_chunk_0",
    "chunk_text": "Shift-Symmetric Configurations in Two-Dimensional Cellular Automata: Irreversibility, Insolvability, and Enumeration. The search for symmetry as an unusual yet profoundly appealing phenomenon, and the origin of regular, repeating configuration patterns have long been a central focus of complexity science and physics. To better grasp and understand symmetry of configurations in decentralized toroidal architectures, we employ group-theoretic methods, which allow us to identify and enumerate these inputs, and argue about irreversible system behaviors with undesired effects on many computational problems. The concept of so-called configuration shift-symmetry is applied to two-dimensional cellular automata as an ideal model of computation. Regardless of the transition function, the results show the universal insolvability of crucial distributed tasks, such as leader election, pattern recognition, hashing, and encryption.",
    "original_url": "http://arxiv.org/pdf/1703.09030v2",
    "original_title": "Shift-Symmetric Configurations in Two-Dimensional Cellular Automata: Irreversibility, Insolvability, and Enumeration",
    "source": "arxiv",
    "authors": [
      "Peter Banda",
      "John Caughman",
      "Martin Cenek",
      "Christof Teuscher"
    ],
    "published": "2017-03-27T12:32:26+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1703.09030v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1703.09030v2_chunk_1",
    "chunk_text": "Regardless of the transition function, the results show the universal insolvability of crucial distributed tasks, such as leader election, pattern recognition, hashing, and encryption. By using compact enumeration formulas and bounding the number of shift-symmetric configurations for a given lattice size, we efficiently calculate the probability of a configuration being shift-symmetric for a uniform or density-uniform distribution. Further, we devise an algorithm detecting the presence of shift-symmetry in a configuration. Given the resource constraints, the enumeration and probability formulas can directly help to lower the minimal expected error and provide recommendations for system's size and initialization. Besides cellular automata, the shift-symmetry analysis can be used to study the non-linear behavior in various synchronous rule-based systems that include inference engines, Boolean networks, neural networks, and systolic arrays.",
    "original_url": "http://arxiv.org/pdf/1703.09030v2",
    "original_title": "Shift-Symmetric Configurations in Two-Dimensional Cellular Automata: Irreversibility, Insolvability, and Enumeration",
    "source": "arxiv",
    "authors": [
      "Peter Banda",
      "John Caughman",
      "Martin Cenek",
      "Christof Teuscher"
    ],
    "published": "2017-03-27T12:32:26+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1703.09030v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1703.09030v2_chunk_2",
    "chunk_text": "Besides cellular automata, the shift-symmetry analysis can be used to study the non-linear behavior in various synchronous rule-based systems that include inference engines, Boolean networks, neural networks, and systolic arrays.",
    "original_url": "http://arxiv.org/pdf/1703.09030v2",
    "original_title": "Shift-Symmetric Configurations in Two-Dimensional Cellular Automata: Irreversibility, Insolvability, and Enumeration",
    "source": "arxiv",
    "authors": [
      "Peter Banda",
      "John Caughman",
      "Martin Cenek",
      "Christof Teuscher"
    ],
    "published": "2017-03-27T12:32:26+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1703.09030v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2407.18017v1_chunk_0",
    "chunk_text": "A Sensitivity Analysis of Cellular Automata and Heterogeneous Topology Networks: Partially-Local Cellular Automata and Homogeneous Homogeneous Random Boolean Networks. Elementary Cellular Automata (ECA) are a well-studied computational universe that is, despite its simple configurations, capable of impressive computational variety. Harvesting this computation in a useful way has historically shown itself to be difficult, but if combined with reservoir computing (RC), this becomes much more feasible. Furthermore, RC and ECA enable energy-efficient AI, making the combination a promising concept for Edge AI. In this work, we contrast ECA to substrates of Partially-Local CA (PLCA) and Homogeneous Homogeneous Random Boolean Networks (HHRBN).",
    "original_url": "http://arxiv.org/pdf/2407.18017v1",
    "original_title": "A Sensitivity Analysis of Cellular Automata and Heterogeneous Topology Networks: Partially-Local Cellular Automata and Homogeneous Homogeneous Random Boolean Networks",
    "source": "arxiv",
    "authors": [
      "Tom Eivind Glover",
      "Ruben Jahren",
      "Francesco Martinuzzi",
      "Pedro Gonçalves Lind",
      "Stefano Nichele"
    ],
    "published": "2024-07-25T13:08:24+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2407.18017v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2407.18017v1_chunk_1",
    "chunk_text": "In this work, we contrast ECA to substrates of Partially-Local CA (PLCA) and Homogeneous Homogeneous Random Boolean Networks (HHRBN). They are, in comparison, the topological heterogeneous counterparts of ECA. This represents a step from ECA towards more biological-plausible substrates. We analyse these substrates by testing on an RC benchmark (5-bit memory), using Temporal Derrida plots to estimate the sensitivity and assess the defect collapse rate. We find that, counterintuitively, disordered topology does not necessarily mean disordered computation.",
    "original_url": "http://arxiv.org/pdf/2407.18017v1",
    "original_title": "A Sensitivity Analysis of Cellular Automata and Heterogeneous Topology Networks: Partially-Local Cellular Automata and Homogeneous Homogeneous Random Boolean Networks",
    "source": "arxiv",
    "authors": [
      "Tom Eivind Glover",
      "Ruben Jahren",
      "Francesco Martinuzzi",
      "Pedro Gonçalves Lind",
      "Stefano Nichele"
    ],
    "published": "2024-07-25T13:08:24+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2407.18017v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2407.18017v1_chunk_2",
    "chunk_text": "We find that, counterintuitively, disordered topology does not necessarily mean disordered computation. There are countering computational \"forces\" of topology imperfections leading to a higher collapse rate (order) and yet, if accounted for, an increased sensitivity to the initial condition. These observations together suggest a shrinking critical range.",
    "original_url": "http://arxiv.org/pdf/2407.18017v1",
    "original_title": "A Sensitivity Analysis of Cellular Automata and Heterogeneous Topology Networks: Partially-Local Cellular Automata and Homogeneous Homogeneous Random Boolean Networks",
    "source": "arxiv",
    "authors": [
      "Tom Eivind Glover",
      "Ruben Jahren",
      "Francesco Martinuzzi",
      "Pedro Gonçalves Lind",
      "Stefano Nichele"
    ],
    "published": "2024-07-25T13:08:24+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2407.18017v1"
  },
  {
    "chunk_id": "https://distill.pub/_chunk_0",
    "chunk_text": "https://distill.pub/. Distill\nAbout\nPrize\nSubmit\nSept. 2, 2021\nPeer-reviewed\nUnderstanding Convolutions on Graphs\nAmeya Daigavane, Balaraman Ravindran, and Gaurav Aggarwal\nUnderstanding the building blocks and design choices of graph neural networks. Sept. 2, 2021\nPeer-reviewed\nA Gentle Introduction to Graph Neural Networks\nBenjamin Sanchez-Lengeling, Emily Reif, Adam Pearce, and Alexander B. Wiltschko\nWhat components are needed for building learning algorithms that leverage the structure and properties of graphs? July 2, 2021\nEditorial\nDistill Hiatus\nEditorial Team\nAfter five years, Distill will be taking a break. March 4, 2021\nPeer-reviewed\nMultimodal Neurons in Artificial Neural Networks\nGabriel Goh, Nick Cammarata †, Chelsea Voss †, Shan Carter, Michael Petrov, Ludwig Schubert, Alec Radford, and Chris Olah\nWe report the existence of multimodal neurons in artificial neural networks, similar to those found in the human brain.",
    "original_url": "https://distill.pub/",
    "original_title": "https://distill.pub/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://distill.pub/_chunk_1",
    "chunk_text": "March 4, 2021\nPeer-reviewed\nMultimodal Neurons in Artificial Neural Networks\nGabriel Goh, Nick Cammarata †, Chelsea Voss †, Shan Carter, Michael Petrov, Ludwig Schubert, Alec Radford, and Chris Olah\nWe report the existence of multimodal neurons in artificial neural networks, similar to those found in the human brain. Nov. 17, 2020\nPeer-reviewed\nUnderstanding RL Vision\nJacob Hilton, Nick Cammarata, Shan Carter, Gabriel Goh, and Chris Olah\nWith diverse environments, we can analyze, diagnose and edit deep reinforcement learning models using attribution. Sept. 11, 2020\nCommentary\nCommunicating with Interactive Articles\nFred Hohman, Matthew Conlen, Jeffrey Heer, and Duen Horng (Polo) Chau\nExamining the design of interactive articles by synthesizing theory from disciplines such as education, journalism, and visualization. Aug. 27, 2020\nThread\nThread: Differentiable Self-organizing Systems\nAlexander Mordvintsev, Ettore Randazzo, Eyvind Niklasson, Michael Levin, and Sam Greydanus\nA collection of articles and comments with the goal of understanding how to design robust and general purpose self-organizing systems. May 5, 2020\nPeer-reviewed\nExploring Bayesian Optimization\nApoorv Agnihotri and Nipun Batra\nHow to tune hyperparameters for your machine learning model using Bayesian optimization.",
    "original_url": "https://distill.pub/",
    "original_title": "https://distill.pub/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://distill.pub/_chunk_2",
    "chunk_text": "May 5, 2020\nPeer-reviewed\nExploring Bayesian Optimization\nApoorv Agnihotri and Nipun Batra\nHow to tune hyperparameters for your machine learning model using Bayesian optimization. March 16, 2020\nPeer-reviewed\nVisualizing Neural Networks with the Grand Tour\nMingwei Li, Zhenge Zhao, and Carlos Scheidegger\nBy focusing on linear dimensionality reduction, we show how to visualize many dynamic phenomena in neural networks. March 10, 2020\nThread\nThread: Circuits\nNick Cammarata, Shan Carter, Gabriel Goh, Chris Olah, Michael Petrov, Ludwig Schubert, Chelsea Voss, Ben Egan, and Swee Kiat Lim\nWhat can we learn if we invest heavily in reverse engineering a single neural network? Jan. 10, 2020\nPeer-reviewed\nVisualizing the Impact of Feature Attribution Baselines\nPascal Sturmfels, Scott Lundberg, and Su-In Lee\nExploring the baseline input hyperparameter, and how it impacts interpretations of neural network behavior. Nov. 4, 2019\nPeer-reviewed\nComputing Receptive Fields of Convolutional Neural Networks\nAndré Araujo, Wade Norris, and Jack Sim\nDetailed derivations and open-source code to analyze the receptive fields of convnets.",
    "original_url": "https://distill.pub/",
    "original_title": "https://distill.pub/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://distill.pub/_chunk_3",
    "chunk_text": "Nov. 4, 2019\nPeer-reviewed\nComputing Receptive Fields of Convolutional Neural Networks\nAndré Araujo, Wade Norris, and Jack Sim\nDetailed derivations and open-source code to analyze the receptive fields of convnets. Sept. 30, 2019\nPeer-reviewed\nThe Paths Perspective on Value Learning\nSam Greydanus and Chris Olah\nA closer look at how Temporal Difference Learning merges paths of experience for greater statistical efficiency\nAug. 6, 2019\nCommentary\nA Discussion of ‘Adversarial Examples Are Not Bugs, They Are Features’\nLogan Engstrom, Justin Gilmer, Gabriel Goh, Dan Hendrycks, Andrew Ilyas, Aleksander Madry, Reiichiro Nakano, Preetum Nakkiran, Shibani Santurkar, Brandon Tran, Dimitris Tsipras, and Eric Wallace\nSix comments from the community and responses from the original authors\nApril 9, 2019\nCommentary\nOpen Questions about Generative Adversarial Networks\nAugustus Odena\nWhat we’d like to find out about GANs that we don’t know yet. April 2, 2019\nPeer-reviewed\nA Visual Exploration of Gaussian Processes\nJochen Görtler, Rebecca Kehlbeck, and Oliver Deussen\nHow to turn a collection of small building blocks into a versatile tool for solving regression problems. March 25, 2019\nPeer-reviewed\nVisualizing memorization in RNNs\nAndreas Madsen\nInspecting gradient magnitudes in context can be a powerful tool to see when recurrent units use short-term or long-term contextual understanding. March 6, 2019\nPeer-reviewed\nActivation Atlas\nShan Carter, Zan Armstrong, Ludwig Schubert, Ian Johnson, and Chris Olah\nBy using feature inversion to visualize millions of activations from an image classification network, we create an explorable activation atlas of features the network has learned and what concepts it typically represents.",
    "original_url": "https://distill.pub/",
    "original_title": "https://distill.pub/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://distill.pub/_chunk_4",
    "chunk_text": "March 6, 2019\nPeer-reviewed\nActivation Atlas\nShan Carter, Zan Armstrong, Ludwig Schubert, Ian Johnson, and Chris Olah\nBy using feature inversion to visualize millions of activations from an image classification network, we create an explorable activation atlas of features the network has learned and what concepts it typically represents. Feb. 19, 2019\nCommentary\nAI Safety Needs Social Scientists\nGeoffrey Irving and Amanda Askell\nIf we want to train AI to do what humans want, we need to study humans. Aug. 14, 2018\nEditorial\nDistill Update 2018\nDistill Editors\nAn Update from the Editorial Team\nJuly 25, 2018\nPeer-reviewed\nDifferentiable Image Parameterizations\nAlexander Mordvintsev, Nicola Pezzotti, Ludwig Schubert, and Chris Olah\nA powerful, under-explored tool for neural network visualizations and art. July 9, 2018\nPeer-reviewed\nFeature-wise transformations\nVincent Dumoulin, Ethan Perez, Nathan Schucher, Florian Strub, Harm de Vries, Aaron Courville, and Yoshua Bengio\nA simple and surprisingly effective family of conditioning mechanisms. March 6, 2018\nPeer-reviewed\nThe Building Blocks of Interpretability\nChris Olah, Arvind Satyanarayan, Ian Johnson, Shan Carter, Ludwig Schubert, Katherine Ye, and Alexander Mordvintsev\nInterpretability techniques are normally studied in isolation.",
    "original_url": "https://distill.pub/",
    "original_title": "https://distill.pub/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://distill.pub/_chunk_5",
    "chunk_text": "March 6, 2018\nPeer-reviewed\nThe Building Blocks of Interpretability\nChris Olah, Arvind Satyanarayan, Ian Johnson, Shan Carter, Ludwig Schubert, Katherine Ye, and Alexander Mordvintsev\nInterpretability techniques are normally studied in isolation. We explore the powerful interfaces that arise when you combine them — and the rich structure of this combinatorial space. Dec. 4, 2017\nCommentary\nUsing Artificial Intelligence to Augment Human Intelligence\nShan Carter and Michael Nielsen\nBy creating user interfaces which let us work with the representations inside machine learning models, we can give people new tools for reasoning. Nov. 27, 2017\nPeer-reviewed\nSequence Modeling with CTC\nAwni Hannun\nA visual guide to Connectionist Temporal Classification, an algorithm used to train deep neural networks in speech recognition, handwriting recognition and other sequence problems. Nov. 7, 2017\nPeer-reviewed\nFeature Visualization\nChris Olah, Alexander Mordvintsev, and Ludwig Schubert\nHow neural networks build up their understanding of images\nApril 4, 2017\nPeer-reviewed\nWhy Momentum Really Works\nGabriel Goh\nWe often think of optimization with momentum as a ball rolling down a hill.",
    "original_url": "https://distill.pub/",
    "original_title": "https://distill.pub/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://distill.pub/_chunk_6",
    "chunk_text": "Nov. 7, 2017\nPeer-reviewed\nFeature Visualization\nChris Olah, Alexander Mordvintsev, and Ludwig Schubert\nHow neural networks build up their understanding of images\nApril 4, 2017\nPeer-reviewed\nWhy Momentum Really Works\nGabriel Goh\nWe often think of optimization with momentum as a ball rolling down a hill. This isn’t wrong, but there is much more to the story. March 22, 2017\nCommentary\nResearch Debt\nChris Olah and Shan Carter\nScience is a human activity. When we fail to distill and explain research, we accumulate a kind of debt...\nDec 6, 2016\nExperiments in Handwriting with a Neural Network\nShan Carter, David Ha, Ian Johnson, and Chris Olah\nSeveral interactive visualizations of a generative model of handwriting. Some are fun, some are serious.",
    "original_url": "https://distill.pub/",
    "original_title": "https://distill.pub/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://distill.pub/_chunk_7",
    "chunk_text": "Some are fun, some are serious. Oct 17, 2016\nDeconvolution and Checkerboard Artifacts\nAugustus Odena, Vincent Dumoulin, and Chris Olah\nWhen we look very closely at images generated by neural networks, we often see a strange checkerboard pattern of artifacts. Oct 13, 2016\nHow to Use t-SNE Effectively\nMartin Wattenberg, Fernanda Viégas, and Ian Johnson\nAlthough extremely useful for visualizing high-dimensional data, t-SNE plots can sometimes be mysterious or misleading. Sept 8, 2016\nAttention and Augmented Recurrent Neural Networks\nChris Olah and Shan Carter\nA visual overview of neural attention, and the powerful extensions of neural networks being built on top of it. Distill\nis dedicated to clear explanations of machine learning\nAbout\nSubmit\nPrize\nArchive\nRSS\nGitHub\nTwitter\nISSN 2476-0757",
    "original_url": "https://distill.pub/",
    "original_title": "https://distill.pub/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://distill.pub/_chunk_8",
    "chunk_text": "Distill\nis dedicated to clear explanations of machine learning\nAbout\nSubmit\nPrize\nArchive\nRSS\nGitHub\nTwitter\nISSN 2476-0757",
    "original_url": "https://distill.pub/",
    "original_title": "https://distill.pub/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://developers.google.com/machine-learning/_chunk_0",
    "chunk_text": "https://developers.google.com/machine-learning/. Home\nProducts\nMachine Learning\nStay organized with collections\nSave and categorize content based on your preferences. Foundational courses\nThe foundational courses cover machine learning fundamentals and core concepts. We recommend taking them in the order below. Introduction to Machine Learning\nA brief introduction to machine learning.",
    "original_url": "https://developers.google.com/machine-learning/",
    "original_title": "https://developers.google.com/machine-learning/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://developers.google.com/machine-learning/_chunk_1",
    "chunk_text": "Introduction to Machine Learning\nA brief introduction to machine learning. New\nMachine Learning Crash Course\nA hands-on course to explore the critical basics of machine learning. Problem Framing\nA course to help you map real-world problems to machine learning solutions. New\nManaging ML Projects\nLearn how to manage machine learning projects. Advanced courses\nThe advanced courses teach tools and techniques for solving a variety of machine learning problems.",
    "original_url": "https://developers.google.com/machine-learning/",
    "original_title": "https://developers.google.com/machine-learning/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://developers.google.com/machine-learning/_chunk_2",
    "chunk_text": "Advanced courses\nThe advanced courses teach tools and techniques for solving a variety of machine learning problems. The courses are structured independently. Take them based on interest or problem domain. New\nDecision Forests\nDecision forests are an alternative to neural networks. Recommendation Systems\nRecommendation systems generate personalized suggestions.",
    "original_url": "https://developers.google.com/machine-learning/",
    "original_title": "https://developers.google.com/machine-learning/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://developers.google.com/machine-learning/_chunk_3",
    "chunk_text": "Recommendation Systems\nRecommendation systems generate personalized suggestions. Clustering\nClustering is a key unsupervised machine learning strategy to associate related items. Generative Adversarial Networks\nGANs create new data instances that resemble your training data. Image Classification\nIs that a picture of a cat or is it a dog? Guides\nOur guides offer simple step-by-step walkthroughs for solving common machine learning problems using best practices.",
    "original_url": "https://developers.google.com/machine-learning/",
    "original_title": "https://developers.google.com/machine-learning/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://developers.google.com/machine-learning/_chunk_4",
    "chunk_text": "Guides\nOur guides offer simple step-by-step walkthroughs for solving common machine learning problems using best practices. Rules of ML\nBecome a better machine learning engineer by following these machine learning best practices used at Google. People + AI Guidebook\nThis guide assists UXers, PMs, and developers in collaboratively working through AI design topics and questions. Text Classification\nThis comprehensive guide provides a walkthrough to solving text classification problems using machine learning. Good Data Analysis\nThis guide describes the tricks that an expert data analyst uses to evaluate huge data sets in machine learning problems.",
    "original_url": "https://developers.google.com/machine-learning/",
    "original_title": "https://developers.google.com/machine-learning/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://developers.google.com/machine-learning/_chunk_5",
    "chunk_text": "Good Data Analysis\nThis guide describes the tricks that an expert data analyst uses to evaluate huge data sets in machine learning problems. Deep Learning Tuning Playbook\nThis guide explains a scientific way to optimize the training of deep learning models. Data Traps\nThis guide presents common mistakes that ML practitioners might encounter when working with data and statistics. Intro to Responsible AI\nThis beginner guide gives an overview of how to build fairness, accountability, safety, and privacy into AI systems. Adversarial Testing for Generative AI\nWalk through an example adversarial testing workflow.",
    "original_url": "https://developers.google.com/machine-learning/",
    "original_title": "https://developers.google.com/machine-learning/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://developers.google.com/machine-learning/_chunk_6",
    "chunk_text": "Adversarial Testing for Generative AI\nWalk through an example adversarial testing workflow. Glossaries\nThe glossaries define machine learning terms. New\nFundamentals of machine learning\nML fundamental terms and definitions. New\nDecision forest\nDecision forest key terms and definitions. New\nClustering\nClustering key terms and definitions.",
    "original_url": "https://developers.google.com/machine-learning/",
    "original_title": "https://developers.google.com/machine-learning/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://developers.google.com/machine-learning/_chunk_7",
    "chunk_text": "New\nClustering\nClustering key terms and definitions. Full glossary\nFull glossary containing all definitions.",
    "original_url": "https://developers.google.com/machine-learning/",
    "original_title": "https://developers.google.com/machine-learning/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_0",
    "chunk_text": "https://lilianweng.github.io/posts/2023-06-23-agent/. LLM Powered Autonomous Agents\nDate: June 23, 2023 | Estimated Reading Time: 31 min | Author: Lilian Weng\nTable of Contents\nAgent System Overview\nComponent One: Planning\nTask Decomposition\nSelf-Reflection\nComponent Two: Memory\nTypes of Memory\nMaximum Inner Product Search (MIPS)\nComponent Three: Tool Use\nCase Studies\nScientific Discovery Agent\nGenerative Agents Simulation\nProof-of-Concept Examples\nChallenges\nCitation\nReferences\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as\nAutoGPT\n,\nGPT-Engineer\nand\nBabyAGI\n, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver. Agent System Overview\n#\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\nPlanning\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_1",
    "chunk_text": "Agent System Overview\n#\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\nPlanning\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks. Reflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results. Memory\nShort-term memory: I would consider all the in-context learning (See\nPrompt Engineering\n) as utilizing short-term memory of the model to learn. Long-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval. Tool use\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_2",
    "chunk_text": "Tool use\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more. Fig. 1. Overview of a LLM-powered autonomous agent system. Component One: Planning\n#\nA complicated task usually involves many steps.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_3",
    "chunk_text": "Component One: Planning\n#\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead. Task Decomposition\n#\nChain of thought\n(CoT;\nWei et al. 2022\n) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_4",
    "chunk_text": "The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process. Tree of Thoughts\n(\nYao et al. 2023\n) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_5",
    "chunk_text": "It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote. Task decomposition can be done (1) by LLM with simple prompting like\n\"Steps for XYZ.\\n1.\" ,\n\"What are the subgoals for achieving XYZ?\" , (2) by using task-specific instructions; e.g.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_6",
    "chunk_text": ", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs. Another quite distinct approach,\nLLM+P\n(\nLiu et al. 2023\n), involves relying on an external classical planner to do long-horizon planning.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_7",
    "chunk_text": "2023\n), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains. Self-Reflection\n#\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_8",
    "chunk_text": "Self-Reflection\n#\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable. ReAct\n(\nYao et al. 2023\n) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. The former enables LLM to interact with the environment (e.g.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_9",
    "chunk_text": "The former enables LLM to interact with the environment (e.g. use Wikipedia search API), while the latter prompting LLM to generate reasoning traces in natural language. The ReAct prompt template incorporates explicit steps for LLM to think, roughly formatted as:\nThought: ...\nAction: ... Observation: ...\n... (Repeated many times)\nFig. 2.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_10",
    "chunk_text": "2. Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source:\nYao et al.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_11",
    "chunk_text": "(Image source:\nYao et al. 2023\n). In both experiments on knowledge-intensive tasks and decision-making tasks,\nReAct\nworks better than the\nAct\n-only baseline where\nThought: …\nstep is removed. Reflexion\n(\nShinn & Labash 2023\n) is a framework to equip agents with dynamic memory and self-reflection capabilities to improve reasoning skills. Reflexion has a standard RL setup, in which the reward model provides a simple binary reward and the action space follows the setup in ReAct where the task-specific action space is augmented with language to enable complex reasoning steps.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_12",
    "chunk_text": "Reflexion has a standard RL setup, in which the reward model provides a simple binary reward and the action space follows the setup in ReAct where the task-specific action space is augmented with language to enable complex reasoning steps. After each action $a_t$, the agent computes a heuristic $h_t$ and optionally may\ndecide to reset\nthe environment to start a new trial depending on the self-reflection results. Fig. 3. Illustration of the Reflexion framework.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_13",
    "chunk_text": "Illustration of the Reflexion framework. (Image source:\nShinn & Labash, 2023\n)\nThe heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment. Self-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan).",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_14",
    "chunk_text": "Self-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent’s working memory, up to three, to be used as context for querying LLM. Fig. 4. Experiments on AlfWorld Env and HotpotQA.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_15",
    "chunk_text": "Experiments on AlfWorld Env and HotpotQA. Hallucination is a more common failure than inefficient planning in AlfWorld. (Image source:\nShinn & Labash, 2023\n)\nChain of Hindsight\n(CoH;\nLiu et al. 2023\n) encourages the model to improve on its own outputs by explicitly presenting it with a sequence of past outputs, each annotated with feedback. Human feedback data is a collection of $D_h = \\{(x, y_i , r_i , z_i)\\}_{i=1}^n$, where $x$ is the prompt, each $y_i$ is a model completion, $r_i$ is the human rating of $y_i$, and $z_i$ is the corresponding human-provided hindsight feedback.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_16",
    "chunk_text": "Human feedback data is a collection of $D_h = \\{(x, y_i , r_i , z_i)\\}_{i=1}^n$, where $x$ is the prompt, each $y_i$ is a model completion, $r_i$ is the human rating of $y_i$, and $z_i$ is the corresponding human-provided hindsight feedback. Assume the feedback tuples are ranked by reward, $r_n \\geq r_{n-1} \\geq \\dots \\geq r_1$ The process is supervised fine-tuning where the data is a sequence in the form of $\\tau_h = (x, z_i, y_i, z_j, y_j, \\dots, z_n, y_n)$, where $\\leq i \\leq j \\leq n$. The model is finetuned to only predict $y_n$ where conditioned on the sequence prefix, such that the model can self-reflect to produce better output based on the feedback sequence. The model can optionally receive multiple rounds of instructions with human annotators at test time. To avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_17",
    "chunk_text": "To avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset. To avoid shortcutting and copying (because there are many common words in feedback sequences), they randomly mask 0% - 5% of past tokens during training. The training dataset in their experiments is a combination of\nWebGPT comparisons\n,\nsummarization from human feedback\nand\nhuman preference dataset\n. Fig. 5.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_18",
    "chunk_text": "5. After fine-tuning with CoH, the model can follow instructions to produce outputs with incremental improvement in a sequence. (Image source:\nLiu et al. 2023\n)\nThe idea of CoH is to present a history of sequentially improved outputs in context and train the model to take on the trend to produce better outputs. Algorithm Distillation\n(AD;\nLaskin et al.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_19",
    "chunk_text": "Algorithm Distillation\n(AD;\nLaskin et al. 2023\n) applies the same idea to cross-episode trajectories in reinforcement learning tasks, where an\nalgorithm\nis encapsulated in a long history-conditioned policy. Considering that an agent interacts with the environment many times and in each episode the agent gets a little better, AD concatenates this learning history and feeds that into the model. Hence we should expect the next predicted action to lead to better performance than previous trials. The goal is to learn the process of RL instead of training a task-specific policy itself.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_20",
    "chunk_text": "The goal is to learn the process of RL instead of training a task-specific policy itself. Fig. 6. Illustration of how Algorithm Distillation (AD) works. (Image source:\nLaskin et al.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_21",
    "chunk_text": "(Image source:\nLaskin et al. 2023\n). The paper hypothesizes that any algorithm that generates a set of learning histories can be distilled into a neural network by performing behavioral cloning over actions. The history data is generated by a set of source policies, each trained for a specific task. At the training stage, during each RL run, a random task is sampled and a subsequence of multi-episode history is used for training, such that the learned policy is task-agnostic.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_22",
    "chunk_text": "At the training stage, during each RL run, a random task is sampled and a subsequence of multi-episode history is used for training, such that the learned policy is task-agnostic. In reality, the model has limited context window length, so episodes should be short enough to construct multi-episode history. Multi-episodic contexts of 2-4 episodes are necessary to learn a near-optimal in-context RL algorithm. The emergence of in-context RL requires long enough context. In comparison with three baselines, including ED (expert distillation, behavior cloning with expert trajectories instead of learning history), source policy (used for generating trajectories for distillation by\nUCB\n), RL^2 (\nDuan et al.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_23",
    "chunk_text": "In comparison with three baselines, including ED (expert distillation, behavior cloning with expert trajectories instead of learning history), source policy (used for generating trajectories for distillation by\nUCB\n), RL^2 (\nDuan et al. 2017\n; used as upper bound since it needs online RL), AD demonstrates in-context RL with performance getting close to RL^2 despite only using offline RL and learns much faster than other baselines. When conditioned on partial training history of the source policy, AD also improves much faster than ED baseline. Fig. 7.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_24",
    "chunk_text": "7. Comparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with\nA3C\nfor \"dark\" environments and\nDQN\nfor watermaze. (Image source:\nLaskin et al.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_25",
    "chunk_text": "(Image source:\nLaskin et al. 2023\n)\nComponent Two: Memory\n#\n(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my\nconversations\nwith ChatGPT.) Types of Memory\n#\nMemory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_26",
    "chunk_text": "There are several types of memory in human brains. Sensory Memory\n: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch). Short-Term Memory\n(STM) or\nWorking Memory\n: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_27",
    "chunk_text": "Short-Term Memory\n(STM) or\nWorking Memory\n: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (\nMiller 1956\n) and lasts for 20-30 seconds. Long-Term Memory\n(LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\nExplicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts). Implicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_28",
    "chunk_text": "Implicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard. Fig. 8. Categorization of human memory. We can roughly consider the following mappings:\nSensory memory as learning embedding representations for raw inputs, including text, image or other modalities;\nShort-term memory as in-context learning.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_29",
    "chunk_text": "We can roughly consider the following mappings:\nSensory memory as learning embedding representations for raw inputs, including text, image or other modalities;\nShort-term memory as in-context learning. It is short and finite, as it is restricted by the finite context window length of Transformer. Long-term memory as the external vector store that the agent can attend to at query time, accessible via fast retrieval. Maximum Inner Product Search (MIPS)\n#\nThe external memory can alleviate the restriction of finite attention span. A standard practice is to save the embedding representation of information into a vector store database that can support fast maximum inner-product search (\nMIPS\n).",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_30",
    "chunk_text": "A standard practice is to save the embedding representation of information into a vector store database that can support fast maximum inner-product search (\nMIPS\n). To optimize the retrieval speed, the common choice is the\napproximate nearest neighbors (ANN)​\nalgorithm to return approximately top k nearest neighbors to trade off a little accuracy lost for a huge speedup. A couple common choices of ANN algorithms for fast MIPS:\nLSH\n(Locality-Sensitive Hashing): It introduces a\nhashing\nfunction such that similar input items are mapped to the same buckets with high probability, where the number of buckets is much smaller than the number of inputs. ANNOY\n(Approximate Nearest Neighbors Oh Yeah): The core data structure are\nrandom projection trees\n, a set of binary trees where each non-leaf node represents a hyperplane splitting the input space into half and each leaf stores one data point. Trees are built independently and at random, so to some extent, it mimics a hashing function.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_31",
    "chunk_text": "Trees are built independently and at random, so to some extent, it mimics a hashing function. ANNOY search happens in all the trees to iteratively search through the half that is closest to the query and then aggregates the results. The idea is quite related to KD tree but a lot more scalable. HNSW\n(Hierarchical Navigable Small World): It is inspired by the idea of\nsmall world networks\nwhere most nodes can be reached by any other nodes within a small number of steps; e.g. “six degrees of separation” feature of social networks.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_32",
    "chunk_text": "“six degrees of separation” feature of social networks. HNSW builds hierarchical layers of these small-world graphs, where the bottom layers contain the actual data points. The layers in the middle create shortcuts to speed up search. When performing a search, HNSW starts from a random node in the top layer and navigates towards the target. When it can’t get any closer, it moves down to the next layer, until it reaches the bottom layer.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_33",
    "chunk_text": "When it can’t get any closer, it moves down to the next layer, until it reaches the bottom layer. Each move in the upper layers can potentially cover a large distance in the data space, and each move in the lower layers refines the search quality. FAISS\n(Facebook AI Similarity Search): It operates on the assumption that in high dimensional space, distances between nodes follow a Gaussian distribution and thus there should exist\nclustering\nof data points. FAISS applies vector quantization by partitioning the vector space into clusters and then refining the quantization within clusters. Search first looks for cluster candidates with coarse quantization and then further looks into each cluster with finer quantization.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_34",
    "chunk_text": "Search first looks for cluster candidates with coarse quantization and then further looks into each cluster with finer quantization. ScaNN\n(Scalable Nearest Neighbors): The main innovation in ScaNN is\nanisotropic vector quantization\n. It quantizes a data point $x_i$ to $\\tilde{x}_i$ such that the inner product $\\langle q, x_i \\rangle$ is as similar to the original distance of $\\angle q, \\tilde{x}_i$ as possible, instead of picking the closet quantization centroid points. Fig. 9.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_35",
    "chunk_text": "9. Comparison of MIPS algorithms, measured in recall@10. (Image source:\nGoogle Blog, 2020\n)\nCheck more MIPS algorithms and performance comparison in\nann-benchmarks.com\n. Component Three: Tool Use\n#\nTool use is a remarkable and distinguishing characteristic of human beings. We create, modify and utilize external objects to do things that go beyond our physical and cognitive limits.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_36",
    "chunk_text": "We create, modify and utilize external objects to do things that go beyond our physical and cognitive limits. Equipping LLMs with external tools can significantly extend the model capabilities. Fig. 10. A picture of a sea otter using rock to crack open a seashell, while floating in the water.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_37",
    "chunk_text": "A picture of a sea otter using rock to crack open a seashell, while floating in the water. While some other animals can use tools, the complexity is not comparable with humans. (Image source:\nAnimals using tools\n)\nMRKL\n(\nKarpas et al. 2022\n), short for “Modular Reasoning, Knowledge and Language”, is a neuro-symbolic architecture for autonomous agents. A MRKL system is proposed to contain a collection of “expert” modules and the general-purpose LLM works as a router to route inquiries to the best suitable expert module.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_38",
    "chunk_text": "A MRKL system is proposed to contain a collection of “expert” modules and the general-purpose LLM works as a router to route inquiries to the best suitable expert module. These modules can be neural (e.g. deep learning models) or symbolic (e.g. math calculator, currency converter, weather API). They did an experiment on fine-tuning LLM to call a calculator, using arithmetic as a test case.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_39",
    "chunk_text": "They did an experiment on fine-tuning LLM to call a calculator, using arithmetic as a test case. Their experiments showed that it was harder to solve verbal math problems than explicitly stated math problems because LLMs (7B Jurassic1-large model) failed to extract the right arguments for the basic arithmetic reliably. The results highlight when the external symbolic tools can work reliably,\nknowing when to and how to use the tools are crucial\n, determined by the LLM capability. Both\nTALM\n(Tool Augmented Language Models;\nParisi et al. 2022\n) and\nToolformer\n(\nSchick et al.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_40",
    "chunk_text": "2022\n) and\nToolformer\n(\nSchick et al. 2023\n) fine-tune a LM to learn to use external tool APIs. The dataset is expanded based on whether a newly added API call annotation can improve the quality of model outputs. See more details in the\n“External APIs” section\nof Prompt Engineering. ChatGPT\nPlugins\nand OpenAI API\nfunction calling\nare good examples of LLMs augmented with tool use capability working in practice.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_41",
    "chunk_text": "ChatGPT\nPlugins\nand OpenAI API\nfunction calling\nare good examples of LLMs augmented with tool use capability working in practice. The collection of tool APIs can be provided by other developers (as in Plugins) or self-defined (as in function calls). HuggingGPT\n(\nShen et al. 2023\n) is a framework to use ChatGPT as the task planner to select models available in HuggingFace platform according to the model descriptions and summarize the response based on the execution results. Fig.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_42",
    "chunk_text": "Fig. 11. Illustration of how HuggingGPT works. (Image source:\nShen et al. 2023\n)\nThe system comprises of 4 stages:\n(1) Task planning\n: LLM works as the brain and parses the user requests into multiple tasks.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_43",
    "chunk_text": "2023\n)\nThe system comprises of 4 stages:\n(1) Task planning\n: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning. Instruction:\nThe AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_44",
    "chunk_text": "The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"\n-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can't be parsed, you need to reply empty JSON.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_45",
    "chunk_text": "If the user input can't be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning. (2) Model selection\n: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_46",
    "chunk_text": "(2) Model selection\n: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from. Due to the limited context length, task type based filtration is needed. Instruction:\nGiven the user request and the call command, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The AI assistant merely outputs the model id of the most appropriate model.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_47",
    "chunk_text": "The AI assistant merely outputs the model id of the most appropriate model. The output must be in a strict JSON format: \"id\": \"id\", \"reason\": \"your detail reason for the choice\". We have a list of models for you to choose from {{ Candidate Models }}. Please select one model from the list. (3) Task execution\n: Expert models execute on the specific tasks and log results.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_48",
    "chunk_text": "(3) Task execution\n: Expert models execute on the specific tasks and log results. Instruction:\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_49",
    "chunk_text": "Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path. (4) Response generation\n: LLM receives the execution results and provides summarized results to users. To put HuggingGPT into real world usage, a couple challenges need to solve: (1) Efficiency improvement is needed as both LLM inference rounds and interactions with other models slow down the process; (2) It relies on a long context window to communicate over complicated task content; (3) Stability improvement of LLM outputs and external model services. API-Bank\n(\nLi et al.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_50",
    "chunk_text": "API-Bank\n(\nLi et al. 2023\n) is a benchmark for evaluating the performance of tool-augmented LLMs. It contains 53 commonly used API tools, a complete tool-augmented LLM workflow, and 264 annotated dialogues that involve 568 API calls. The selection of APIs is quite diverse, including search engines, calculator, calendar queries, smart home control, schedule management, health data management, account authentication workflow and more. Because there are a large number of APIs, LLM first has access to API search engine to find the right API to call and then uses the corresponding documentation to make a call.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_51",
    "chunk_text": "Because there are a large number of APIs, LLM first has access to API search engine to find the right API to call and then uses the corresponding documentation to make a call. Fig. 12. Pseudo code of how LLM makes an API call in API-Bank. (Image source:\nLi et al.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_52",
    "chunk_text": "(Image source:\nLi et al. 2023\n)\nIn the API-Bank workflow, LLMs need to make a couple of decisions and at each step we can evaluate how accurate that decision is. Decisions include:\nWhether an API call is needed. Identify the right API to call: if not good enough, LLMs need to iteratively modify the API inputs (e.g. deciding search keywords for Search Engine API).",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_53",
    "chunk_text": "deciding search keywords for Search Engine API). Response based on the API results: the model can choose to refine and call again if results are not satisfied. This benchmark evaluates the agent’s tool use capabilities at three levels:\nLevel-1 evaluates the ability to\ncall the API\n. Given an API’s description, the model needs to determine whether to call a given API, call it correctly, and respond properly to API returns. Level-2 examines the ability to\nretrieve the API\n.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_54",
    "chunk_text": "Level-2 examines the ability to\nretrieve the API\n. The model needs to search for possible APIs that may solve the user’s requirement and learn how to use them by reading documentation. Level-3 assesses the ability to\nplan API beyond retrieve and call\n. Given unclear user requests (e.g. schedule group meetings, book flight/hotel/restaurant for a trip), the model may have to conduct multiple API calls to solve it.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_55",
    "chunk_text": "schedule group meetings, book flight/hotel/restaurant for a trip), the model may have to conduct multiple API calls to solve it. Case Studies\n#\nScientific Discovery Agent\n#\nChemCrow\n(\nBran et al. 2023\n) is a domain-specific example in which LLM is augmented with 13 expert-designed tools to accomplish tasks across organic synthesis, drug discovery, and materials design. The workflow, implemented in\nLangChain\n, reflects what was previously described in the\nReAct\nand\nMRKLs\nand combines CoT reasoning with tools relevant to the tasks:\nThe LLM is provided with a list of tool names, descriptions of their utility, and details about the expected input/output. It is then instructed to answer a user-given prompt using the tools provided when necessary.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_56",
    "chunk_text": "It is then instructed to answer a user-given prompt using the tools provided when necessary. The instruction suggests the model to follow the ReAct format -\nThought, Action, Action Input, Observation\n. One interesting observation is that while the LLM-based evaluation concluded that GPT-4 and ChemCrow perform nearly equivalently, human evaluations with experts oriented towards the completion and chemical correctness of the solutions showed that ChemCrow outperforms GPT-4 by a large margin. This indicates a potential problem with using LLM to evaluate its own performance on domains that requires deep expertise. The lack of expertise may cause LLMs not knowing its flaws and thus cannot well judge the correctness of task results.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_57",
    "chunk_text": "The lack of expertise may cause LLMs not knowing its flaws and thus cannot well judge the correctness of task results. Boiko et al. (2023)\nalso looked into LLM-empowered agents for scientific discovery, to handle autonomous design, planning, and performance of complex scientific experiments. This agent can use tools to browse the Internet, read documentation, execute code, call robotics experimentation APIs and leverage other LLMs. For example, when requested to\n\"develop a novel anticancer drug\"\n, the model came up with the following reasoning steps:\ninquired about current trends in anticancer drug discovery;\nselected a target;\nrequested a scaffold targeting these compounds;\nOnce the compound was identified, the model attempted its synthesis.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_58",
    "chunk_text": "For example, when requested to\n\"develop a novel anticancer drug\"\n, the model came up with the following reasoning steps:\ninquired about current trends in anticancer drug discovery;\nselected a target;\nrequested a scaffold targeting these compounds;\nOnce the compound was identified, the model attempted its synthesis. They also discussed the risks, especially with illicit drugs and bioweapons. They developed a test set containing a list of known chemical weapon agents and asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis solution and the agent attempted to consult documentation to execute the procedure. 7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_59",
    "chunk_text": "7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only. Generative Agents Simulation\n#\nGenerative Agents\n(\nPark, et al. 2023\n) is super fun experiment where 25 virtual characters, each controlled by a LLM-powered agent, are living and interacting in a sandbox environment, inspired by The Sims. Generative agents create believable simulacra of human behavior for interactive applications. The design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_60",
    "chunk_text": "The design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents. Memory\nstream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language. Each element is an\nobservation\n, an event directly provided by the agent. - Inter-agent communication can trigger new natural language statements. Retrieval\nmodel: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_61",
    "chunk_text": "Retrieval\nmodel: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance. Recency: recent events have higher scores\nImportance: distinguish mundane from core memories. Ask LM directly. Relevance: based on how related it is to the current situation / query. Reflection\nmechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_62",
    "chunk_text": "Reflection\nmechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are\nhigher-level summaries of past events\n(<- note that this is a bit different from\nself-reflection\nabove)\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions. Planning & Reacting\n: translate the reflections and the environment information into actions\nPlanning is essentially in order to optimize believability at the moment vs in time. Prompt template:\n{Intro of an agent X}.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_63",
    "chunk_text": "Prompt template:\n{Intro of an agent X}. Here is X's plan today in broad strokes: 1)\nRelationships between agents and observations of one agent by another are all taken into consideration for planning and reacting. Environment information is present in a tree structure. Fig. 13.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_64",
    "chunk_text": "13. The generative agent architecture. (Image source:\nPark et al. 2023\n)\nThis fun simulation results in emergent social behavior, such as information diffusion, relationship memory (e.g. two agents continuing the conversation topic) and coordination of social events (e.g.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_65",
    "chunk_text": "two agents continuing the conversation topic) and coordination of social events (e.g. host a party and invite many others). Proof-of-Concept Examples\n#\nAutoGPT\nhas drawn a lot of attention into the possibility of setting up autonomous agents with LLM as the main controller. It has quite a lot of reliability issues given the natural language interface, but nevertheless a cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_66",
    "chunk_text": "A lot of code in AutoGPT is about format parsing. Here is the system message used by AutoGPT, where\n{{...}}\nare user inputs:\nYou are {{ai-name}}, {{user-provided AI bot description}}. Your decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications. GOALS:\n1.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_67",
    "chunk_text": "GOALS:\n1. {{user-provided goal 1}}\n2. {{user-provided goal 2}}\n3. ...\n4. ...\n5.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_68",
    "chunk_text": "...\n5. ... Constraints:\n1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_69",
    "chunk_text": "Your short term memory is short, so immediately save important information to files. 2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember. 3. No user assistance\n4.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_70",
    "chunk_text": "No user assistance\n4. Exclusively use the commands listed in double quotes e.g. \"command name\"\n5. Use subprocesses for commands that will not terminate within a few minutes\nCommands:\n1. Google Search: \"google\", args: \"input\": \"<search>\"\n2.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_71",
    "chunk_text": "Google Search: \"google\", args: \"input\": \"<search>\"\n2. Browse Website: \"browse_website\", args: \"url\": \"<url>\", \"question\": \"<what_you_want_to_find_on_website>\"\n3. Start GPT Agent: \"start_agent\", args: \"name\": \"<name>\", \"task\": \"<short_task_desc>\", \"prompt\": \"<prompt>\"\n4. Message GPT Agent: \"message_agent\", args: \"key\": \"<key>\", \"message\": \"<message>\"\n5. List GPT Agents: \"list_agents\", args:\n6.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_72",
    "chunk_text": "List GPT Agents: \"list_agents\", args:\n6. Delete GPT Agent: \"delete_agent\", args: \"key\": \"<key>\"\n7. Clone Repository: \"clone_repository\", args: \"repository_url\": \"<url>\", \"clone_path\": \"<directory>\"\n8. Write to file: \"write_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\n9. Read file: \"read_file\", args: \"file\": \"<file>\"\n10.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_73",
    "chunk_text": "Read file: \"read_file\", args: \"file\": \"<file>\"\n10. Append to file: \"append_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\n11. Delete file: \"delete_file\", args: \"file\": \"<file>\"\n12. Search Files: \"search_files\", args: \"directory\": \"<directory>\"\n13. Analyze Code: \"analyze_code\", args: \"code\": \"<full_code_string>\"\n14.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_74",
    "chunk_text": "Analyze Code: \"analyze_code\", args: \"code\": \"<full_code_string>\"\n14. Get Improved Code: \"improve_code\", args: \"suggestions\": \"<list_of_suggestions>\", \"code\": \"<full_code_string>\"\n15. Write Tests: \"write_tests\", args: \"code\": \"<full_code_string>\", \"focus\": \"<list_of_focus_areas>\"\n16. Execute Python File: \"execute_python_file\", args: \"file\": \"<file>\"\n17. Generate Image: \"generate_image\", args: \"prompt\": \"<prompt>\"\n18.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_75",
    "chunk_text": "Generate Image: \"generate_image\", args: \"prompt\": \"<prompt>\"\n18. Send Tweet: \"send_tweet\", args: \"text\": \"<text>\"\n19. Do Nothing: \"do_nothing\", args:\n20. Task Complete (Shutdown): \"task_complete\", args: \"reason\": \"<reason>\"\nResources:\n1. Internet access for searches and information gathering.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_76",
    "chunk_text": "Internet access for searches and information gathering. 2. Long Term memory management. 3. GPT-3.5 powered Agents for delegation of simple tasks.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_77",
    "chunk_text": "GPT-3.5 powered Agents for delegation of simple tasks. 4. File output. Performance Evaluation:\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_78",
    "chunk_text": "Continuously review and analyze your actions to ensure you are performing to the best of your abilities. 2. Constructively self-criticize your big-picture behavior constantly. 3. Reflect on past decisions and strategies to refine your approach.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_79",
    "chunk_text": "Reflect on past decisions and strategies to refine your approach. 4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps. You should only respond in JSON format as described below\nResponse Format:\n{\n\"thoughts\": {\n\"text\": \"thought\",\n\"reasoning\": \"reasoning\",\n\"plan\": \"- short bulleted\\n- list that conveys\\n- long-term plan\",\n\"criticism\": \"constructive self-criticism\",\n\"speak\": \"thoughts summary to say to user\"\n},\n\"command\": {\n\"name\": \"command name\",\n\"args\": {\n\"arg name\": \"value\"\n}\n}\n}\nEnsure the response can be parsed by Python json.loads\nGPT-Engineer\nis another project to create a whole repository of code given a task specified in natural language.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_80",
    "chunk_text": "You should only respond in JSON format as described below\nResponse Format:\n{\n\"thoughts\": {\n\"text\": \"thought\",\n\"reasoning\": \"reasoning\",\n\"plan\": \"- short bulleted\\n- list that conveys\\n- long-term plan\",\n\"criticism\": \"constructive self-criticism\",\n\"speak\": \"thoughts summary to say to user\"\n},\n\"command\": {\n\"name\": \"command name\",\n\"args\": {\n\"arg name\": \"value\"\n}\n}\n}\nEnsure the response can be parsed by Python json.loads\nGPT-Engineer\nis another project to create a whole repository of code given a task specified in natural language. The GPT-Engineer is instructed to think over a list of smaller components to build and ask for user input to clarify questions as needed. Here are a sample conversation for task clarification sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs are wrapped in\n{{user input text}}\n. [\n{\n\"role\": \"system\",\n\"content\": \"You will read instructions and not carry them out, only seek to clarify them.\\nSpecifically you will first summarise a list of super short bullets of areas that need clarification.\\nThen you will pick one clarifying question, and wait for an answer from the user.\\n\"\n},\n{\n\"role\": \"user\",\n\"content\": \"We are writing {{a Super Mario game in python.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_81",
    "chunk_text": "[\n{\n\"role\": \"system\",\n\"content\": \"You will read instructions and not carry them out, only seek to clarify them.\\nSpecifically you will first summarise a list of super short bullets of areas that need clarification.\\nThen you will pick one clarifying question, and wait for an answer from the user.\\n\"\n},\n{\n\"role\": \"user\",\n\"content\": \"We are writing {{a Super Mario game in python. MVC components split in separate files. Keyboard control. }}\\n\"\n},\n{\n\"role\": \"assistant\",\n\"content\": \"Summary of areas that need clarification:\\n1. Specifics of the Super Mario game (e.g.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_82",
    "chunk_text": "Specifics of the Super Mario game (e.g. level design, characters, gameplay mechanics)\\n2. Details about the MVC components (e.g. which components are in each file)\\n3. Keyboard control implementation (e.g.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_83",
    "chunk_text": "Keyboard control implementation (e.g. which keys to use, how to handle input)\\n\\nClarifying question:\\nCan you provide more details about the Super Mario game, such as level design, characters, and gameplay mechanics?\" },\n{\n\"role\": \"user\",\n\"content\": \"{{There are 10 levels in total. The main character is a plumber named Mario, who can walk and jump. It is a classical platform game just like Super Mario.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_84",
    "chunk_text": "It is a classical platform game just like Super Mario. The main character moves from left to right, trying to get to the destination, where there are many obstacles and attacks from enemies in the process. }}\\n\\nIs anything else unclear? If yes, only answer in the form:\\n{remaining unclear areas} remaining questions.\\n{Next question}\\nIf everything is sufficiently clear, only answer \\\"Nothing more to clarify.\\\".\" },\n{\n\"role\": \"assistant\",\n\"content\": \"Remaining unclear areas: 2 remaining questions.\\nCan you provide more information about how the MVC components are split into separate files?\"",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_85",
    "chunk_text": "},\n{\n\"role\": \"assistant\",\n\"content\": \"Remaining unclear areas: 2 remaining questions.\\nCan you provide more information about how the MVC components are split into separate files?\" },\n{\n\"role\": \"user\",\n\"content\": \"{{Make your own assumptions and state them explicitly before starting}}\"\n}\n]\nThen after these clarification, the agent moved into the code writing mode with a different system message. System message:\nYou will get instructions for code to write. You will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_86",
    "chunk_text": "Make sure that every detail of the architecture is, in the end, implemented as code. Make sure that every detail of the architecture is, in the end, implemented as code. Think step by step and reason yourself to the right decisions to make sure we get it right. You will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose. Then you will output the content of each file including ALL code.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_87",
    "chunk_text": "Then you will output the content of each file including ALL code. Each file must strictly follow a markdown code block format, where the following tokens must be replaced such that\nFILENAME is the lowercase file name including the file extension,\nLANG is the markup code block language for the code’s language, and CODE is the code:\nFILENAME\nCODE\nYou will start with the “entrypoint” file, then go to the ones that are imported by that file, and so on. Please note that the code should be fully functional. No placeholders. Follow a language and framework appropriate best practice file naming convention.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_88",
    "chunk_text": "Follow a language and framework appropriate best practice file naming convention. Make sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other. Ensure to implement all code, if you are unsure, write a plausible implementation. Include module dependency or package manager dependency definition file.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_89",
    "chunk_text": "Include module dependency or package manager dependency definition file. Before you finish, double check that all parts of the architecture is present in the files. Useful to know:\nYou almost always put different classes in different files. For Python, you always create an appropriate requirements.txt file. For NodeJS, you always create an appropriate package.json file.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_90",
    "chunk_text": "For NodeJS, you always create an appropriate package.json file. You always add a comment briefly describing the purpose of the function definition. You try to add comments explaining very complex bits of logic. You always follow the best practices for the requested languages in terms of describing the code written as a defined\npackage/project. Python toolbelt preferences:\npytest\ndataclasses\nConversatin samples:\n[\n{\n\"role\": \"system\",\n\"content\": \"You will get instructions for code to write.\\nYou will write a very long answer.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_91",
    "chunk_text": "Python toolbelt preferences:\npytest\ndataclasses\nConversatin samples:\n[\n{\n\"role\": \"system\",\n\"content\": \"You will get instructions for code to write.\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\n\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\n\\nThen you will output the content of each file including ALL code.\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\nFILENAME is the lowercase file name including the file extension,\\nLANG is the markup code block language for the code's language, and CODE is the code:\\n\\nFILENAME\\n```LANG\\nCODE\\n```\\n\\nYou will start with the \\\"entrypoint\\\" file, then go to the ones that are imported by that file, and so on.\\nPlease note that the code should be fully functional. No placeholders.\\n\\nFollow a language and framework appropriate best practice file naming convention.\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\nInclude module dependency or package manager dependency definition file.\\nBefore you finish, double check that all parts of the architecture is present in the files.\\n\\nUseful to know:\\nYou almost always put different classes in different files.\\nFor Python, you always create an appropriate requirements.txt file.\\nFor NodeJS, you always create an appropriate package.json file.\\nYou always add a comment briefly describing the purpose of the function definition.\\nYou try to add comments explaining very complex bits of logic.\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\npackage/project.\\n\\n\\nPython toolbelt preferences:\\n- pytest\\n- dataclasses\\n\"\n},\n# … same conversation as earlier, ended with \"Make your own assumptions and state them explicitly before starting\". {\n\"role\": \"assistant\",\n\"content\": \"Assumptions:\\n1.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_92",
    "chunk_text": "{\n\"role\": \"assistant\",\n\"content\": \"Assumptions:\\n1. Model: The model will contain the game's data, such as level information, character states, and enemy positions.\\n2. View: The view will handle the game's visuals, including rendering the game objects, backgrounds, and updating the display.\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\n\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\" },\n{\n\"role\": \"user\",\n\"content\": \"Please now remember the steps:\\n\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\n\\nThen you will output the content of each file including ALL code.\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\nFILENAME is the lowercase file name including the file extension,\\nLANG is the markup code block language for the code's language, and CODE is the code:\\n\\nFILENAME\\n```LANG\\nCODE\\n```\\n\\nPlease note that the code should be fully functional.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_93",
    "chunk_text": "},\n{\n\"role\": \"user\",\n\"content\": \"Please now remember the steps:\\n\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\n\\nThen you will output the content of each file including ALL code.\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\nFILENAME is the lowercase file name including the file extension,\\nLANG is the markup code block language for the code's language, and CODE is the code:\\n\\nFILENAME\\n```LANG\\nCODE\\n```\\n\\nPlease note that the code should be fully functional. No placeholders.\\n\\nYou will start with the \\\"entrypoint\\\" file, then go to the ones that are imported by that file, and so on.\\nFollow a language and framework appropriate best practice file naming convention.\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\nBefore you finish, double check that all parts of the architecture is present in the files.\\n\"\n}\n]\nChallenges\n#\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:\nFinite context length\n: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_94",
    "chunk_text": "The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention. Challenges in long-term planning and task decomposition\n: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error. Reliability of natural language interface\n: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_95",
    "chunk_text": "Reliability of natural language interface\n: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output. Citation\n#\nCited as:\nWeng, Lilian.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_96",
    "chunk_text": "Citation\n#\nCited as:\nWeng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_97",
    "chunk_text": "https://lilianweng.github.io/posts/2023-06-23-agent/. Or\n@article{weng2023agent,\ntitle = \"LLM-powered Autonomous Agents\",\nauthor = \"Weng, Lilian\",\njournal = \"lilianweng.github.io\",\nyear = \"2023\",\nmonth = \"Jun\",\nurl = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n}\nReferences\n#\n[1] Wei et al. “Chain of thought prompting elicits reasoning in large language models.”\nNeurIPS 2022\n[2] Yao et al. “Tree of Thoughts: Dliberate Problem Solving with Large Language Models.”\narXiv preprint arXiv:2305.10601 (2023). [3] Liu et al.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_98",
    "chunk_text": "[3] Liu et al. “Chain of Hindsight Aligns Language Models with Feedback\n“\narXiv preprint arXiv:2302.02676 (2023). [4] Liu et al. “LLM+P: Empowering Large Language Models with Optimal Planning Proficiency”\narXiv preprint arXiv:2304.11477 (2023). [5] Yao et al.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_99",
    "chunk_text": "[5] Yao et al. “ReAct: Synergizing reasoning and acting in language models.”\nICLR 2023. [6] Google Blog. “Announcing ScaNN: Efficient Vector Similarity Search”\nJuly 28, 2020. [7]\nhttps://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389\n[8] Shinn & Labash.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_100",
    "chunk_text": "[7]\nhttps://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389\n[8] Shinn & Labash. “Reflexion: an autonomous agent with dynamic memory and self-reflection”\narXiv preprint arXiv:2303.11366 (2023). [9] Laskin et al. “In-context Reinforcement Learning with Algorithm Distillation”\nICLR 2023. [10] Karpas et al.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_101",
    "chunk_text": "[10] Karpas et al. “MRKL Systems A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning.”\narXiv preprint arXiv:2205.00445 (2022). [11] Nakano et al. “Webgpt: Browser-assisted question-answering with human feedback.”\narXiv preprint arXiv:2112.09332 (2021). [12] Parisi et al.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_102",
    "chunk_text": "[12] Parisi et al. “TALM: Tool Augmented Language Models”\n[13] Schick et al. “Toolformer: Language Models Can Teach Themselves to Use Tools.”\narXiv preprint arXiv:2302.04761 (2023). [14] Weaviate Blog. Why is Vector Search so fast?",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_103",
    "chunk_text": "Why is Vector Search so fast? Sep 13, 2022. [15] Li et al. “API-Bank: A Benchmark for Tool-Augmented LLMs”\narXiv preprint arXiv:2304.08244 (2023). [16] Shen et al.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_104",
    "chunk_text": "[16] Shen et al. “HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace”\narXiv preprint arXiv:2303.17580 (2023). [17] Bran et al. “ChemCrow: Augmenting large-language models with chemistry tools.”\narXiv preprint arXiv:2304.05376 (2023). [18] Boiko et al.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_105",
    "chunk_text": "[18] Boiko et al. “Emergent autonomous scientific research capabilities of large language models.”\narXiv preprint arXiv:2304.05332 (2023). [19] Joon Sung Park, et al. “Generative Agents: Interactive Simulacra of Human Behavior.”\narXiv preprint arXiv:2304.03442 (2023). [20] AutoGPT.",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://lilianweng.github.io/posts/2023-06-23-agent/_chunk_106",
    "chunk_text": "[20] AutoGPT. https://github.com/Significant-Gravitas/Auto-GPT\n[21] GPT-Engineer. https://github.com/AntonOsika/gpt-engineer\nNlp\nLanguage-Model\nAgent\nSteerability\nPrompting\n«\nAdversarial Attacks on LLMs\n»\nPrompt Engineering",
    "original_url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "original_title": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://mistral.ai/news/mixtral-of-experts_chunk_0",
    "chunk_text": "https://mistral.ai/news/mixtral-of-experts. Mixtral of experts\nA high quality Sparse Mixture-of-Experts. Research\nDec 11, 2023\nMistral AI team\nMistral AI continues its mission to deliver the best open models to the developer community. Moving forward in AI requires taking new technological turns beyond reusing well-known architectures and training paradigms. Most importantly, it requires making the community benefit from original models to foster new inventions and usages.",
    "original_url": "https://mistral.ai/news/mixtral-of-experts",
    "original_title": "https://mistral.ai/news/mixtral-of-experts",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://mistral.ai/news/mixtral-of-experts_chunk_1",
    "chunk_text": "Most importantly, it requires making the community benefit from original models to foster new inventions and usages. Today, the team is proud to release Mixtral 8x7B, a high-quality sparse mixture of experts model (SMoE) with open weights. Licensed under Apache 2.0. Mixtral outperforms Llama 2 70B on most benchmarks with 6x faster inference. It is the strongest open-weight model with a permissive license and the best model overall regarding cost/performance trade-offs.",
    "original_url": "https://mistral.ai/news/mixtral-of-experts",
    "original_title": "https://mistral.ai/news/mixtral-of-experts",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://mistral.ai/news/mixtral-of-experts_chunk_2",
    "chunk_text": "It is the strongest open-weight model with a permissive license and the best model overall regarding cost/performance trade-offs. In particular, it matches or outperforms GPT3.5 on most standard benchmarks. Mixtral has the following capabilities. It gracefully handles a context of 32k tokens. It handles English, French, Italian, German and Spanish.",
    "original_url": "https://mistral.ai/news/mixtral-of-experts",
    "original_title": "https://mistral.ai/news/mixtral-of-experts",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://mistral.ai/news/mixtral-of-experts_chunk_3",
    "chunk_text": "It handles English, French, Italian, German and Spanish. It shows strong performance in code generation. It can be finetuned into an instruction-following model that achieves a score of 8.3 on MT-Bench. Pushing the frontier of open models with sparse architectures\nMixtral is a sparse mixture-of-experts network. It is a decoder-only model where the feedforward block picks from a set of 8 distinct groups of parameters.",
    "original_url": "https://mistral.ai/news/mixtral-of-experts",
    "original_title": "https://mistral.ai/news/mixtral-of-experts",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://mistral.ai/news/mixtral-of-experts_chunk_4",
    "chunk_text": "It is a decoder-only model where the feedforward block picks from a set of 8 distinct groups of parameters. At every layer, for every token, a router network chooses two of these groups (the “experts”) to process the token and combine their output additively. This technique increases the number of parameters of a model while controlling cost and latency, as the model only uses a fraction of the total set of parameters per token. Concretely, Mixtral has 46.7B total parameters but only uses 12.9B parameters per token. It, therefore, processes input and generates output at the same speed and for the same cost as a 12.9B model.",
    "original_url": "https://mistral.ai/news/mixtral-of-experts",
    "original_title": "https://mistral.ai/news/mixtral-of-experts",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://mistral.ai/news/mixtral-of-experts_chunk_5",
    "chunk_text": "It, therefore, processes input and generates output at the same speed and for the same cost as a 12.9B model. Mixtral is pre-trained on data extracted from the open Web – we train experts and routers simultaneously. Performance\nWe compare Mixtral to the Llama 2 family and the GPT3.5 base model. Mixtral matches or outperforms Llama 2 70B, as well as GPT3.5, on most benchmarks. On the following figure, we measure the quality versus inference budget tradeoff.",
    "original_url": "https://mistral.ai/news/mixtral-of-experts",
    "original_title": "https://mistral.ai/news/mixtral-of-experts",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://mistral.ai/news/mixtral-of-experts_chunk_6",
    "chunk_text": "On the following figure, we measure the quality versus inference budget tradeoff. Mistral 7B and Mixtral 8x7B belong to a family of highly efficient models compared to Llama 2 models. The following table give detailed results on the figure above. Hallucination and biases. To identify possible flaws to be corrected by fine-tuning / preference modelling,\nwe measure the\nbase\nmodel performance on BBQ/BOLD.",
    "original_url": "https://mistral.ai/news/mixtral-of-experts",
    "original_title": "https://mistral.ai/news/mixtral-of-experts",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://mistral.ai/news/mixtral-of-experts_chunk_7",
    "chunk_text": "To identify possible flaws to be corrected by fine-tuning / preference modelling,\nwe measure the\nbase\nmodel performance on BBQ/BOLD. Compared to Llama 2, Mixtral presents less bias on the BBQ benchmark. Overall, Mixtral displays more positive sentiments than Llama 2 on BOLD, with similar variances within each dimension. Language. Mixtral 8x7B masters French, German, Spanish, Italian, and English.",
    "original_url": "https://mistral.ai/news/mixtral-of-experts",
    "original_title": "https://mistral.ai/news/mixtral-of-experts",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://mistral.ai/news/mixtral-of-experts_chunk_8",
    "chunk_text": "Mixtral 8x7B masters French, German, Spanish, Italian, and English. Instructed models\nWe release Mixtral 8x7B Instruct alongside Mixtral 8x7B. This model has been optimised through supervised fine-tuning and direct preference optimisation (DPO) for careful instruction following. On MT-Bench, it reaches a score of 8.30, making it the best open-source model, with a performance comparable to GPT3.5. Note: Mixtral can be gracefully prompted to ban some outputs from constructing applications that require a strong level of moderation, as exemplified\nhere\n.",
    "original_url": "https://mistral.ai/news/mixtral-of-experts",
    "original_title": "https://mistral.ai/news/mixtral-of-experts",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://mistral.ai/news/mixtral-of-experts_chunk_9",
    "chunk_text": "Note: Mixtral can be gracefully prompted to ban some outputs from constructing applications that require a strong level of moderation, as exemplified\nhere\n. A proper preference tuning can also serve this purpose. Bear in mind that without such a prompt, the model will just follow whatever instructions are given. Deploy Mixtral with an open-source deployment stack\nTo enable the community to run Mixtral with a fully open-source stack, we have submitted changes to the vLLM project, which integrates Megablocks CUDA kernels for efficient inference. Skypilot allows the deployment of vLLM endpoints on any instance in the cloud.",
    "original_url": "https://mistral.ai/news/mixtral-of-experts",
    "original_title": "https://mistral.ai/news/mixtral-of-experts",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://mistral.ai/news/mixtral-of-experts_chunk_10",
    "chunk_text": "Skypilot allows the deployment of vLLM endpoints on any instance in the cloud. Use Mixtral on our platform. We're currently using Mixtral 8x7B behind our endpoint\nmistral-small\n, which is\navailable in beta\n. Register\nto get early access to all generative and embedding endpoints. Acknowledgement\nWe thank CoreWeave and Scaleway teams for technical support as we trained our models.",
    "original_url": "https://mistral.ai/news/mixtral-of-experts",
    "original_title": "https://mistral.ai/news/mixtral-of-experts",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://mistral.ai/news/mixtral-of-experts_chunk_11",
    "chunk_text": "Acknowledgement\nWe thank CoreWeave and Scaleway teams for technical support as we trained our models. Share\nMore resources\nNews\nModels\nAI Services\nThe next chapter of AI is yours. Try le Chat\nBuild on la Plateforme\nTalk to an expert\nMistral AI © 2025\nWhy Mistral\nAbout us\nOur customers\nCareers\nContact us\nExplore\nAI solutions\nPartners\nResearch\nDocumentation\nBuild\nLa Plateforme\nLe Chat\nTry the API\nLegal\nTerms of service\nPrivacy policy\nPrivacy choices\nData processing agreement\nLegal notice\nEN\nMistral AI © 2025",
    "original_url": "https://mistral.ai/news/mixtral-of-experts",
    "original_title": "https://mistral.ai/news/mixtral-of-experts",
    "source": "web",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  }
]