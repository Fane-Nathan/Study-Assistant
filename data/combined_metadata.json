[
  {
    "chunk_id": "http://arxiv.org/pdf/2410.15578v1_chunk_0",
    "chunk_text": "Generalized Probabilistic Attention Mechanism in Transformers\n\nThe Transformer architecture has become widely adopted due to its demonstrated success, attributed to the attention mechanism at its core. Despite these successes, the attention mechanism of Transformers is associated with two well-known issues: rank-collapse and gradient vanishing. In this paper, we present a theoretical analysis that it is inherently difficult to address both issues simultaneously in the conventional attention mechanism. To handle these issues, we introduce a novel class of attention mechanism, referred to as generalized probabilistic attention mechanism (GPAM), and its dual-attention implementation within the Transformer architecture. Unlike conventional attention mechanisms, GPAM allows for negative attention scores while preserving a fixed total sum.",
    "original_url": "http://arxiv.org/pdf/2410.15578v1",
    "original_title": "Generalized Probabilistic Attention Mechanism in Transformers",
    "source": "arxiv",
    "authors": [
      "DongNyeong Heo",
      "Heeyoul Choi"
    ],
    "published": "2024-10-21T01:55:52+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2410.15578v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2410.15578v1_chunk_1",
    "chunk_text": "Unlike conventional attention mechanisms, GPAM allows for negative attention scores while preserving a fixed total sum. We provide theoretical evidence that the proposed dual-attention GPAM (daGPAM) effectively mitigates both the rank-collapse and gradient vanishing issues which are difficult to resolve simultaneously with the conventional attention mechanisms. Furthermore, we empirically validate this theoretical evidence, demonstrating the superiority of daGPAM compared to other alternative attention mechanisms that were proposed to address the same issues. Additionally, we demonstrate the practical benefits of GPAM in natural language processing tasks, such as language modeling and neural machine translation.",
    "original_url": "http://arxiv.org/pdf/2410.15578v1",
    "original_title": "Generalized Probabilistic Attention Mechanism in Transformers",
    "source": "arxiv",
    "authors": [
      "DongNyeong Heo",
      "Heeyoul Choi"
    ],
    "published": "2024-10-21T01:55:52+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2410.15578v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2209.15176v1_chunk_0",
    "chunk_text": "Adaptive Sparse and Monotonic Attention for Transformer-based Automatic Speech Recognition\n\nThe Transformer architecture model, based on self-attention and multi-head attention, has achieved remarkable success in offline end-to-end Automatic Speech Recognition (ASR). However, self-attention and multi-head attention cannot be easily applied for streaming or online ASR. For self-attention in Transformer ASR, the softmax normalization function-based attention mechanism makes it impossible to highlight important speech information. For multi-head attention in Transformer ASR, it is not easy to model monotonic alignments in different heads. To overcome these two limits, we integrate sparse attention and monotonic attention into Transformer-based ASR.",
    "original_url": "http://arxiv.org/pdf/2209.15176v1",
    "original_title": "Adaptive Sparse and Monotonic Attention for Transformer-based Automatic Speech Recognition",
    "source": "arxiv",
    "authors": [
      "Chendong Zhao",
      "Jianzong Wang",
      "Wen qi Wei",
      "Xiaoyang Qu",
      "Haoqian Wang",
      "Jing Xiao"
    ],
    "published": "2022-09-30T01:55:57+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2209.15176v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2209.15176v1_chunk_1",
    "chunk_text": "To overcome these two limits, we integrate sparse attention and monotonic attention into Transformer-based ASR. The sparse mechanism introduces a learned sparsity scheme to enable each self-attention structure to fit the corresponding head better. The monotonic attention deploys regularization to prune redundant heads for the multi-head attention structure. The experiments show that our method can effectively improve the attention mechanism on widely used benchmarks of speech recognition.",
    "original_url": "http://arxiv.org/pdf/2209.15176v1",
    "original_title": "Adaptive Sparse and Monotonic Attention for Transformer-based Automatic Speech Recognition",
    "source": "arxiv",
    "authors": [
      "Chendong Zhao",
      "Jianzong Wang",
      "Wen qi Wei",
      "Xiaoyang Qu",
      "Haoqian Wang",
      "Jing Xiao"
    ],
    "published": "2022-09-30T01:55:57+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2209.15176v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2303.01542v1_chunk_0",
    "chunk_text": "Self-attention in Vision Transformers Performs Perceptual Grouping, Not Attention\n\nRecently, a considerable number of studies in computer vision involves deep neural architectures called vision transformers. Visual processing in these models incorporates computational models that are claimed to implement attention mechanisms. Despite an increasing body of work that attempts to understand the role of attention mechanisms in vision transformers, their effect is largely unknown. Here, we asked if the attention mechanisms in vision transformers exhibit similar effects as those known in human visual attention. To answer this question, we revisited the attention formulation in these models and found that despite the name, computationally, these models perform a special class of relaxation labeling with similarity grouping effects.",
    "original_url": "http://arxiv.org/pdf/2303.01542v1",
    "original_title": "Self-attention in Vision Transformers Performs Perceptual Grouping, Not Attention",
    "source": "arxiv",
    "authors": [
      "Paria Mehrani",
      "John K. Tsotsos"
    ],
    "published": "2023-03-02T19:18:11+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2303.01542v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2303.01542v1_chunk_1",
    "chunk_text": "To answer this question, we revisited the attention formulation in these models and found that despite the name, computationally, these models perform a special class of relaxation labeling with similarity grouping effects. Additionally, whereas modern experimental findings reveal that human visual attention involves both feed-forward and feedback mechanisms, the purely feed-forward architecture of vision transformers suggests that attention in these models will not have the same effects as those known in humans. To quantify these observations, we evaluated grouping performance in a family of vision transformers. Our results suggest that self-attention modules group figures in the stimuli based on similarity in visual features such as color. Also, in a singleton detection experiment as an instance of saliency detection, we studied if these models exhibit similar effects as those of feed-forward visual salience mechanisms utilized in human visual attention.",
    "original_url": "http://arxiv.org/pdf/2303.01542v1",
    "original_title": "Self-attention in Vision Transformers Performs Perceptual Grouping, Not Attention",
    "source": "arxiv",
    "authors": [
      "Paria Mehrani",
      "John K. Tsotsos"
    ],
    "published": "2023-03-02T19:18:11+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2303.01542v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2303.01542v1_chunk_2",
    "chunk_text": "Also, in a singleton detection experiment as an instance of saliency detection, we studied if these models exhibit similar effects as those of feed-forward visual salience mechanisms utilized in human visual attention. We found that generally, the transformer-based attention modules assign more salience either to distractors or the ground. Together, our study suggests that the attention mechanisms in vision transformers perform similarity grouping and not attention.",
    "original_url": "http://arxiv.org/pdf/2303.01542v1",
    "original_title": "Self-attention in Vision Transformers Performs Perceptual Grouping, Not Attention",
    "source": "arxiv",
    "authors": [
      "Paria Mehrani",
      "John K. Tsotsos"
    ],
    "published": "2023-03-02T19:18:11+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2303.01542v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2108.01778v1_chunk_0",
    "chunk_text": "Armour: Generalizable Compact Self-Attention for Vision Transformers\n\nAttention-based transformer networks have demonstrated promising potential as their applications extend from natural language processing to vision. However, despite the recent improvements, such as sub-quadratic attention approximation and various training enhancements, the compact vision transformers to date using the regular attention still fall short in comparison with its convnet counterparts, in terms of \\textit{accuracy,} \\textit{model size}, \\textit{and} \\textit{throughput}. This paper introduces a compact self-attention mechanism that is fundamental and highly generalizable. The proposed method reduces redundancy and improves efficiency on top of the existing attention optimizations. We show its drop-in applicability for both the regular attention mechanism and some most recent variants in vision transformers.",
    "original_url": "http://arxiv.org/pdf/2108.01778v1",
    "original_title": "Armour: Generalizable Compact Self-Attention for Vision Transformers",
    "source": "arxiv",
    "authors": [
      "Lingchuan Meng"
    ],
    "published": "2021-08-03T22:33:58+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2108.01778v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2108.01778v1_chunk_1",
    "chunk_text": "We show its drop-in applicability for both the regular attention mechanism and some most recent variants in vision transformers. As a result, we produced smaller and faster models with the same or better accuracies.",
    "original_url": "http://arxiv.org/pdf/2108.01778v1",
    "original_title": "Armour: Generalizable Compact Self-Attention for Vision Transformers",
    "source": "arxiv",
    "authors": [
      "Lingchuan Meng"
    ],
    "published": "2021-08-03T22:33:58+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2108.01778v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2103.15679v1_chunk_0",
    "chunk_text": "Generic Attention-model Explainability for Interpreting Bi-Modal and Encoder-Decoder Transformers\n\nTransformers are increasingly dominating multi-modal reasoning tasks, such as visual question answering, achieving state-of-the-art results thanks to their ability to contextualize information using the self-attention and co-attention mechanisms. These attention modules also play a role in other computer vision tasks including object detection and image segmentation. Unlike Transformers that only use self-attention, Transformers with co-attention require to consider multiple attention maps in parallel in order to highlight the information that is relevant to the prediction in the model's input. In this work, we propose the first method to explain prediction by any Transformer-based architecture, including bi-modal Transformers and Transformers with co-attentions. We provide generic solutions and apply these to the three most commonly used of these architectures: (i) pure self-attention, (ii) self-attention combined with co-attention, and (iii) encoder-decoder attention.",
    "original_url": "http://arxiv.org/pdf/2103.15679v1",
    "original_title": "Generic Attention-model Explainability for Interpreting Bi-Modal and Encoder-Decoder Transformers",
    "source": "arxiv",
    "authors": [
      "Hila Chefer",
      "Shir Gur",
      "Lior Wolf"
    ],
    "published": "2021-03-29T15:03:11+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2103.15679v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2103.15679v1_chunk_1",
    "chunk_text": "We provide generic solutions and apply these to the three most commonly used of these architectures: (i) pure self-attention, (ii) self-attention combined with co-attention, and (iii) encoder-decoder attention. We show that our method is superior to all existing methods which are adapted from single modality explainability.",
    "original_url": "http://arxiv.org/pdf/2103.15679v1",
    "original_title": "Generic Attention-model Explainability for Interpreting Bi-Modal and Encoder-Decoder Transformers",
    "source": "arxiv",
    "authors": [
      "Hila Chefer",
      "Shir Gur",
      "Lior Wolf"
    ],
    "published": "2021-03-29T15:03:11+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2103.15679v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2112.14000v1_chunk_0",
    "chunk_text": "Pale Transformer: A General Vision Transformer Backbone with Pale-Shaped Attention\n\nRecently, Transformers have shown promising performance in various vision tasks. To reduce the quadratic computation complexity caused by the global self-attention, various methods constrain the range of attention within a local region to improve its efficiency. Consequently, their receptive fields in a single attention layer are not large enough, resulting in insufficient context modeling. To address this issue, we propose a Pale-Shaped self-Attention (PS-Attention), which performs self-attention within a pale-shaped region. Compared to the global self-attention, PS-Attention can reduce the computation and memory costs significantly.",
    "original_url": "http://arxiv.org/pdf/2112.14000v1",
    "original_title": "Pale Transformer: A General Vision Transformer Backbone with Pale-Shaped Attention",
    "source": "arxiv",
    "authors": [
      "Sitong Wu",
      "Tianyi Wu",
      "Haoru Tan",
      "Guodong Guo"
    ],
    "published": "2021-12-28T05:37:24+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2112.14000v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2112.14000v1_chunk_1",
    "chunk_text": "Compared to the global self-attention, PS-Attention can reduce the computation and memory costs significantly. Meanwhile, it can capture richer contextual information under the similar computation complexity with previous local self-attention mechanisms. Based on the PS-Attention, we develop a general Vision Transformer backbone with a hierarchical architecture, named Pale Transformer, which achieves 83.4%, 84.3%, and 84.9% Top-1 accuracy with the model size of 22M, 48M, and 85M respectively for 224 ImageNet-1K classification, outperforming the previous Vision Transformer backbones. For downstream tasks, our Pale Transformer backbone performs better than the recent state-of-the-art CSWin Transformer by a large margin on ADE20K semantic segmentation and COCO object detection & instance segmentation. The code will be released on https://github.com/BR-IDL/PaddleViT.",
    "original_url": "http://arxiv.org/pdf/2112.14000v1",
    "original_title": "Pale Transformer: A General Vision Transformer Backbone with Pale-Shaped Attention",
    "source": "arxiv",
    "authors": [
      "Sitong Wu",
      "Tianyi Wu",
      "Haoru Tan",
      "Guodong Guo"
    ],
    "published": "2021-12-28T05:37:24+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2112.14000v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2112.14000v1_chunk_2",
    "chunk_text": "The code will be released on https://github.com/BR-IDL/PaddleViT.",
    "original_url": "http://arxiv.org/pdf/2112.14000v1",
    "original_title": "Pale Transformer: A General Vision Transformer Backbone with Pale-Shaped Attention",
    "source": "arxiv",
    "authors": [
      "Sitong Wu",
      "Tianyi Wu",
      "Haoru Tan",
      "Guodong Guo"
    ],
    "published": "2021-12-28T05:37:24+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2112.14000v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2407.01548v1_chunk_0",
    "chunk_text": "From Cognition to Computation: A Comparative Review of Human Attention and Transformer Architectures\n\nAttention is a cornerstone of human cognition that facilitates the efficient extraction of information in everyday life. Recent developments in artificial intelligence like the Transformer architecture also incorporate the idea of attention in model designs. However, despite the shared fundamental principle of selectively attending to information, human attention and the Transformer model display notable differences, particularly in their capacity constraints, attention pathways, and intentional mechanisms. Our review aims to provide a comparative analysis of these mechanisms from a cognitive-functional perspective, thereby shedding light on several open research questions. The exploration encourages interdisciplinary efforts to derive insights from human attention mechanisms in the pursuit of developing more generalized artificial intelligence.",
    "original_url": "http://arxiv.org/pdf/2407.01548v1",
    "original_title": "From Cognition to Computation: A Comparative Review of Human Attention and Transformer Architectures",
    "source": "arxiv",
    "authors": [
      "Minglu Zhao",
      "Dehong Xu",
      "Tao Gao"
    ],
    "published": "2024-04-25T05:13:38+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2407.01548v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2407.01548v1_chunk_1",
    "chunk_text": "The exploration encourages interdisciplinary efforts to derive insights from human attention mechanisms in the pursuit of developing more generalized artificial intelligence.",
    "original_url": "http://arxiv.org/pdf/2407.01548v1",
    "original_title": "From Cognition to Computation: A Comparative Review of Human Attention and Transformer Architectures",
    "source": "arxiv",
    "authors": [
      "Minglu Zhao",
      "Dehong Xu",
      "Tao Gao"
    ],
    "published": "2024-04-25T05:13:38+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2407.01548v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1907.06607v1_chunk_0",
    "chunk_text": "Agglomerative Attention\n\nNeural networks using transformer-based architectures have recently demonstrated great power and flexibility in modeling sequences of many types. One of the core components of transformer networks is the attention layer, which allows contextual information to be exchanged among sequence elements. While many of the prevalent network structures thus far have utilized full attention -- which operates on all pairs of sequence elements -- the quadratic scaling of this attention mechanism significantly constrains the size of models that can be trained. In this work, we present an attention model that has only linear requirements in memory and computation time. We show that, despite the simpler attention model, networks using this attention mechanism can attain comparable performance to full attention networks on language modeling tasks.",
    "original_url": "http://arxiv.org/pdf/1907.06607v1",
    "original_title": "Agglomerative Attention",
    "source": "arxiv",
    "authors": [
      "Matthew Spellings"
    ],
    "published": "2019-07-15T17:11:05+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1907.06607v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1907.06607v1_chunk_1",
    "chunk_text": "We show that, despite the simpler attention model, networks using this attention mechanism can attain comparable performance to full attention networks on language modeling tasks.",
    "original_url": "http://arxiv.org/pdf/1907.06607v1",
    "original_title": "Agglomerative Attention",
    "source": "arxiv",
    "authors": [
      "Matthew Spellings"
    ],
    "published": "2019-07-15T17:11:05+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1907.06607v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2405.04515v2_chunk_0",
    "chunk_text": "A Transformer with Stack Attention\n\nNatural languages are believed to be (mildly) context-sensitive. Despite underpinning remarkably capable large language models, transformers are unable to model many context-free language tasks. In an attempt to address this limitation in the modeling power of transformer-based language models, we propose augmenting them with a differentiable, stack-based attention mechanism. Our stack-based attention mechanism can be incorporated into any transformer-based language model and adds a level of interpretability to the model. We show that the addition of our stack-based attention mechanism enables the transformer to model some, but not all, deterministic context-free languages.",
    "original_url": "http://arxiv.org/pdf/2405.04515v2",
    "original_title": "A Transformer with Stack Attention",
    "source": "arxiv",
    "authors": [
      "Jiaoda Li",
      "Jennifer C. White",
      "Mrinmaya Sachan",
      "Ryan Cotterell"
    ],
    "published": "2024-05-07T17:47:57+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2405.04515v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2405.04515v2_chunk_1",
    "chunk_text": "We show that the addition of our stack-based attention mechanism enables the transformer to model some, but not all, deterministic context-free languages.",
    "original_url": "http://arxiv.org/pdf/2405.04515v2",
    "original_title": "A Transformer with Stack Attention",
    "source": "arxiv",
    "authors": [
      "Jiaoda Li",
      "Jennifer C. White",
      "Mrinmaya Sachan",
      "Ryan Cotterell"
    ],
    "published": "2024-05-07T17:47:57+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2405.04515v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1904.05873v1_chunk_0",
    "chunk_text": "An Empirical Study of Spatial Attention Mechanisms in Deep Networks\n\nAttention mechanisms have become a popular component in deep neural networks, yet there has been little examination of how different influencing factors and methods for computing attention from these factors affect performance. Toward a better general understanding of attention mechanisms, we present an empirical study that ablates various spatial attention elements within a generalized attention formulation, encompassing the dominant Transformer attention as well as the prevalent deformable convolution and dynamic convolution modules. Conducted on a variety of applications, the study yields significant findings about spatial attention in deep networks, some of which run counter to conventional understanding. For example, we find that the query and key content comparison in Transformer attention is negligible for self-attention, but vital for encoder-decoder attention. A proper combination of deformable convolution with key content only saliency achieves the best accuracy-efficiency tradeoff in self-attention.",
    "original_url": "http://arxiv.org/pdf/1904.05873v1",
    "original_title": "An Empirical Study of Spatial Attention Mechanisms in Deep Networks",
    "source": "arxiv",
    "authors": [
      "Xizhou Zhu",
      "Dazhi Cheng",
      "Zheng Zhang",
      "Stephen Lin",
      "Jifeng Dai"
    ],
    "published": "2019-04-11T17:58:37+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1904.05873v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1904.05873v1_chunk_1",
    "chunk_text": "A proper combination of deformable convolution with key content only saliency achieves the best accuracy-efficiency tradeoff in self-attention. Our results suggest that there exists much room for improvement in the design of attention mechanisms.",
    "original_url": "http://arxiv.org/pdf/1904.05873v1",
    "original_title": "An Empirical Study of Spatial Attention Mechanisms in Deep Networks",
    "source": "arxiv",
    "authors": [
      "Xizhou Zhu",
      "Dazhi Cheng",
      "Zheng Zhang",
      "Stephen Lin",
      "Jifeng Dai"
    ],
    "published": "2019-04-11T17:58:37+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1904.05873v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2201.13027v2_chunk_0",
    "chunk_text": "BOAT: Bilateral Local Attention Vision Transformer\n\nVision Transformers achieved outstanding performance in many computer vision tasks. Early Vision Transformers such as ViT and DeiT adopt global self-attention, which is computationally expensive when the number of patches is large. To improve efficiency, recent Vision Transformers adopt local self-attention mechanisms, where self-attention is computed within local windows. Despite the fact that window-based local self-attention significantly boosts efficiency, it fails to capture the relationships between distant but similar patches in the image plane. To overcome this limitation of image-space local attention, in this paper, we further exploit the locality of patches in the feature space.",
    "original_url": "http://arxiv.org/pdf/2201.13027v2",
    "original_title": "BOAT: Bilateral Local Attention Vision Transformer",
    "source": "arxiv",
    "authors": [
      "Tan Yu",
      "Gangming Zhao",
      "Ping Li",
      "Yizhou Yu"
    ],
    "published": "2022-01-31T07:09:50+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2201.13027v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2201.13027v2_chunk_1",
    "chunk_text": "To overcome this limitation of image-space local attention, in this paper, we further exploit the locality of patches in the feature space. We group the patches into multiple clusters using their features, and self-attention is computed within every cluster. Such feature-space local attention effectively captures the connections between patches across different local windows but still relevant. We propose a Bilateral lOcal Attention vision Transformer (BOAT), which integrates feature-space local attention with image-space local attention. We further integrate BOAT with both Swin and CSWin models, and extensive experiments on several benchmark datasets demonstrate that our BOAT-CSWin model clearly and consistently outperforms existing state-of-the-art CNN models and vision Transformers.",
    "original_url": "http://arxiv.org/pdf/2201.13027v2",
    "original_title": "BOAT: Bilateral Local Attention Vision Transformer",
    "source": "arxiv",
    "authors": [
      "Tan Yu",
      "Gangming Zhao",
      "Ping Li",
      "Yizhou Yu"
    ],
    "published": "2022-01-31T07:09:50+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2201.13027v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2201.13027v2_chunk_2",
    "chunk_text": "We further integrate BOAT with both Swin and CSWin models, and extensive experiments on several benchmark datasets demonstrate that our BOAT-CSWin model clearly and consistently outperforms existing state-of-the-art CNN models and vision Transformers.",
    "original_url": "http://arxiv.org/pdf/2201.13027v2",
    "original_title": "BOAT: Bilateral Local Attention Vision Transformer",
    "source": "arxiv",
    "authors": [
      "Tan Yu",
      "Gangming Zhao",
      "Ping Li",
      "Yizhou Yu"
    ],
    "published": "2022-01-31T07:09:50+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2201.13027v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2410.02703v2_chunk_0",
    "chunk_text": "Selective Attention Improves Transformer\n\nUnneeded elements in the attention's context degrade performance. We introduce Selective Attention, a simple parameter-free change to the standard attention mechanism which reduces attention to unneeded elements. Selective attention consistently improves language modeling and downstream task performance in a variety of model sizes and context lengths. For example, transformers trained with the language modeling objective on C4 with selective attention perform language modeling equivalently to standard transformers with ~2X more heads and parameters in their attention modules. Selective attention also allows decreasing the size of the attention's context buffer, leading to meaningful reductions in the memory and compute requirements during inference.",
    "original_url": "http://arxiv.org/pdf/2410.02703v2",
    "original_title": "Selective Attention Improves Transformer",
    "source": "arxiv",
    "authors": [
      "Yaniv Leviathan",
      "Matan Kalman",
      "Yossi Matias"
    ],
    "published": "2024-10-03T17:27:30+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2410.02703v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2410.02703v2_chunk_1",
    "chunk_text": "Selective attention also allows decreasing the size of the attention's context buffer, leading to meaningful reductions in the memory and compute requirements during inference. For example, transformers trained on C4 with context sizes of 512, 1,024, and 2,048 need 16X, 25X, and 47X less memory for their attention module, respectively, when equipped with selective attention, as those without selective attention, with the same validation perplexity.",
    "original_url": "http://arxiv.org/pdf/2410.02703v2",
    "original_title": "Selective Attention Improves Transformer",
    "source": "arxiv",
    "authors": [
      "Yaniv Leviathan",
      "Matan Kalman",
      "Yossi Matias"
    ],
    "published": "2024-10-03T17:27:30+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2410.02703v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2307.04723v3_chunk_0",
    "chunk_text": "Quark/Gluon Discrimination and Top Tagging with Dual Attention Transformer\n\nJet tagging is a crucial classification task in high energy physics. Recently the performance of jet tagging has been significantly improved by the application of deep learning techniques. In this study, we introduce a new architecture for jet tagging: the Particle Dual Attention Transformer (P-DAT). This novel transformer architecture stands out by concurrently capturing both global and local information, while maintaining computational efficiency. Regarding the self attention mechanism, we have extended the established attention mechanism between particles to encompass the attention mechanism between particle features.",
    "original_url": "http://arxiv.org/pdf/2307.04723v3",
    "original_title": "Quark/Gluon Discrimination and Top Tagging with Dual Attention Transformer",
    "source": "arxiv",
    "authors": [
      "Minxuan He",
      "Daohan Wang"
    ],
    "published": "2023-07-10T17:33:54+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2307.04723v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2307.04723v3_chunk_1",
    "chunk_text": "Regarding the self attention mechanism, we have extended the established attention mechanism between particles to encompass the attention mechanism between particle features. The particle attention module computes particle level interactions across all the particles, while the channel attention module computes attention scores between particle features, which naturally captures jet level interactions by taking all particles into account. These two kinds of attention mechanisms can complement each other. Further, we incorporate both the pairwise particle interactions and the pairwise jet feature interactions in the attention mechanism. We demonstrate the effectiveness of the P-DAT architecture in classic top tagging and quark-gluon discrimination tasks, achieving competitive performance compared to other benchmark strategies.",
    "original_url": "http://arxiv.org/pdf/2307.04723v3",
    "original_title": "Quark/Gluon Discrimination and Top Tagging with Dual Attention Transformer",
    "source": "arxiv",
    "authors": [
      "Minxuan He",
      "Daohan Wang"
    ],
    "published": "2023-07-10T17:33:54+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2307.04723v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2307.04723v3_chunk_2",
    "chunk_text": "We demonstrate the effectiveness of the P-DAT architecture in classic top tagging and quark-gluon discrimination tasks, achieving competitive performance compared to other benchmark strategies.",
    "original_url": "http://arxiv.org/pdf/2307.04723v3",
    "original_title": "Quark/Gluon Discrimination and Top Tagging with Dual Attention Transformer",
    "source": "arxiv",
    "authors": [
      "Minxuan He",
      "Daohan Wang"
    ],
    "published": "2023-07-10T17:33:54+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2307.04723v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2411.14109v1_chunk_0",
    "chunk_text": "Global and Local Attention-Based Transformer for Hyperspectral Image Change Detection\n\nRecently Transformer-based hyperspectral image (HSI) change detection methods have shown remarkable performance. Nevertheless, existing attention mechanisms in Transformers have limitations in local feature representation. To address this issue, we propose Global and Local Attention-based Transformer (GLAFormer), which incorporates a global and local attention module (GLAM) to combine high-frequency and low-frequency signals. Furthermore, we introduce a cross-gating mechanism, called cross-gated feed-forward network (CGFN), to emphasize salient features and suppress noise interference. Specifically, the GLAM splits attention heads into global and local attention components to capture comprehensive spatial-spectral features.",
    "original_url": "http://arxiv.org/pdf/2411.14109v1",
    "original_title": "Global and Local Attention-Based Transformer for Hyperspectral Image Change Detection",
    "source": "arxiv",
    "authors": [
      "Ziyi Wang",
      "Feng Gao",
      "Junyu Dong",
      "Qian Du"
    ],
    "published": "2024-11-21T13:17:52+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2411.14109v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2411.14109v1_chunk_1",
    "chunk_text": "Specifically, the GLAM splits attention heads into global and local attention components to capture comprehensive spatial-spectral features. The global attention component employs global attention on downsampled feature maps to capture low-frequency information, while the local attention component focuses on high-frequency details using non-overlapping window-based local attention. The CGFN enhances the feature representation via convolutions and cross-gating mechanism in parallel paths. The proposed GLAFormer is evaluated on three HSI datasets. The results demonstrate its superiority over state-of-the-art HSI change detection methods.",
    "original_url": "http://arxiv.org/pdf/2411.14109v1",
    "original_title": "Global and Local Attention-Based Transformer for Hyperspectral Image Change Detection",
    "source": "arxiv",
    "authors": [
      "Ziyi Wang",
      "Feng Gao",
      "Junyu Dong",
      "Qian Du"
    ],
    "published": "2024-11-21T13:17:52+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2411.14109v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2411.14109v1_chunk_2",
    "chunk_text": "The results demonstrate its superiority over state-of-the-art HSI change detection methods. The source code of GLAFormer is available at \\url{https://github.com/summitgao/GLAFormer}.",
    "original_url": "http://arxiv.org/pdf/2411.14109v1",
    "original_title": "Global and Local Attention-Based Transformer for Hyperspectral Image Change Detection",
    "source": "arxiv",
    "authors": [
      "Ziyi Wang",
      "Feng Gao",
      "Junyu Dong",
      "Qian Du"
    ],
    "published": "2024-11-21T13:17:52+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2411.14109v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2305.14768v1_chunk_0",
    "chunk_text": "Dual Path Transformer with Partition Attention\n\nThis paper introduces a novel attention mechanism, called dual attention, which is both efficient and effective. The dual attention mechanism consists of two parallel components: local attention generated by Convolutional Neural Networks (CNNs) and long-range attention generated by Vision Transformers (ViTs). To address the high computational complexity and memory footprint of vanilla Multi-Head Self-Attention (MHSA), we introduce a novel Multi-Head Partition-wise Attention (MHPA) mechanism. The partition-wise attention approach models both intra-partition and inter-partition attention simultaneously. Building on the dual attention block and partition-wise attention mechanism, we present a hierarchical vision backbone called DualFormer.",
    "original_url": "http://arxiv.org/pdf/2305.14768v1",
    "original_title": "Dual Path Transformer with Partition Attention",
    "source": "arxiv",
    "authors": [
      "Zhengkai Jiang",
      "Liang Liu",
      "Jiangning Zhang",
      "Yabiao Wang",
      "Mingang Chen",
      "Chengjie Wang"
    ],
    "published": "2023-05-24T06:17:53+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2305.14768v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2305.14768v1_chunk_1",
    "chunk_text": "Building on the dual attention block and partition-wise attention mechanism, we present a hierarchical vision backbone called DualFormer. We evaluate the effectiveness of our model on several computer vision tasks, including image classification on ImageNet, object detection on COCO, and semantic segmentation on Cityscapes. Specifically, the proposed DualFormer-XS achieves 81.5\\% top-1 accuracy on ImageNet, outperforming the recent state-of-the-art MPViT-XS by 0.6\\% top-1 accuracy with much higher throughput.",
    "original_url": "http://arxiv.org/pdf/2305.14768v1",
    "original_title": "Dual Path Transformer with Partition Attention",
    "source": "arxiv",
    "authors": [
      "Zhengkai Jiang",
      "Liang Liu",
      "Jiangning Zhang",
      "Yabiao Wang",
      "Mingang Chen",
      "Chengjie Wang"
    ],
    "published": "2023-05-24T06:17:53+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2305.14768v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2106.05786v1_chunk_0",
    "chunk_text": "CAT: Cross Attention in Vision Transformer\n\nSince Transformer has found widespread use in NLP, the potential of Transformer in CV has been realized and has inspired many new approaches. However, the computation required for replacing word tokens with image patches for Transformer after the tokenization of the image is vast(e.g., ViT), which bottlenecks model training and inference. In this paper, we propose a new attention mechanism in Transformer termed Cross Attention, which alternates attention inner the image patch instead of the whole image to capture local information and apply attention between image patches which are divided from single-channel feature maps capture global information. Both operations have less computation than standard self-attention in Transformer. By alternately applying attention inner patch and between patches, we implement cross attention to maintain the performance with lower computational cost and build a hierarchical network called Cross Attention Transformer(CAT) for other vision tasks.",
    "original_url": "http://arxiv.org/pdf/2106.05786v1",
    "original_title": "CAT: Cross Attention in Vision Transformer",
    "source": "arxiv",
    "authors": [
      "Hezheng Lin",
      "Xing Cheng",
      "Xiangyu Wu",
      "Fan Yang",
      "Dong Shen",
      "Zhongyuan Wang",
      "Qing Song",
      "Wei Yuan"
    ],
    "published": "2021-06-10T14:38:32+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2106.05786v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2106.05786v1_chunk_1",
    "chunk_text": "By alternately applying attention inner patch and between patches, we implement cross attention to maintain the performance with lower computational cost and build a hierarchical network called Cross Attention Transformer(CAT) for other vision tasks. Our base model achieves state-of-the-arts on ImageNet-1K, and improves the performance of other methods on COCO and ADE20K, illustrating that our network has the potential to serve as general backbones. The code and models are available at \\url{https://github.com/linhezheng19/CAT}.",
    "original_url": "http://arxiv.org/pdf/2106.05786v1",
    "original_title": "CAT: Cross Attention in Vision Transformer",
    "source": "arxiv",
    "authors": [
      "Hezheng Lin",
      "Xing Cheng",
      "Xiangyu Wu",
      "Fan Yang",
      "Dong Shen",
      "Zhongyuan Wang",
      "Qing Song",
      "Wei Yuan"
    ],
    "published": "2021-06-10T14:38:32+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2106.05786v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2406.00427v1_chunk_0",
    "chunk_text": "You Only Need Less Attention at Each Stage in Vision Transformers\n\nThe advent of Vision Transformers (ViTs) marks a substantial paradigm shift in the realm of computer vision. ViTs capture the global information of images through self-attention modules, which perform dot product computations among patchified image tokens. While self-attention modules empower ViTs to capture long-range dependencies, the computational complexity grows quadratically with the number of tokens, which is a major hindrance to the practical application of ViTs. Moreover, the self-attention mechanism in deep ViTs is also susceptible to the attention saturation issue. Accordingly, we argue against the necessity of computing the attention scores in every layer, and we propose the Less-Attention Vision Transformer (LaViT), which computes only a few attention operations at each stage and calculates the subsequent feature alignments in other layers via attention transformations that leverage the previously calculated attention scores.",
    "original_url": "http://arxiv.org/pdf/2406.00427v1",
    "original_title": "You Only Need Less Attention at Each Stage in Vision Transformers",
    "source": "arxiv",
    "authors": [
      "Shuoxi Zhang",
      "Hanpeng Liu",
      "Stephen Lin",
      "Kun He"
    ],
    "published": "2024-06-01T12:49:16+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2406.00427v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2406.00427v1_chunk_1",
    "chunk_text": "Accordingly, we argue against the necessity of computing the attention scores in every layer, and we propose the Less-Attention Vision Transformer (LaViT), which computes only a few attention operations at each stage and calculates the subsequent feature alignments in other layers via attention transformations that leverage the previously calculated attention scores. This novel approach can mitigate two primary issues plaguing traditional self-attention modules: the heavy computational burden and attention saturation. Our proposed architecture offers superior efficiency and ease of implementation, merely requiring matrix multiplications that are highly optimized in contemporary deep learning frameworks. Moreover, our architecture demonstrates exceptional performance across various vision tasks including classification, detection and segmentation.",
    "original_url": "http://arxiv.org/pdf/2406.00427v1",
    "original_title": "You Only Need Less Attention at Each Stage in Vision Transformers",
    "source": "arxiv",
    "authors": [
      "Shuoxi Zhang",
      "Hanpeng Liu",
      "Stephen Lin",
      "Kun He"
    ],
    "published": "2024-06-01T12:49:16+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2406.00427v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2203.02636v1_chunk_0",
    "chunk_text": "Boosting Crowd Counting via Multifaceted Attention\n\nThis paper focuses on the challenging crowd counting task. As large-scale variations often exist within crowd images, neither fixed-size convolution kernel of CNN nor fixed-size attention of recent vision transformers can well handle this kind of variation. To address this problem, we propose a Multifaceted Attention Network (MAN) to improve transformer models in local spatial relation encoding. MAN incorporates global attention from a vanilla transformer, learnable local attention, and instance attention into a counting model. Firstly, the local Learnable Region Attention (LRA) is proposed to assign attention exclusively for each feature location dynamically.",
    "original_url": "http://arxiv.org/pdf/2203.02636v1",
    "original_title": "Boosting Crowd Counting via Multifaceted Attention",
    "source": "arxiv",
    "authors": [
      "Hui Lin",
      "Zhiheng Ma",
      "Rongrong Ji",
      "Yaowei Wang",
      "Xiaopeng Hong"
    ],
    "published": "2022-03-05T01:36:43+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2203.02636v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2203.02636v1_chunk_1",
    "chunk_text": "Firstly, the local Learnable Region Attention (LRA) is proposed to assign attention exclusively for each feature location dynamically. Secondly, we design the Local Attention Regularization to supervise the training of LRA by minimizing the deviation among the attention for different feature locations. Finally, we provide an Instance Attention mechanism to focus on the most important instances dynamically during training. Extensive experiments on four challenging crowd counting datasets namely ShanghaiTech, UCF-QNRF, JHU++, and NWPU have validated the proposed method. Codes: https://github.com/LoraLinH/Boosting-Crowd-Counting-via-Multifaceted-Attention.",
    "original_url": "http://arxiv.org/pdf/2203.02636v1",
    "original_title": "Boosting Crowd Counting via Multifaceted Attention",
    "source": "arxiv",
    "authors": [
      "Hui Lin",
      "Zhiheng Ma",
      "Rongrong Ji",
      "Yaowei Wang",
      "Xiaopeng Hong"
    ],
    "published": "2022-03-05T01:36:43+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2203.02636v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2203.02636v1_chunk_2",
    "chunk_text": "Codes: https://github.com/LoraLinH/Boosting-Crowd-Counting-via-Multifaceted-Attention.",
    "original_url": "http://arxiv.org/pdf/2203.02636v1",
    "original_title": "Boosting Crowd Counting via Multifaceted Attention",
    "source": "arxiv",
    "authors": [
      "Hui Lin",
      "Zhiheng Ma",
      "Rongrong Ji",
      "Yaowei Wang",
      "Xiaopeng Hong"
    ],
    "published": "2022-03-05T01:36:43+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2203.02636v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2502.19281v1_chunk_0",
    "chunk_text": "Integrating Biological and Machine Intelligence: Attention Mechanisms in Brain-Computer Interfaces\n\nWith the rapid advancement of deep learning, attention mechanisms have become indispensable in electroencephalography (EEG) signal analysis, significantly enhancing Brain-Computer Interface (BCI) applications. This paper presents a comprehensive review of traditional and Transformer-based attention mechanisms, their embedding strategies, and their applications in EEG-based BCI, with a particular emphasis on multimodal data fusion. By capturing EEG variations across time, frequency, and spatial channels, attention mechanisms improve feature extraction, representation learning, and model robustness. These methods can be broadly categorized into traditional attention mechanisms, which typically integrate with convolutional and recurrent networks, and Transformer-based multi-head self-attention, which excels in capturing long-range dependencies. Beyond single-modality analysis, attention mechanisms also enhance multimodal EEG applications, facilitating effective fusion between EEG and other physiological or sensory data.",
    "original_url": "http://arxiv.org/pdf/2502.19281v1",
    "original_title": "Integrating Biological and Machine Intelligence: Attention Mechanisms in Brain-Computer Interfaces",
    "source": "arxiv",
    "authors": [
      "Jiyuan Wang",
      "Weishan Ye",
      "Jialin He",
      "Li Zhang",
      "Gan Huang",
      "Zhuliang Yu",
      "Zhen Liang"
    ],
    "published": "2025-02-26T16:38:28+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2502.19281v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2502.19281v1_chunk_1",
    "chunk_text": "Beyond single-modality analysis, attention mechanisms also enhance multimodal EEG applications, facilitating effective fusion between EEG and other physiological or sensory data. Finally, we discuss existing challenges and emerging trends in attention-based EEG modeling, highlighting future directions for advancing BCI technology. This review aims to provide valuable insights for researchers seeking to leverage attention mechanisms for improved EEG interpretation and application.",
    "original_url": "http://arxiv.org/pdf/2502.19281v1",
    "original_title": "Integrating Biological and Machine Intelligence: Attention Mechanisms in Brain-Computer Interfaces",
    "source": "arxiv",
    "authors": [
      "Jiyuan Wang",
      "Weishan Ye",
      "Jialin He",
      "Li Zhang",
      "Gan Huang",
      "Zhuliang Yu",
      "Zhen Liang"
    ],
    "published": "2025-02-26T16:38:28+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2502.19281v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2210.00641v1_chunk_0",
    "chunk_text": "DARTFormer: Finding The Best Type Of Attention\n\nGiven the wide and ever growing range of different efficient Transformer attention mechanisms, it is important to identify which attention is most effective when given a task. In this work, we are also interested in combining different attention types to build heterogeneous Transformers. We first propose a DARTS-like Neural Architecture Search (NAS) method to find the best attention for a given task, in this setup, all heads use the same attention (homogeneous models). Our results suggest that NAS is highly effective on this task, and it identifies the best attention mechanisms for IMDb byte level text classification and Listops. We then extend our framework to search for and build Transformers with multiple different attention types, and call them heterogeneous Transformers.",
    "original_url": "http://arxiv.org/pdf/2210.00641v1",
    "original_title": "DARTFormer: Finding The Best Type Of Attention",
    "source": "arxiv",
    "authors": [
      "Jason Ross Brown",
      "Yiren Zhao",
      "Ilia Shumailov",
      "Robert D Mullins"
    ],
    "published": "2022-10-02T21:56:29+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2210.00641v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2210.00641v1_chunk_1",
    "chunk_text": "We then extend our framework to search for and build Transformers with multiple different attention types, and call them heterogeneous Transformers. We show that whilst these heterogeneous Transformers are better than the average homogeneous models, they cannot outperform the best. We explore the reasons why heterogeneous attention makes sense, and why it ultimately fails.",
    "original_url": "http://arxiv.org/pdf/2210.00641v1",
    "original_title": "DARTFormer: Finding The Best Type Of Attention",
    "source": "arxiv",
    "authors": [
      "Jason Ross Brown",
      "Yiren Zhao",
      "Ilia Shumailov",
      "Robert D Mullins"
    ],
    "published": "2022-10-02T21:56:29+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2210.00641v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2103.05103v1_chunk_0",
    "chunk_text": "Image Captioning using Multiple Transformers for Self-Attention Mechanism\n\nReal-time image captioning, along with adequate precision, is the main challenge of this research field. The present work, Multiple Transformers for Self-Attention Mechanism (MTSM), utilizes multiple transformers to address these problems. The proposed algorithm, MTSM, acquires region proposals using a transformer detector (DETR). Consequently, MTSM achieves the self-attention mechanism by transferring these region proposals and their visual and geometrical features through another transformer and learns the objects' local and global interconnections. The qualitative and quantitative results of the proposed algorithm, MTSM, are shown on the MSCOCO dataset.",
    "original_url": "http://arxiv.org/pdf/2103.05103v1",
    "original_title": "Image Captioning using Multiple Transformers for Self-Attention Mechanism",
    "source": "arxiv",
    "authors": [
      "Farrukh Olimov",
      "Shikha Dubey",
      "Labina Shrestha",
      "Tran Trung Tin",
      "Moongu Jeon"
    ],
    "published": "2021-02-14T05:35:54+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2103.05103v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2103.05103v1_chunk_1",
    "chunk_text": "The qualitative and quantitative results of the proposed algorithm, MTSM, are shown on the MSCOCO dataset.",
    "original_url": "http://arxiv.org/pdf/2103.05103v1",
    "original_title": "Image Captioning using Multiple Transformers for Self-Attention Mechanism",
    "source": "arxiv",
    "authors": [
      "Farrukh Olimov",
      "Shikha Dubey",
      "Labina Shrestha",
      "Tran Trung Tin",
      "Moongu Jeon"
    ],
    "published": "2021-02-14T05:35:54+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2103.05103v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2311.04653v2_chunk_0",
    "chunk_text": "Hybrid Focal and Full-Range Attention Based Graph Transformers\n\nThe paradigm of Transformers using the self-attention mechanism has manifested its advantage in learning graph-structured data. Yet, Graph Transformers are capable of modeling full range dependencies but are often deficient in extracting information from locality. A common practice is to utilize Message Passing Neural Networks (MPNNs) as an auxiliary to capture local information, which however are still inadequate for comprehending substructures. In this paper, we present a purely attention-based architecture, namely Focal and Full-Range Graph Transformer (FFGT), which can mitigate the loss of local information in learning global correlations. The core component of FFGT is a new mechanism of compound attention, which combines the conventional full-range attention with K-hop focal attention on ego-nets to aggregate both global and local information.",
    "original_url": "http://arxiv.org/pdf/2311.04653v2",
    "original_title": "Hybrid Focal and Full-Range Attention Based Graph Transformers",
    "source": "arxiv",
    "authors": [
      "Minhong Zhu",
      "Zhenhao Zhao",
      "Weiran Cai"
    ],
    "published": "2023-11-08T12:53:07+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2311.04653v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2311.04653v2_chunk_1",
    "chunk_text": "The core component of FFGT is a new mechanism of compound attention, which combines the conventional full-range attention with K-hop focal attention on ego-nets to aggregate both global and local information. Beyond the scope of canonical Transformers, the FFGT has the merit of being more substructure-aware. Our approach enhances the performance of existing Graph Transformers on various open datasets, while achieves compatible SOTA performance on several Long-Range Graph Benchmark (LRGB) datasets even with a vanilla transformer. We further examine influential factors on the optimal focal length of attention via introducing a novel synthetic dataset based on SBM-PATTERN.",
    "original_url": "http://arxiv.org/pdf/2311.04653v2",
    "original_title": "Hybrid Focal and Full-Range Attention Based Graph Transformers",
    "source": "arxiv",
    "authors": [
      "Minhong Zhu",
      "Zhenhao Zhao",
      "Weiran Cai"
    ],
    "published": "2023-11-08T12:53:07+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2311.04653v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1810.07595v1_chunk_0",
    "chunk_text": "An Analysis of Attention Mechanisms: The Case of Word Sense Disambiguation in Neural Machine Translation\n\nRecent work has shown that the encoder-decoder attention mechanisms in neural machine translation (NMT) are different from the word alignment in statistical machine translation. In this paper, we focus on analyzing encoder-decoder attention mechanisms, in the case of word sense disambiguation (WSD) in NMT models. We hypothesize that attention mechanisms pay more attention to context tokens when translating ambiguous words. We explore the attention distribution patterns when translating ambiguous nouns. Counter-intuitively, we find that attention mechanisms are likely to distribute more attention to the ambiguous noun itself rather than context tokens, in comparison to other nouns.",
    "original_url": "http://arxiv.org/pdf/1810.07595v1",
    "original_title": "An Analysis of Attention Mechanisms: The Case of Word Sense Disambiguation in Neural Machine Translation",
    "source": "arxiv",
    "authors": [
      "Gongbo Tang",
      "Rico Sennrich",
      "Joakim Nivre"
    ],
    "published": "2018-10-17T14:58:54+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1810.07595v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1810.07595v1_chunk_1",
    "chunk_text": "Counter-intuitively, we find that attention mechanisms are likely to distribute more attention to the ambiguous noun itself rather than context tokens, in comparison to other nouns. We conclude that attention mechanism is not the main mechanism used by NMT models to incorporate contextual information for WSD. The experimental results suggest that NMT models learn to encode contextual information necessary for WSD in the encoder hidden states. For the attention mechanism in Transformer models, we reveal that the first few layers gradually learn to \"align\" source and target tokens and the last few layers learn to extract features from the related but unaligned context tokens.",
    "original_url": "http://arxiv.org/pdf/1810.07595v1",
    "original_title": "An Analysis of Attention Mechanisms: The Case of Word Sense Disambiguation in Neural Machine Translation",
    "source": "arxiv",
    "authors": [
      "Gongbo Tang",
      "Rico Sennrich",
      "Joakim Nivre"
    ],
    "published": "2018-10-17T14:58:54+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1810.07595v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2108.09193v3_chunk_0",
    "chunk_text": "Smart Bird: Learnable Sparse Attention for Efficient and Effective Transformer\n\nTransformer has achieved great success in NLP. However, the quadratic complexity of the self-attention mechanism in Transformer makes it inefficient in handling long sequences. Many existing works explore to accelerate Transformers by computing sparse self-attention instead of a dense one, which usually attends to tokens at certain positions or randomly selected tokens. However, manually selected or random tokens may be uninformative for context modeling. In this paper, we propose Smart Bird, which is an efficient and effective Transformer with learnable sparse attention.",
    "original_url": "http://arxiv.org/pdf/2108.09193v3",
    "original_title": "Smart Bird: Learnable Sparse Attention for Efficient and Effective Transformer",
    "source": "arxiv",
    "authors": [
      "Chuhan Wu",
      "Fangzhao Wu",
      "Tao Qi",
      "Binxing Jiao",
      "Daxin Jiang",
      "Yongfeng Huang",
      "Xing Xie"
    ],
    "published": "2021-08-20T14:22:00+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2108.09193v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2108.09193v3_chunk_1",
    "chunk_text": "In this paper, we propose Smart Bird, which is an efficient and effective Transformer with learnable sparse attention. In Smart Bird, we first compute a sketched attention matrix with a single-head low-dimensional Transformer, which aims to find potential important interactions between tokens. We then sample token pairs based on their probability scores derived from the sketched attention matrix to generate different sparse attention index matrices for different attention heads. Finally, we select token embeddings according to the index matrices to form the input of sparse attention networks. Extensive experiments on six benchmark datasets for different tasks validate the efficiency and effectiveness of Smart Bird in text modeling.",
    "original_url": "http://arxiv.org/pdf/2108.09193v3",
    "original_title": "Smart Bird: Learnable Sparse Attention for Efficient and Effective Transformer",
    "source": "arxiv",
    "authors": [
      "Chuhan Wu",
      "Fangzhao Wu",
      "Tao Qi",
      "Binxing Jiao",
      "Daxin Jiang",
      "Yongfeng Huang",
      "Xing Xie"
    ],
    "published": "2021-08-20T14:22:00+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2108.09193v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2108.09193v3_chunk_2",
    "chunk_text": "Extensive experiments on six benchmark datasets for different tasks validate the efficiency and effectiveness of Smart Bird in text modeling.",
    "original_url": "http://arxiv.org/pdf/2108.09193v3",
    "original_title": "Smart Bird: Learnable Sparse Attention for Efficient and Effective Transformer",
    "source": "arxiv",
    "authors": [
      "Chuhan Wu",
      "Fangzhao Wu",
      "Tao Qi",
      "Binxing Jiao",
      "Daxin Jiang",
      "Yongfeng Huang",
      "Xing Xie"
    ],
    "published": "2021-08-20T14:22:00+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2108.09193v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2004.10102v2_chunk_0",
    "chunk_text": "Attention is Not Only a Weight: Analyzing Transformers with Vector Norms\n\nAttention is a key component of Transformers, which have recently achieved considerable success in natural language processing. Hence, attention is being extensively studied to investigate various linguistic capabilities of Transformers, focusing on analyzing the parallels between attention weights and specific linguistic phenomena. This paper shows that attention weights alone are only one of the two factors that determine the output of attention and proposes a norm-based analysis that incorporates the second factor, the norm of the transformed input vectors. The findings of our norm-based analyses of BERT and a Transformer-based neural machine translation system include the following: (i) contrary to previous studies, BERT pays poor attention to special tokens, and (ii) reasonable word alignment can be extracted from attention mechanisms of Transformer. These findings provide insights into the inner workings of Transformers.",
    "original_url": "http://arxiv.org/pdf/2004.10102v2",
    "original_title": "Attention is Not Only a Weight: Analyzing Transformers with Vector Norms",
    "source": "arxiv",
    "authors": [
      "Goro Kobayashi",
      "Tatsuki Kuribayashi",
      "Sho Yokoi",
      "Kentaro Inui"
    ],
    "published": "2020-04-21T15:22:27+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2004.10102v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2004.10102v2_chunk_1",
    "chunk_text": "These findings provide insights into the inner workings of Transformers.",
    "original_url": "http://arxiv.org/pdf/2004.10102v2",
    "original_title": "Attention is Not Only a Weight: Analyzing Transformers with Vector Norms",
    "source": "arxiv",
    "authors": [
      "Goro Kobayashi",
      "Tatsuki Kuribayashi",
      "Sho Yokoi",
      "Kentaro Inui"
    ],
    "published": "2020-04-21T15:22:27+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2004.10102v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2209.12816v2_chunk_0",
    "chunk_text": "Fast-FNet: Accelerating Transformer Encoder Models via Efficient Fourier Layers\n\nTransformer-based language models utilize the attention mechanism for substantial performance improvements in almost all natural language processing (NLP) tasks. Similar attention structures are also extensively studied in several other areas. Although the attention mechanism enhances the model performances significantly, its quadratic complexity prevents efficient processing of long sequences. Recent works focused on eliminating the disadvantages of computational inefficiency and showed that transformer-based models can still reach competitive results without the attention layer. A pioneering study proposed the FNet, which replaces the attention layer with the Fourier Transform (FT) in the transformer encoder architecture.",
    "original_url": "http://arxiv.org/pdf/2209.12816v2",
    "original_title": "Fast-FNet: Accelerating Transformer Encoder Models via Efficient Fourier Layers",
    "source": "arxiv",
    "authors": [
      "Nurullah Sevim",
      "Ege Ozan zyedek",
      "Furkan ahinu",
      "Aykut Ko"
    ],
    "published": "2022-09-26T16:23:02+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2209.12816v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2209.12816v2_chunk_1",
    "chunk_text": "A pioneering study proposed the FNet, which replaces the attention layer with the Fourier Transform (FT) in the transformer encoder architecture. FNet achieves competitive performances concerning the original transformer encoder model while accelerating training process by removing the computational burden of the attention mechanism. However, the FNet model ignores essential properties of the FT from the classical signal processing that can be leveraged to increase model efficiency further. We propose different methods to deploy FT efficiently in transformer encoder models. Our proposed architectures have smaller number of model parameters, shorter training times, less memory usage, and some additional performance improvements.",
    "original_url": "http://arxiv.org/pdf/2209.12816v2",
    "original_title": "Fast-FNet: Accelerating Transformer Encoder Models via Efficient Fourier Layers",
    "source": "arxiv",
    "authors": [
      "Nurullah Sevim",
      "Ege Ozan zyedek",
      "Furkan ahinu",
      "Aykut Ko"
    ],
    "published": "2022-09-26T16:23:02+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2209.12816v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2209.12816v2_chunk_2",
    "chunk_text": "Our proposed architectures have smaller number of model parameters, shorter training times, less memory usage, and some additional performance improvements. We demonstrate these improvements through extensive experiments on common benchmarks.",
    "original_url": "http://arxiv.org/pdf/2209.12816v2",
    "original_title": "Fast-FNet: Accelerating Transformer Encoder Models via Efficient Fourier Layers",
    "source": "arxiv",
    "authors": [
      "Nurullah Sevim",
      "Ege Ozan zyedek",
      "Furkan ahinu",
      "Aykut Ko"
    ],
    "published": "2022-09-26T16:23:02+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2209.12816v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2412.18288v1_chunk_0",
    "chunk_text": "Towards understanding how attention mechanism works in deep learning\n\nAttention mechanism has been extensively integrated within mainstream neural network architectures, such as Transformers and graph attention networks. Yet, its underlying working principles remain somewhat elusive. What is its essence? Are there any connections between it and traditional machine learning algorithms? In this study, we inspect the process of computing similarity using classic metrics and vector space properties in manifold learning, clustering, and supervised learning.",
    "original_url": "http://arxiv.org/pdf/2412.18288v1",
    "original_title": "Towards understanding how attention mechanism works in deep learning",
    "source": "arxiv",
    "authors": [
      "Tianyu Ruan",
      "Shihua Zhang"
    ],
    "published": "2024-12-24T08:52:06+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2412.18288v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2412.18288v1_chunk_1",
    "chunk_text": "In this study, we inspect the process of computing similarity using classic metrics and vector space properties in manifold learning, clustering, and supervised learning. We identify the key characteristics of similarity computation and information propagation in these methods and demonstrate that the self-attention mechanism in deep learning adheres to the same principles but operates more flexibly and adaptively. We decompose the self-attention mechanism into a learnable pseudo-metric function and an information propagation process based on similarity computation. We prove that the self-attention mechanism converges to a drift-diffusion process through continuous modeling provided the pseudo-metric is a transformation of a metric and certain reasonable assumptions hold. This equation could be transformed into a heat equation under a new metric.",
    "original_url": "http://arxiv.org/pdf/2412.18288v1",
    "original_title": "Towards understanding how attention mechanism works in deep learning",
    "source": "arxiv",
    "authors": [
      "Tianyu Ruan",
      "Shihua Zhang"
    ],
    "published": "2024-12-24T08:52:06+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2412.18288v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2412.18288v1_chunk_2",
    "chunk_text": "This equation could be transformed into a heat equation under a new metric. In addition, we give a first-order analysis of attention mechanism with a general pseudo-metric function. This study aids in understanding the effects and principle of attention mechanism through physical intuition. Finally, we propose a modified attention mechanism called metric-attention by leveraging the concept of metric learning to facilitate the ability to learn desired metrics more effectively. Experimental results demonstrate that it outperforms self-attention regarding training efficiency, accuracy, and robustness.",
    "original_url": "http://arxiv.org/pdf/2412.18288v1",
    "original_title": "Towards understanding how attention mechanism works in deep learning",
    "source": "arxiv",
    "authors": [
      "Tianyu Ruan",
      "Shihua Zhang"
    ],
    "published": "2024-12-24T08:52:06+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2412.18288v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2412.18288v1_chunk_3",
    "chunk_text": "Experimental results demonstrate that it outperforms self-attention regarding training efficiency, accuracy, and robustness.",
    "original_url": "http://arxiv.org/pdf/2412.18288v1",
    "original_title": "Towards understanding how attention mechanism works in deep learning",
    "source": "arxiv",
    "authors": [
      "Tianyu Ruan",
      "Shihua Zhang"
    ],
    "published": "2024-12-24T08:52:06+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2412.18288v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2411.01537v1_chunk_0",
    "chunk_text": "LinRec: Linear Attention Mechanism for Long-term Sequential Recommender Systems\n\nTransformer models have achieved remarkable success in sequential recommender systems (SRSs). However, computing the attention matrix in traditional dot-product attention mechanisms results in a quadratic complexity with sequence lengths, leading to high computational costs for long-term sequential recommendation. Motivated by the above observation, we propose a novel L2-Normalized Linear Attention for the Transformer-based Sequential Recommender Systems (LinRec), which theoretically improves efficiency while preserving the learning capabilities of the traditional dot-product attention. Specifically, by thoroughly examining the equivalence conditions of efficient attention mechanisms, we show that LinRec possesses linear complexity while preserving the property of attention mechanisms. In addition, we reveal its latent efficiency properties by interpreting the proposed LinRec mechanism through a statistical lens.",
    "original_url": "http://arxiv.org/pdf/2411.01537v1",
    "original_title": "LinRec: Linear Attention Mechanism for Long-term Sequential Recommender Systems",
    "source": "arxiv",
    "authors": [
      "Langming Liu",
      "Xiangyu Zhao",
      "Chi Zhang",
      "Jingtong Gao",
      "Wanyu Wang",
      "Wenqi Fan",
      "Yiqi Wang",
      "Ming He",
      "Zitao Liu",
      "Qing Li"
    ],
    "published": "2024-11-03T11:56:00+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2411.01537v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2411.01537v1_chunk_1",
    "chunk_text": "In addition, we reveal its latent efficiency properties by interpreting the proposed LinRec mechanism through a statistical lens. Extensive experiments are conducted based on two public benchmark datasets, demonstrating that the combination of LinRec and Transformer models achieves comparable or even superior performance than state-of-the-art Transformer-based SRS models while significantly improving time and memory efficiency.",
    "original_url": "http://arxiv.org/pdf/2411.01537v1",
    "original_title": "LinRec: Linear Attention Mechanism for Long-term Sequential Recommender Systems",
    "source": "arxiv",
    "authors": [
      "Langming Liu",
      "Xiangyu Zhao",
      "Chi Zhang",
      "Jingtong Gao",
      "Wanyu Wang",
      "Wenqi Fan",
      "Yiqi Wang",
      "Ming He",
      "Zitao Liu",
      "Qing Li"
    ],
    "published": "2024-11-03T11:56:00+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2411.01537v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2312.08874v3_chunk_0",
    "chunk_text": "Agent Attention: On the Integration of Softmax and Linear Attention\n\nThe attention module is the key component in Transformers. While the global attention mechanism offers high expressiveness, its excessive computational cost restricts its applicability in various scenarios. In this paper, we propose a novel attention paradigm, Agent Attention, to strike a favorable balance between computational efficiency and representation power. Specifically, the Agent Attention, denoted as a quadruple $(Q, A, K, V)$, introduces an additional set of agent tokens $A$ into the conventional attention module. The agent tokens first act as the agent for the query tokens $Q$ to aggregate information from $K$ and $V$, and then broadcast the information back to $Q$.",
    "original_url": "http://arxiv.org/pdf/2312.08874v3",
    "original_title": "Agent Attention: On the Integration of Softmax and Linear Attention",
    "source": "arxiv",
    "authors": [
      "Dongchen Han",
      "Tianzhu Ye",
      "Yizeng Han",
      "Zhuofan Xia",
      "Siyuan Pan",
      "Pengfei Wan",
      "Shiji Song",
      "Gao Huang"
    ],
    "published": "2023-12-14T16:26:29+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2312.08874v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2312.08874v3_chunk_1",
    "chunk_text": "The agent tokens first act as the agent for the query tokens $Q$ to aggregate information from $K$ and $V$, and then broadcast the information back to $Q$. Given the number of agent tokens can be designed to be much smaller than the number of query tokens, the agent attention is significantly more efficient than the widely adopted Softmax attention, while preserving global context modelling capability. Interestingly, we show that the proposed agent attention is equivalent to a generalized form of linear attention. Therefore, agent attention seamlessly integrates the powerful Softmax attention and the highly efficient linear attention. Extensive experiments demonstrate the effectiveness of agent attention with various vision Transformers and across diverse vision tasks, including image classification, object detection, semantic segmentation and image generation.",
    "original_url": "http://arxiv.org/pdf/2312.08874v3",
    "original_title": "Agent Attention: On the Integration of Softmax and Linear Attention",
    "source": "arxiv",
    "authors": [
      "Dongchen Han",
      "Tianzhu Ye",
      "Yizeng Han",
      "Zhuofan Xia",
      "Siyuan Pan",
      "Pengfei Wan",
      "Shiji Song",
      "Gao Huang"
    ],
    "published": "2023-12-14T16:26:29+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2312.08874v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2312.08874v3_chunk_2",
    "chunk_text": "Extensive experiments demonstrate the effectiveness of agent attention with various vision Transformers and across diverse vision tasks, including image classification, object detection, semantic segmentation and image generation. Notably, agent attention has shown remarkable performance in high-resolution scenarios, owning to its linear attention nature. For instance, when applied to Stable Diffusion, our agent attention accelerates generation and substantially enhances image generation quality without any additional training. Code is available at https://github.com/LeapLabTHU/Agent-Attention.",
    "original_url": "http://arxiv.org/pdf/2312.08874v3",
    "original_title": "Agent Attention: On the Integration of Softmax and Linear Attention",
    "source": "arxiv",
    "authors": [
      "Dongchen Han",
      "Tianzhu Ye",
      "Yizeng Han",
      "Zhuofan Xia",
      "Siyuan Pan",
      "Pengfei Wan",
      "Shiji Song",
      "Gao Huang"
    ],
    "published": "2023-12-14T16:26:29+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2312.08874v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2406.13781v1_chunk_0",
    "chunk_text": "A Primal-Dual Framework for Transformers and Neural Networks\n\nSelf-attention is key to the remarkable success of transformers in sequence modeling tasks including many applications in natural language processing and computer vision. Like neural network layers, these attention mechanisms are often developed by heuristics and experience. To provide a principled framework for constructing attention layers in transformers, we show that the self-attention corresponds to the support vector expansion derived from a support vector regression problem, whose primal formulation has the form of a neural network layer. Using our framework, we derive popular attention layers used in practice and propose two new attentions: 1) the Batch Normalized Attention (Attention-BN) derived from the batch normalization layer and 2) the Attention with Scaled Head (Attention-SH) derived from using less training data to fit the SVR model. We empirically demonstrate the advantages of the Attention-BN and Attention-SH in reducing head redundancy, increasing the model's accuracy, and improving the model's efficiency in a variety of practical applications including image and time-series classification.",
    "original_url": "http://arxiv.org/pdf/2406.13781v1",
    "original_title": "A Primal-Dual Framework for Transformers and Neural Networks",
    "source": "arxiv",
    "authors": [
      "Tan M. Nguyen",
      "Tam Nguyen",
      "Nhat Ho",
      "Andrea L. Bertozzi",
      "Richard G. Baraniuk",
      "Stanley J. Osher"
    ],
    "published": "2024-06-19T19:11:22+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2406.13781v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2406.13781v1_chunk_1",
    "chunk_text": "We empirically demonstrate the advantages of the Attention-BN and Attention-SH in reducing head redundancy, increasing the model's accuracy, and improving the model's efficiency in a variety of practical applications including image and time-series classification.",
    "original_url": "http://arxiv.org/pdf/2406.13781v1",
    "original_title": "A Primal-Dual Framework for Transformers and Neural Networks",
    "source": "arxiv",
    "authors": [
      "Tan M. Nguyen",
      "Tam Nguyen",
      "Nhat Ho",
      "Andrea L. Bertozzi",
      "Richard G. Baraniuk",
      "Stanley J. Osher"
    ],
    "published": "2024-06-19T19:11:22+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2406.13781v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2501.19215v2_chunk_0",
    "chunk_text": "Strassen Attention: Unlocking Compositional Abilities in Transformers Based on a New Lower Bound Method\n\nWe propose a novel method to evaluate the theoretical limits of Transformers, allowing us to prove the first lower bounds against one-layer softmax Transformers with infinite precision. We establish those bounds for three tasks that require advanced reasoning. The first task, Match3 (Sanford et al., 2023), requires looking at all triples of positions. The second and third tasks address compositionality-based reasoning: one is composition of functions (Peng et al., 2024) and the other is composition of binary relations. We formally prove the inability of one-layer softmax Transformers to solve any of these tasks.",
    "original_url": "http://arxiv.org/pdf/2501.19215v2",
    "original_title": "Strassen Attention: Unlocking Compositional Abilities in Transformers Based on a New Lower Bound Method",
    "source": "arxiv",
    "authors": [
      "Alexander Kozachinskiy",
      "Felipe Urrutia",
      "Hector Jimenez",
      "Tomasz Steifer",
      "Germn Pizarro",
      "Matas Fuentes",
      "Francisco Meza",
      "Cristian B. Calderon",
      "Cristbal Rojas"
    ],
    "published": "2025-01-31T15:21:54+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2501.19215v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2501.19215v2_chunk_1",
    "chunk_text": "We formally prove the inability of one-layer softmax Transformers to solve any of these tasks. In an attempt to overcome these limitations, we introduce Strassen attention and prove that with this mechanism a one-layer Transformer can in principle solve all these tasks. We also show that it enjoys sub-cubic running-time complexity, making it more scalable than similar previously proposed mechanisms, such as higher-order attention (Sanford et al., 2023). To complement our theoretical findings, we experimentally studied Strassen attention and compared it against standard (Vaswani et al, 2017), higher-order attention (Sanford et al., 2023) and triangular attention (Bergen et al. 2021).",
    "original_url": "http://arxiv.org/pdf/2501.19215v2",
    "original_title": "Strassen Attention: Unlocking Compositional Abilities in Transformers Based on a New Lower Bound Method",
    "source": "arxiv",
    "authors": [
      "Alexander Kozachinskiy",
      "Felipe Urrutia",
      "Hector Jimenez",
      "Tomasz Steifer",
      "Germn Pizarro",
      "Matas Fuentes",
      "Francisco Meza",
      "Cristian B. Calderon",
      "Cristbal Rojas"
    ],
    "published": "2025-01-31T15:21:54+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2501.19215v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2501.19215v2_chunk_2",
    "chunk_text": "2021). Our results help to disentangle all these attention mechanisms, highlighting their strengths and limitations. In particular, Strassen attention outperforms standard attention significantly on all the tasks. Altogether, understanding the theoretical limitations can guide research towards scalable attention mechanisms that improve the reasoning abilities of Transformers.",
    "original_url": "http://arxiv.org/pdf/2501.19215v2",
    "original_title": "Strassen Attention: Unlocking Compositional Abilities in Transformers Based on a New Lower Bound Method",
    "source": "arxiv",
    "authors": [
      "Alexander Kozachinskiy",
      "Felipe Urrutia",
      "Hector Jimenez",
      "Tomasz Steifer",
      "Germn Pizarro",
      "Matas Fuentes",
      "Francisco Meza",
      "Cristian B. Calderon",
      "Cristbal Rojas"
    ],
    "published": "2025-01-31T15:21:54+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2501.19215v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2412.02344v1_chunk_0",
    "chunk_text": "UniForm: A Reuse Attention Mechanism Optimized for Efficient Vision Transformers on Edge Devices\n\nTransformer-based architectures have demonstrated remarkable success across various domains, but their deployment on edge devices remains challenging due to high memory and computational demands. In this paper, we introduce a novel Reuse Attention mechanism, tailored for efficient memory access and computational optimization, enabling seamless operation on resource-constrained platforms without compromising performance. Unlike traditional multi-head attention (MHA), which redundantly computes separate attention matrices for each head, Reuse Attention consolidates these computations into a shared attention matrix, significantly reducing memory overhead and computational complexity. Comprehensive experiments on ImageNet-1K and downstream tasks show that the proposed UniForm models leveraging Reuse Attention achieve state-of-the-art imagenet classification accuracy while outperforming existing attention mechanisms, such as Linear Attention and Flash Attention, in inference speed and memory scalability. Notably, UniForm-l achieves a 76.7% Top-1 accuracy on ImageNet-1K with 21.8ms inference time on edge devices like the Jetson AGX Orin, representing up to a 5x speedup over competing benchmark methods.",
    "original_url": "http://arxiv.org/pdf/2412.02344v1",
    "original_title": "UniForm: A Reuse Attention Mechanism Optimized for Efficient Vision Transformers on Edge Devices",
    "source": "arxiv",
    "authors": [
      "Seul-Ki Yeom",
      "Tae-Ho Kim"
    ],
    "published": "2024-12-03T10:04:15+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2412.02344v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2412.02344v1_chunk_1",
    "chunk_text": "Notably, UniForm-l achieves a 76.7% Top-1 accuracy on ImageNet-1K with 21.8ms inference time on edge devices like the Jetson AGX Orin, representing up to a 5x speedup over competing benchmark methods. These results demonstrate the versatility of Reuse Attention across high-performance GPUs and edge platforms, paving the way for broader real-time applications",
    "original_url": "http://arxiv.org/pdf/2412.02344v1",
    "original_title": "UniForm: A Reuse Attention Mechanism Optimized for Efficient Vision Transformers on Edge Devices",
    "source": "arxiv",
    "authors": [
      "Seul-Ki Yeom",
      "Tae-Ho Kim"
    ],
    "published": "2024-12-03T10:04:15+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2412.02344v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2407.13806v1_chunk_0",
    "chunk_text": "Revisiting Attention for Multivariate Time Series Forecasting\n\nCurrent Transformer methods for Multivariate Time-Series Forecasting (MTSF) are all based on the conventional attention mechanism. They involve sequence embedding and performing a linear projection of Q, K, and V, and then computing attention within this latent space. We have never delved into the attention mechanism to explore whether such a mapping space is optimal for MTSF. To investigate this issue, this study first proposes Frequency Spectrum attention (FSatten), a novel attention mechanism based on the frequency domain space. It employs the Fourier transform for embedding and introduces Multi-head Spectrum Scaling (MSS) to replace the conventional linear mapping of Q and K. FSatten can accurately capture the periodic dependencies between sequences and outperform the conventional attention without changing mainstream architectures.",
    "original_url": "http://arxiv.org/pdf/2407.13806v1",
    "original_title": "Revisiting Attention for Multivariate Time Series Forecasting",
    "source": "arxiv",
    "authors": [
      "Haixiang Wu"
    ],
    "published": "2024-07-18T06:28:20+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2407.13806v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2407.13806v1_chunk_1",
    "chunk_text": "It employs the Fourier transform for embedding and introduces Multi-head Spectrum Scaling (MSS) to replace the conventional linear mapping of Q and K. FSatten can accurately capture the periodic dependencies between sequences and outperform the conventional attention without changing mainstream architectures. We further design a more general method dubbed Scaled Orthogonal attention (SOatten). We propose an orthogonal embedding and a Head-Coupling Convolution (HCC) based on the neighboring similarity bias to guide the model in learning comprehensive dependency patterns. Experiments show that FSatten and SOatten surpass the SOTA which uses conventional attention, making it a good alternative as a basic attention mechanism for MTSF. The codes and log files will be released at: https://github.com/Joeland4/FSatten-SOatten.",
    "original_url": "http://arxiv.org/pdf/2407.13806v1",
    "original_title": "Revisiting Attention for Multivariate Time Series Forecasting",
    "source": "arxiv",
    "authors": [
      "Haixiang Wu"
    ],
    "published": "2024-07-18T06:28:20+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2407.13806v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2407.13806v1_chunk_2",
    "chunk_text": "The codes and log files will be released at: https://github.com/Joeland4/FSatten-SOatten.",
    "original_url": "http://arxiv.org/pdf/2407.13806v1",
    "original_title": "Revisiting Attention for Multivariate Time Series Forecasting",
    "source": "arxiv",
    "authors": [
      "Haixiang Wu"
    ],
    "published": "2024-07-18T06:28:20+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2407.13806v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2210.17115v1_chunk_0",
    "chunk_text": "ViT-LSLA: Vision Transformer with Light Self-Limited-Attention\n\nTransformers have demonstrated a competitive performance across a wide range of vision tasks, while it is very expensive to compute the global self-attention. Many methods limit the range of attention within a local window to reduce computation complexity. However, their approaches cannot save the number of parameters; meanwhile, the self-attention and inner position bias (inside the softmax function) cause each query to focus on similar and close patches. Consequently, this paper presents a light self-limited-attention (LSLA) consisting of a light self-attention mechanism (LSA) to save the computation cost and the number of parameters, and a self-limited-attention mechanism (SLA) to improve the performance. Firstly, the LSA replaces the K (Key) and V (Value) of self-attention with the X(origin input).",
    "original_url": "http://arxiv.org/pdf/2210.17115v1",
    "original_title": "ViT-LSLA: Vision Transformer with Light Self-Limited-Attention",
    "source": "arxiv",
    "authors": [
      "Zhenzhe Hechen",
      "Wei Huang",
      "Yixin Zhao"
    ],
    "published": "2022-10-31T07:46:45+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2210.17115v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2210.17115v1_chunk_1",
    "chunk_text": "Firstly, the LSA replaces the K (Key) and V (Value) of self-attention with the X(origin input). Applying it in vision Transformers which have encoder architecture and self-attention mechanism, can simplify the computation. Secondly, the SLA has a positional information module and a limited-attention module. The former contains a dynamic scale and an inner position bias to adjust the distribution of the self-attention scores and enhance the positional information. The latter uses an outer position bias after the softmax function to limit some large values of attention weights.",
    "original_url": "http://arxiv.org/pdf/2210.17115v1",
    "original_title": "ViT-LSLA: Vision Transformer with Light Self-Limited-Attention",
    "source": "arxiv",
    "authors": [
      "Zhenzhe Hechen",
      "Wei Huang",
      "Yixin Zhao"
    ],
    "published": "2022-10-31T07:46:45+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2210.17115v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2210.17115v1_chunk_2",
    "chunk_text": "The latter uses an outer position bias after the softmax function to limit some large values of attention weights. Finally, a hierarchical Vision Transformer with Light self-Limited-attention (ViT-LSLA) is presented. The experiments show that ViT-LSLA achieves 71.6% top-1 accuracy on IP102 (2.4% absolute improvement of Swin-T); 87.2% top-1 accuracy on Mini-ImageNet (3.7% absolute improvement of Swin-T). Furthermore, it greatly reduces FLOPs (3.5GFLOPs vs. 4.5GFLOPs of Swin-T) and parameters (18.9M vs. 27.6M of Swin-T).",
    "original_url": "http://arxiv.org/pdf/2210.17115v1",
    "original_title": "ViT-LSLA: Vision Transformer with Light Self-Limited-Attention",
    "source": "arxiv",
    "authors": [
      "Zhenzhe Hechen",
      "Wei Huang",
      "Yixin Zhao"
    ],
    "published": "2022-10-31T07:46:45+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2210.17115v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1912.11959v2_chunk_0",
    "chunk_text": "Is Attention All What You Need? -- An Empirical Investigation on Convolution-Based Active Memory and Self-Attention\n\nThe key to a Transformer model is the self-attention mechanism, which allows the model to analyze an entire sequence in a computationally efficient manner. Recent work has suggested the possibility that general attention mechanisms used by RNNs could be replaced by active-memory mechanisms. In this work, we evaluate whether various active-memory mechanisms could replace self-attention in a Transformer. Our experiments suggest that active-memory alone achieves comparable results to the self-attention mechanism for language modelling, but optimal results are mostly achieved by using both active-memory and self-attention mechanisms together.",
    "original_url": "http://arxiv.org/pdf/1912.11959v2",
    "original_title": "Is Attention All What You Need? -- An Empirical Investigation on Convolution-Based Active Memory and Self-Attention",
    "source": "arxiv",
    "authors": [
      "Thomas Dowdell",
      "Hongyu Zhang"
    ],
    "published": "2019-12-27T02:01:13+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1912.11959v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1912.11959v2_chunk_1",
    "chunk_text": "Our experiments suggest that active-memory alone achieves comparable results to the self-attention mechanism for language modelling, but optimal results are mostly achieved by using both active-memory and self-attention mechanisms together. We also note that, for some specific algorithmic tasks, active-memory mechanisms alone outperform both self-attention and a combination of the two.",
    "original_url": "http://arxiv.org/pdf/1912.11959v2",
    "original_title": "Is Attention All What You Need? -- An Empirical Investigation on Convolution-Based Active Memory and Self-Attention",
    "source": "arxiv",
    "authors": [
      "Thomas Dowdell",
      "Hongyu Zhang"
    ],
    "published": "2019-12-27T02:01:13+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1912.11959v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2202.08371v1_chunk_0",
    "chunk_text": "The Quarks of Attention\n\nAttention plays a fundamental role in both natural and artificial intelligence systems. In deep learning, attention-based neural architectures, such as transformer architectures, are widely used to tackle problems in natural language processing and beyond. Here we investigate the fundamental building blocks of attention and their computational properties. Within the standard model of deep learning, we classify all possible fundamental building blocks of attention in terms of their source, target, and computational mechanism. We identify and study three most important mechanisms: additive activation attention, multiplicative output attention (output gating), and multiplicative synaptic attention (synaptic gating).",
    "original_url": "http://arxiv.org/pdf/2202.08371v1",
    "original_title": "The Quarks of Attention",
    "source": "arxiv",
    "authors": [
      "Pierre Baldi",
      "Roman Vershynin"
    ],
    "published": "2022-02-15T18:47:19+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2202.08371v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2202.08371v1_chunk_1",
    "chunk_text": "We identify and study three most important mechanisms: additive activation attention, multiplicative output attention (output gating), and multiplicative synaptic attention (synaptic gating). The gating mechanisms correspond to multiplicative extensions of the standard model and are used across all current attention-based deep learning architectures. We study their functional properties and estimate the capacity of several attentional building blocks in the case of linear and polynomial threshold gates. Surprisingly, additive activation attention plays a central role in the proofs of the lower bounds. Attention mechanisms reduce the depth of certain basic circuits and leverage the power of quadratic activations without incurring their full cost.",
    "original_url": "http://arxiv.org/pdf/2202.08371v1",
    "original_title": "The Quarks of Attention",
    "source": "arxiv",
    "authors": [
      "Pierre Baldi",
      "Roman Vershynin"
    ],
    "published": "2022-02-15T18:47:19+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2202.08371v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2202.08371v1_chunk_2",
    "chunk_text": "Attention mechanisms reduce the depth of certain basic circuits and leverage the power of quadratic activations without incurring their full cost.",
    "original_url": "http://arxiv.org/pdf/2202.08371v1",
    "original_title": "The Quarks of Attention",
    "source": "arxiv",
    "authors": [
      "Pierre Baldi",
      "Roman Vershynin"
    ],
    "published": "2022-02-15T18:47:19+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2202.08371v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2502.07244v1_chunk_0",
    "chunk_text": "Linear Transformers as VAR Models: Aligning Autoregressive Attention Mechanisms with Autoregressive Forecasting\n\nAutoregressive attention-based time series forecasting (TSF) has drawn increasing interest, with mechanisms like linear attention sometimes outperforming vanilla attention. However, deeper Transformer architectures frequently misalign with autoregressive objectives, obscuring the underlying VAR structure embedded within linear attention and hindering their ability to capture the data generative processes in TSF. In this work, we first show that a single linear attention layer can be interpreted as a dynamic vector autoregressive (VAR) structure. We then explain that existing multi-layer Transformers have structural mismatches with the autoregressive forecasting objective, which impair interpretability and generalization ability. To address this, we show that by rearranging the MLP, attention, and input-output flow, multi-layer linear attention can also be aligned as a VAR model.",
    "original_url": "http://arxiv.org/pdf/2502.07244v1",
    "original_title": "Linear Transformers as VAR Models: Aligning Autoregressive Attention Mechanisms with Autoregressive Forecasting",
    "source": "arxiv",
    "authors": [
      "Jiecheng Lu",
      "Shihao Yang"
    ],
    "published": "2025-02-11T04:24:43+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2502.07244v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2502.07244v1_chunk_1",
    "chunk_text": "To address this, we show that by rearranging the MLP, attention, and input-output flow, multi-layer linear attention can also be aligned as a VAR model. Then, we propose Structural Aligned Mixture of VAR (SAMoVAR), a linear Transformer variant that integrates interpretable dynamic VAR weights for multivariate TSF. By aligning the Transformer architecture with autoregressive objectives, SAMoVAR delivers improved performance, interpretability, and computational efficiency, comparing to SOTA TSF models.",
    "original_url": "http://arxiv.org/pdf/2502.07244v1",
    "original_title": "Linear Transformers as VAR Models: Aligning Autoregressive Attention Mechanisms with Autoregressive Forecasting",
    "source": "arxiv",
    "authors": [
      "Jiecheng Lu",
      "Shihao Yang"
    ],
    "published": "2025-02-11T04:24:43+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2502.07244v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2504.19414v1_chunk_0",
    "chunk_text": "GMAR: Gradient-Driven Multi-Head Attention Rollout for Vision Transformer Interpretability\n\nThe Vision Transformer (ViT) has made significant advancements in computer vision, utilizing self-attention mechanisms to achieve state-of-the-art performance across various tasks, including image classification, object detection, and segmentation. Its architectural flexibility and capabilities have made it a preferred choice among researchers and practitioners. However, the intricate multi-head attention mechanism of ViT presents significant challenges to interpretability, as the underlying prediction process remains opaque. A critical limitation arises from an observation commonly noted in transformer architectures: \"Not all attention heads are equally meaningful.\" Overlooking the relative importance of specific heads highlights the limitations of existing interpretability methods.",
    "original_url": "http://arxiv.org/pdf/2504.19414v1",
    "original_title": "GMAR: Gradient-Driven Multi-Head Attention Rollout for Vision Transformer Interpretability",
    "source": "arxiv",
    "authors": [
      "Sehyeong Jo",
      "Gangjae Jang",
      "Haesol Park"
    ],
    "published": "2025-04-28T01:58:39+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2504.19414v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2504.19414v1_chunk_1",
    "chunk_text": "Overlooking the relative importance of specific heads highlights the limitations of existing interpretability methods. To address these challenges, we introduce Gradient-Driven Multi-Head Attention Rollout (GMAR), a novel method that quantifies the importance of each attention head using gradient-based scores. These scores are normalized to derive a weighted aggregate attention score, effectively capturing the relative contributions of individual heads. GMAR clarifies the role of each head in the prediction process, enabling more precise interpretability at the head level. Experimental results demonstrate that GMAR consistently outperforms traditional attention rollout techniques.",
    "original_url": "http://arxiv.org/pdf/2504.19414v1",
    "original_title": "GMAR: Gradient-Driven Multi-Head Attention Rollout for Vision Transformer Interpretability",
    "source": "arxiv",
    "authors": [
      "Sehyeong Jo",
      "Gangjae Jang",
      "Haesol Park"
    ],
    "published": "2025-04-28T01:58:39+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2504.19414v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2504.19414v1_chunk_2",
    "chunk_text": "Experimental results demonstrate that GMAR consistently outperforms traditional attention rollout techniques. This work provides a practical contribution to transformer-based architectures, establishing a robust framework for enhancing the interpretability of Vision Transformer models.",
    "original_url": "http://arxiv.org/pdf/2504.19414v1",
    "original_title": "GMAR: Gradient-Driven Multi-Head Attention Rollout for Vision Transformer Interpretability",
    "source": "arxiv",
    "authors": [
      "Sehyeong Jo",
      "Gangjae Jang",
      "Haesol Park"
    ],
    "published": "2025-04-28T01:58:39+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2504.19414v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2210.15497v1_chunk_0",
    "chunk_text": "LSG Attention: Extrapolation of pretrained Transformers to long sequences\n\nTransformer models achieve state-of-the-art performance on a wide range of NLP tasks. They however suffer from a prohibitive limitation due to the self-attention mechanism, inducing $O(n^2)$ complexity with regard to sequence length. To answer this limitation we introduce the LSG architecture which relies on Local, Sparse and Global attention. We show that LSG attention is fast, efficient and competitive in classification and summarization tasks on long documents. Interestingly, it can also be used to adapt existing pretrained models to efficiently extrapolate to longer sequences with no additional training.",
    "original_url": "http://arxiv.org/pdf/2210.15497v1",
    "original_title": "LSG Attention: Extrapolation of pretrained Transformers to long sequences",
    "source": "arxiv",
    "authors": [
      "Charles Condevaux",
      "Sbastien Harispe"
    ],
    "published": "2022-10-13T13:10:41+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2210.15497v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2210.15497v1_chunk_1",
    "chunk_text": "Interestingly, it can also be used to adapt existing pretrained models to efficiently extrapolate to longer sequences with no additional training. Along with the introduction of the LSG attention mechanism, we propose tools to train new models and adapt existing ones based on this mechanism.",
    "original_url": "http://arxiv.org/pdf/2210.15497v1",
    "original_title": "LSG Attention: Extrapolation of pretrained Transformers to long sequences",
    "source": "arxiv",
    "authors": [
      "Charles Condevaux",
      "Sbastien Harispe"
    ],
    "published": "2022-10-13T13:10:41+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2210.15497v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2411.18003v3_chunk_0",
    "chunk_text": "HAAT: Hybrid Attention Aggregation Transformer for Image Super-Resolution\n\nIn the research area of image super-resolution, Swin-transformer-based models are favored for their global spatial modeling and shifting window attention mechanism. However, existing methods often limit self-attention to non overlapping windows to cut costs and ignore the useful information that exists across channels. To address this issue, this paper introduces a novel model, the Hybrid Attention Aggregation Transformer (HAAT), designed to better leverage feature information. HAAT is constructed by integrating Swin-Dense-Residual-Connected Blocks (SDRCB) with Hybrid Grid Attention Blocks (HGAB). SDRCB expands the receptive field while maintaining a streamlined architecture, resulting in enhanced performance.",
    "original_url": "http://arxiv.org/pdf/2411.18003v3",
    "original_title": "HAAT: Hybrid Attention Aggregation Transformer for Image Super-Resolution",
    "source": "arxiv",
    "authors": [
      "Song-Jiang Lai",
      "Tsun-Hin Cheung",
      "Ka-Chun Fung",
      "Kai-wen Xue",
      "Kin-Man Lam"
    ],
    "published": "2024-11-27T02:47:17+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2411.18003v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2411.18003v3_chunk_1",
    "chunk_text": "SDRCB expands the receptive field while maintaining a streamlined architecture, resulting in enhanced performance. HGAB incorporates channel attention, sparse attention, and window attention to improve nonlocal feature fusion and achieve more visually compelling results. Experimental evaluations demonstrate that HAAT surpasses state-of-the-art methods on benchmark datasets. Keywords: Image super-resolution, Computer vision, Attention mechanism, Transformer",
    "original_url": "http://arxiv.org/pdf/2411.18003v3",
    "original_title": "HAAT: Hybrid Attention Aggregation Transformer for Image Super-Resolution",
    "source": "arxiv",
    "authors": [
      "Song-Jiang Lai",
      "Tsun-Hin Cheung",
      "Ka-Chun Fung",
      "Kai-wen Xue",
      "Kin-Man Lam"
    ],
    "published": "2024-11-27T02:47:17+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2411.18003v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2405.16727v2_chunk_0",
    "chunk_text": "Disentangling and Integrating Relational and Sensory Information in Transformer Architectures\n\nRelational reasoning is a central component of generally intelligent systems, enabling robust and data-efficient inductive generalization. Recent empirical evidence shows that many existing neural architectures, including Transformers, struggle with tasks requiring relational reasoning. In this work, we distinguish between two types of information: sensory information about the properties of individual objects, and relational information about the relationships between objects. While neural attention provides a powerful mechanism for controlling the flow of sensory information between objects, the Transformer lacks an explicit computational mechanism for routing and processing relational information. To address this limitation, we propose an architectural extension of the Transformer framework that we call the Dual Attention Transformer (DAT), featuring two distinct attention mechanisms: sensory attention for directing the flow of sensory information, and a novel relational attention mechanism for directing the flow of relational information.",
    "original_url": "http://arxiv.org/pdf/2405.16727v2",
    "original_title": "Disentangling and Integrating Relational and Sensory Information in Transformer Architectures",
    "source": "arxiv",
    "authors": [
      "Awni Altabaa",
      "John Lafferty"
    ],
    "published": "2024-05-26T23:52:51+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2405.16727v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2405.16727v2_chunk_1",
    "chunk_text": "To address this limitation, we propose an architectural extension of the Transformer framework that we call the Dual Attention Transformer (DAT), featuring two distinct attention mechanisms: sensory attention for directing the flow of sensory information, and a novel relational attention mechanism for directing the flow of relational information. We empirically evaluate DAT on a diverse set of tasks ranging from synthetic relational benchmarks to complex real-world tasks such as language modeling and visual processing. Our results demonstrate that integrating explicit relational computational mechanisms into the Transformer architecture leads to significant performance gains in terms of data efficiency and parameter efficiency.",
    "original_url": "http://arxiv.org/pdf/2405.16727v2",
    "original_title": "Disentangling and Integrating Relational and Sensory Information in Transformer Architectures",
    "source": "arxiv",
    "authors": [
      "Awni Altabaa",
      "John Lafferty"
    ],
    "published": "2024-05-26T23:52:51+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2405.16727v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2411.03493v1_chunk_0",
    "chunk_text": "LASER: Attention with Exponential Transformation\n\nTransformers have had tremendous impact for several sequence related tasks, largely due to their ability to retrieve from any part of the sequence via softmax based dot-product attention. This mechanism plays a crucial role in Transformer's performance. We analyze the gradients backpropagated through the softmax operation in the attention mechanism and observe that these gradients can often be small. This poor gradient signal backpropagation can lead to inefficient learning of parameters preceeding the attention operations. To this end, we introduce a new attention mechanism called LASER, which we analytically show to admit a larger gradient signal.",
    "original_url": "http://arxiv.org/pdf/2411.03493v1",
    "original_title": "LASER: Attention with Exponential Transformation",
    "source": "arxiv",
    "authors": [
      "Sai Surya Duvvuri",
      "Inderjit S. Dhillon"
    ],
    "published": "2024-11-05T20:18:28+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2411.03493v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2411.03493v1_chunk_1",
    "chunk_text": "To this end, we introduce a new attention mechanism called LASER, which we analytically show to admit a larger gradient signal. We show that LASER Attention can be implemented by making small modifications to existing attention implementations. We conduct experiments on autoregressive large language models (LLMs) with upto 2.2 billion parameters where we show upto 3.38% and an average of ~1% improvement over standard attention on downstream evaluations. Using LASER gives the following relative improvements in generalization performance across a variety of tasks (vision, text and speech): 4.67% accuracy in Vision Transformer (ViT) on Imagenet, 2.25% error rate in Conformer on the Librispeech speech-to-text and 0.93% fraction of incorrect predictions in BERT with 2.2 billion parameters.",
    "original_url": "http://arxiv.org/pdf/2411.03493v1",
    "original_title": "LASER: Attention with Exponential Transformation",
    "source": "arxiv",
    "authors": [
      "Sai Surya Duvvuri",
      "Inderjit S. Dhillon"
    ],
    "published": "2024-11-05T20:18:28+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2411.03493v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2412.17019v1_chunk_0",
    "chunk_text": "Reversed Attention: On The Gradient Descent Of Attention Layers In GPT\n\nThe success of Transformer-based Language Models (LMs) stems from their attention mechanism. While this mechanism has been extensively studied in explainability research, particularly through the attention values obtained during the forward pass of LMs, the backward pass of attention has been largely overlooked. In this work, we study the mathematics of the backward pass of attention, revealing that it implicitly calculates an attention matrix we refer to as \"Reversed Attention\". We examine the properties of Reversed Attention and demonstrate its ability to elucidate the models' behavior and edit dynamics. In an experimental setup, we showcase the ability of Reversed Attention to directly alter the forward pass of attention, without modifying the model's weights, using a novel method called \"attention patching\".",
    "original_url": "http://arxiv.org/pdf/2412.17019v1",
    "original_title": "Reversed Attention: On The Gradient Descent Of Attention Layers In GPT",
    "source": "arxiv",
    "authors": [
      "Shahar Katz",
      "Lior Wolf"
    ],
    "published": "2024-12-22T13:48:04+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2412.17019v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2412.17019v1_chunk_1",
    "chunk_text": "In an experimental setup, we showcase the ability of Reversed Attention to directly alter the forward pass of attention, without modifying the model's weights, using a novel method called \"attention patching\". In addition to enhancing the comprehension of how LM configure attention layers during backpropagation, Reversed Attention maps contribute to a more interpretable backward pass.",
    "original_url": "http://arxiv.org/pdf/2412.17019v1",
    "original_title": "Reversed Attention: On The Gradient Descent Of Attention Layers In GPT",
    "source": "arxiv",
    "authors": [
      "Shahar Katz",
      "Lior Wolf"
    ],
    "published": "2024-12-22T13:48:04+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2412.17019v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2112.10809v1_chunk_0",
    "chunk_text": "Lite Vision Transformer with Enhanced Self-Attention\n\nDespite the impressive representation capacity of vision transformer models, current light-weight vision transformer models still suffer from inconsistent and incorrect dense predictions at local regions. We suspect that the power of their self-attention mechanism is limited in shallower and thinner networks. We propose Lite Vision Transformer (LVT), a novel light-weight transformer network with two enhanced self-attention mechanisms to improve the model performances for mobile deployment. For the low-level features, we introduce Convolutional Self-Attention (CSA). Unlike previous approaches of merging convolution and self-attention, CSA introduces local self-attention into the convolution within a kernel of size 3x3 to enrich low-level features in the first stage of LVT.",
    "original_url": "http://arxiv.org/pdf/2112.10809v1",
    "original_title": "Lite Vision Transformer with Enhanced Self-Attention",
    "source": "arxiv",
    "authors": [
      "Chenglin Yang",
      "Yilin Wang",
      "Jianming Zhang",
      "He Zhang",
      "Zijun Wei",
      "Zhe Lin",
      "Alan Yuille"
    ],
    "published": "2021-12-20T19:11:53+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2112.10809v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2112.10809v1_chunk_1",
    "chunk_text": "Unlike previous approaches of merging convolution and self-attention, CSA introduces local self-attention into the convolution within a kernel of size 3x3 to enrich low-level features in the first stage of LVT. For the high-level features, we propose Recursive Atrous Self-Attention (RASA), which utilizes the multi-scale context when calculating the similarity map and a recursive mechanism to increase the representation capability with marginal extra parameter cost. The superiority of LVT is demonstrated on ImageNet recognition, ADE20K semantic segmentation, and COCO panoptic segmentation. The code is made publicly available.",
    "original_url": "http://arxiv.org/pdf/2112.10809v1",
    "original_title": "Lite Vision Transformer with Enhanced Self-Attention",
    "source": "arxiv",
    "authors": [
      "Chenglin Yang",
      "Yilin Wang",
      "Jianming Zhang",
      "He Zhang",
      "Zijun Wei",
      "Zhe Lin",
      "Alan Yuille"
    ],
    "published": "2021-12-20T19:11:53+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2112.10809v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2207.08569v3_chunk_0",
    "chunk_text": "Multi-manifold Attention for Vision Transformers\n\nVision Transformers are very popular nowadays due to their state-of-the-art performance in several computer vision tasks, such as image classification and action recognition. Although their performance has been greatly enhanced through highly descriptive patch embeddings and hierarchical structures, there is still limited research on utilizing additional data representations so as to refine the selfattention map of a Transformer. To address this problem, a novel attention mechanism, called multi-manifold multihead attention, is proposed in this work to substitute the vanilla self-attention of a Transformer. The proposed mechanism models the input space in three distinct manifolds, namely Euclidean, Symmetric Positive Definite and Grassmann, thus leveraging different statistical and geometrical properties of the input for the computation of a highly descriptive attention map. In this way, the proposed attention mechanism can guide a Vision Transformer to become more attentive towards important appearance, color and texture features of an image, leading to improved classification and segmentation results, as shown by the experimental results on well-known datasets.",
    "original_url": "http://arxiv.org/pdf/2207.08569v3",
    "original_title": "Multi-manifold Attention for Vision Transformers",
    "source": "arxiv",
    "authors": [
      "Dimitrios Konstantinidis",
      "Ilias Papastratis",
      "Kosmas Dimitropoulos",
      "Petros Daras"
    ],
    "published": "2022-07-18T12:53:53+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2207.08569v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2207.08569v3_chunk_1",
    "chunk_text": "In this way, the proposed attention mechanism can guide a Vision Transformer to become more attentive towards important appearance, color and texture features of an image, leading to improved classification and segmentation results, as shown by the experimental results on well-known datasets.",
    "original_url": "http://arxiv.org/pdf/2207.08569v3",
    "original_title": "Multi-manifold Attention for Vision Transformers",
    "source": "arxiv",
    "authors": [
      "Dimitrios Konstantinidis",
      "Ilias Papastratis",
      "Kosmas Dimitropoulos",
      "Petros Daras"
    ],
    "published": "2022-07-18T12:53:53+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2207.08569v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1906.09777v3_chunk_0",
    "chunk_text": "A Tensorized Transformer for Language Modeling\n\nLatest development of neural models has connected the encoder and decoder through a self-attention mechanism. In particular, Transformer, which is solely based on self-attention, has led to breakthroughs in Natural Language Processing (NLP) tasks. However, the multi-head attention mechanism, as a key component of Transformer, limits the effective deployment of the model to a resource-limited setting. In this paper, based on the ideas of tensor decomposition and parameters sharing, we propose a novel self-attention model (namely Multi-linear attention) with Block-Term Tensor Decomposition (BTD). We test and verify the proposed attention method on three language modeling tasks (i.e., PTB, WikiText-103 and One-billion) and a neural machine translation task (i.e., WMT-2016 English-German).",
    "original_url": "http://arxiv.org/pdf/1906.09777v3",
    "original_title": "A Tensorized Transformer for Language Modeling",
    "source": "arxiv",
    "authors": [
      "Xindian Ma",
      "Peng Zhang",
      "Shuai Zhang",
      "Nan Duan",
      "Yuexian Hou",
      "Dawei Song",
      "Ming Zhou"
    ],
    "published": "2019-06-24T08:28:37+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1906.09777v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/1906.09777v3_chunk_1",
    "chunk_text": "We test and verify the proposed attention method on three language modeling tasks (i.e., PTB, WikiText-103 and One-billion) and a neural machine translation task (i.e., WMT-2016 English-German). Multi-linear attention can not only largely compress the model parameters but also obtain performance improvements, compared with a number of language modeling approaches, such as Transformer, Transformer-XL, and Transformer with tensor train decomposition.",
    "original_url": "http://arxiv.org/pdf/1906.09777v3",
    "original_title": "A Tensorized Transformer for Language Modeling",
    "source": "arxiv",
    "authors": [
      "Xindian Ma",
      "Peng Zhang",
      "Shuai Zhang",
      "Nan Duan",
      "Yuexian Hou",
      "Dawei Song",
      "Ming Zhou"
    ],
    "published": "2019-06-24T08:28:37+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/1906.09777v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2110.01655v1_chunk_0",
    "chunk_text": "VTAMIQ: Transformers for Attention Modulated Image Quality Assessment\n\nFollowing the major successes of self-attention and Transformers for image analysis, we investigate the use of such attention mechanisms in the context of Image Quality Assessment (IQA) and propose a novel full-reference IQA method, Vision Transformer for Attention Modulated Image Quality (VTAMIQ). Our method achieves competitive or state-of-the-art performance on the existing IQA datasets and significantly outperforms previous metrics in cross-database evaluations. Most patch-wise IQA methods treat each patch independently; this partially discards global information and limits the ability to model long-distance interactions. We avoid this problem altogether by employing a transformer to encode a sequence of patches as a single global representation, which by design considers interdependencies between patches. We rely on various attention mechanisms -- first with self-attention within the Transformer, and second with channel attention within our difference modulation network -- specifically to reveal and enhance the more salient features throughout our architecture.",
    "original_url": "http://arxiv.org/pdf/2110.01655v1",
    "original_title": "VTAMIQ: Transformers for Attention Modulated Image Quality Assessment",
    "source": "arxiv",
    "authors": [
      "Andrei Chubarau",
      "James Clark"
    ],
    "published": "2021-10-04T18:35:29+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2110.01655v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2110.01655v1_chunk_1",
    "chunk_text": "We rely on various attention mechanisms -- first with self-attention within the Transformer, and second with channel attention within our difference modulation network -- specifically to reveal and enhance the more salient features throughout our architecture. With large-scale pre-training for both classification and IQA tasks, VTAMIQ generalizes well to unseen sets of images and distortions, further demonstrating the strength of transformer-based networks for vision modelling.",
    "original_url": "http://arxiv.org/pdf/2110.01655v1",
    "original_title": "VTAMIQ: Transformers for Attention Modulated Image Quality Assessment",
    "source": "arxiv",
    "authors": [
      "Andrei Chubarau",
      "James Clark"
    ],
    "published": "2021-10-04T18:35:29+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2110.01655v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2503.11233v1_chunk_0",
    "chunk_text": "Addressing Information Loss and Interaction Collapse: A Dual Enhanced Attention Framework for Feature Interaction\n\nThe Transformer has proven to be a significant approach in feature interaction for CTR prediction, achieving considerable success in previous works. However, it also presents potential challenges in handling feature interactions. Firstly, Transformers may encounter information loss when capturing feature interactions. By relying on inner products to represent pairwise relationships, they compress raw interaction information, which can result in a degradation of fidelity. Secondly, due to the long-tail features distribution, feature fields with low information-abundance embeddings constrain the information abundance of other fields, leading to collapsed embedding matrices.",
    "original_url": "http://arxiv.org/pdf/2503.11233v1",
    "original_title": "Addressing Information Loss and Interaction Collapse: A Dual Enhanced Attention Framework for Feature Interaction",
    "source": "arxiv",
    "authors": [
      "Yi Xu",
      "Zhiyuan Lu",
      "Xiaochen Li",
      "Jinxin Hu",
      "Hong Wen",
      "Zulong Chen",
      "Yu Zhang",
      "Jing Zhang"
    ],
    "published": "2025-03-14T09:31:03+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2503.11233v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2503.11233v1_chunk_1",
    "chunk_text": "Secondly, due to the long-tail features distribution, feature fields with low information-abundance embeddings constrain the information abundance of other fields, leading to collapsed embedding matrices. To tackle these issues, we propose a Dual Attention Framework for Enhanced Feature Interaction, known as Dual Enhanced Attention. This framework integrates two attention mechanisms: the Combo-ID attention mechanism and the collapse-avoiding attention mechanism. The Combo-ID attention mechanism directly retains feature interaction pairs to mitigate information loss, while the collapse-avoiding attention mechanism adaptively filters out low information-abundance interaction pairs to prevent interaction collapse. Extensive experiments conducted on industrial datasets have shown the effectiveness of Dual Enhanced Attention.",
    "original_url": "http://arxiv.org/pdf/2503.11233v1",
    "original_title": "Addressing Information Loss and Interaction Collapse: A Dual Enhanced Attention Framework for Feature Interaction",
    "source": "arxiv",
    "authors": [
      "Yi Xu",
      "Zhiyuan Lu",
      "Xiaochen Li",
      "Jinxin Hu",
      "Hong Wen",
      "Zulong Chen",
      "Yu Zhang",
      "Jing Zhang"
    ],
    "published": "2025-03-14T09:31:03+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2503.11233v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2503.11233v1_chunk_2",
    "chunk_text": "Extensive experiments conducted on industrial datasets have shown the effectiveness of Dual Enhanced Attention.",
    "original_url": "http://arxiv.org/pdf/2503.11233v1",
    "original_title": "Addressing Information Loss and Interaction Collapse: A Dual Enhanced Attention Framework for Feature Interaction",
    "source": "arxiv",
    "authors": [
      "Yi Xu",
      "Zhiyuan Lu",
      "Xiaochen Li",
      "Jinxin Hu",
      "Hong Wen",
      "Zulong Chen",
      "Yu Zhang",
      "Jing Zhang"
    ],
    "published": "2025-03-14T09:31:03+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2503.11233v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2311.13508v1_chunk_0",
    "chunk_text": "Naturalness of Attention: Revisiting Attention in Code Language Models\n\nLanguage models for code such as CodeBERT offer the capability to learn advanced source code representation, but their opacity poses barriers to understanding of captured properties. Recent attention analysis studies provide initial interpretability insights by focusing solely on attention weights rather than considering the wider context modeling of Transformers. This study aims to shed some light on the previously ignored factors of the attention mechanism beyond the attention weights. We conduct an initial empirical study analyzing both attention distributions and transformed representations in CodeBERT. Across two programming languages, Java and Python, we find that the scaled transformation norms of the input better capture syntactic structure compared to attention weights alone.",
    "original_url": "http://arxiv.org/pdf/2311.13508v1",
    "original_title": "Naturalness of Attention: Revisiting Attention in Code Language Models",
    "source": "arxiv",
    "authors": [
      "Mootez Saad",
      "Tushar Sharma"
    ],
    "published": "2023-11-22T16:34:12+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2311.13508v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2311.13508v1_chunk_1",
    "chunk_text": "Across two programming languages, Java and Python, we find that the scaled transformation norms of the input better capture syntactic structure compared to attention weights alone. Our analysis reveals characterization of how CodeBERT embeds syntactic code properties. The findings demonstrate the importance of incorporating factors beyond just attention weights for rigorously understanding neural code models. This lays the groundwork for developing more interpretable models and effective uses of attention mechanisms in program analysis.",
    "original_url": "http://arxiv.org/pdf/2311.13508v1",
    "original_title": "Naturalness of Attention: Revisiting Attention in Code Language Models",
    "source": "arxiv",
    "authors": [
      "Mootez Saad",
      "Tushar Sharma"
    ],
    "published": "2023-11-22T16:34:12+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2311.13508v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2502.12352v2_chunk_0",
    "chunk_text": "Towards Mechanistic Interpretability of Graph Transformers via Attention Graphs\n\nWe introduce Attention Graphs, a new tool for mechanistic interpretability of Graph Neural Networks (GNNs) and Graph Transformers based on the mathematical equivalence between message passing in GNNs and the self-attention mechanism in Transformers. Attention Graphs aggregate attention matrices across Transformer layers and heads to describe how information flows among input nodes. Through experiments on homophilous and heterophilous node classification tasks, we analyze Attention Graphs from a network science perspective and find that: (1) When Graph Transformers are allowed to learn the optimal graph structure using all-to-all attention among input nodes, the Attention Graphs learned by the model do not tend to correlate with the input/original graph structure; and (2) For heterophilous graphs, different Graph Transformer variants can achieve similar performance while utilising distinct information flow patterns. Open source code: https://github.com/batu-el/understanding-inductive-biases-of-gnns",
    "original_url": "http://arxiv.org/pdf/2502.12352v2",
    "original_title": "Towards Mechanistic Interpretability of Graph Transformers via Attention Graphs",
    "source": "arxiv",
    "authors": [
      "Batu El",
      "Deepro Choudhury",
      "Pietro Li",
      "Chaitanya K. Joshi"
    ],
    "published": "2025-02-17T22:35:16+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2502.12352v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2406.11274v1_chunk_0",
    "chunk_text": "Skip-Layer Attention: Bridging Abstract and Detailed Dependencies in Transformers\n\nThe Transformer architecture has significantly advanced deep learning, particularly in natural language processing, by effectively managing long-range dependencies. However, as the demand for understanding complex relationships grows, refining the Transformer's architecture becomes critical. This paper introduces Skip-Layer Attention (SLA) to enhance Transformer models by enabling direct attention between non-adjacent layers. This method improves the model's ability to capture dependencies between high-level abstract features and low-level details. By facilitating direct attention between these diverse feature levels, our approach overcomes the limitations of current Transformers, which often rely on suboptimal intra-layer attention.",
    "original_url": "http://arxiv.org/pdf/2406.11274v1",
    "original_title": "Skip-Layer Attention: Bridging Abstract and Detailed Dependencies in Transformers",
    "source": "arxiv",
    "authors": [
      "Qian Chen",
      "Wen Wang",
      "Qinglin Zhang",
      "Siqi Zheng",
      "Shiliang Zhang",
      "Chong Deng",
      "Hai Yu",
      "Jiaqing Liu",
      "Yukun Ma",
      "Chong Zhang"
    ],
    "published": "2024-06-17T07:24:38+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2406.11274v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2406.11274v1_chunk_1",
    "chunk_text": "By facilitating direct attention between these diverse feature levels, our approach overcomes the limitations of current Transformers, which often rely on suboptimal intra-layer attention. Our implementation extends the Transformer's functionality by enabling queries in a given layer to interact with keys and values from both the current layer and one preceding layer, thus enhancing the diversity of multi-head attention without additional computational burden. Extensive experiments demonstrate that our enhanced Transformer model achieves superior performance in language modeling tasks, highlighting the effectiveness of our skip-layer attention mechanism.",
    "original_url": "http://arxiv.org/pdf/2406.11274v1",
    "original_title": "Skip-Layer Attention: Bridging Abstract and Detailed Dependencies in Transformers",
    "source": "arxiv",
    "authors": [
      "Qian Chen",
      "Wen Wang",
      "Qinglin Zhang",
      "Siqi Zheng",
      "Shiliang Zhang",
      "Chong Deng",
      "Hai Yu",
      "Jiaqing Liu",
      "Yukun Ma",
      "Chong Zhang"
    ],
    "published": "2024-06-17T07:24:38+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2406.11274v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2102.12871v3_chunk_0",
    "chunk_text": "SparseBERT: Rethinking the Importance Analysis in Self-attention\n\nTransformer-based models are popularly used in natural language processing (NLP). Its core component, self-attention, has aroused widespread interest. To understand the self-attention mechanism, a direct method is to visualize the attention map of a pre-trained model. Based on the patterns observed, a series of efficient Transformers with different sparse attention masks have been proposed. From a theoretical perspective, universal approximability of Transformer-based models is also recently proved.",
    "original_url": "http://arxiv.org/pdf/2102.12871v3",
    "original_title": "SparseBERT: Rethinking the Importance Analysis in Self-attention",
    "source": "arxiv",
    "authors": [
      "Han Shi",
      "Jiahui Gao",
      "Xiaozhe Ren",
      "Hang Xu",
      "Xiaodan Liang",
      "Zhenguo Li",
      "James T. Kwok"
    ],
    "published": "2021-02-25T14:13:44+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2102.12871v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2102.12871v3_chunk_1",
    "chunk_text": "From a theoretical perspective, universal approximability of Transformer-based models is also recently proved. However, the above understanding and analysis of self-attention is based on a pre-trained model. To rethink the importance analysis in self-attention, we study the significance of different positions in attention matrix during pre-training. A surprising result is that diagonal elements in the attention map are the least important compared with other attention positions. We provide a proof showing that these diagonal elements can indeed be removed without deteriorating model performance.",
    "original_url": "http://arxiv.org/pdf/2102.12871v3",
    "original_title": "SparseBERT: Rethinking the Importance Analysis in Self-attention",
    "source": "arxiv",
    "authors": [
      "Han Shi",
      "Jiahui Gao",
      "Xiaozhe Ren",
      "Hang Xu",
      "Xiaodan Liang",
      "Zhenguo Li",
      "James T. Kwok"
    ],
    "published": "2021-02-25T14:13:44+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2102.12871v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2102.12871v3_chunk_2",
    "chunk_text": "We provide a proof showing that these diagonal elements can indeed be removed without deteriorating model performance. Furthermore, we propose a Differentiable Attention Mask (DAM) algorithm, which further guides the design of the SparseBERT. Extensive experiments verify our interesting findings and illustrate the effect of the proposed algorithm.",
    "original_url": "http://arxiv.org/pdf/2102.12871v3",
    "original_title": "SparseBERT: Rethinking the Importance Analysis in Self-attention",
    "source": "arxiv",
    "authors": [
      "Han Shi",
      "Jiahui Gao",
      "Xiaozhe Ren",
      "Hang Xu",
      "Xiaodan Liang",
      "Zhenguo Li",
      "James T. Kwok"
    ],
    "published": "2021-02-25T14:13:44+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2102.12871v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2202.06258v2_chunk_0",
    "chunk_text": "Flowformer: Linearizing Transformers with Conservation Flows\n\nTransformers based on the attention mechanism have achieved impressive success in various areas. However, the attention mechanism has a quadratic complexity, significantly impeding Transformers from dealing with numerous tokens and scaling up to bigger models. Previous methods mainly utilize the similarity decomposition and the associativity of matrix multiplication to devise linear-time attention mechanisms. They avoid degeneration of attention to a trivial distribution by reintroducing inductive biases such as the locality, thereby at the expense of model generality and expressiveness. In this paper, we linearize Transformers free from specific inductive biases based on the flow network theory.",
    "original_url": "http://arxiv.org/pdf/2202.06258v2",
    "original_title": "Flowformer: Linearizing Transformers with Conservation Flows",
    "source": "arxiv",
    "authors": [
      "Haixu Wu",
      "Jialong Wu",
      "Jiehui Xu",
      "Jianmin Wang",
      "Mingsheng Long"
    ],
    "published": "2022-02-13T08:44:10+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2202.06258v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2202.06258v2_chunk_1",
    "chunk_text": "In this paper, we linearize Transformers free from specific inductive biases based on the flow network theory. We cast attention as the information flow aggregated from the sources (values) to the sinks (results) through the learned flow capacities (attentions). Within this framework, we apply the property of flow conservation into attention and propose the Flow-Attention mechanism of linear complexity. By respectively conserving the incoming flow of sinks for source competition and the outgoing flow of sources for sink allocation, Flow-Attention inherently generates informative attentions without using specific inductive biases. Empowered by the Flow-Attention, Flowformer yields strong performance in linear time for wide areas, including long sequence, time series, vision, natural language, and reinforcement learning.",
    "original_url": "http://arxiv.org/pdf/2202.06258v2",
    "original_title": "Flowformer: Linearizing Transformers with Conservation Flows",
    "source": "arxiv",
    "authors": [
      "Haixu Wu",
      "Jialong Wu",
      "Jiehui Xu",
      "Jianmin Wang",
      "Mingsheng Long"
    ],
    "published": "2022-02-13T08:44:10+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2202.06258v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2202.06258v2_chunk_2",
    "chunk_text": "Empowered by the Flow-Attention, Flowformer yields strong performance in linear time for wide areas, including long sequence, time series, vision, natural language, and reinforcement learning. The code and settings are available at this repository: https://github.com/thuml/Flowformer.",
    "original_url": "http://arxiv.org/pdf/2202.06258v2",
    "original_title": "Flowformer: Linearizing Transformers with Conservation Flows",
    "source": "arxiv",
    "authors": [
      "Haixu Wu",
      "Jialong Wu",
      "Jiehui Xu",
      "Jianmin Wang",
      "Mingsheng Long"
    ],
    "published": "2022-02-13T08:44:10+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2202.06258v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2104.06399v2_chunk_0",
    "chunk_text": "Co-Scale Conv-Attentional Image Transformers\n\nIn this paper, we present Co-scale conv-attentional image Transformers (CoaT), a Transformer-based image classifier equipped with co-scale and conv-attentional mechanisms. First, the co-scale mechanism maintains the integrity of Transformers' encoder branches at individual scales, while allowing representations learned at different scales to effectively communicate with each other; we design a series of serial and parallel blocks to realize the co-scale mechanism. Second, we devise a conv-attentional mechanism by realizing a relative position embedding formulation in the factorized attention module with an efficient convolution-like implementation. CoaT empowers image Transformers with enriched multi-scale and contextual modeling capabilities. On ImageNet, relatively small CoaT models attain superior classification results compared with similar-sized convolutional neural networks and image/vision Transformers.",
    "original_url": "http://arxiv.org/pdf/2104.06399v2",
    "original_title": "Co-Scale Conv-Attentional Image Transformers",
    "source": "arxiv",
    "authors": [
      "Weijian Xu",
      "Yifan Xu",
      "Tyler Chang",
      "Zhuowen Tu"
    ],
    "published": "2021-04-13T17:58:29+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2104.06399v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2104.06399v2_chunk_1",
    "chunk_text": "On ImageNet, relatively small CoaT models attain superior classification results compared with similar-sized convolutional neural networks and image/vision Transformers. The effectiveness of CoaT's backbone is also illustrated on object detection and instance segmentation, demonstrating its applicability to downstream computer vision tasks.",
    "original_url": "http://arxiv.org/pdf/2104.06399v2",
    "original_title": "Co-Scale Conv-Attentional Image Transformers",
    "source": "arxiv",
    "authors": [
      "Weijian Xu",
      "Yifan Xu",
      "Tyler Chang",
      "Zhuowen Tu"
    ],
    "published": "2021-04-13T17:58:29+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2104.06399v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2211.11052v1_chunk_0",
    "chunk_text": "Convexifying Transformers: Improving optimization and understanding of transformer networks\n\nUnderstanding the fundamental mechanism behind the success of transformer networks is still an open problem in the deep learning literature. Although their remarkable performance has been mostly attributed to the self-attention mechanism, the literature still lacks a solid analysis of these networks and interpretation of the functions learned by them. To this end, we study the training problem of attention/transformer networks and introduce a novel convex analytic approach to improve the understanding and optimization of these networks. Particularly, we first introduce a convex alternative to the self-attention mechanism and reformulate the regularized training problem of transformer networks with our alternative convex attention. Then, we cast the reformulation as a convex optimization problem that is interpretable and easier to optimize.",
    "original_url": "http://arxiv.org/pdf/2211.11052v1",
    "original_title": "Convexifying Transformers: Improving optimization and understanding of transformer networks",
    "source": "arxiv",
    "authors": [
      "Tolga Ergen",
      "Behnam Neyshabur",
      "Harsh Mehta"
    ],
    "published": "2022-11-20T18:17:47+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2211.11052v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2211.11052v1_chunk_1",
    "chunk_text": "Then, we cast the reformulation as a convex optimization problem that is interpretable and easier to optimize. Moreover, as a byproduct of our convex analysis, we reveal an implicit regularization mechanism, which promotes sparsity across tokens. Therefore, we not only improve the optimization of attention/transformer networks but also provide a solid theoretical understanding of the functions learned by them. We also demonstrate the effectiveness of our theory through several numerical experiments.",
    "original_url": "http://arxiv.org/pdf/2211.11052v1",
    "original_title": "Convexifying Transformers: Improving optimization and understanding of transformer networks",
    "source": "arxiv",
    "authors": [
      "Tolga Ergen",
      "Behnam Neyshabur",
      "Harsh Mehta"
    ],
    "published": "2022-11-20T18:17:47+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2211.11052v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2310.05296v1_chunk_0",
    "chunk_text": "Tailoring Self-Attention for Graph via Rooted Subtrees\n\nAttention mechanisms have made significant strides in graph learning, yet they still exhibit notable limitations: local attention faces challenges in capturing long-range information due to the inherent problems of the message-passing scheme, while global attention cannot reflect the hierarchical neighborhood structure and fails to capture fine-grained local information. In this paper, we propose a novel multi-hop graph attention mechanism, named Subtree Attention (STA), to address the aforementioned issues. STA seamlessly bridges the fully-attentional structure and the rooted subtree, with theoretical proof that STA approximates the global attention under extreme settings. By allowing direct computation of attention weights among multi-hop neighbors, STA mitigates the inherent problems in existing graph attention mechanisms. Further we devise an efficient form for STA by employing kernelized softmax, which yields a linear time complexity.",
    "original_url": "http://arxiv.org/pdf/2310.05296v1",
    "original_title": "Tailoring Self-Attention for Graph via Rooted Subtrees",
    "source": "arxiv",
    "authors": [
      "Siyuan Huang",
      "Yunchong Song",
      "Jiayue Zhou",
      "Zhouhan Lin"
    ],
    "published": "2023-10-08T21:47:18+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2310.05296v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2310.05296v1_chunk_1",
    "chunk_text": "Further we devise an efficient form for STA by employing kernelized softmax, which yields a linear time complexity. Our resulting GNN architecture, the STAGNN, presents a simple yet performant STA-based graph neural network leveraging a hop-aware attention strategy. Comprehensive evaluations on ten node classification datasets demonstrate that STA-based models outperform existing graph transformers and mainstream GNNs. The code is available at https://github.com/LUMIA-Group/SubTree-Attention.",
    "original_url": "http://arxiv.org/pdf/2310.05296v1",
    "original_title": "Tailoring Self-Attention for Graph via Rooted Subtrees",
    "source": "arxiv",
    "authors": [
      "Siyuan Huang",
      "Yunchong Song",
      "Jiayue Zhou",
      "Zhouhan Lin"
    ],
    "published": "2023-10-08T21:47:18+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2310.05296v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2410.05258v2_chunk_0",
    "chunk_text": "Differential Transformer\n\nTransformer tends to overallocate attention to irrelevant context. In this work, we introduce Diff Transformer, which amplifies attention to the relevant context while canceling noise. Specifically, the differential attention mechanism calculates attention scores as the difference between two separate softmax attention maps. The subtraction cancels noise, promoting the emergence of sparse attention patterns. Experimental results on language modeling show that Diff Transformer outperforms Transformer in various settings of scaling up model size and training tokens.",
    "original_url": "http://arxiv.org/pdf/2410.05258v2",
    "original_title": "Differential Transformer",
    "source": "arxiv",
    "authors": [
      "Tianzhu Ye",
      "Li Dong",
      "Yuqing Xia",
      "Yutao Sun",
      "Yi Zhu",
      "Gao Huang",
      "Furu Wei"
    ],
    "published": "2024-10-07T17:57:38+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2410.05258v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2410.05258v2_chunk_1",
    "chunk_text": "Experimental results on language modeling show that Diff Transformer outperforms Transformer in various settings of scaling up model size and training tokens. More intriguingly, it offers notable advantages in practical applications, such as long-context modeling, key information retrieval, hallucination mitigation, in-context learning, and reduction of activation outliers. By being less distracted by irrelevant context, Diff Transformer can mitigate hallucination in question answering and text summarization. For in-context learning, Diff Transformer not only enhances accuracy but is also more robust to order permutation, which was considered as a chronic robustness issue. The results position Diff Transformer as a highly effective and promising architecture to advance large language models.",
    "original_url": "http://arxiv.org/pdf/2410.05258v2",
    "original_title": "Differential Transformer",
    "source": "arxiv",
    "authors": [
      "Tianzhu Ye",
      "Li Dong",
      "Yuqing Xia",
      "Yutao Sun",
      "Yi Zhu",
      "Gao Huang",
      "Furu Wei"
    ],
    "published": "2024-10-07T17:57:38+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2410.05258v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2410.05258v2_chunk_2",
    "chunk_text": "The results position Diff Transformer as a highly effective and promising architecture to advance large language models.",
    "original_url": "http://arxiv.org/pdf/2410.05258v2",
    "original_title": "Differential Transformer",
    "source": "arxiv",
    "authors": [
      "Tianzhu Ye",
      "Li Dong",
      "Yuqing Xia",
      "Yutao Sun",
      "Yi Zhu",
      "Gao Huang",
      "Furu Wei"
    ],
    "published": "2024-10-07T17:57:38+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2410.05258v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2305.17937v1_chunk_0",
    "chunk_text": "Attention Mechanisms in Medical Image Segmentation: A Survey\n\nMedical image segmentation plays an important role in computer-aided diagnosis. Attention mechanisms that distinguish important parts from irrelevant parts have been widely used in medical image segmentation tasks. This paper systematically reviews the basic principles of attention mechanisms and their applications in medical image segmentation. First, we review the basic concepts of attention mechanism and formulation. Second, we surveyed over 300 articles related to medical image segmentation, and divided them into two groups based on their attention mechanisms, non-Transformer attention and Transformer attention.",
    "original_url": "http://arxiv.org/pdf/2305.17937v1",
    "original_title": "Attention Mechanisms in Medical Image Segmentation: A Survey",
    "source": "arxiv",
    "authors": [
      "Yutong Xie",
      "Bing Yang",
      "Qingbiao Guan",
      "Jianpeng Zhang",
      "Qi Wu",
      "Yong Xia"
    ],
    "published": "2023-05-29T08:00:54+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2305.17937v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2305.17937v1_chunk_1",
    "chunk_text": "Second, we surveyed over 300 articles related to medical image segmentation, and divided them into two groups based on their attention mechanisms, non-Transformer attention and Transformer attention. In each group, we deeply analyze the attention mechanisms from three aspects based on the current literature work, i.e., the principle of the mechanism (what to use), implementation methods (how to use), and application tasks (where to use). We also thoroughly analyzed the advantages and limitations of their applications to different tasks. Finally, we summarize the current state of research and shortcomings in the field, and discuss the potential challenges in the future, including task specificity, robustness, standard evaluation, etc. We hope that this review can showcase the overall research context of traditional and Transformer attention methods, provide a clear reference for subsequent research, and inspire more advanced attention research, not only in medical image segmentation, but also in other image analysis scenarios.",
    "original_url": "http://arxiv.org/pdf/2305.17937v1",
    "original_title": "Attention Mechanisms in Medical Image Segmentation: A Survey",
    "source": "arxiv",
    "authors": [
      "Yutong Xie",
      "Bing Yang",
      "Qingbiao Guan",
      "Jianpeng Zhang",
      "Qi Wu",
      "Yong Xia"
    ],
    "published": "2023-05-29T08:00:54+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2305.17937v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2305.17937v1_chunk_2",
    "chunk_text": "We hope that this review can showcase the overall research context of traditional and Transformer attention methods, provide a clear reference for subsequent research, and inspire more advanced attention research, not only in medical image segmentation, but also in other image analysis scenarios.",
    "original_url": "http://arxiv.org/pdf/2305.17937v1",
    "original_title": "Attention Mechanisms in Medical Image Segmentation: A Survey",
    "source": "arxiv",
    "authors": [
      "Yutong Xie",
      "Bing Yang",
      "Qingbiao Guan",
      "Jianpeng Zhang",
      "Qi Wu",
      "Yong Xia"
    ],
    "published": "2023-05-29T08:00:54+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2305.17937v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2304.13276v1_chunk_0",
    "chunk_text": "The Closeness of In-Context Learning and Weight Shifting for Softmax Regression\n\nLarge language models (LLMs) are known for their exceptional performance in natural language processing, making them highly effective in many human life-related or even job-related tasks. The attention mechanism in the Transformer architecture is a critical component of LLMs, as it allows the model to selectively focus on specific input parts. The softmax unit, which is a key part of the attention mechanism, normalizes the attention scores. Hence, the performance of LLMs in various NLP tasks depends significantly on the crucial role played by the attention mechanism with the softmax unit. In-context learning, as one of the celebrated abilities of recent LLMs, is an important concept in querying LLMs such as ChatGPT.",
    "original_url": "http://arxiv.org/pdf/2304.13276v1",
    "original_title": "The Closeness of In-Context Learning and Weight Shifting for Softmax Regression",
    "source": "arxiv",
    "authors": [
      "Shuai Li",
      "Zhao Song",
      "Yu Xia",
      "Tong Yu",
      "Tianyi Zhou"
    ],
    "published": "2023-04-26T04:33:41+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2304.13276v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2304.13276v1_chunk_1",
    "chunk_text": "In-context learning, as one of the celebrated abilities of recent LLMs, is an important concept in querying LLMs such as ChatGPT. Without further parameter updates, Transformers can learn to predict based on few in-context examples. However, the reason why Transformers becomes in-context learners is not well understood. Recently, several works [ASA+22,GTLV22,ONR+22] have studied the in-context learning from a mathematical perspective based on a linear regression formulation $\\min_x\\| Ax - b \\|_2$, which show Transformers' capability of learning linear functions in context. In this work, we study the in-context learning based on a softmax regression formulation $\\min_{x} \\| \\langle \\exp(Ax), {\\bf 1}_n \\rangle^{-1} \\exp(Ax) - b \\|_2$ of Transformer's attention mechanism.",
    "original_url": "http://arxiv.org/pdf/2304.13276v1",
    "original_title": "The Closeness of In-Context Learning and Weight Shifting for Softmax Regression",
    "source": "arxiv",
    "authors": [
      "Shuai Li",
      "Zhao Song",
      "Yu Xia",
      "Tong Yu",
      "Tianyi Zhou"
    ],
    "published": "2023-04-26T04:33:41+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2304.13276v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2304.13276v1_chunk_2",
    "chunk_text": "In this work, we study the in-context learning based on a softmax regression formulation $\\min_{x} \\| \\langle \\exp(Ax), {\\bf 1}_n \\rangle^{-1} \\exp(Ax) - b \\|_2$ of Transformer's attention mechanism. We show the upper bounds of the data transformations induced by a single self-attention layer and by gradient-descent on a $\\ell_2$ regression loss for softmax prediction function, which imply that when training self-attention-only Transformers for fundamental regression tasks, the models learned by gradient-descent and Transformers show great similarity.",
    "original_url": "http://arxiv.org/pdf/2304.13276v1",
    "original_title": "The Closeness of In-Context Learning and Weight Shifting for Softmax Regression",
    "source": "arxiv",
    "authors": [
      "Shuai Li",
      "Zhao Song",
      "Yu Xia",
      "Tong Yu",
      "Tianyi Zhou"
    ],
    "published": "2023-04-26T04:33:41+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2304.13276v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2309.16354v2_chunk_0",
    "chunk_text": "Transformer-VQ: Linear-Time Transformers via Vector Quantization\n\nWe introduce Transformer-VQ, a decoder-only transformer computing softmax-based dense self-attention in linear time. Transformer-VQ's efficient attention is enabled by vector-quantized keys and a novel caching mechanism. In our large-scale experiments, Transformer-VQ is shown highly competitive in quality, obtaining 0.99 bpb on Enwik8, 26.6 ppl on PG-19, and 3.16 bpb on ImageNet64. In addition, the optimized implementation of Transformer-VQ is over 3x faster than a comparable quadratic-time transformer at sequence length 8k, is over 12x faster at 32k, and can scale to 131k with similar throughput. Code available: \\url{https://github.com/transformer-vq/transformer_vq}",
    "original_url": "http://arxiv.org/pdf/2309.16354v2",
    "original_title": "Transformer-VQ: Linear-Time Transformers via Vector Quantization",
    "source": "arxiv",
    "authors": [
      "Lucas D. Lingle"
    ],
    "published": "2023-09-28T11:26:52+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2309.16354v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2309.16354v2_chunk_1",
    "chunk_text": "Code available: \\url{https://github.com/transformer-vq/transformer_vq}",
    "original_url": "http://arxiv.org/pdf/2309.16354v2",
    "original_title": "Transformer-VQ: Linear-Time Transformers via Vector Quantization",
    "source": "arxiv",
    "authors": [
      "Lucas D. Lingle"
    ],
    "published": "2023-09-28T11:26:52+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2309.16354v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2110.05036v2_chunk_0",
    "chunk_text": "Multi-View Self-Attention Based Transformer for Speaker Recognition\n\nInitially developed for natural language processing (NLP), Transformer model is now widely used for speech processing tasks such as speaker recognition, due to its powerful sequence modeling capabilities. However, conventional self-attention mechanisms are originally designed for modeling textual sequence without considering the characteristics of speech and speaker modeling. Besides, different Transformer variants for speaker recognition have not been well studied. In this work, we propose a novel multi-view self-attention mechanism and present an empirical study of different Transformer variants with or without the proposed attention mechanism for speaker recognition. Specifically, to balance the capabilities of capturing global dependencies and modeling the locality, we propose a multi-view self-attention mechanism for speaker Transformer, in which different attention heads can attend to different ranges of the receptive field.",
    "original_url": "http://arxiv.org/pdf/2110.05036v2",
    "original_title": "Multi-View Self-Attention Based Transformer for Speaker Recognition",
    "source": "arxiv",
    "authors": [
      "Rui Wang",
      "Junyi Ao",
      "Long Zhou",
      "Shujie Liu",
      "Zhihua Wei",
      "Tom Ko",
      "Qing Li",
      "Yu Zhang"
    ],
    "published": "2021-10-11T07:03:23+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2110.05036v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2110.05036v2_chunk_1",
    "chunk_text": "Specifically, to balance the capabilities of capturing global dependencies and modeling the locality, we propose a multi-view self-attention mechanism for speaker Transformer, in which different attention heads can attend to different ranges of the receptive field. Furthermore, we introduce and compare five Transformer variants with different network architectures, embedding locations, and pooling methods to learn speaker embeddings. Experimental results on the VoxCeleb1 and VoxCeleb2 datasets show that the proposed multi-view self-attention mechanism achieves improvement in the performance of speaker recognition, and the proposed speaker Transformer network attains excellent results compared with state-of-the-art models.",
    "original_url": "http://arxiv.org/pdf/2110.05036v2",
    "original_title": "Multi-View Self-Attention Based Transformer for Speaker Recognition",
    "source": "arxiv",
    "authors": [
      "Rui Wang",
      "Junyi Ao",
      "Long Zhou",
      "Shujie Liu",
      "Zhihua Wei",
      "Tom Ko",
      "Qing Li",
      "Yu Zhang"
    ],
    "published": "2021-10-11T07:03:23+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2110.05036v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2012.02144v1_chunk_0",
    "chunk_text": "Do We Really Need That Many Parameters In Transformer For Extractive Summarization? Discourse Can Help ! The multi-head self-attention of popular transformer models is widely used within Natural Language Processing (NLP), including for the task of extractive summarization. With the goal of analyzing and pruning the parameter-heavy self-attention mechanism, there are multiple approaches proposing more parameter-light self-attention alternatives. In this paper, we present a novel parameter-lean self-attention mechanism using discourse priors.",
    "original_url": "http://arxiv.org/pdf/2012.02144v1",
    "original_title": "Do We Really Need That Many Parameters In Transformer For Extractive Summarization? Discourse Can Help !",
    "source": "arxiv",
    "authors": [
      "Wen Xiao",
      "Patrick Huber",
      "Giuseppe Carenini"
    ],
    "published": "2020-12-03T18:23:21+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2012.02144v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2012.02144v1_chunk_1",
    "chunk_text": "In this paper, we present a novel parameter-lean self-attention mechanism using discourse priors. Our new tree self-attention is based on document-level discourse information, extending the recently proposed \"Synthesizer\" framework with another lightweight alternative. We show empirical results that our tree self-attention approach achieves competitive ROUGE-scores on the task of extractive summarization. When compared to the original single-head transformer model, the tree attention approach reaches similar performance on both, EDU and sentence level, despite the significant reduction of parameters in the attention component. We further significantly outperform the 8-head transformer model on sentence level when applying a more balanced hyper-parameter setting, requiring an order of magnitude less parameters.",
    "original_url": "http://arxiv.org/pdf/2012.02144v1",
    "original_title": "Do We Really Need That Many Parameters In Transformer For Extractive Summarization? Discourse Can Help !",
    "source": "arxiv",
    "authors": [
      "Wen Xiao",
      "Patrick Huber",
      "Giuseppe Carenini"
    ],
    "published": "2020-12-03T18:23:21+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2012.02144v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2012.02144v1_chunk_2",
    "chunk_text": "We further significantly outperform the 8-head transformer model on sentence level when applying a more balanced hyper-parameter setting, requiring an order of magnitude less parameters.",
    "original_url": "http://arxiv.org/pdf/2012.02144v1",
    "original_title": "Do We Really Need That Many Parameters In Transformer For Extractive Summarization? Discourse Can Help !",
    "source": "arxiv",
    "authors": [
      "Wen Xiao",
      "Patrick Huber",
      "Giuseppe Carenini"
    ],
    "published": "2020-12-03T18:23:21+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2012.02144v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2110.06553v2_chunk_0",
    "chunk_text": "Spatial-temporal Transformers for EEG Emotion Recognition\n\nElectroencephalography (EEG) is a popular and effective tool for emotion recognition. However, the propagation mechanisms of EEG in the human brain and its intrinsic correlation with emotions are still obscure to researchers. This work proposes four variant transformer frameworks~(spatial attention, temporal attention, sequential spatial-temporal attention and simultaneous spatial-temporal attention) for EEG emotion recognition to explore the relationship between emotion and spatial-temporal EEG features. Specifically, spatial attention and temporal attention are to learn the topological structure information and time-varying EEG characteristics for emotion recognition respectively. Sequential spatial-temporal attention does the spatial attention within a one-second segment and temporal attention within one sample sequentially to explore the influence degree of emotional stimulation on EEG signals of diverse EEG electrodes in the same temporal segment.",
    "original_url": "http://arxiv.org/pdf/2110.06553v2",
    "original_title": "Spatial-temporal Transformers for EEG Emotion Recognition",
    "source": "arxiv",
    "authors": [
      "Jiyao Liu",
      "Hao Wu",
      "Li Zhang",
      "Yanxi Zhao"
    ],
    "published": "2021-10-13T08:08:57+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2110.06553v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2110.06553v2_chunk_1",
    "chunk_text": "Sequential spatial-temporal attention does the spatial attention within a one-second segment and temporal attention within one sample sequentially to explore the influence degree of emotional stimulation on EEG signals of diverse EEG electrodes in the same temporal segment. The simultaneous spatial-temporal attention, whose spatial and temporal attention are performed simultaneously, is used to model the relationship between different spatial features in different time segments. The experimental results demonstrate that simultaneous spatial-temporal attention leads to the best emotion recognition accuracy among the design choices, indicating modeling the correlation of spatial and temporal features of EEG signals is significant to emotion recognition.",
    "original_url": "http://arxiv.org/pdf/2110.06553v2",
    "original_title": "Spatial-temporal Transformers for EEG Emotion Recognition",
    "source": "arxiv",
    "authors": [
      "Jiyao Liu",
      "Hao Wu",
      "Li Zhang",
      "Yanxi Zhao"
    ],
    "published": "2021-10-13T08:08:57+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2110.06553v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2501.06480v2_chunk_0",
    "chunk_text": "Flash Window Attention: speedup the attention computation for Swin Transformer\n\nTo address the high resolution of image pixels, the Swin Transformer introduces window attention. This mechanism divides an image into non-overlapping windows and restricts attention computation to within each window, significantly enhancing computational efficiency. To further optimize this process, one might consider replacing standard attention with flash attention, which has proven to be more efficient in language models. However, a direct substitution is ineffective. Flash attention is designed for long sequences, whereas window attention deals with shorter sequences but must handle numerous of them in parallel.",
    "original_url": "http://arxiv.org/pdf/2501.06480v2",
    "original_title": "Flash Window Attention: speedup the attention computation for Swin Transformer",
    "source": "arxiv",
    "authors": [
      "Zhendong Zhang"
    ],
    "published": "2025-01-11T08:13:13+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2501.06480v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2501.06480v2_chunk_1",
    "chunk_text": "Flash attention is designed for long sequences, whereas window attention deals with shorter sequences but must handle numerous of them in parallel. In this report, we present an optimized solution called Flash Window Attention, tailored specifically for window attention. Flash Window Attention improves attention computation efficiency by up to 300% and enhances end-to-end runtime efficiency by up to 30%. Our code is available online.",
    "original_url": "http://arxiv.org/pdf/2501.06480v2",
    "original_title": "Flash Window Attention: speedup the attention computation for Swin Transformer",
    "source": "arxiv",
    "authors": [
      "Zhendong Zhang"
    ],
    "published": "2025-01-11T08:13:13+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2501.06480v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2312.08268v1_chunk_0",
    "chunk_text": "Efficient Multi-Object Pose Estimation using Multi-Resolution Deformable Attention and Query Aggregation\n\nObject pose estimation is a long-standing problem in computer vision. Recently, attention-based vision transformer models have achieved state-of-the-art results in many computer vision applications. Exploiting the permutation-invariant nature of the attention mechanism, a family of vision transformer models formulate multi-object pose estimation as a set prediction problem. However, existing vision transformer models for multi-object pose estimation rely exclusively on the attention mechanism. Convolutional neural networks, on the other hand, hard-wire various inductive biases into their architecture.",
    "original_url": "http://arxiv.org/pdf/2312.08268v1",
    "original_title": "Efficient Multi-Object Pose Estimation using Multi-Resolution Deformable Attention and Query Aggregation",
    "source": "arxiv",
    "authors": [
      "Arul Selvam Periyasamy",
      "Vladimir Tsaturyan",
      "Sven Behnke"
    ],
    "published": "2023-12-13T16:30:00+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2312.08268v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2312.08268v1_chunk_1",
    "chunk_text": "Convolutional neural networks, on the other hand, hard-wire various inductive biases into their architecture. In this paper, we investigate incorporating inductive biases in vision transformer models for multi-object pose estimation, which facilitates learning long-range dependencies while circumventing the costly global attention. In particular, we use multi-resolution deformable attention, where the attention operation is performed only between a few deformed reference points. Furthermore, we propose a query aggregation mechanism that enables increasing the number of object queries without increasing the computational complexity. We evaluate the proposed model on the challenging YCB-Video dataset and report state-of-the-art results.",
    "original_url": "http://arxiv.org/pdf/2312.08268v1",
    "original_title": "Efficient Multi-Object Pose Estimation using Multi-Resolution Deformable Attention and Query Aggregation",
    "source": "arxiv",
    "authors": [
      "Arul Selvam Periyasamy",
      "Vladimir Tsaturyan",
      "Sven Behnke"
    ],
    "published": "2023-12-13T16:30:00+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2312.08268v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2312.08268v1_chunk_2",
    "chunk_text": "We evaluate the proposed model on the challenging YCB-Video dataset and report state-of-the-art results.",
    "original_url": "http://arxiv.org/pdf/2312.08268v1",
    "original_title": "Efficient Multi-Object Pose Estimation using Multi-Resolution Deformable Attention and Query Aggregation",
    "source": "arxiv",
    "authors": [
      "Arul Selvam Periyasamy",
      "Vladimir Tsaturyan",
      "Sven Behnke"
    ],
    "published": "2023-12-13T16:30:00+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2312.08268v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2405.09285v1_chunk_0",
    "chunk_text": "Positional Knowledge is All You Need: Position-induced Transformer (PiT) for Operator Learning\n\nOperator learning for Partial Differential Equations (PDEs) is rapidly emerging as a promising approach for surrogate modeling of intricate systems. Transformers with the self-attention mechanism$\\unicode{x2013}$a powerful tool originally designed for natural language processing$\\unicode{x2013}$have recently been adapted for operator learning. However, they confront challenges, including high computational demands and limited interpretability. This raises a critical question: Is there a more efficient attention mechanism for Transformer-based operator learning? This paper proposes the Position-induced Transformer (PiT), built on an innovative position-attention mechanism, which demonstrates significant advantages over the classical self-attention in operator learning.",
    "original_url": "http://arxiv.org/pdf/2405.09285v1",
    "original_title": "Positional Knowledge is All You Need: Position-induced Transformer (PiT) for Operator Learning",
    "source": "arxiv",
    "authors": [
      "Junfeng Chen",
      "Kailiang Wu"
    ],
    "published": "2024-05-15T12:09:24+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2405.09285v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2405.09285v1_chunk_1",
    "chunk_text": "This paper proposes the Position-induced Transformer (PiT), built on an innovative position-attention mechanism, which demonstrates significant advantages over the classical self-attention in operator learning. Position-attention draws inspiration from numerical methods for PDEs. Different from self-attention, position-attention is induced by only the spatial interrelations of sampling positions for input functions of the operators, and does not rely on the input function values themselves, thereby greatly boosting efficiency. PiT exhibits superior performance over current state-of-the-art neural operators in a variety of complex operator learning tasks across diverse PDE benchmarks. Additionally, PiT possesses an enhanced discretization convergence feature, compared to the widely-used Fourier neural operator.",
    "original_url": "http://arxiv.org/pdf/2405.09285v1",
    "original_title": "Positional Knowledge is All You Need: Position-induced Transformer (PiT) for Operator Learning",
    "source": "arxiv",
    "authors": [
      "Junfeng Chen",
      "Kailiang Wu"
    ],
    "published": "2024-05-15T12:09:24+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2405.09285v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2405.09285v1_chunk_2",
    "chunk_text": "Additionally, PiT possesses an enhanced discretization convergence feature, compared to the widely-used Fourier neural operator.",
    "original_url": "http://arxiv.org/pdf/2405.09285v1",
    "original_title": "Positional Knowledge is All You Need: Position-induced Transformer (PiT) for Operator Learning",
    "source": "arxiv",
    "authors": [
      "Junfeng Chen",
      "Kailiang Wu"
    ],
    "published": "2024-05-15T12:09:24+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2405.09285v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2503.15983v1_chunk_0",
    "chunk_text": "InhibiDistilbert: Knowledge Distillation for a ReLU and Addition-based Transformer\n\nThis work explores optimizing transformer-based language models by integrating model compression techniques with inhibitor attention, a novel alternative attention mechanism. Inhibitor attention employs Manhattan distances and ReLU activations instead of the matrix multiplications and softmax activation of the conventional scaled dot-product attention. This shift offers potential computational and energy savings while maintaining model effectiveness. We propose further adjustments to improve the inhibitor mechanism's training efficiency and evaluate its performance on the DistilBERT architecture. Our knowledge distillation experiments indicate that the modified inhibitor transformer model can achieve competitive performance on standard NLP benchmarks, including General Language Understanding Evaluation (GLUE) and sentiment analysis tasks.",
    "original_url": "http://arxiv.org/pdf/2503.15983v1",
    "original_title": "InhibiDistilbert: Knowledge Distillation for a ReLU and Addition-based Transformer",
    "source": "arxiv",
    "authors": [
      "Tony Zhang",
      "Rickard Brnnvall"
    ],
    "published": "2025-03-20T09:30:35+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2503.15983v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2503.15983v1_chunk_1",
    "chunk_text": "Our knowledge distillation experiments indicate that the modified inhibitor transformer model can achieve competitive performance on standard NLP benchmarks, including General Language Understanding Evaluation (GLUE) and sentiment analysis tasks.",
    "original_url": "http://arxiv.org/pdf/2503.15983v1",
    "original_title": "InhibiDistilbert: Knowledge Distillation for a ReLU and Addition-based Transformer",
    "source": "arxiv",
    "authors": [
      "Tony Zhang",
      "Rickard Brnnvall"
    ],
    "published": "2025-03-20T09:30:35+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2503.15983v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2006.03265v2_chunk_0",
    "chunk_text": "Understanding Self-Attention of Self-Supervised Audio Transformers\n\nSelf-supervised Audio Transformers (SAT) enable great success in many downstream speech applications like ASR, but how they work has not been widely explored yet. In this work, we present multiple strategies for the analysis of attention mechanisms in SAT. We categorize attentions into explainable categories, where we discover each category possesses its own unique functionality. We provide a visualization tool for understanding multi-head self-attention, importance ranking strategies for identifying critical attention, and attention refinement techniques to improve model performance.",
    "original_url": "http://arxiv.org/pdf/2006.03265v2",
    "original_title": "Understanding Self-Attention of Self-Supervised Audio Transformers",
    "source": "arxiv",
    "authors": [
      "Shu-wen Yang",
      "Andy T. Liu",
      "Hung-yi Lee"
    ],
    "published": "2020-06-05T07:23:03+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2006.03265v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2102.12895v1_chunk_0",
    "chunk_text": "Evolving Attention with Residual Convolutions\n\nTransformer is a ubiquitous model for natural language processing and has attracted wide attentions in computer vision. The attention maps are indispensable for a transformer model to encode the dependencies among input tokens. However, they are learned independently in each layer and sometimes fail to capture precise patterns. In this paper, we propose a novel and generic mechanism based on evolving attention to improve the performance of transformers. On one hand, the attention maps in different layers share common knowledge, thus the ones in preceding layers can instruct the attention in succeeding layers through residual connections.",
    "original_url": "http://arxiv.org/pdf/2102.12895v1",
    "original_title": "Evolving Attention with Residual Convolutions",
    "source": "arxiv",
    "authors": [
      "Yujing Wang",
      "Yaming Yang",
      "Jiangang Bai",
      "Mingliang Zhang",
      "Jing Bai",
      "Jing Yu",
      "Ce Zhang",
      "Gao Huang",
      "Yunhai Tong"
    ],
    "published": "2021-02-20T15:24:06+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2102.12895v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2102.12895v1_chunk_1",
    "chunk_text": "On one hand, the attention maps in different layers share common knowledge, thus the ones in preceding layers can instruct the attention in succeeding layers through residual connections. On the other hand, low-level and high-level attentions vary in the level of abstraction, so we adopt convolutional layers to model the evolutionary process of attention maps. The proposed evolving attention mechanism achieves significant performance improvement over various state-of-the-art models for multiple tasks, including image classification, natural language understanding and machine translation.",
    "original_url": "http://arxiv.org/pdf/2102.12895v1",
    "original_title": "Evolving Attention with Residual Convolutions",
    "source": "arxiv",
    "authors": [
      "Yujing Wang",
      "Yaming Yang",
      "Jiangang Bai",
      "Mingliang Zhang",
      "Jing Bai",
      "Jing Yu",
      "Ce Zhang",
      "Gao Huang",
      "Yunhai Tong"
    ],
    "published": "2021-02-20T15:24:06+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2102.12895v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2108.04962v1_chunk_0",
    "chunk_text": "Adaptive Multi-Resolution Attention with Linear Complexity\n\nTransformers have improved the state-of-the-art across numerous tasks in sequence modeling. Besides the quadratic computational and memory complexity w.r.t the sequence length, the self-attention mechanism only processes information at the same scale, i.e., all attention heads are in the same resolution, resulting in the limited power of the Transformer. To remedy this, we propose a novel and efficient structure named Adaptive Multi-Resolution Attention (AdaMRA for short), which scales linearly to sequence length in terms of time and space. Specifically, we leverage a multi-resolution multi-head attention mechanism, enabling attention heads to capture long-range contextual information in a coarse-to-fine fashion. Moreover, to capture the potential relations between query representation and clues of different attention granularities, we leave the decision of which resolution of attention to use to query, which further improves the model's capacity compared to vanilla Transformer.",
    "original_url": "http://arxiv.org/pdf/2108.04962v1",
    "original_title": "Adaptive Multi-Resolution Attention with Linear Complexity",
    "source": "arxiv",
    "authors": [
      "Yao Zhang",
      "Yunpu Ma",
      "Thomas Seidl",
      "Volker Tresp"
    ],
    "published": "2021-08-10T23:17:16+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2108.04962v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2108.04962v1_chunk_1",
    "chunk_text": "Moreover, to capture the potential relations between query representation and clues of different attention granularities, we leave the decision of which resolution of attention to use to query, which further improves the model's capacity compared to vanilla Transformer. In an effort to reduce complexity, we adopt kernel attention without degrading the performance. Extensive experiments on several benchmarks demonstrate the effectiveness and efficiency of our model by achieving a state-of-the-art performance-efficiency-memory trade-off. To facilitate AdaMRA utilization by the scientific community, the code implementation will be made publicly available.",
    "original_url": "http://arxiv.org/pdf/2108.04962v1",
    "original_title": "Adaptive Multi-Resolution Attention with Linear Complexity",
    "source": "arxiv",
    "authors": [
      "Yao Zhang",
      "Yunpu Ma",
      "Thomas Seidl",
      "Volker Tresp"
    ],
    "published": "2021-08-10T23:17:16+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2108.04962v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2207.09603v2_chunk_0",
    "chunk_text": "AiATrack: Attention in Attention for Transformer Visual Tracking\n\nTransformer trackers have achieved impressive advancements recently, where the attention mechanism plays an important role. However, the independent correlation computation in the attention mechanism could result in noisy and ambiguous attention weights, which inhibits further performance improvement. To address this issue, we propose an attention in attention (AiA) module, which enhances appropriate correlations and suppresses erroneous ones by seeking consensus among all correlation vectors. Our AiA module can be readily applied to both self-attention blocks and cross-attention blocks to facilitate feature aggregation and information propagation for visual tracking. Moreover, we propose a streamlined Transformer tracking framework, dubbed AiATrack, by introducing efficient feature reuse and target-background embeddings to make full use of temporal references.",
    "original_url": "http://arxiv.org/pdf/2207.09603v2",
    "original_title": "AiATrack: Attention in Attention for Transformer Visual Tracking",
    "source": "arxiv",
    "authors": [
      "Shenyuan Gao",
      "Chunluan Zhou",
      "Chao Ma",
      "Xinggang Wang",
      "Junsong Yuan"
    ],
    "published": "2022-07-20T00:44:03+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2207.09603v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2207.09603v2_chunk_1",
    "chunk_text": "Moreover, we propose a streamlined Transformer tracking framework, dubbed AiATrack, by introducing efficient feature reuse and target-background embeddings to make full use of temporal references. Experiments show that our tracker achieves state-of-the-art performance on six tracking benchmarks while running at a real-time speed.",
    "original_url": "http://arxiv.org/pdf/2207.09603v2",
    "original_title": "AiATrack: Attention in Attention for Transformer Visual Tracking",
    "source": "arxiv",
    "authors": [
      "Shenyuan Gao",
      "Chunluan Zhou",
      "Chao Ma",
      "Xinggang Wang",
      "Junsong Yuan"
    ],
    "published": "2022-07-20T00:44:03+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2207.09603v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2112.12748v1_chunk_0",
    "chunk_text": "Assessing the Impact of Attention and Self-Attention Mechanisms on the Classification of Skin Lesions\n\nAttention mechanisms have raised significant interest in the research community, since they promise significant improvements in the performance of neural network architectures. However, in any specific problem, we still lack a principled way to choose specific mechanisms and hyper-parameters that lead to guaranteed improvements. More recently, self-attention has been proposed and widely used in transformer-like architectures, leading to significant breakthroughs in some applications. In this work we focus on two forms of attention mechanisms: attention modules and self-attention. Attention modules are used to reweight the features of each layer input tensor.",
    "original_url": "http://arxiv.org/pdf/2112.12748v1",
    "original_title": "Assessing the Impact of Attention and Self-Attention Mechanisms on the Classification of Skin Lesions",
    "source": "arxiv",
    "authors": [
      "Rafael Pedro",
      "Arlindo L. Oliveira"
    ],
    "published": "2021-12-23T18:02:48+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2112.12748v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2112.12748v1_chunk_1",
    "chunk_text": "Attention modules are used to reweight the features of each layer input tensor. Different modules have different ways to perform this reweighting in fully connected or convolutional layers. The attention models studied are completely modular and in this work they will be used with the popular ResNet architecture. Self-Attention, originally proposed in the area of Natural Language Processing makes it possible to relate all the items in an input sequence. Self-Attention is becoming increasingly popular in Computer Vision, where it is sometimes combined with convolutional layers, although some recent architectures do away entirely with convolutions.",
    "original_url": "http://arxiv.org/pdf/2112.12748v1",
    "original_title": "Assessing the Impact of Attention and Self-Attention Mechanisms on the Classification of Skin Lesions",
    "source": "arxiv",
    "authors": [
      "Rafael Pedro",
      "Arlindo L. Oliveira"
    ],
    "published": "2021-12-23T18:02:48+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2112.12748v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2112.12748v1_chunk_2",
    "chunk_text": "Self-Attention is becoming increasingly popular in Computer Vision, where it is sometimes combined with convolutional layers, although some recent architectures do away entirely with convolutions. In this work, we study and perform an objective comparison of a number of different attention mechanisms in a specific computer vision task, the classification of samples in the widely used Skin Cancer MNIST dataset. The results show that attention modules do sometimes improve the performance of convolutional neural network architectures, but also that this improvement, although noticeable and statistically significant, is not consistent in different settings. The results obtained with self-attention mechanisms, on the other hand, show consistent and significant improvements, leading to the best results even in architectures with a reduced number of parameters.",
    "original_url": "http://arxiv.org/pdf/2112.12748v1",
    "original_title": "Assessing the Impact of Attention and Self-Attention Mechanisms on the Classification of Skin Lesions",
    "source": "arxiv",
    "authors": [
      "Rafael Pedro",
      "Arlindo L. Oliveira"
    ],
    "published": "2021-12-23T18:02:48+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2112.12748v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2207.04399v1_chunk_0",
    "chunk_text": "Horizontal and Vertical Attention in Transformers\n\nTransformers are built upon multi-head scaled dot-product attention and positional encoding, which aim to learn the feature representations and token dependencies. In this work, we focus on enhancing the distinctive representation by learning to augment the feature maps with the self-attention mechanism in Transformers. Specifically, we propose the horizontal attention to re-weight the multi-head output of the scaled dot-product attention before dimensionality reduction, and propose the vertical attention to adaptively re-calibrate channel-wise feature responses by explicitly modelling inter-dependencies among different channels. We demonstrate the Transformer models equipped with the two attentions have a high generalization capability across different supervised learning tasks, with a very minor additional computational cost overhead. The proposed horizontal and vertical attentions are highly modular, which can be inserted into various Transformer models to further improve the performance.",
    "original_url": "http://arxiv.org/pdf/2207.04399v1",
    "original_title": "Horizontal and Vertical Attention in Transformers",
    "source": "arxiv",
    "authors": [
      "Litao Yu",
      "Jian Zhang"
    ],
    "published": "2022-07-10T07:08:18+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2207.04399v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2207.04399v1_chunk_1",
    "chunk_text": "The proposed horizontal and vertical attentions are highly modular, which can be inserted into various Transformer models to further improve the performance. Our code is available in the supplementary material.",
    "original_url": "http://arxiv.org/pdf/2207.04399v1",
    "original_title": "Horizontal and Vertical Attention in Transformers",
    "source": "arxiv",
    "authors": [
      "Litao Yu",
      "Jian Zhang"
    ],
    "published": "2022-07-10T07:08:18+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2207.04399v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2302.14346v1_chunk_0",
    "chunk_text": "Sampled Transformer for Point Sets\n\nThe sparse transformer can reduce the computational complexity of the self-attention layers to $O(n)$, whilst still being a universal approximator of continuous sequence-to-sequence functions. However, this permutation variant operation is not appropriate for direct application to sets. In this paper, we proposed an $O(n)$ complexity sampled transformer that can process point set elements directly without any additional inductive bias. Our sampled transformer introduces random element sampling, which randomly splits point sets into subsets, followed by applying a shared Hamiltonian self-attention mechanism to each subset. The overall attention mechanism can be viewed as a Hamiltonian cycle in the complete attention graph, and the permutation of point set elements is equivalent to randomly sampling Hamiltonian cycles.",
    "original_url": "http://arxiv.org/pdf/2302.14346v1",
    "original_title": "Sampled Transformer for Point Sets",
    "source": "arxiv",
    "authors": [
      "Shidi Li",
      "Christian Walder",
      "Alexander Soen",
      "Lexing Xie",
      "Miaomiao Liu"
    ],
    "published": "2023-02-28T06:38:05+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2302.14346v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2302.14346v1_chunk_1",
    "chunk_text": "The overall attention mechanism can be viewed as a Hamiltonian cycle in the complete attention graph, and the permutation of point set elements is equivalent to randomly sampling Hamiltonian cycles. This mechanism implements a Monte Carlo simulation of the $O(n^2)$ dense attention connections. We show that it is a universal approximator for continuous set-to-set functions. Experimental results on point-clouds show comparable or better accuracy with significantly reduced computational complexity compared to the dense transformer or alternative sparse attention schemes.",
    "original_url": "http://arxiv.org/pdf/2302.14346v1",
    "original_title": "Sampled Transformer for Point Sets",
    "source": "arxiv",
    "authors": [
      "Shidi Li",
      "Christian Walder",
      "Alexander Soen",
      "Lexing Xie",
      "Miaomiao Liu"
    ],
    "published": "2023-02-28T06:38:05+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2302.14346v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2109.03888v1_chunk_0",
    "chunk_text": "Sparsity and Sentence Structure in Encoder-Decoder Attention of Summarization Systems\n\nTransformer models have achieved state-of-the-art results in a wide range of NLP tasks including summarization. Training and inference using large transformer models can be computationally expensive. Previous work has focused on one important bottleneck, the quadratic self-attention mechanism in the encoder. Modified encoder architectures such as LED or LoBART use local attention patterns to address this problem for summarization. In contrast, this work focuses on the transformer's encoder-decoder attention mechanism.",
    "original_url": "http://arxiv.org/pdf/2109.03888v1",
    "original_title": "Sparsity and Sentence Structure in Encoder-Decoder Attention of Summarization Systems",
    "source": "arxiv",
    "authors": [
      "Potsawee Manakul",
      "Mark J. F. Gales"
    ],
    "published": "2021-09-08T19:32:42+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2109.03888v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2109.03888v1_chunk_1",
    "chunk_text": "In contrast, this work focuses on the transformer's encoder-decoder attention mechanism. The cost of this attention becomes more significant in inference or training approaches that require model-generated histories. First, we examine the complexity of the encoder-decoder attention. We demonstrate empirically that there is a sparse sentence structure in document summarization that can be exploited by constraining the attention mechanism to a subset of input sentences, whilst maintaining system performance. Second, we propose a modified architecture that selects the subset of sentences to constrain the encoder-decoder attention.",
    "original_url": "http://arxiv.org/pdf/2109.03888v1",
    "original_title": "Sparsity and Sentence Structure in Encoder-Decoder Attention of Summarization Systems",
    "source": "arxiv",
    "authors": [
      "Potsawee Manakul",
      "Mark J. F. Gales"
    ],
    "published": "2021-09-08T19:32:42+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2109.03888v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2109.03888v1_chunk_2",
    "chunk_text": "Second, we propose a modified architecture that selects the subset of sentences to constrain the encoder-decoder attention. Experiments are carried out on abstractive summarization tasks, including CNN/DailyMail, XSum, Spotify Podcast, and arXiv.",
    "original_url": "http://arxiv.org/pdf/2109.03888v1",
    "original_title": "Sparsity and Sentence Structure in Encoder-Decoder Attention of Summarization Systems",
    "source": "arxiv",
    "authors": [
      "Potsawee Manakul",
      "Mark J. F. Gales"
    ],
    "published": "2021-09-08T19:32:42+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2109.03888v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2402.07901v1_chunk_0",
    "chunk_text": "FAST: Factorizable Attention for Speeding up Transformers\n\nMotivated by the factorization inherent in the original fast multipole method and the improved fast Gauss transform we introduce a factorable form of attention that operates efficiently in high dimensions. This approach reduces the computational and memory complexity of the attention mechanism in transformers from $O(N^2)$ to $O(N)$. In comparison to previous attempts, our work presents a linearly scaled attention mechanism that maintains the full representation of the attention matrix without compromising on sparsification and incorporates the all-to-all relationship between tokens. We explore the properties of our new attention metric and conduct tests in various standard settings. Results indicate that our attention mechanism has a robust performance and holds significant promise for diverse applications where self-attention is used.",
    "original_url": "http://arxiv.org/pdf/2402.07901v1",
    "original_title": "FAST: Factorizable Attention for Speeding up Transformers",
    "source": "arxiv",
    "authors": [
      "Armin Gerami",
      "Monte Hoover",
      "Pranav S. Dulepet",
      "Ramani Duraiswami"
    ],
    "published": "2024-02-12T18:59:39+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2402.07901v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2402.07901v1_chunk_1",
    "chunk_text": "Results indicate that our attention mechanism has a robust performance and holds significant promise for diverse applications where self-attention is used.",
    "original_url": "http://arxiv.org/pdf/2402.07901v1",
    "original_title": "FAST: Factorizable Attention for Speeding up Transformers",
    "source": "arxiv",
    "authors": [
      "Armin Gerami",
      "Monte Hoover",
      "Pranav S. Dulepet",
      "Ramani Duraiswami"
    ],
    "published": "2024-02-12T18:59:39+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2402.07901v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2310.01749v2_chunk_0",
    "chunk_text": "Stack Attention: Improving the Ability of Transformers to Model Hierarchical Patterns\n\nAttention, specifically scaled dot-product attention, has proven effective for natural language, but it does not have a mechanism for handling hierarchical patterns of arbitrary nesting depth, which limits its ability to recognize certain syntactic structures. To address this shortcoming, we propose stack attention: an attention operator that incorporates stacks, inspired by their theoretical connections to context-free languages (CFLs). We show that stack attention is analogous to standard attention, but with a latent model of syntax that requires no syntactic supervision. We propose two variants: one related to deterministic pushdown automata (PDAs) and one based on nondeterministic PDAs, which allows transformers to recognize arbitrary CFLs. We show that transformers with stack attention are very effective at learning CFLs that standard transformers struggle on, achieving strong results on a CFL with theoretically maximal parsing difficulty.",
    "original_url": "http://arxiv.org/pdf/2310.01749v2",
    "original_title": "Stack Attention: Improving the Ability of Transformers to Model Hierarchical Patterns",
    "source": "arxiv",
    "authors": [
      "Brian DuSell",
      "David Chiang"
    ],
    "published": "2023-10-03T02:18:06+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2310.01749v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2310.01749v2_chunk_1",
    "chunk_text": "We show that transformers with stack attention are very effective at learning CFLs that standard transformers struggle on, achieving strong results on a CFL with theoretically maximal parsing difficulty. We also show that stack attention is more effective at natural language modeling under a constrained parameter budget, and we include results on machine translation.",
    "original_url": "http://arxiv.org/pdf/2310.01749v2",
    "original_title": "Stack Attention: Improving the Ability of Transformers to Model Hierarchical Patterns",
    "source": "arxiv",
    "authors": [
      "Brian DuSell",
      "David Chiang"
    ],
    "published": "2023-10-03T02:18:06+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2310.01749v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2502.17206v1_chunk_0",
    "chunk_text": "Neural Attention: A Novel Mechanism for Enhanced Expressive Power in Transformer Models\n\nTransformer models typically calculate attention matrices using dot products, which have limitations when capturing nonlinear relationships between embedding vectors. We propose Neural Attention, a technique that replaces dot products with feed-forward networks, enabling a more expressive representation of relationships between tokens. This approach modifies only the attention matrix calculation while preserving the matrix dimensions, making it easily adaptable to existing transformer-based architectures. We provide a detailed mathematical justification for why Neural Attention increases representational capacity and conduct controlled experiments to validate this claim. When comparing Neural Attention and Dot-Product Attention, NLP experiments on WikiText-103 show a reduction in perplexity of over 5 percent.",
    "original_url": "http://arxiv.org/pdf/2502.17206v1",
    "original_title": "Neural Attention: A Novel Mechanism for Enhanced Expressive Power in Transformer Models",
    "source": "arxiv",
    "authors": [
      "Andrew DiGiugno",
      "Ausif Mahmood"
    ],
    "published": "2025-02-24T14:39:40+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2502.17206v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2502.17206v1_chunk_1",
    "chunk_text": "When comparing Neural Attention and Dot-Product Attention, NLP experiments on WikiText-103 show a reduction in perplexity of over 5 percent. Similarly, experiments on CIFAR-10 and CIFAR-100 show comparable improvements for image classification tasks. While Neural Attention introduces higher computational demands, we develop techniques to mitigate these challenges, ensuring practical usability without sacrificing the increased expressivity it provides. This work establishes Neural Attention as an effective means of enhancing the predictive capabilities of transformer models across a variety of applications.",
    "original_url": "http://arxiv.org/pdf/2502.17206v1",
    "original_title": "Neural Attention: A Novel Mechanism for Enhanced Expressive Power in Transformer Models",
    "source": "arxiv",
    "authors": [
      "Andrew DiGiugno",
      "Ausif Mahmood"
    ],
    "published": "2025-02-24T14:39:40+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2502.17206v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2501.02393v3_chunk_0",
    "chunk_text": "Graph-Aware Isomorphic Attention for Adaptive Dynamics in Transformers\n\nWe present an approach to modifying Transformer architectures by integrating graph-aware relational reasoning into the attention mechanism, merging concepts from graph neural networks and language modeling. Building on the inherent connection between attention and graph theory, we reformulate the Transformer's attention mechanism as a graph operation and propose Graph-Aware Isomorphic Attention. This method leverages advanced graph modeling strategies, including Graph Isomorphism Networks (GIN) and Principal Neighborhood Aggregation (PNA), to enrich the representation of relational structures. Our approach captures complex dependencies and generalizes across tasks, as evidenced by a reduced generalization gap and improved learning performance. Additionally, we expand the concept of graph-aware attention to introduce Sparse GIN-Attention, a fine-tuning approach that employs sparse GINs.",
    "original_url": "http://arxiv.org/pdf/2501.02393v3",
    "original_title": "Graph-Aware Isomorphic Attention for Adaptive Dynamics in Transformers",
    "source": "arxiv",
    "authors": [
      "Markus J. Buehler"
    ],
    "published": "2025-01-04T22:30:21+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2501.02393v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2501.02393v3_chunk_1",
    "chunk_text": "Additionally, we expand the concept of graph-aware attention to introduce Sparse GIN-Attention, a fine-tuning approach that employs sparse GINs. By interpreting attention matrices as sparse adjacency graphs, this technique enhances the adaptability of pre-trained foundational models with minimal computational overhead, endowing them with graph-aware capabilities. Sparse GIN-Attention fine-tuning achieves improved training dynamics and better generalization compared to alternative methods like low-rank adaption (LoRA). We discuss latent graph-like structures within traditional attention mechanisms, offering a new lens through which Transformers can be understood. By evolving Transformers as hierarchical GIN models for relational reasoning.",
    "original_url": "http://arxiv.org/pdf/2501.02393v3",
    "original_title": "Graph-Aware Isomorphic Attention for Adaptive Dynamics in Transformers",
    "source": "arxiv",
    "authors": [
      "Markus J. Buehler"
    ],
    "published": "2025-01-04T22:30:21+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2501.02393v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2501.02393v3_chunk_2",
    "chunk_text": "By evolving Transformers as hierarchical GIN models for relational reasoning. This perspective suggests profound implications for foundational model development, enabling the design of architectures that dynamically adapt to both local and global dependencies. Applications in bioinformatics, materials science, language modeling, and beyond could benefit from this synthesis of relational and sequential data modeling, setting the stage for interpretable and generalizable modeling strategies.",
    "original_url": "http://arxiv.org/pdf/2501.02393v3",
    "original_title": "Graph-Aware Isomorphic Attention for Adaptive Dynamics in Transformers",
    "source": "arxiv",
    "authors": [
      "Markus J. Buehler"
    ],
    "published": "2025-01-04T22:30:21+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2501.02393v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2503.02130v2_chunk_0",
    "chunk_text": "Forgetting Transformer: Softmax Attention with a Forget Gate\n\nAn essential component of modern recurrent sequence models is the forget gate. While Transformers do not have an explicit recurrent form, we show that a forget gate can be naturally incorporated into Transformers by down-weighting the unnormalized attention scores in a data-dependent way. We name this attention mechanism Forgetting Attention and the resulting model the Forgetting Transformer (FoX). We show that FoX outperforms the Transformer on long-context language modeling, length extrapolation, and short-context downstream tasks, while performing on par with the Transformer on long-context downstream tasks. Moreover, it is compatible with the FlashAttention algorithm and does not require any positional embeddings.",
    "original_url": "http://arxiv.org/pdf/2503.02130v2",
    "original_title": "Forgetting Transformer: Softmax Attention with a Forget Gate",
    "source": "arxiv",
    "authors": [
      "Zhixuan Lin",
      "Evgenii Nikishin",
      "Xu Owen He",
      "Aaron Courville"
    ],
    "published": "2025-03-03T23:35:23+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2503.02130v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2503.02130v2_chunk_1",
    "chunk_text": "Moreover, it is compatible with the FlashAttention algorithm and does not require any positional embeddings. Several analyses, including the needle-in-the-haystack test, show that FoX also retains the Transformer's superior long-context capabilities over recurrent sequence models such as Mamba-2, HGRN2, and DeltaNet. We also introduce a \"Pro\" block design that incorporates some common architectural components in recurrent sequence models and find it significantly improves the performance of both FoX and the Transformer. Our code is available at https://github.com/zhixuan-lin/forgetting-transformer.",
    "original_url": "http://arxiv.org/pdf/2503.02130v2",
    "original_title": "Forgetting Transformer: Softmax Attention with a Forget Gate",
    "source": "arxiv",
    "authors": [
      "Zhixuan Lin",
      "Evgenii Nikishin",
      "Xu Owen He",
      "Aaron Courville"
    ],
    "published": "2025-03-03T23:35:23+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2503.02130v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2207.13955v1_chunk_0",
    "chunk_text": "Neural Architecture Search on Efficient Transformers and Beyond\n\nRecently, numerous efficient Transformers have been proposed to reduce the quadratic computational complexity of standard Transformers caused by the Softmax attention. However, most of them simply swap Softmax with an efficient attention mechanism without considering the customized architectures specially for the efficient attention. In this paper, we argue that the handcrafted vanilla Transformer architectures for Softmax attention may not be suitable for efficient Transformers. To address this issue, we propose a new framework to find optimal architectures for efficient Transformers with the neural architecture search (NAS) technique. The proposed method is validated on popular machine translation and image classification tasks.",
    "original_url": "http://arxiv.org/pdf/2207.13955v1",
    "original_title": "Neural Architecture Search on Efficient Transformers and Beyond",
    "source": "arxiv",
    "authors": [
      "Zexiang Liu",
      "Dong Li",
      "Kaiyue Lu",
      "Zhen Qin",
      "Weixuan Sun",
      "Jiacheng Xu",
      "Yiran Zhong"
    ],
    "published": "2022-07-28T08:41:41+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2207.13955v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2207.13955v1_chunk_1",
    "chunk_text": "The proposed method is validated on popular machine translation and image classification tasks. We observe that the optimal architecture of the efficient Transformer has the reduced computation compared with that of the standard Transformer, but the general accuracy is less comparable. It indicates that the Softmax attention and efficient attention have their own distinctions but neither of them can simultaneously balance the accuracy and efficiency well. This motivates us to mix the two types of attention to reduce the performance imbalance. Besides the search spaces that commonly used in existing NAS Transformer approaches, we propose a new search space that allows the NAS algorithm to automatically search the attention variants along with architectures.",
    "original_url": "http://arxiv.org/pdf/2207.13955v1",
    "original_title": "Neural Architecture Search on Efficient Transformers and Beyond",
    "source": "arxiv",
    "authors": [
      "Zexiang Liu",
      "Dong Li",
      "Kaiyue Lu",
      "Zhen Qin",
      "Weixuan Sun",
      "Jiacheng Xu",
      "Yiran Zhong"
    ],
    "published": "2022-07-28T08:41:41+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2207.13955v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2207.13955v1_chunk_2",
    "chunk_text": "Besides the search spaces that commonly used in existing NAS Transformer approaches, we propose a new search space that allows the NAS algorithm to automatically search the attention variants along with architectures. Extensive experiments on WMT' 14 En-De and CIFAR-10 demonstrate that our searched architecture maintains comparable accuracy to the standard Transformer with notably improved computational efficiency.",
    "original_url": "http://arxiv.org/pdf/2207.13955v1",
    "original_title": "Neural Architecture Search on Efficient Transformers and Beyond",
    "source": "arxiv",
    "authors": [
      "Zexiang Liu",
      "Dong Li",
      "Kaiyue Lu",
      "Zhen Qin",
      "Weixuan Sun",
      "Jiacheng Xu",
      "Yiran Zhong"
    ],
    "published": "2022-07-28T08:41:41+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2207.13955v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2410.03805v2_chunk_0",
    "chunk_text": "Local Attention Mechanism: Boosting the Transformer Architecture for Long-Sequence Time Series Forecasting\n\nTransformers have become the leading choice in natural language processing over other deep learning architectures. This trend has also permeated the field of time series analysis, especially for long-horizon forecasting, showcasing promising results both in performance and running time. In this paper, we introduce Local Attention Mechanism (LAM), an efficient attention mechanism tailored for time series analysis. This mechanism exploits the continuity properties of time series to reduce the number of attention scores computed. We present an algorithm for implementing LAM in tensor algebra that runs in time and memory O(nlogn), significantly improving upon the O(n^2) time and memory complexity of traditional attention mechanisms.",
    "original_url": "http://arxiv.org/pdf/2410.03805v2",
    "original_title": "Local Attention Mechanism: Boosting the Transformer Architecture for Long-Sequence Time Series Forecasting",
    "source": "arxiv",
    "authors": [
      "Ignacio Aguilera-Martos",
      "Andrs Herrera-Poyatos",
      "Julin Luengo",
      "Francisco Herrera"
    ],
    "published": "2024-10-04T11:32:02+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2410.03805v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2410.03805v2_chunk_1",
    "chunk_text": "We present an algorithm for implementing LAM in tensor algebra that runs in time and memory O(nlogn), significantly improving upon the O(n^2) time and memory complexity of traditional attention mechanisms. We also note the lack of proper datasets to evaluate long-horizon forecast models. Thus, we propose a novel set of datasets to improve the evaluation of models addressing long-horizon forecasting challenges. Our experimental analysis demonstrates that the vanilla transformer architecture magnified with LAM surpasses state-of-the-art models, including the vanilla attention mechanism. These results confirm the effectiveness of our approach and highlight a range of future challenges in long-sequence time series forecasting.",
    "original_url": "http://arxiv.org/pdf/2410.03805v2",
    "original_title": "Local Attention Mechanism: Boosting the Transformer Architecture for Long-Sequence Time Series Forecasting",
    "source": "arxiv",
    "authors": [
      "Ignacio Aguilera-Martos",
      "Andrs Herrera-Poyatos",
      "Julin Luengo",
      "Francisco Herrera"
    ],
    "published": "2024-10-04T11:32:02+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2410.03805v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2410.03805v2_chunk_2",
    "chunk_text": "These results confirm the effectiveness of our approach and highlight a range of future challenges in long-sequence time series forecasting.",
    "original_url": "http://arxiv.org/pdf/2410.03805v2",
    "original_title": "Local Attention Mechanism: Boosting the Transformer Architecture for Long-Sequence Time Series Forecasting",
    "source": "arxiv",
    "authors": [
      "Ignacio Aguilera-Martos",
      "Andrs Herrera-Poyatos",
      "Julin Luengo",
      "Francisco Herrera"
    ],
    "published": "2024-10-04T11:32:02+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2410.03805v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2406.14100v1_chunk_0",
    "chunk_text": "Self-Attention in Transformer Networks Explains Monkeys' Gaze Pattern in Pac-Man Game\n\nWe proactively direct our eyes and attention to collect information during problem solving and decision making. Understanding gaze patterns is crucial for gaining insights into the computation underlying the problem-solving process. However, there is a lack of interpretable models that can account for how the brain directs the eyes to collect information and utilize it, especially in the context of complex problem solving. In the current study, we analyzed the gaze patterns of two monkeys playing the Pac-Man game. We trained a transformer network to mimic the monkeys' gameplay and found its attention pattern captures the monkeys' eye movements.",
    "original_url": "http://arxiv.org/pdf/2406.14100v1",
    "original_title": "Self-Attention in Transformer Networks Explains Monkeys' Gaze Pattern in Pac-Man Game",
    "source": "arxiv",
    "authors": [
      "Zhongqiao Lin",
      "Yunwei Li",
      "Tianming Yang"
    ],
    "published": "2024-06-20T08:32:35+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2406.14100v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2406.14100v1_chunk_1",
    "chunk_text": "We trained a transformer network to mimic the monkeys' gameplay and found its attention pattern captures the monkeys' eye movements. In addition, the prediction based on the transformer network's attention outperforms the human subjects' predictions. Importantly, we dissected the computation underlying the attention mechanism of the transformer network, revealing its layered structures reflecting a value-based attention component and a component that captures the interactions between Pac-Man and other game objects. Based on these findings, we built a condensed attention model that is not only as accurate as the transformer network but also fully interpretable. Our results highlight the potential of using transformer neural networks to model and understand the cognitive processes underlying complex problem solving in the brain, opening new avenues for investigating the neural basis of cognition.",
    "original_url": "http://arxiv.org/pdf/2406.14100v1",
    "original_title": "Self-Attention in Transformer Networks Explains Monkeys' Gaze Pattern in Pac-Man Game",
    "source": "arxiv",
    "authors": [
      "Zhongqiao Lin",
      "Yunwei Li",
      "Tianming Yang"
    ],
    "published": "2024-06-20T08:32:35+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2406.14100v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2406.14100v1_chunk_2",
    "chunk_text": "Our results highlight the potential of using transformer neural networks to model and understand the cognitive processes underlying complex problem solving in the brain, opening new avenues for investigating the neural basis of cognition.",
    "original_url": "http://arxiv.org/pdf/2406.14100v1",
    "original_title": "Self-Attention in Transformer Networks Explains Monkeys' Gaze Pattern in Pac-Man Game",
    "source": "arxiv",
    "authors": [
      "Zhongqiao Lin",
      "Yunwei Li",
      "Tianming Yang"
    ],
    "published": "2024-06-20T08:32:35+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2406.14100v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2210.07661v4_chunk_0",
    "chunk_text": "CAB: Comprehensive Attention Benchmarking on Long Sequence Modeling\n\nTransformer has achieved remarkable success in language, image, and speech processing. Recently, various efficient attention architectures have been proposed to improve transformer's efficiency while largely preserving its efficacy, especially in modeling long sequences. A widely-used benchmark to test these efficient methods' capability on long-range modeling is Long Range Arena (LRA). However, LRA only focuses on the standard bidirectional (or noncausal) self attention, and completely ignores cross attentions and unidirectional (or causal) attentions, which are equally important to downstream applications. In this paper, we propose Comprehensive Attention Benchmark (CAB) under a fine-grained attention taxonomy with four distinguishable attention patterns, namely, noncausal self, causal self, noncausal cross, and causal cross attentions.",
    "original_url": "http://arxiv.org/pdf/2210.07661v4",
    "original_title": "CAB: Comprehensive Attention Benchmarking on Long Sequence Modeling",
    "source": "arxiv",
    "authors": [
      "Jun Zhang",
      "Shuyang Jiang",
      "Jiangtao Feng",
      "Lin Zheng",
      "Lingpeng Kong"
    ],
    "published": "2022-10-14T09:25:47+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2210.07661v4"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2210.07661v4_chunk_1",
    "chunk_text": "In this paper, we propose Comprehensive Attention Benchmark (CAB) under a fine-grained attention taxonomy with four distinguishable attention patterns, namely, noncausal self, causal self, noncausal cross, and causal cross attentions. CAB collects seven real-world tasks from different research areas to evaluate efficient attentions under the four attention patterns. Among these tasks, CAB validates efficient attentions in eight backbone networks to show their generalization across neural architectures. We conduct exhaustive experiments to benchmark the performances of nine widely-used efficient attention architectures designed with different philosophies on CAB. Extensive experimental results also shed light on the fundamental problems of efficient attentions, such as efficiency length against vanilla attention, performance consistency across attention patterns, the benefit of attention mechanisms, and interpolation/extrapolation on long-context language modeling.",
    "original_url": "http://arxiv.org/pdf/2210.07661v4",
    "original_title": "CAB: Comprehensive Attention Benchmarking on Long Sequence Modeling",
    "source": "arxiv",
    "authors": [
      "Jun Zhang",
      "Shuyang Jiang",
      "Jiangtao Feng",
      "Lin Zheng",
      "Lingpeng Kong"
    ],
    "published": "2022-10-14T09:25:47+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2210.07661v4"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2210.07661v4_chunk_2",
    "chunk_text": "Extensive experimental results also shed light on the fundamental problems of efficient attentions, such as efficiency length against vanilla attention, performance consistency across attention patterns, the benefit of attention mechanisms, and interpolation/extrapolation on long-context language modeling.",
    "original_url": "http://arxiv.org/pdf/2210.07661v4",
    "original_title": "CAB: Comprehensive Attention Benchmarking on Long Sequence Modeling",
    "source": "arxiv",
    "authors": [
      "Jun Zhang",
      "Shuyang Jiang",
      "Jiangtao Feng",
      "Lin Zheng",
      "Lingpeng Kong"
    ],
    "published": "2022-10-14T09:25:47+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2210.07661v4"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2211.03495v1_chunk_0",
    "chunk_text": "How Much Does Attention Actually Attend? Questioning the Importance of Attention in Pretrained Transformers\n\nThe attention mechanism is considered the backbone of the widely-used Transformer architecture. It contextualizes the input by computing input-specific attention matrices. We find that this mechanism, while powerful and elegant, is not as important as typically thought for pretrained language models. We introduce PAPA, a new probing method that replaces the input-dependent attention matrices with constant ones -- the average attention weights over multiple inputs.",
    "original_url": "http://arxiv.org/pdf/2211.03495v1",
    "original_title": "How Much Does Attention Actually Attend? Questioning the Importance of Attention in Pretrained Transformers",
    "source": "arxiv",
    "authors": [
      "Michael Hassid",
      "Hao Peng",
      "Daniel Rotem",
      "Jungo Kasai",
      "Ivan Montero",
      "Noah A. Smith",
      "Roy Schwartz"
    ],
    "published": "2022-11-07T12:37:54+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2211.03495v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2211.03495v1_chunk_1",
    "chunk_text": "We introduce PAPA, a new probing method that replaces the input-dependent attention matrices with constant ones -- the average attention weights over multiple inputs. We use PAPA to analyze several established pretrained Transformers on six downstream tasks. We find that without any input-dependent attention, all models achieve competitive performance -- an average relative drop of only 8% from the probing baseline. Further, little or no performance drop is observed when replacing half of the input-dependent attention matrices with constant (input-independent) ones. Interestingly, we show that better-performing models lose more from applying our method than weaker models, suggesting that the utilization of the input-dependent attention mechanism might be a factor in their success.",
    "original_url": "http://arxiv.org/pdf/2211.03495v1",
    "original_title": "How Much Does Attention Actually Attend? Questioning the Importance of Attention in Pretrained Transformers",
    "source": "arxiv",
    "authors": [
      "Michael Hassid",
      "Hao Peng",
      "Daniel Rotem",
      "Jungo Kasai",
      "Ivan Montero",
      "Noah A. Smith",
      "Roy Schwartz"
    ],
    "published": "2022-11-07T12:37:54+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2211.03495v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2211.03495v1_chunk_2",
    "chunk_text": "Interestingly, we show that better-performing models lose more from applying our method than weaker models, suggesting that the utilization of the input-dependent attention mechanism might be a factor in their success. Our results motivate research on simpler alternatives to input-dependent attention, as well as on methods for better utilization of this mechanism in the Transformer architecture.",
    "original_url": "http://arxiv.org/pdf/2211.03495v1",
    "original_title": "How Much Does Attention Actually Attend? Questioning the Importance of Attention in Pretrained Transformers",
    "source": "arxiv",
    "authors": [
      "Michael Hassid",
      "Hao Peng",
      "Daniel Rotem",
      "Jungo Kasai",
      "Ivan Montero",
      "Noah A. Smith",
      "Roy Schwartz"
    ],
    "published": "2022-11-07T12:37:54+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2211.03495v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2303.07585v1_chunk_0",
    "chunk_text": "Input-length-shortening and text generation via attention values\n\nIdentifying words that impact a task's performance more than others is a challenge in natural language processing. Transformers models have recently addressed this issue by incorporating an attention mechanism that assigns greater attention (i.e., relevance) scores to some words than others. Because of the attention mechanism's high computational cost, transformer models usually have an input-length limitation caused by hardware constraints. This limitation applies to many transformers, including the well-known bidirectional encoder representations of the transformer (BERT) model. In this paper, we examined BERT's attention assignment mechanism, focusing on two questions: (1) How can attention be employed to reduce input length?",
    "original_url": "http://arxiv.org/pdf/2303.07585v1",
    "original_title": "Input-length-shortening and text generation via attention values",
    "source": "arxiv",
    "authors": [
      "Neet zkan Tan",
      "Alex Yuxuan Peng",
      "Joshua Bensemann",
      "Qiming Bao",
      "Tim Hartill",
      "Mark Gahegan",
      "Michael Witbrock"
    ],
    "published": "2023-03-14T02:11:24+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2303.07585v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2303.07585v1_chunk_1",
    "chunk_text": "In this paper, we examined BERT's attention assignment mechanism, focusing on two questions: (1) How can attention be employed to reduce input length? (2) How can attention be used as a control mechanism for conditional text generation? We investigated these questions in the context of a text classification task. We discovered that BERT's early layers assign more critical attention scores for text classification tasks compared to later layers. We demonstrated that the first layer's attention sums could be used to filter tokens in a given sequence, considerably decreasing the input length while maintaining good test accuracy.",
    "original_url": "http://arxiv.org/pdf/2303.07585v1",
    "original_title": "Input-length-shortening and text generation via attention values",
    "source": "arxiv",
    "authors": [
      "Neet zkan Tan",
      "Alex Yuxuan Peng",
      "Joshua Bensemann",
      "Qiming Bao",
      "Tim Hartill",
      "Mark Gahegan",
      "Michael Witbrock"
    ],
    "published": "2023-03-14T02:11:24+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2303.07585v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2303.07585v1_chunk_2",
    "chunk_text": "We demonstrated that the first layer's attention sums could be used to filter tokens in a given sequence, considerably decreasing the input length while maintaining good test accuracy. We also applied filtering, which uses a compute-efficient semantic similarities algorithm, and discovered that retaining approximately 6\\% of the original sequence is sufficient to obtain 86.5\\% accuracy. Finally, we showed that we could generate data in a stable manner and indistinguishable from the original one by only using a small percentage (10\\%) of the tokens with high attention scores according to BERT's first layer.",
    "original_url": "http://arxiv.org/pdf/2303.07585v1",
    "original_title": "Input-length-shortening and text generation via attention values",
    "source": "arxiv",
    "authors": [
      "Neet zkan Tan",
      "Alex Yuxuan Peng",
      "Joshua Bensemann",
      "Qiming Bao",
      "Tim Hartill",
      "Mark Gahegan",
      "Michael Witbrock"
    ],
    "published": "2023-03-14T02:11:24+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2303.07585v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2504.00068v1_chunk_0",
    "chunk_text": "Integrating Quantum-Classical Attention in Patch Transformers for Enhanced Time Series Forecasting\n\nQCAAPatchTF is a quantum attention network integrated with an advanced patch-based transformer, designed for multivariate time series forecasting, classification, and anomaly detection. Leveraging quantum superpositions, entanglement, and variational quantum eigensolver principles, the model introduces a quantum-classical hybrid self-attention mechanism to capture multivariate correlations across time points. For multivariate long-term time series, the quantum self-attention mechanism can reduce computational complexity while maintaining temporal relationships. It then applies the quantum-classical hybrid self-attention mechanism alongside a feed-forward network in the encoder stage of the advanced patch-based transformer. While the feed-forward network learns nonlinear representations for each variable frame, the quantum self-attention mechanism processes individual series to enhance multivariate relationships.",
    "original_url": "http://arxiv.org/pdf/2504.00068v1",
    "original_title": "Integrating Quantum-Classical Attention in Patch Transformers for Enhanced Time Series Forecasting",
    "source": "arxiv",
    "authors": [
      "Sanjay Chakraborty",
      "Fredrik Heintz"
    ],
    "published": "2025-03-31T17:23:36+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2504.00068v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2504.00068v1_chunk_1",
    "chunk_text": "While the feed-forward network learns nonlinear representations for each variable frame, the quantum self-attention mechanism processes individual series to enhance multivariate relationships. The advanced patch-based transformer computes the optimized patch length by dividing the sequence length into a fixed number of patches instead of using an arbitrary set of values. The stride is then set to half of the patch length to ensure efficient overlapping representations while maintaining temporal continuity. QCAAPatchTF achieves state-of-the-art performance in both long-term and short-term forecasting, classification, and anomaly detection tasks, demonstrating state-of-the-art accuracy and efficiency on complex real-world datasets.",
    "original_url": "http://arxiv.org/pdf/2504.00068v1",
    "original_title": "Integrating Quantum-Classical Attention in Patch Transformers for Enhanced Time Series Forecasting",
    "source": "arxiv",
    "authors": [
      "Sanjay Chakraborty",
      "Fredrik Heintz"
    ],
    "published": "2025-03-31T17:23:36+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2504.00068v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2112.05425v1_chunk_0",
    "chunk_text": "Couplformer:Rethinking Vision Transformer with Coupling Attention Map\n\nWith the development of the self-attention mechanism, the Transformer model has demonstrated its outstanding performance in the computer vision domain. However, the massive computation brought from the full attention mechanism became a heavy burden for memory consumption. Sequentially, the limitation of memory reduces the possibility of improving the Transformer model. To remedy this problem, we propose a novel memory economy attention mechanism named Couplformer, which decouples the attention map into two sub-matrices and generates the alignment scores from spatial information. A series of different scale image classification tasks are applied to evaluate the effectiveness of our model.",
    "original_url": "http://arxiv.org/pdf/2112.05425v1",
    "original_title": "Couplformer:Rethinking Vision Transformer with Coupling Attention Map",
    "source": "arxiv",
    "authors": [
      "Hai Lan",
      "Xihao Wang",
      "Xian Wei"
    ],
    "published": "2021-12-10T10:05:35+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2112.05425v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2112.05425v1_chunk_1",
    "chunk_text": "A series of different scale image classification tasks are applied to evaluate the effectiveness of our model. The result of experiments shows that on the ImageNet-1k classification task, the Couplformer can significantly decrease 28% memory consumption compared with regular Transformer while accessing sufficient accuracy requirements and outperforming 0.92% on Top-1 accuracy while occupying the same memory footprint. As a result, the Couplformer can serve as an efficient backbone in visual tasks, and provide a novel perspective on the attention mechanism for researchers.",
    "original_url": "http://arxiv.org/pdf/2112.05425v1",
    "original_title": "Couplformer:Rethinking Vision Transformer with Coupling Attention Map",
    "source": "arxiv",
    "authors": [
      "Hai Lan",
      "Xihao Wang",
      "Xian Wei"
    ],
    "published": "2021-12-10T10:05:35+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2112.05425v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2205.12231v1_chunk_0",
    "chunk_text": "ASSET: Autoregressive Semantic Scene Editing with Transformers at High Resolutions\n\nWe present ASSET, a neural architecture for automatically modifying an input high-resolution image according to a user's edits on its semantic segmentation map. Our architecture is based on a transformer with a novel attention mechanism. Our key idea is to sparsify the transformer's attention matrix at high resolutions, guided by dense attention extracted at lower image resolutions. While previous attention mechanisms are computationally too expensive for handling high-resolution images or are overly constrained within specific image regions hampering long-range interactions, our novel attention mechanism is both computationally efficient and effective. Our sparsified attention mechanism is able to capture long-range interactions and context, leading to synthesizing interesting phenomena in scenes, such as reflections of landscapes onto water or flora consistent with the rest of the landscape, that were not possible to generate reliably with previous convnets and transformer approaches.",
    "original_url": "http://arxiv.org/pdf/2205.12231v1",
    "original_title": "ASSET: Autoregressive Semantic Scene Editing with Transformers at High Resolutions",
    "source": "arxiv",
    "authors": [
      "Difan Liu",
      "Sandesh Shetty",
      "Tobias Hinz",
      "Matthew Fisher",
      "Richard Zhang",
      "Taesung Park",
      "Evangelos Kalogerakis"
    ],
    "published": "2022-05-24T17:39:53+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2205.12231v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2205.12231v1_chunk_1",
    "chunk_text": "Our sparsified attention mechanism is able to capture long-range interactions and context, leading to synthesizing interesting phenomena in scenes, such as reflections of landscapes onto water or flora consistent with the rest of the landscape, that were not possible to generate reliably with previous convnets and transformer approaches. We present qualitative and quantitative results, along with user studies, demonstrating the effectiveness of our method.",
    "original_url": "http://arxiv.org/pdf/2205.12231v1",
    "original_title": "ASSET: Autoregressive Semantic Scene Editing with Transformers at High Resolutions",
    "source": "arxiv",
    "authors": [
      "Difan Liu",
      "Sandesh Shetty",
      "Tobias Hinz",
      "Matthew Fisher",
      "Richard Zhang",
      "Taesung Park",
      "Evangelos Kalogerakis"
    ],
    "published": "2022-05-24T17:39:53+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2205.12231v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2309.06282v1_chunk_0",
    "chunk_text": "IBAFormer: Intra-batch Attention Transformer for Domain Generalized Semantic Segmentation\n\nDomain generalized semantic segmentation (DGSS) is a critical yet challenging task, where the model is trained only on source data without access to any target data. Despite the proposal of numerous DGSS strategies, the generalization capability remains limited in CNN architectures. Though some Transformer-based segmentation models show promising performance, they primarily focus on capturing intra-sample attentive relationships, disregarding inter-sample correlations which can potentially benefit DGSS. To this end, we enhance the attention modules in Transformer networks for improving DGSS by incorporating information from other independent samples in the same batch, enriching contextual information, and diversifying the training data for each attention block. Specifically, we propose two alternative intra-batch attention mechanisms, namely mean-based intra-batch attention (MIBA) and element-wise intra-batch attention (EIBA), to capture correlations between different samples, enhancing feature representation and generalization capabilities.",
    "original_url": "http://arxiv.org/pdf/2309.06282v1",
    "original_title": "IBAFormer: Intra-batch Attention Transformer for Domain Generalized Semantic Segmentation",
    "source": "arxiv",
    "authors": [
      "Qiyu Sun",
      "Huilin Chen",
      "Meng Zheng",
      "Ziyan Wu",
      "Michael Felsberg",
      "Yang Tang"
    ],
    "published": "2023-09-12T14:42:22+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2309.06282v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2309.06282v1_chunk_1",
    "chunk_text": "Specifically, we propose two alternative intra-batch attention mechanisms, namely mean-based intra-batch attention (MIBA) and element-wise intra-batch attention (EIBA), to capture correlations between different samples, enhancing feature representation and generalization capabilities. Building upon intra-batch attention, we introduce IBAFormer, which integrates self-attention modules with the proposed intra-batch attention for DGSS. Extensive experiments demonstrate that IBAFormer achieves SOTA performance in DGSS, and ablation studies further confirm the effectiveness of each introduced component.",
    "original_url": "http://arxiv.org/pdf/2309.06282v1",
    "original_title": "IBAFormer: Intra-batch Attention Transformer for Domain Generalized Semantic Segmentation",
    "source": "arxiv",
    "authors": [
      "Qiyu Sun",
      "Huilin Chen",
      "Meng Zheng",
      "Ziyan Wu",
      "Michael Felsberg",
      "Yang Tang"
    ],
    "published": "2023-09-12T14:42:22+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2309.06282v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2212.13504v3_chunk_0",
    "chunk_text": "DAE-Former: Dual Attention-guided Efficient Transformer for Medical Image Segmentation\n\nTransformers have recently gained attention in the computer vision domain due to their ability to model long-range dependencies. However, the self-attention mechanism, which is the core part of the Transformer model, usually suffers from quadratic computational complexity with respect to the number of tokens. Many architectures attempt to reduce model complexity by limiting the self-attention mechanism to local regions or by redesigning the tokenization process. In this paper, we propose DAE-Former, a novel method that seeks to provide an alternative perspective by efficiently designing the self-attention mechanism. More specifically, we reformulate the self-attention mechanism to capture both spatial and channel relations across the whole feature dimension while staying computationally efficient.",
    "original_url": "http://arxiv.org/pdf/2212.13504v3",
    "original_title": "DAE-Former: Dual Attention-guided Efficient Transformer for Medical Image Segmentation",
    "source": "arxiv",
    "authors": [
      "Reza Azad",
      "Ren Arimond",
      "Ehsan Khodapanah Aghdam",
      "Amirhossein Kazerouni",
      "Dorit Merhof"
    ],
    "published": "2022-12-27T14:39:39+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2212.13504v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2212.13504v3_chunk_1",
    "chunk_text": "More specifically, we reformulate the self-attention mechanism to capture both spatial and channel relations across the whole feature dimension while staying computationally efficient. Furthermore, we redesign the skip connection path by including the cross-attention module to ensure the feature reusability and enhance the localization power. Our method outperforms state-of-the-art methods on multi-organ cardiac and skin lesion segmentation datasets without requiring pre-training weights. The code is publicly available at https://github.com/mindflow-institue/DAEFormer.",
    "original_url": "http://arxiv.org/pdf/2212.13504v3",
    "original_title": "DAE-Former: Dual Attention-guided Efficient Transformer for Medical Image Segmentation",
    "source": "arxiv",
    "authors": [
      "Reza Azad",
      "Ren Arimond",
      "Ehsan Khodapanah Aghdam",
      "Amirhossein Kazerouni",
      "Dorit Merhof"
    ],
    "published": "2022-12-27T14:39:39+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2212.13504v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2112.13310v1_chunk_0",
    "chunk_text": "Miti-DETR: Object Detection based on Transformers with Mitigatory Self-Attention Convergence\n\nObject Detection with Transformers (DETR) and related works reach or even surpass the highly-optimized Faster-RCNN baseline with self-attention network architectures. Inspired by the evidence that pure self-attention possesses a strong inductive bias that leads to the transformer losing the expressive power with respect to network depth, we propose a transformer architecture with a mitigatory self-attention mechanism by applying possible direct mapping connections in the transformer architecture to mitigate the rank collapse so as to counteract feature expression loss and enhance the model performance. We apply this proposal in object detection tasks and develop a model named Miti-DETR. Miti-DETR reserves the inputs of each single attention layer to the outputs of that layer so that the \"non-attention\" information has participated in any attention propagation. The formed residual self-attention network addresses two critical issues: (1) stop the self-attention networks from degenerating to rank-1 to the maximized degree; and (2) further diversify the path distribution of parameter update so that easier attention learning is expected.",
    "original_url": "http://arxiv.org/pdf/2112.13310v1",
    "original_title": "Miti-DETR: Object Detection based on Transformers with Mitigatory Self-Attention Convergence",
    "source": "arxiv",
    "authors": [
      "Wenchi Ma",
      "Tianxiao Zhang",
      "Guanghui Wang"
    ],
    "published": "2021-12-26T03:23:59+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2112.13310v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2112.13310v1_chunk_1",
    "chunk_text": "The formed residual self-attention network addresses two critical issues: (1) stop the self-attention networks from degenerating to rank-1 to the maximized degree; and (2) further diversify the path distribution of parameter update so that easier attention learning is expected. Miti-DETR significantly enhances the average detection precision and convergence speed towards existing DETR-based models on the challenging COCO object detection dataset. Moreover, the proposed transformer with the residual self-attention network can be easily generalized or plugged in other related task models without specific customization.",
    "original_url": "http://arxiv.org/pdf/2112.13310v1",
    "original_title": "Miti-DETR: Object Detection based on Transformers with Mitigatory Self-Attention Convergence",
    "source": "arxiv",
    "authors": [
      "Wenchi Ma",
      "Tianxiao Zhang",
      "Guanghui Wang"
    ],
    "published": "2021-12-26T03:23:59+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2112.13310v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2404.04617v2_chunk_0",
    "chunk_text": "Empowering Image Recovery_ A Multi-Attention Approach\n\nWe propose Diverse Restormer (DART), a novel image restoration method that effectively integrates information from various sources (long sequences, local and global regions, feature dimensions, and positional dimensions) to address restoration challenges. While Transformer models have demonstrated excellent performance in image restoration due to their self-attention mechanism, they face limitations in complex scenarios. Leveraging recent advancements in Transformers and various attention mechanisms, our method utilizes customized attention mechanisms to enhance overall performance. DART, our novel network architecture, employs windowed attention to mimic the selective focusing mechanism of human eyes. By dynamically adjusting receptive fields, it optimally captures the fundamental features crucial for image resolution reconstruction.",
    "original_url": "http://arxiv.org/pdf/2404.04617v2",
    "original_title": "Empowering Image Recovery_ A Multi-Attention Approach",
    "source": "arxiv",
    "authors": [
      "Juan Wen",
      "Yawei Li",
      "Chao Zhang",
      "Weiyan Hou",
      "Radu Timofte",
      "Luc Van Gool"
    ],
    "published": "2024-04-06T12:50:08+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2404.04617v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2404.04617v2_chunk_1",
    "chunk_text": "By dynamically adjusting receptive fields, it optimally captures the fundamental features crucial for image resolution reconstruction. Efficiency and performance balance are achieved through the LongIR attention mechanism for long sequence image restoration. Integration of attention mechanisms across feature and positional dimensions further enhances the recovery of fine details. Evaluation across five restoration tasks consistently positions DART at the forefront. Upon acceptance, we commit to providing publicly accessible code and models to ensure reproducibility and facilitate further research.",
    "original_url": "http://arxiv.org/pdf/2404.04617v2",
    "original_title": "Empowering Image Recovery_ A Multi-Attention Approach",
    "source": "arxiv",
    "authors": [
      "Juan Wen",
      "Yawei Li",
      "Chao Zhang",
      "Weiyan Hou",
      "Radu Timofte",
      "Luc Van Gool"
    ],
    "published": "2024-04-06T12:50:08+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2404.04617v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2404.04617v2_chunk_2",
    "chunk_text": "Upon acceptance, we commit to providing publicly accessible code and models to ensure reproducibility and facilitate further research.",
    "original_url": "http://arxiv.org/pdf/2404.04617v2",
    "original_title": "Empowering Image Recovery_ A Multi-Attention Approach",
    "source": "arxiv",
    "authors": [
      "Juan Wen",
      "Yawei Li",
      "Chao Zhang",
      "Weiyan Hou",
      "Radu Timofte",
      "Luc Van Gool"
    ],
    "published": "2024-04-06T12:50:08+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2404.04617v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2206.11990v2_chunk_0",
    "chunk_text": "Equiformer: Equivariant Graph Attention Transformer for 3D Atomistic Graphs\n\nDespite their widespread success in various domains, Transformer networks have yet to perform well across datasets in the domain of 3D atomistic graphs such as molecules even when 3D-related inductive biases like translational invariance and rotational equivariance are considered. In this paper, we demonstrate that Transformers can generalize well to 3D atomistic graphs and present Equiformer, a graph neural network leveraging the strength of Transformer architectures and incorporating SE(3)/E(3)-equivariant features based on irreducible representations (irreps). First, we propose a simple and effective architecture by only replacing original operations in Transformers with their equivariant counterparts and including tensor products. Using equivariant operations enables encoding equivariant information in channels of irreps features without complicating graph structures. With minimal modifications to Transformers, this architecture has already achieved strong empirical results.",
    "original_url": "http://arxiv.org/pdf/2206.11990v2",
    "original_title": "Equiformer: Equivariant Graph Attention Transformer for 3D Atomistic Graphs",
    "source": "arxiv",
    "authors": [
      "Yi-Lun Liao",
      "Tess Smidt"
    ],
    "published": "2022-06-23T21:40:37+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2206.11990v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2206.11990v2_chunk_1",
    "chunk_text": "With minimal modifications to Transformers, this architecture has already achieved strong empirical results. Second, we propose a novel attention mechanism called equivariant graph attention, which improves upon typical attention in Transformers through replacing dot product attention with multi-layer perceptron attention and including non-linear message passing. With these two innovations, Equiformer achieves competitive results to previous models on QM9, MD17 and OC20 datasets.",
    "original_url": "http://arxiv.org/pdf/2206.11990v2",
    "original_title": "Equiformer: Equivariant Graph Attention Transformer for 3D Atomistic Graphs",
    "source": "arxiv",
    "authors": [
      "Yi-Lun Liao",
      "Tess Smidt"
    ],
    "published": "2022-06-23T21:40:37+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2206.11990v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2501.17486v1_chunk_0",
    "chunk_text": "DINT Transformer\n\nDIFF Transformer addresses the issue of irrelevant context interference by introducing a differential attention mechanism that enhances the robustness of local attention. However, it has two critical limitations: the lack of global context modeling, which is essential for identifying globally significant tokens, and numerical instability due to the absence of strict row normalization in the attention matrix. To overcome these challenges, we propose DINT Transformer, which extends DIFF Transformer by incorporating a differential-integral mechanism. By computing global importance scores and integrating them into the attention matrix, DINT Transformer improves its ability to capture global dependencies. Moreover, the unified parameter design enforces row-normalized attention matrices, improving numerical stability.",
    "original_url": "http://arxiv.org/pdf/2501.17486v1",
    "original_title": "DINT Transformer",
    "source": "arxiv",
    "authors": [
      "Yueyang Cang",
      "Yuhang Liu",
      "Xiaoteng Zhang",
      "Erlu Zhao",
      "Li Shi"
    ],
    "published": "2025-01-29T08:53:29+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2501.17486v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2501.17486v1_chunk_1",
    "chunk_text": "Moreover, the unified parameter design enforces row-normalized attention matrices, improving numerical stability. Experimental results demonstrate that DINT Transformer excels in accuracy and robustness across various practical applications, such as long-context language modeling and key information retrieval. These results position DINT Transformer as a highly effective and promising architecture.",
    "original_url": "http://arxiv.org/pdf/2501.17486v1",
    "original_title": "DINT Transformer",
    "source": "arxiv",
    "authors": [
      "Yueyang Cang",
      "Yuhang Liu",
      "Xiaoteng Zhang",
      "Erlu Zhao",
      "Li Shi"
    ],
    "published": "2025-01-29T08:53:29+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2501.17486v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2311.13541v4_chunk_0",
    "chunk_text": "Linear Log-Normal Attention with Unbiased Concentration\n\nTransformer models have achieved remarkable results in a wide range of applications. However, their scalability is hampered by the quadratic time and memory complexity of the self-attention mechanism concerning the sequence length. This limitation poses a substantial obstacle when dealing with long documents or high-resolution images. In this work, we study the self-attention mechanism by analyzing the distribution of the attention matrix and its concentration ability. Furthermore, we propose instruments to measure these quantities and introduce a novel self-attention mechanism, Linear Log-Normal Attention, designed to emulate the distribution and concentration behavior of the original self-attention.",
    "original_url": "http://arxiv.org/pdf/2311.13541v4",
    "original_title": "Linear Log-Normal Attention with Unbiased Concentration",
    "source": "arxiv",
    "authors": [
      "Yury Nahshan",
      "Joseph Kampeas",
      "Emir Haleva"
    ],
    "published": "2023-11-22T17:30:41+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2311.13541v4"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2311.13541v4_chunk_1",
    "chunk_text": "Furthermore, we propose instruments to measure these quantities and introduce a novel self-attention mechanism, Linear Log-Normal Attention, designed to emulate the distribution and concentration behavior of the original self-attention. Our experimental results on popular natural language benchmarks reveal that our proposed Linear Log-Normal Attention outperforms other linearized attention alternatives, offering a promising avenue for enhancing the scalability of transformer models.",
    "original_url": "http://arxiv.org/pdf/2311.13541v4",
    "original_title": "Linear Log-Normal Attention with Unbiased Concentration",
    "source": "arxiv",
    "authors": [
      "Yury Nahshan",
      "Joseph Kampeas",
      "Emir Haleva"
    ],
    "published": "2023-11-22T17:30:41+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2311.13541v4"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2009.14658v2_chunk_0",
    "chunk_text": "Learning Hard Retrieval Decoder Attention for Transformers\n\nThe Transformer translation model is based on the multi-head attention mechanism, which can be parallelized easily. The multi-head attention network performs the scaled dot-product attention function in parallel, empowering the model by jointly attending to information from different representation subspaces at different positions. In this paper, we present an approach to learning a hard retrieval attention where an attention head only attends to one token in the sentence rather than all tokens. The matrix multiplication between attention probabilities and the value sequence in the standard scaled dot-product attention can thus be replaced by a simple and efficient retrieval operation. We show that our hard retrieval attention mechanism is 1.43 times faster in decoding, while preserving translation quality on a wide range of machine translation tasks when used in the decoder self- and cross-attention networks.",
    "original_url": "http://arxiv.org/pdf/2009.14658v2",
    "original_title": "Learning Hard Retrieval Decoder Attention for Transformers",
    "source": "arxiv",
    "authors": [
      "Hongfei Xu",
      "Qiuhui Liu",
      "Josef van Genabith",
      "Deyi Xiong"
    ],
    "published": "2020-09-30T13:18:57+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2009.14658v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2009.14658v2_chunk_1",
    "chunk_text": "We show that our hard retrieval attention mechanism is 1.43 times faster in decoding, while preserving translation quality on a wide range of machine translation tasks when used in the decoder self- and cross-attention networks.",
    "original_url": "http://arxiv.org/pdf/2009.14658v2",
    "original_title": "Learning Hard Retrieval Decoder Attention for Transformers",
    "source": "arxiv",
    "authors": [
      "Hongfei Xu",
      "Qiuhui Liu",
      "Josef van Genabith",
      "Deyi Xiong"
    ],
    "published": "2020-09-30T13:18:57+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2009.14658v2"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2009.07053v1_chunk_0",
    "chunk_text": "Attention Flows: Analyzing and Comparing Attention Mechanisms in Language Models\n\nAdvances in language modeling have led to the development of deep attention-based models that are performant across a wide variety of natural language processing (NLP) problems. These language models are typified by a pre-training process on large unlabeled text corpora and subsequently fine-tuned for specific tasks. Although considerable work has been devoted to understanding the attention mechanisms of pre-trained models, it is less understood how a model's attention mechanisms change when trained for a target NLP task. In this paper, we propose a visual analytics approach to understanding fine-tuning in attention-based language models. Our visualization, Attention Flows, is designed to support users in querying, tracing, and comparing attention within layers, across layers, and amongst attention heads in Transformer-based language models.",
    "original_url": "http://arxiv.org/pdf/2009.07053v1",
    "original_title": "Attention Flows: Analyzing and Comparing Attention Mechanisms in Language Models",
    "source": "arxiv",
    "authors": [
      "Joseph F DeRose",
      "Jiayao Wang",
      "Matthew Berger"
    ],
    "published": "2020-09-03T19:56:30+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2009.07053v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2009.07053v1_chunk_1",
    "chunk_text": "Our visualization, Attention Flows, is designed to support users in querying, tracing, and comparing attention within layers, across layers, and amongst attention heads in Transformer-based language models. To help users gain insight on how a classification decision is made, our design is centered on depicting classification-based attention at the deepest layer and how attention from prior layers flows throughout words in the input. Attention Flows supports the analysis of a single model, as well as the visual comparison between pre-trained and fine-tuned models via their similarities and differences. We use Attention Flows to study attention mechanisms in various sentence understanding tasks and highlight how attention evolves to address the nuances of solving these tasks.",
    "original_url": "http://arxiv.org/pdf/2009.07053v1",
    "original_title": "Attention Flows: Analyzing and Comparing Attention Mechanisms in Language Models",
    "source": "arxiv",
    "authors": [
      "Joseph F DeRose",
      "Jiayao Wang",
      "Matthew Berger"
    ],
    "published": "2020-09-03T19:56:30+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2009.07053v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2204.07756v3_chunk_0",
    "chunk_text": "Visual Attention Methods in Deep Learning: An In-Depth Survey\n\nInspired by the human cognitive system, attention is a mechanism that imitates the human cognitive awareness about specific information, amplifying critical details to focus more on the essential aspects of data. Deep learning has employed attention to boost performance for many applications. Interestingly, the same attention design can suit processing different data modalities and can easily be incorporated into large networks. Furthermore, multiple complementary attention mechanisms can be incorporated into one network. Hence, attention techniques have become extremely attractive.",
    "original_url": "http://arxiv.org/pdf/2204.07756v3",
    "original_title": "Visual Attention Methods in Deep Learning: An In-Depth Survey",
    "source": "arxiv",
    "authors": [
      "Mohammed Hassanin",
      "Saeed Anwar",
      "Ibrahim Radwan",
      "Fahad S Khan",
      "Ajmal Mian"
    ],
    "published": "2022-04-16T08:57:00+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2204.07756v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2204.07756v3_chunk_1",
    "chunk_text": "Hence, attention techniques have become extremely attractive. However, the literature lacks a comprehensive survey on attention techniques to guide researchers in employing attention in their deep models. Note that, besides being demanding in terms of training data and computational resources, transformers only cover a single category in self-attention out of the many categories available. We fill this gap and provide an in-depth survey of 50 attention techniques, categorizing them by their most prominent features. We initiate our discussion by introducing the fundamental concepts behind the success of the attention mechanism.",
    "original_url": "http://arxiv.org/pdf/2204.07756v3",
    "original_title": "Visual Attention Methods in Deep Learning: An In-Depth Survey",
    "source": "arxiv",
    "authors": [
      "Mohammed Hassanin",
      "Saeed Anwar",
      "Ibrahim Radwan",
      "Fahad S Khan",
      "Ajmal Mian"
    ],
    "published": "2022-04-16T08:57:00+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2204.07756v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2204.07756v3_chunk_2",
    "chunk_text": "We initiate our discussion by introducing the fundamental concepts behind the success of the attention mechanism. Next, we furnish some essentials such as the strengths and limitations of each attention category, describe their fundamental building blocks, basic formulations with primary usage, and applications specifically for computer vision. We also discuss the challenges and general open questions related to attention mechanisms. Finally, we recommend possible future research directions for deep attention. All the information about visual attention methods in deep learning is provided at \\href{https://github.com/saeed-anwar/VisualAttention}{https://github.com/saeed-anwar/VisualAttention}",
    "original_url": "http://arxiv.org/pdf/2204.07756v3",
    "original_title": "Visual Attention Methods in Deep Learning: An In-Depth Survey",
    "source": "arxiv",
    "authors": [
      "Mohammed Hassanin",
      "Saeed Anwar",
      "Ibrahim Radwan",
      "Fahad S Khan",
      "Ajmal Mian"
    ],
    "published": "2022-04-16T08:57:00+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2204.07756v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2204.07756v3_chunk_3",
    "chunk_text": "All the information about visual attention methods in deep learning is provided at \\href{https://github.com/saeed-anwar/VisualAttention}{https://github.com/saeed-anwar/VisualAttention}",
    "original_url": "http://arxiv.org/pdf/2204.07756v3",
    "original_title": "Visual Attention Methods in Deep Learning: An In-Depth Survey",
    "source": "arxiv",
    "authors": [
      "Mohammed Hassanin",
      "Saeed Anwar",
      "Ibrahim Radwan",
      "Fahad S Khan",
      "Ajmal Mian"
    ],
    "published": "2022-04-16T08:57:00+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2204.07756v3"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2412.17810v1_chunk_0",
    "chunk_text": "Token Statistics Transformer: Linear-Time Attention via Variational Rate Reduction\n\nThe attention operator is arguably the key distinguishing factor of transformer architectures, which have demonstrated state-of-the-art performance on a variety of tasks. However, transformer attention operators often impose a significant computational burden, with the computational complexity scaling quadratically with the number of tokens. In this work, we propose a novel transformer attention operator whose computational complexity scales linearly with the number of tokens. We derive our network architecture by extending prior work which has shown that a transformer style architecture naturally arises by \"white-box\" architecture design, where each layer of the network is designed to implement an incremental optimization step of a maximal coding rate reduction objective (MCR$^2$). Specifically, we derive a novel variational form of the MCR$^2$ objective and show that the architecture that results from unrolled gradient descent of this variational objective leads to a new attention module called Token Statistics Self-Attention (TSSA).",
    "original_url": "http://arxiv.org/pdf/2412.17810v1",
    "original_title": "Token Statistics Transformer: Linear-Time Attention via Variational Rate Reduction",
    "source": "arxiv",
    "authors": [
      "Ziyang Wu",
      "Tianjiao Ding",
      "Yifu Lu",
      "Druv Pai",
      "Jingyuan Zhang",
      "Weida Wang",
      "Yaodong Yu",
      "Yi Ma",
      "Benjamin D. Haeffele"
    ],
    "published": "2024-12-23T18:59:21+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2412.17810v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2412.17810v1_chunk_1",
    "chunk_text": "Specifically, we derive a novel variational form of the MCR$^2$ objective and show that the architecture that results from unrolled gradient descent of this variational objective leads to a new attention module called Token Statistics Self-Attention (TSSA). TSSA has linear computational and memory complexity and radically departs from the typical attention architecture that computes pairwise similarities between tokens. Experiments on vision, language, and long sequence tasks show that simply swapping TSSA for standard self-attention, which we refer to as the Token Statistics Transformer (ToST), achieves competitive performance with conventional transformers while being significantly more computationally efficient and interpretable. Our results also somewhat call into question the conventional wisdom that pairwise similarity style attention mechanisms are critical to the success of transformer architectures. Code will be available at https://github.com/RobinWu218/ToST.",
    "original_url": "http://arxiv.org/pdf/2412.17810v1",
    "original_title": "Token Statistics Transformer: Linear-Time Attention via Variational Rate Reduction",
    "source": "arxiv",
    "authors": [
      "Ziyang Wu",
      "Tianjiao Ding",
      "Yifu Lu",
      "Druv Pai",
      "Jingyuan Zhang",
      "Weida Wang",
      "Yaodong Yu",
      "Yi Ma",
      "Benjamin D. Haeffele"
    ],
    "published": "2024-12-23T18:59:21+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2412.17810v1"
  },
  {
    "chunk_id": "http://arxiv.org/pdf/2412.17810v1_chunk_2",
    "chunk_text": "Code will be available at https://github.com/RobinWu218/ToST.",
    "original_url": "http://arxiv.org/pdf/2412.17810v1",
    "original_title": "Token Statistics Transformer: Linear-Time Attention via Variational Rate Reduction",
    "source": "arxiv",
    "authors": [
      "Ziyang Wu",
      "Tianjiao Ding",
      "Yifu Lu",
      "Druv Pai",
      "Jingyuan Zhang",
      "Weida Wang",
      "Yaodong Yu",
      "Yi Ma",
      "Benjamin D. Haeffele"
    ],
    "published": "2024-12-23T18:59:21+00:00",
    "arxiv_entry_id": "http://arxiv.org/abs/2412.17810v1"
  },
  {
    "chunk_id": "https://distill.pub/_chunk_0",
    "chunk_text": "Ameya Daigavane, Balaraman Ravindran, and Gaurav Aggarwal\nUnderstanding the building blocks and design choices of graph neural networks. Benjamin Sanchez-Lengeling, Emily Reif, Adam Pearce, and Alexander B. Wiltschko\nWhat components are needed for building learning algorithms that leverage the structure and properties of graphs? Editorial Team\nAfter five years, Distill will be taking a break. Gabriel Goh, Nick Cammarata , Chelsea Voss , Shan Carter, Michael Petrov, Ludwig Schubert, Alec Radford, and Chris Olah\nWe report the existence of multimodal neurons in artificial neural networks, similar to those found in the human brain. Jacob Hilton, Nick Cammarata, Shan Carter, Gabriel Goh, and Chris Olah\nWith diverse environments, we can analyze, diagnose and edit deep reinforcement learning models using attribution.",
    "original_url": "https://distill.pub/",
    "original_title": "Distill  Latest articles about machine learning",
    "source": "html",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://distill.pub/_chunk_1",
    "chunk_text": "Jacob Hilton, Nick Cammarata, Shan Carter, Gabriel Goh, and Chris Olah\nWith diverse environments, we can analyze, diagnose and edit deep reinforcement learning models using attribution. Fred Hohman, Matthew Conlen, Jeffrey Heer, and Duen Horng (Polo) Chau\nExamining the design of interactive articles by synthesizing theory from disciplines such as education, journalism, and visualization. Alexander Mordvintsev, Ettore Randazzo, Eyvind Niklasson, Michael Levin, and Sam Greydanus\nA collection of articles and comments with the goal of understanding how to design robust and general purpose self-organizing systems. Apoorv Agnihotri and Nipun Batra\nHow to tune hyperparameters for your machine learning model using Bayesian optimization. Mingwei Li, Zhenge Zhao, and Carlos Scheidegger\nBy focusing on linear dimensionality reduction, we show how to visualize many dynamic phenomena in neural networks.",
    "original_url": "https://distill.pub/",
    "original_title": "Distill  Latest articles about machine learning",
    "source": "html",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://distill.pub/_chunk_2",
    "chunk_text": "Mingwei Li, Zhenge Zhao, and Carlos Scheidegger\nBy focusing on linear dimensionality reduction, we show how to visualize many dynamic phenomena in neural networks. Nick Cammarata, Shan Carter, Gabriel Goh, Chris Olah, Michael Petrov, Ludwig Schubert, Chelsea Voss, Ben Egan, and Swee Kiat Lim\nWhat can we learn if we invest heavily in reverse engineering a single neural network? Pascal Sturmfels, Scott Lundberg, and Su-In Lee\nExploring the baseline input hyperparameter, and how it impacts interpretations of neural network behavior. Andr Araujo, Wade Norris, and Jack Sim\nDetailed derivations and open-source code to analyze the receptive fields of convnets. Sam Greydanus and Chris Olah\nA closer look at how Temporal Difference Learning merges paths of experience for greater statistical efficiency\nLogan Engstrom, Justin Gilmer, Gabriel Goh, Dan Hendrycks, Andrew Ilyas, Aleksander Madry, Reiichiro Nakano, Preetum Nakkiran, Shibani Santurkar, Brandon Tran, Dimitris Tsipras, and Eric Wallace\nSix comments from the community and responses from the original authors\nAugustus Odena\nWhat wed like to find out about GANs that we dont know yet.",
    "original_url": "https://distill.pub/",
    "original_title": "Distill  Latest articles about machine learning",
    "source": "html",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://distill.pub/_chunk_3",
    "chunk_text": "Sam Greydanus and Chris Olah\nA closer look at how Temporal Difference Learning merges paths of experience for greater statistical efficiency\nLogan Engstrom, Justin Gilmer, Gabriel Goh, Dan Hendrycks, Andrew Ilyas, Aleksander Madry, Reiichiro Nakano, Preetum Nakkiran, Shibani Santurkar, Brandon Tran, Dimitris Tsipras, and Eric Wallace\nSix comments from the community and responses from the original authors\nAugustus Odena\nWhat wed like to find out about GANs that we dont know yet. Jochen Grtler, Rebecca Kehlbeck, and Oliver Deussen\nHow to turn a collection of small building blocks into a versatile tool for solving regression problems. Andreas Madsen\nInspecting gradient magnitudes in context can be a powerful tool to see when recurrent units use short-term or long-term contextual understanding. Shan Carter, Zan Armstrong, Ludwig Schubert, Ian Johnson, and Chris Olah\nBy using feature inversion to visualize millions of activations from an image classification network, we create an explorable activation atlas of features the network has learned and what concepts it typically represents. Geoffrey Irving and Amanda Askell\nIf we want to train AI to do what humans want, we need to study humans.",
    "original_url": "https://distill.pub/",
    "original_title": "Distill  Latest articles about machine learning",
    "source": "html",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://distill.pub/_chunk_4",
    "chunk_text": "Geoffrey Irving and Amanda Askell\nIf we want to train AI to do what humans want, we need to study humans. Distill Editors\nAn Update from the Editorial Team\nAlexander Mordvintsev, Nicola Pezzotti, Ludwig Schubert, and Chris Olah\nA powerful, under-explored tool for neural network visualizations and art. Vincent Dumoulin, Ethan Perez, Nathan Schucher, Florian Strub, Harm de Vries, Aaron Courville, and Yoshua Bengio\nA simple and surprisingly effective family of conditioning mechanisms. Chris Olah, Arvind Satyanarayan, Ian Johnson, Shan Carter, Ludwig Schubert, Katherine Ye, and Alexander Mordvintsev\nInterpretability techniques are normally studied in isolation. We explore the powerful interfaces that arise when you combine them  and the rich structure of this combinatorial space.",
    "original_url": "https://distill.pub/",
    "original_title": "Distill  Latest articles about machine learning",
    "source": "html",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://distill.pub/_chunk_5",
    "chunk_text": "We explore the powerful interfaces that arise when you combine them  and the rich structure of this combinatorial space. Shan Carter and Michael Nielsen\nBy creating user interfaces which let us work with the representations inside machine learning models, we can give people new tools for reasoning. Awni Hannun\nA visual guide to Connectionist Temporal Classification, an algorithm used to train deep neural networks in speech recognition, handwriting recognition and other sequence problems. Chris Olah, Alexander Mordvintsev, and Ludwig Schubert\nHow neural networks build up their understanding of images\nGabriel Goh\nWe often think of optimization with momentum as a ball rolling down a hill. This isnt wrong, but there is much more to the story.",
    "original_url": "https://distill.pub/",
    "original_title": "Distill  Latest articles about machine learning",
    "source": "html",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://distill.pub/_chunk_6",
    "chunk_text": "This isnt wrong, but there is much more to the story. Chris Olah and Shan Carter\nScience is a human activity. When we fail to distill and explain research, we accumulate a kind of debt...\nShan Carter, David Ha, Ian Johnson, and Chris Olah\nSeveral interactive visualizations of a generative model of handwriting. Some are fun, some are serious. Augustus Odena, Vincent Dumoulin, and Chris Olah\nWhen we look very closely at images generated by neural networks, we often see a strange checkerboard pattern of artifacts.",
    "original_url": "https://distill.pub/",
    "original_title": "Distill  Latest articles about machine learning",
    "source": "html",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  },
  {
    "chunk_id": "https://distill.pub/_chunk_7",
    "chunk_text": "Augustus Odena, Vincent Dumoulin, and Chris Olah\nWhen we look very closely at images generated by neural networks, we often see a strange checkerboard pattern of artifacts. Martin Wattenberg, Fernanda Vigas, and Ian Johnson\nAlthough extremely useful for visualizing high-dimensional data, t-SNE plots can sometimes be mysterious or misleading. A visual overview of neural attention, and the powerful extensions of neural networks being built on top of it.",
    "original_url": "https://distill.pub/",
    "original_title": "Distill  Latest articles about machine learning",
    "source": "html",
    "authors": [],
    "published": null,
    "arxiv_entry_id": null
  }
]