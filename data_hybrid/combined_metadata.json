[
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1210.6111v1",
    "title": "Model Validation in Ontology Based Transformations",
    "content": "Model Validation in Ontology Based Transformations. Model Driven Engineering (MDE) is an emerging approach of software engineering. MDE emphasizes the construction of models from which the implementation should be derived by applying model transformations. The Ontology Definition Meta-model (ODM) has been proposed as a profile for UML models of the Web Ontology Language (OWL). In this context, transformations of UML models can be mapped into ODM/OWL transformations. On the other hand, model validation is a crucial task in model transformation. Meta-modeling permits to give a syntactic structure to source and target models. However, semantic requirements have to be imposed on source and target models. A given transformation will be sound when source and target models fulfill the syntactic and semantic requirements. In this paper, we present an approach for model validation in ODM based transformations. Adopting a logic programming based transformational approach we will show how it is possible to transform and validate models. Properties to be validated range from structural and semantic requirements of models (pre and post conditions) to properties of the transformation (invariants). The approach has been applied to a well-known example of model transformation: the Entity-Relationship (ER) to Relational Model (RM) transformation.",
    "authors": [
      "Jesús M. Almendros-Jiménez",
      "Luis Iribarne"
    ],
    "published": "2012-10-23T02:54:50+00:00",
    "url": "http://arxiv.org/pdf/1210.6111v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1310.2279v1",
    "title": "A Mathematical Model, Implementation and Study of a Swarm System",
    "content": "A Mathematical Model, Implementation and Study of a Swarm System. The work reported in this paper is motivated towards the development of a mathematical model for swarm systems based on macroscopic primitives. A pattern formation and transformation model is proposed. The pattern transformation model comprises two general methods for pattern transformation, namely a macroscopic transformation and mathematical transformation method. The problem of transformation is formally expressed and four special cases of transformation are considered. Simulations to confirm the feasibility of the proposed models and transformation methods are presented. Comparison between the two transformation methods is also reported.",
    "authors": [
      "Blesson Varghese",
      "Gerard McKee"
    ],
    "published": "2013-10-08T21:03:51+00:00",
    "url": "http://arxiv.org/pdf/1310.2279v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2204.07780v1",
    "title": "Towards Lightweight Transformer via Group-wise Transformation for Vision-and-Language Tasks",
    "content": "Towards Lightweight Transformer via Group-wise Transformation for Vision-and-Language Tasks. Despite the exciting performance, Transformer is criticized for its excessive parameters and computation cost. However, compressing Transformer remains as an open problem due to its internal complexity of the layer designs, i.e., Multi-Head Attention (MHA) and Feed-Forward Network (FFN). To address this issue, we introduce Group-wise Transformation towards a universal yet lightweight Transformer for vision-and-language tasks, termed as LW-Transformer. LW-Transformer applies Group-wise Transformation to reduce both the parameters and computations of Transformer, while also preserving its two main properties, i.e., the efficient attention modeling on diverse subspaces of MHA, and the expanding-scaling feature transformation of FFN. We apply LW-Transformer to a set of Transformer-based networks, and quantitatively measure them on three vision-and-language tasks and six benchmark datasets. Experimental results show that while saving a large number of parameters and computations, LW-Transformer achieves very competitive performance against the original Transformer networks for vision-and-language tasks. To examine the generalization ability, we also apply our optimization strategy to a recently proposed image Transformer called Swin-Transformer for image classification, where the effectiveness can be also confirmed",
    "authors": [
      "Gen Luo",
      "Yiyi Zhou",
      "Xiaoshuai Sun",
      "Yan Wang",
      "Liujuan Cao",
      "Yongjian Wu",
      "Feiyue Huang",
      "Rongrong Ji"
    ],
    "published": "2022-04-16T11:30:26+00:00",
    "url": "http://arxiv.org/pdf/2204.07780v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2207.04285v1",
    "title": "A Closer Look into Transformer-Based Code Intelligence Through Code Transformation: Challenges and Opportunities",
    "content": "A Closer Look into Transformer-Based Code Intelligence Through Code Transformation: Challenges and Opportunities. Transformer-based models have demonstrated state-of-the-art performance in many intelligent coding tasks such as code comment generation and code completion. Previous studies show that deep learning models are sensitive to the input variations, but few studies have systematically studied the robustness of Transformer under perturbed input code. In this work, we empirically study the effect of semantic-preserving code transformation on the performance of Transformer. Specifically, 24 and 27 code transformation strategies are implemented for two popular programming languages, Java and Python, respectively. For facilitating analysis, the strategies are grouped into five categories: block transformation, insertion/deletion transformation, grammatical statement transformation, grammatical token transformation, and identifier transformation. Experiments on three popular code intelligence tasks, including code completion, code summarization and code search, demonstrate insertion/deletion transformation and identifier transformation show the greatest impact on the performance of Transformer. Our results also suggest that Transformer based on abstract syntax trees (ASTs) shows more robust performance than the model based on only code sequence under most code transformations. Besides, the design of positional encoding can impact the robustness of Transformer under code transformation. Based on our findings, we distill some insights about the challenges and opportunities for Transformer-based code intelligence.",
    "authors": [
      "Yaoxian Li",
      "Shiyi Qi",
      "Cuiyun Gao",
      "Yun Peng",
      "David Lo",
      "Zenglin Xu",
      "Michael R. Lyu"
    ],
    "published": "2022-07-09T15:02:39+00:00",
    "url": "http://arxiv.org/pdf/2207.04285v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2410.21217v3",
    "title": "Symmetric similarity 3D coordinate transformation based on dual quaternion algorithm",
    "content": "Symmetric similarity 3D coordinate transformation based on dual quaternion algorithm. Nowadays, we have seen that dual quaternion algorithms are used in 3D coordinate transformation problems due to their advantages. 3D coordinate transformation problem is one of the important problems in geodesy. This transformation problem is encountered in many application areas other than geodesy. Although there are many coordinate transformation methods (similarity, affine, projective, etc.), similarity transformation is used because of its simplicity. The asymmetric transformation is preferred to the symmetric coordinate transformation because of its ease of use. In terms of error theory, the symmetric transformation should be preferred. In this study, the topic of symmetric similarity 3D coordinate transformation based on the dual quaternion algorithm is discussed, and the bottlenecks encountered in solving the problem and the solution method are discussed. A new iterative algorithm based on the dual quaternion is presented. The solution is implemented in two different models: with constraint equations and without constraint equations. The advantages and disadvantages of the two models compared to each other are also evaluated. Not only the transformation parameters but also the errors of the transformation parameters are determined. The detailed derivation of the formulas for estimating the symmetric similarity of 3D transformation parameters is presented step by step. Since symmetric transformation is the general form of asymmetric transformation, we can also obtain asymmetric transformation results with a simple modification of the model we developed for symmetric transformation. The proposed algorithm is capable of performing both 2D and 3D symmetric and asymmetric similarity transformations. For the 2D transformation, it is sufficient to replace the z and Z coordinates in both systems with zero.",
    "authors": [
      "Sebahattin Bektaş"
    ],
    "published": "2024-10-28T17:02:07+00:00",
    "url": "http://arxiv.org/pdf/2410.21217v3"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1302.5174v1",
    "title": "Assembling the Proofs of Ordered Model Transformations",
    "content": "Assembling the Proofs of Ordered Model Transformations. In model-driven development, an ordered model transformation is a nested set of transformations between source and target classes, in which each transformation is governed by its own pre and post- conditions, but structurally dependent on its parent. Following the proofs-as-model-transformations approach, in this paper we consider a formalisation in Constructive Type Theory of the concepts of model and model transformation, and show how the correctness proofs of potentially large ordered model transformations can be systematically assembled from the proofs of the specifications of their parts, making them easier to derive.",
    "authors": [
      "Maribel Fernández",
      "Jeffrey Terrell"
    ],
    "published": "2013-02-21T04:00:11+00:00",
    "url": "http://arxiv.org/pdf/1302.5174v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2105.14424v1",
    "title": "Gaze Estimation using Transformer",
    "content": "Gaze Estimation using Transformer. Recent work has proven the effectiveness of transformers in many computer vision tasks. However, the performance of transformers in gaze estimation is still unexplored. In this paper, we employ transformers and assess their effectiveness for gaze estimation. We consider two forms of vision transformer which are pure transformers and hybrid transformers. We first follow the popular ViT and employ a pure transformer to estimate gaze from images. On the other hand, we preserve the convolutional layers and integrate CNNs as well as transformers. The transformer serves as a component to complement CNNs. We compare the performance of the two transformers in gaze estimation. The Hybrid transformer significantly outperforms the pure transformer in all evaluation datasets with less parameters. We further conduct experiments to assess the effectiveness of the hybrid transformer and explore the advantage of self-attention mechanism. Experiments show the hybrid transformer can achieve state-of-the-art performance in all benchmarks with pre-training.To facilitate further research, we release codes and models in https://github.com/yihuacheng/GazeTR.",
    "authors": [
      "Yihua Cheng",
      "Feng Lu"
    ],
    "published": "2021-05-30T04:06:29+00:00",
    "url": "http://arxiv.org/pdf/2105.14424v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1511.05366v1",
    "title": "Systematically Deriving Domain-Specific Transformation Languages",
    "content": "Systematically Deriving Domain-Specific Transformation Languages. Model transformations are helpful to evolve, refactor, refine and maintain models. While domain-specific languages are normally intuitive for modelers, common model transformation approaches (regardless of whether they transform graphical or textual models) are based on the modeling language's abstract syntax requiring the modeler to learn the internal representation of the model to describe transformations. This paper presents a process that allows to systematically derive a textual domainspecific transformation language from the grammar of a given textual modeling language. As example, we apply this systematic derivation to UML class diagrams to obtain a domain-specific transformation language called CDTrans. CDTrans incorporates the concrete syntax of class diagrams which is already familiar to the modeler and extends it with a few transformation operators. To demonstrate the usefulness of the derived transformation language, we describe several refactoring transformations.",
    "authors": [
      "Katrin Hölldobler",
      "Bernhard Rumpe Ingo Weisemöller"
    ],
    "published": "2015-11-17T12:03:35+00:00",
    "url": "http://arxiv.org/pdf/1511.05366v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2004.08838v1",
    "title": "Extended Abstract of Performance Analysis and Prediction of Model Transformation",
    "content": "Extended Abstract of Performance Analysis and Prediction of Model Transformation. In the software development process, model transformation is increasingly assimilated. However, systems being developed with model transformation sometimes grow in size and become complex. Meanwhile, the performance of model transformation tends to decrease. Hence, performance is an important quality of model transformation. According to current research model transformation performance focuses on optimising the engines internally. However, there exists no research activities to support transformation engineer to identify performance bottleneck in the transformation rules and hence, to predict the overall performance. In this paper we vision our aim at providing an approach of monitoring and profiling to identify the root cause of performance issues in the transformation rules and to predict the performance of model transformation. This will enable software engineers to systematically identify performance issues as well as predict the performance of model transformation.",
    "authors": [
      "Vijayshree Vijayshree",
      "Markus Frank",
      "Steffen Becker"
    ],
    "published": "2020-04-19T12:36:10+00:00",
    "url": "http://arxiv.org/pdf/2004.08838v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1506.08507v1",
    "title": "Shrinking cloaks in expanding spacetimes: the role of coordinates and the meaning of transformations in Transformation Optics",
    "content": "Shrinking cloaks in expanding spacetimes: the role of coordinates and the meaning of transformations in Transformation Optics. The fully covariant formulation of transformation optics is used to find the configuration of a cloaking device operating in an expanding universe modelled by a Friedmann-Lema\\^itre-Robertson-Walker spacetime. This spacetime cloak is used as a platform for probing the covariant formulation of transformation optics, thereby rigorously enhancing the conceptual understanding of the theory. By studying the problem in both comoving and physical coordinates we explicitly demonstrate the preservation of general covariance of electrodynamics under the transformation optics procedure. This platform also enables a detailed study of the various transformations that arise in transformation optics. We define a corporeal transformation as the \"transformation\" of transformation optics, and distinguish it from coordinate and frame transformations. We find that corporeal transformations considered in the literature have generally been restricted to a subset of all possible corporeal transformations, providing a potential mechanism for increased functionality of transformation optics.",
    "authors": [
      "Robert T. Thompson",
      "Mohsen Fathi"
    ],
    "published": "2015-06-29T04:47:37+00:00",
    "url": "http://arxiv.org/pdf/1506.08507v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1906.08628v3",
    "title": "Learning Generalized Transformation Equivariant Representations via Autoencoding Transformations",
    "content": "Learning Generalized Transformation Equivariant Representations via Autoencoding Transformations. Transformation Equivariant Representations (TERs) aim to capture the intrinsic visual structures that equivary to various transformations by expanding the notion of {\\em translation} equivariance underlying the success of Convolutional Neural Networks (CNNs). For this purpose, we present both deterministic AutoEncoding Transformations (AET) and probabilistic AutoEncoding Variational Transformations (AVT) models to learn visual representations from generic groups of transformations. While the AET is trained by directly decoding the transformations from the learned representations, the AVT is trained by maximizing the joint mutual information between the learned representation and transformations. This results in Generalized TERs (GTERs) equivariant against transformations in a more general fashion by capturing complex patterns of visual structures beyond the conventional linear equivariance under a transformation group. The presented approach can be extended to (semi-)supervised models by jointly maximizing the mutual information of the learned representation with both labels and transformations. Experiments demonstrate the proposed models outperform the state-of-the-art models in both unsupervised and (semi-)supervised tasks.",
    "authors": [
      "Guo-Jun Qi",
      "Liheng Zhang",
      "Xiao Wang"
    ],
    "published": "2019-06-19T06:17:56+00:00",
    "url": "http://arxiv.org/pdf/1906.08628v3"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1803.01980v1",
    "title": "Learning Filter Bank Sparsifying Transforms",
    "content": "Learning Filter Bank Sparsifying Transforms. Data is said to follow the transform (or analysis) sparsity model if it becomes sparse when acted on by a linear operator called a sparsifying transform. Several algorithms have been designed to learn such a transform directly from data, and data-adaptive sparsifying transforms have demonstrated excellent performance in signal restoration tasks. Sparsifying transforms are typically learned using small sub-regions of data called patches, but these algorithms often ignore redundant information shared between neighboring patches.   We show that many existing transform and analysis sparse representations can be viewed as filter banks, thus linking the local properties of patch-based model to the global properties of a convolutional model. We propose a new transform learning framework where the sparsifying transform is an undecimated perfect reconstruction filter bank. Unlike previous transform learning algorithms, the filter length can be chosen independently of the number of filter bank channels. Numerical results indicate filter bank sparsifying transforms outperform existing patch-based transform learning for image denoising while benefiting from additional flexibility in the design process.",
    "authors": [
      "Luke Pfister",
      "Yoram Bresler"
    ],
    "published": "2018-03-06T01:35:01+00:00",
    "url": "http://arxiv.org/pdf/1803.01980v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2012.11152v1",
    "title": "Explainable Machine Learning based Transform Coding for High Efficiency Intra Prediction",
    "content": "Explainable Machine Learning based Transform Coding for High Efficiency Intra Prediction. Machine learning techniques provide a chance to explore the coding performance potential of transform. In this work, we propose an explainable transform based intra video coding to improve the coding efficiency. Firstly, we model machine learning based transform design as an optimization problem of maximizing the energy compaction or decorrelation capability. The explainable machine learning based transform, i.e., Subspace Approximation with Adjusted Bias (Saab) transform, is analyzed and compared with the mainstream Discrete Cosine Transform (DCT) on their energy compaction and decorrelation capabilities. Secondly, we propose a Saab transform based intra video coding framework with off-line Saab transform learning. Meanwhile, intra mode dependent Saab transform is developed. Then, Rate Distortion (RD) gain of Saab transform based intra video coding is theoretically and experimentally analyzed in detail. Finally, three strategies on integrating the Saab transform and DCT in intra video coding are developed to improve the coding efficiency. Experimental results demonstrate that the proposed 8$\\times$8 Saab transform based intra video coding can achieve Bj{\\o}nteggard Delta Bit Rate (BDBR) from -1.19% to -10.00% and -3.07% on average as compared with the mainstream 8$\\times$8 DCT based coding scheme.",
    "authors": [
      "Na Li",
      "Yun Zhang",
      "C. -C. Jay Kuo"
    ],
    "published": "2020-12-21T07:02:33+00:00",
    "url": "http://arxiv.org/pdf/2012.11152v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2108.11993v2",
    "title": "Evaluating Transformer-based Semantic Segmentation Networks for Pathological Image Segmentation",
    "content": "Evaluating Transformer-based Semantic Segmentation Networks for Pathological Image Segmentation. Histopathology has played an essential role in cancer diagnosis. With the rapid advances in convolutional neural networks (CNN). Various CNN-based automated pathological image segmentation approaches have been developed in computer-assisted pathological image analysis. In the past few years, Transformer neural networks (Transformer) have shown the unique merit of capturing the global long-distance dependencies across the entire image as a new deep learning paradigm. Such merit is appealing for exploring spatially heterogeneous pathological images. However, there have been very few, if any, studies that have systematically evaluated the current Transformer-based approaches in pathological image segmentation. To assess the performance of Transformer segmentation models on whole slide images (WSI), we quantitatively evaluated six prevalent transformer-based models on tumor segmentation, using the widely used PAIP liver histopathological dataset. For a more comprehensive analysis, we also compare the transformer-based models with six major traditional CNN-based models. The results show that the Transformer-based models exhibit a general superior performance over the CNN-based models. In particular, Segmenter, Swin-Transformer and TransUNet-all transformer-based-came out as the best performers among the twelve evaluated models.",
    "authors": [
      "Cam Nguyen",
      "Zuhayr Asad",
      "Yuankai Huo"
    ],
    "published": "2021-08-26T18:46:43+00:00",
    "url": "http://arxiv.org/pdf/2108.11993v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2403.14552v1",
    "title": "Token Transformation Matters: Towards Faithful Post-hoc Explanation for Vision Transformer",
    "content": "Token Transformation Matters: Towards Faithful Post-hoc Explanation for Vision Transformer. While Transformers have rapidly gained popularity in various computer vision applications, post-hoc explanations of their internal mechanisms remain largely unexplored. Vision Transformers extract visual information by representing image regions as transformed tokens and integrating them via attention weights. However, existing post-hoc explanation methods merely consider these attention weights, neglecting crucial information from the transformed tokens, which fails to accurately illustrate the rationales behind the models' predictions. To incorporate the influence of token transformation into interpretation, we propose TokenTM, a novel post-hoc explanation method that utilizes our introduced measurement of token transformation effects. Specifically, we quantify token transformation effects by measuring changes in token lengths and correlations in their directions pre- and post-transformation. Moreover, we develop initialization and aggregation rules to integrate both attention weights and token transformation effects across all layers, capturing holistic token contributions throughout the model. Experimental results on segmentation and perturbation tests demonstrate the superiority of our proposed TokenTM compared to state-of-the-art Vision Transformer explanation methods.",
    "authors": [
      "Junyi Wu",
      "Bin Duan",
      "Weitai Kang",
      "Hao Tang",
      "Yan Yan"
    ],
    "published": "2024-03-21T16:52:27+00:00",
    "url": "http://arxiv.org/pdf/2403.14552v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2004.11886v1",
    "title": "Lite Transformer with Long-Short Range Attention",
    "content": "Lite Transformer with Long-Short Range Attention. Transformer has become ubiquitous in natural language processing (e.g., machine translation, question answering); however, it requires enormous amount of computations to achieve high performance, which makes it not suitable for mobile applications that are tightly constrained by the hardware resources and battery. In this paper, we present an efficient mobile NLP architecture, Lite Transformer to facilitate deploying mobile NLP applications on edge devices. The key primitive is the Long-Short Range Attention (LSRA), where one group of heads specializes in the local context modeling (by convolution) while another group specializes in the long-distance relationship modeling (by attention). Such specialization brings consistent improvement over the vanilla transformer on three well-established language tasks: machine translation, abstractive summarization, and language modeling. Under constrained resources (500M/100M MACs), Lite Transformer outperforms transformer on WMT'14 English-French by 1.2/1.7 BLEU, respectively. Lite Transformer reduces the computation of transformer base model by 2.5x with 0.3 BLEU score degradation. Combining with pruning and quantization, we further compressed the model size of Lite Transformer by 18.2x. For language modeling, Lite Transformer achieves 1.8 lower perplexity than the transformer at around 500M MACs. Notably, Lite Transformer outperforms the AutoML-based Evolved Transformer by 0.5 higher BLEU for the mobile NLP setting without the costly architecture search that requires more than 250 GPU years. Code has been made available at https://github.com/mit-han-lab/lite-transformer.",
    "authors": [
      "Zhanghao Wu",
      "Zhijian Liu",
      "Ji Lin",
      "Yujun Lin",
      "Song Han"
    ],
    "published": "2020-04-24T17:52:25+00:00",
    "url": "http://arxiv.org/pdf/2004.11886v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/nlin/0302002v1",
    "title": "Some notes on Ishimori's magnet model",
    "content": "Some notes on Ishimori's magnet model. Gauge transformation properties for an associated linear system of model Ishimori's magnet model have been discussed. Explicit formulas for the gauge transformation matrix have been obtained. Darboux Transformation has been suggested and appropriate dressing relations have been found.",
    "authors": [
      "E. Sh. Gutshabash"
    ],
    "published": "2003-02-02T22:11:47+00:00",
    "url": "http://arxiv.org/pdf/nlin/0302002v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1312.0341v1",
    "title": "The TTC 2013 Flowgraphs Case",
    "content": "The TTC 2013 Flowgraphs Case. This case for the Transformation Tool Contest 2013 is about evaluating the scope and usability of transformation languages and tools for a set of four tasks requiring very different capabilities. One task deals with typical model-to-model transformation problem, there's a model-to-text problem, there are two in-place transformation problems, and finally there's a task dealing with validation of models resulting from the transformations.   The tasks build upon each other, but the transformation case project also provides all intermediate models, thus making it possible to skip tasks that are not suited for a particular tool, or for parallelizing the work among members of participating teams.",
    "authors": [
      "Tassilo Horn"
    ],
    "published": "2013-12-02T06:57:47+00:00",
    "url": "http://arxiv.org/pdf/1312.0341v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2207.08079v1",
    "title": "Performance degradation of ImageNet trained models by simple image transformations",
    "content": "Performance degradation of ImageNet trained models by simple image transformations. ImageNet trained PyTorch models are generally preferred as the off-the-shelf models for direct use or for initialisation in most computer vision tasks. In this paper, we simply test a representative set of these convolution and transformer based models under many simple image transformations like horizontal shifting, vertical shifting, scaling, rotation, presence of Gaussian noise, cutout, horizontal flip and vertical flip and report the performance drop caused by such transformations. We find that even simple transformations like rotating the image by 10{\\deg} or zooming in by 20% can reduce the top-1 accuracy of models like ResNet152 by 1%+. The code is available at https://github.com/harshm121/imagenet-transformation-degradation.",
    "authors": [
      "Harsh Maheshwari"
    ],
    "published": "2022-07-17T05:23:48+00:00",
    "url": "http://arxiv.org/pdf/2207.08079v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2407.09777v1",
    "title": "Graph Transformers: A Survey",
    "content": "Graph Transformers: A Survey. Graph transformers are a recent advancement in machine learning, offering a new class of neural network models for graph-structured data. The synergy between transformers and graph learning demonstrates strong performance and versatility across various graph-related tasks. This survey provides an in-depth review of recent progress and challenges in graph transformer research. We begin with foundational concepts of graphs and transformers. We then explore design perspectives of graph transformers, focusing on how they integrate graph inductive biases and graph attention mechanisms into the transformer architecture. Furthermore, we propose a taxonomy classifying graph transformers based on depth, scalability, and pre-training strategies, summarizing key principles for effective development of graph transformer models. Beyond technical analysis, we discuss the applications of graph transformer models for node-level, edge-level, and graph-level tasks, exploring their potential in other application scenarios as well. Finally, we identify remaining challenges in the field, such as scalability and efficiency, generalization and robustness, interpretability and explainability, dynamic and complex graphs, as well as data quality and diversity, charting future directions for graph transformer research.",
    "authors": [
      "Ahsan Shehzad",
      "Feng Xia",
      "Shagufta Abid",
      "Ciyuan Peng",
      "Shuo Yu",
      "Dongyu Zhang",
      "Karin Verspoor"
    ],
    "published": "2024-07-13T05:15:24+00:00",
    "url": "http://arxiv.org/pdf/2407.09777v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/quant-ph/9904009v1",
    "title": "New possibilities for supersymmetry breakdown in quantum mechanics and second order irreducible Darboux transformations",
    "content": "New possibilities for supersymmetry breakdown in quantum mechanics and second order irreducible Darboux transformations. New types of irreducible second order Darboux transformations for the one dimensional Schroedinger equation are described. The main feature of such transformations is that the transformation functions have the eigenvalues grater then the ground state energy of the initial (or reference) Hamiltonian. When such a transformation is presented as a chain of two first order transformations, an intermediate potential is singular and therefore intermediate Hamiltonian can not be Hermitian while the final potential is regular and the final Hamiltonian is Hermitian. Second derivative supersymmetric quantum mechanical model based on a transformation of this kind exhibits properties inherent to models with exact and broken supersymmetry at once.",
    "authors": [
      "Boris F. Samsonov"
    ],
    "published": "1999-04-02T03:13:02+00:00",
    "url": "http://arxiv.org/pdf/quant-ph/9904009v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2210.08288v1",
    "title": "Transformer-based dimensionality reduction",
    "content": "Transformer-based dimensionality reduction. Recently, Transformer is much popular and plays an important role in the fields of Machine Learning (ML), Natural Language Processing (NLP), and Computer Vision (CV), etc. In this paper, based on the Vision Transformer (ViT) model, a new dimensionality reduction (DR) model is proposed, named Transformer-DR. From data visualization, image reconstruction and face recognition, the representation ability of Transformer-DR after dimensionality reduction is studied, and it is compared with some representative DR methods to understand the difference between Transformer-DR and existing DR methods. The experimental results show that Transformer-DR is an effective dimensionality reduction method.",
    "authors": [
      "Ruisheng Ran",
      "Tianyu Gao",
      "Bin Fang"
    ],
    "published": "2022-10-15T13:24:43+00:00",
    "url": "http://arxiv.org/pdf/2210.08288v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2311.11378v1",
    "title": "Inspecting Explainability of Transformer Models with Additional Statistical Information",
    "content": "Inspecting Explainability of Transformer Models with Additional Statistical Information. Transformer becomes more popular in the vision domain in recent years so there is a need for finding an effective way to interpret the Transformer model by visualizing it. In recent work, Chefer et al. can visualize the Transformer on vision and multi-modal tasks effectively by combining attention layers to show the importance of each image patch. However, when applying to other variants of Transformer such as the Swin Transformer, this method can not focus on the predicted object. Our method, by considering the statistics of tokens in layer normalization layers, shows a great ability to interpret the explainability of Swin Transformer and ViT.",
    "authors": [
      "Hoang C. Nguyen",
      "Haeil Lee",
      "Junmo Kim"
    ],
    "published": "2023-11-19T17:22:50+00:00",
    "url": "http://arxiv.org/pdf/2311.11378v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1107.1126v2",
    "title": "A New Viewpoint to the Discrete Approximation: Discrete Yang-Fourier Transforms of Discrete-time Fractal Signal",
    "content": "A New Viewpoint to the Discrete Approximation: Discrete Yang-Fourier Transforms of Discrete-time Fractal Signal. It is suggest that a new fractal model for the Yang-Fourier transforms of discrete approximation based on local fractional calculus and the Discrete Yang-Fourier transforms are investigated in detail.",
    "authors": [
      "Xiao-Jun Yang"
    ],
    "published": "2011-07-06T13:21:42+00:00",
    "url": "http://arxiv.org/pdf/1107.1126v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2406.16111v1",
    "title": "Multi-Scale Temporal Difference Transformer for Video-Text Retrieval",
    "content": "Multi-Scale Temporal Difference Transformer for Video-Text Retrieval. Currently, in the field of video-text retrieval, there are many transformer-based methods. Most of them usually stack frame features and regrade frames as tokens, then use transformers for video temporal modeling. However, they commonly neglect the inferior ability of the transformer modeling local temporal information. To tackle this problem, we propose a transformer variant named Multi-Scale Temporal Difference Transformer (MSTDT). MSTDT mainly addresses the defects of the traditional transformer which has limited ability to capture local temporal information. Besides, in order to better model the detailed dynamic information, we make use of the difference feature between frames, which practically reflects the dynamic movement of a video. We extract the inter-frame difference feature and integrate the difference and frame feature by the multi-scale temporal transformer. In general, our proposed MSTDT consists of a short-term multi-scale temporal difference transformer and a long-term temporal transformer. The former focuses on modeling local temporal information, the latter aims at modeling global temporal information. At last, we propose a new loss to narrow the distance of similar samples. Extensive experiments show that backbone, such as CLIP, with MSTDT has attained a new state-of-the-art result.",
    "authors": [
      "Ni Wang",
      "Dongliang Liao",
      "Xing Xu"
    ],
    "published": "2024-06-23T13:59:31+00:00",
    "url": "http://arxiv.org/pdf/2406.16111v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2004.14996v2",
    "title": "Segatron: Segment-Aware Transformer for Language Modeling and Understanding",
    "content": "Segatron: Segment-Aware Transformer for Language Modeling and Understanding. Transformers are powerful for sequence modeling. Nearly all state-of-the-art language models and pre-trained language models are based on the Transformer architecture. However, it distinguishes sequential tokens only with the token position index. We hypothesize that better contextual representations can be generated from the Transformer with richer positional information. To verify this, we propose a segment-aware Transformer (Segatron), by replacing the original token position encoding with a combined position encoding of paragraph, sentence, and token. We first introduce the segment-aware mechanism to Transformer-XL, which is a popular Transformer-based language model with memory extension and relative position encoding. We find that our method can further improve the Transformer-XL base model and large model, achieving 17.1 perplexity on the WikiText-103 dataset. We further investigate the pre-training masked language modeling task with Segatron. Experimental results show that BERT pre-trained with Segatron (SegaBERT) can outperform BERT with vanilla Transformer on various NLP tasks, and outperforms RoBERTa on zero-shot sentence representation learning.",
    "authors": [
      "He Bai",
      "Peng Shi",
      "Jimmy Lin",
      "Yuqing Xie",
      "Luchen Tan",
      "Kun Xiong",
      "Wen Gao",
      "Ming Li"
    ],
    "published": "2020-04-30T17:38:27+00:00",
    "url": "http://arxiv.org/pdf/2004.14996v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2106.01040v3",
    "title": "Hi-Transformer: Hierarchical Interactive Transformer for Efficient and Effective Long Document Modeling",
    "content": "Hi-Transformer: Hierarchical Interactive Transformer for Efficient and Effective Long Document Modeling. Transformer is important for text modeling. However, it has difficulty in handling long documents due to the quadratic complexity with input text length. In order to handle this problem, we propose a hierarchical interactive Transformer (Hi-Transformer) for efficient and effective long document modeling. Hi-Transformer models documents in a hierarchical way, i.e., first learns sentence representations and then learns document representations. It can effectively reduce the complexity and meanwhile capture global document context in the modeling of each sentence. More specifically, we first use a sentence Transformer to learn the representations of each sentence. Then we use a document Transformer to model the global document context from these sentence representations. Next, we use another sentence Transformer to enhance sentence modeling using the global document context. Finally, we use hierarchical pooling method to obtain document embedding. Extensive experiments on three benchmark datasets validate the efficiency and effectiveness of Hi-Transformer in long document modeling.",
    "authors": [
      "Chuhan Wu",
      "Fangzhao Wu",
      "Tao Qi",
      "Yongfeng Huang"
    ],
    "published": "2021-06-02T09:30:29+00:00",
    "url": "http://arxiv.org/pdf/2106.01040v3"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1901.09458v2",
    "title": "Learning Transformation Synchronization",
    "content": "Learning Transformation Synchronization. Reconstructing the 3D model of a physical object typically requires us to align the depth scans obtained from different camera poses into the same coordinate system. Solutions to this global alignment problem usually proceed in two steps. The first step estimates relative transformations between pairs of scans using an off-the-shelf technique. Due to limited information presented between pairs of scans, the resulting relative transformations are generally noisy. The second step then jointly optimizes the relative transformations among all input depth scans. A natural constraint used in this step is the cycle-consistency constraint, which allows us to prune incorrect relative transformations by detecting inconsistent cycles. The performance of such approaches, however, heavily relies on the quality of the input relative transformations. Instead of merely using the relative transformations as the input to perform transformation synchronization, we propose to use a neural network to learn the weights associated with each relative transformation. Our approach alternates between transformation synchronization using weighted relative transformations and predicting new weights of the input relative transformations using a neural network. We demonstrate the usefulness of this approach across a wide range of datasets.",
    "authors": [
      "Xiangru Huang",
      "Zhenxiao Liang",
      "Xiaowei Zhou",
      "Yao Xie",
      "Leonidas Guibas",
      "Qixing Huang"
    ],
    "published": "2019-01-27T23:09:21+00:00",
    "url": "http://arxiv.org/pdf/1901.09458v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2501.08712v1",
    "title": "Self-supervised Transformation Learning for Equivariant Representations",
    "content": "Self-supervised Transformation Learning for Equivariant Representations. Unsupervised representation learning has significantly advanced various machine learning tasks. In the computer vision domain, state-of-the-art approaches utilize transformations like random crop and color jitter to achieve invariant representations, embedding semantically the same inputs despite transformations. However, this can degrade performance in tasks requiring precise features, such as localization or flower classification. To address this, recent research incorporates equivariant representation learning, which captures transformation-sensitive information. However, current methods depend on transformation labels and thus struggle with interdependency and complex transformations. We propose Self-supervised Transformation Learning (STL), replacing transformation labels with transformation representations derived from image pairs. The proposed method ensures transformation representation is image-invariant and learns corresponding equivariant transformations, enhancing performance without increased batch complexity. We demonstrate the approach's effectiveness across diverse classification and detection tasks, outperforming existing methods in 7 out of 11 benchmarks and excelling in detection. By integrating complex transformations like AugMix, unusable by prior equivariant methods, this approach enhances performance across tasks, underscoring its adaptability and resilience. Additionally, its compatibility with various base models highlights its flexibility and broad applicability. The code is available at https://github.com/jaemyung-u/stl.",
    "authors": [
      "Jaemyung Yu",
      "Jaehyun Choi",
      "Dong-Jae Lee",
      "HyeongGwon Hong",
      "Junmo Kim"
    ],
    "published": "2025-01-15T10:54:21+00:00",
    "url": "http://arxiv.org/pdf/2501.08712v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1001.2604v1",
    "title": "On the Model Transform in Stochastic Network Calculus",
    "content": "On the Model Transform in Stochastic Network Calculus. Stochastic network calculus requires special care in the search of proper stochastic traffic arrival models and stochastic service models. Tradeoff must be considered between the feasibility for the analysis of performance bounds, the usefulness of performance bounds, and the ease of their numerical calculation. In theory, transform between different traffic arrival models and transform between different service models are possible. Nevertheless, the impact of the model transform on performance bounds has not been thoroughly investigated. This paper is to investigate the effect of the model transform and to provide practical guidance in the model selection in stochastic network calculus.",
    "authors": [
      "Kui Wu",
      "Yuming Jiang",
      "Jie Li"
    ],
    "published": "2010-01-15T03:12:55+00:00",
    "url": "http://arxiv.org/pdf/1001.2604v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1308.0982v1",
    "title": "Finite field dependent BRST transformations and its applications to gauge field theories",
    "content": "Finite field dependent BRST transformations and its applications to gauge field theories. The Becchi-Rouet-Stora and Tyutin (BRST) transformation plays a crucial role in the quantization of gauge theories. The BRST transformation is also very important tool in characterizing the various renormalizable field theoretic models. The generalization of the usual BRST transformation, by making the infinitesimal global parameter finite and field dependent, is commonly known as the finite field dependent BRST (FFBRST) transformation. In this thesis, we have extended the FFBRST transformation in an auxiliary field formulation and have developed both on-shell and off-shell FF-anti-BRST transformations. The different aspects of such transformation are studied in Batalin-Vilkovisky (BV) formulation. FFBRST transformation has further been used to study the celebrated Gribov problem and to analyze the constrained dynamics in gauge theories. A new finite field dependent symmetry (combination of FFBRST and FF-anti-BRST) transformation has been invented. The FFBRST transformation is shown useful in connection of first-class constrained theory to that of second-class also. Further, we have applied the Batalin-Fradkin-Vilkovisky (BFV) technique to quantize a field theoretic model in the Hamiltonian framework. The Hodge de Rham theorem for differential geometry has also been studied in such context.",
    "authors": [
      "Sudhaker Upadhyay"
    ],
    "published": "2013-08-05T13:45:04+00:00",
    "url": "http://arxiv.org/pdf/1308.0982v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1409.6611v1",
    "title": "Model Transformations in Practice Workshop (MTiP)",
    "content": "Model Transformations in Practice Workshop (MTiP). Model Transformations in Practice (MTiP) 2005 was a workshop which provided a forum for the model transformation community to discuss practical model transformation issues. Although many different model transformation approaches have been proposed and explored in recent years, there has been little work on comparing and contrasting various approaches. Without such comparisons, it is hard to assess new model transformation approaches such as the upcoming OMG MOF/QVT recommendation, or to discern sensible future paths for the area. Our aims with the workshop were to create a forum that would help lead to an increased understanding of the relative merits of different model transformation techniques and approaches. A more advanced understanding of such merits is of considerable benefit to both the model transformation and wider modelling communities.",
    "authors": [
      "Jean Bézivin",
      "Bernhard Rumpe",
      "Andy Schürr",
      "Laurence Tratt"
    ],
    "published": "2014-09-22T17:12:43+00:00",
    "url": "http://arxiv.org/pdf/1409.6611v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2110.13711v2",
    "title": "Hierarchical Transformers Are More Efficient Language Models",
    "content": "Hierarchical Transformers Are More Efficient Language Models. Transformer models yield impressive results on many NLP and sequence modeling tasks. Remarkably, Transformers can handle long sequences which allows them to produce long coherent outputs: full paragraphs produced by GPT-3 or well-structured images produced by DALL-E. These large language models are impressive but also very inefficient and costly, which limits their applications and accessibility. We postulate that having an explicit hierarchical architecture is the key to Transformers that efficiently handle long sequences. To verify this claim, we first study different ways to downsample and upsample activations in Transformers so as to make them hierarchical. We use the best performing upsampling and downsampling layers to create Hourglass - a hierarchical Transformer language model. Hourglass improves upon the Transformer baseline given the same amount of computation and can yield the same results as Transformers more efficiently. In particular, Hourglass sets new state-of-the-art for Transformer models on the ImageNet32 generation task and improves language modeling efficiency on the widely studied enwik8 benchmark.",
    "authors": [
      "Piotr Nawrot",
      "Szymon Tworkowski",
      "Michał Tyrolski",
      "Łukasz Kaiser",
      "Yuhuai Wu",
      "Christian Szegedy",
      "Henryk Michalewski"
    ],
    "published": "2021-10-26T14:00:49+00:00",
    "url": "http://arxiv.org/pdf/2110.13711v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2411.00252v1",
    "title": "IO Transformer: Evaluating SwinV2-Based Reward Models for Computer Vision",
    "content": "IO Transformer: Evaluating SwinV2-Based Reward Models for Computer Vision. Transformers and their derivatives have achieved state-of-the-art performance across text, vision, and speech recognition tasks. However, minimal effort has been made to train transformers capable of evaluating the output quality of other models. This paper examines SwinV2-based reward models, called the Input-Output Transformer (IO Transformer) and the Output Transformer. These reward models can be leveraged for tasks such as inference quality evaluation, data categorization, and policy optimization. Our experiments demonstrate highly accurate model output quality assessment across domains where the output is entirely dependent on the input, with the IO Transformer achieving perfect evaluation accuracy on the Change Dataset 25 (CD25). We also explore modified Swin V2 architectures. Ultimately Swin V2 remains on top with a score of 95.41 % on the IO Segmentation Dataset, outperforming the IO Transformer in scenarios where the output is not entirely dependent on the input. Our work expands the application of transformer architectures to reward modeling in computer vision and provides critical insights into optimizing these models for various tasks.",
    "authors": [
      "Maxwell Meyer",
      "Jack Spruyt"
    ],
    "published": "2024-10-31T23:16:09+00:00",
    "url": "http://arxiv.org/pdf/2411.00252v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2012.12071v1",
    "title": "Disentangling images with Lie group transformations and sparse coding",
    "content": "Disentangling images with Lie group transformations and sparse coding. Discrete spatial patterns and their continuous transformations are two important regularities contained in natural signals. Lie groups and representation theory are mathematical tools that have been used in previous works to model continuous image transformations. On the other hand, sparse coding is an important tool for learning dictionaries of patterns in natural signals. In this paper, we combine these ideas in a Bayesian generative model that learns to disentangle spatial patterns and their continuous transformations in a completely unsupervised manner. Images are modeled as a sparse superposition of shape components followed by a transformation that is parameterized by n continuous variables. The shape components and transformations are not predefined, but are instead adapted to learn the symmetries in the data, with the constraint that the transformations form a representation of an n-dimensional torus. Training the model on a dataset consisting of controlled geometric transformations of specific MNIST digits shows that it can recover these transformations along with the digits. Training on the full MNIST dataset shows that it can learn both the basic digit shapes and the natural transformations such as shearing and stretching that are contained in this data.",
    "authors": [
      "Ho Yin Chau",
      "Frank Qiu",
      "Yubei Chen",
      "Bruno Olshausen"
    ],
    "published": "2020-12-11T19:11:32+00:00",
    "url": "http://arxiv.org/pdf/2012.12071v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2110.07160v1",
    "title": "Transformer over Pre-trained Transformer for Neural Text Segmentation with Enhanced Topic Coherence",
    "content": "Transformer over Pre-trained Transformer for Neural Text Segmentation with Enhanced Topic Coherence. This paper proposes a transformer over transformer framework, called Transformer$^2$, to perform neural text segmentation. It consists of two components: bottom-level sentence encoders using pre-trained transformers, and an upper-level transformer-based segmentation model based on the sentence embeddings. The bottom-level component transfers the pre-trained knowledge learned from large external corpora under both single and pair-wise supervised NLP tasks to model the sentence embeddings for the documents. Given the sentence embeddings, the upper-level transformer is trained to recover the segmentation boundaries as well as the topic labels of each sentence. Equipped with a multi-task loss and the pre-trained knowledge, Transformer$^2$ can better capture the semantic coherence within the same segments. Our experiments show that (1) Transformer$^2$ manages to surpass state-of-the-art text segmentation models in terms of a commonly-used semantic coherence measure; (2) in most cases, both single and pair-wise pre-trained knowledge contribute to the model performance; (3) bottom-level sentence encoders pre-trained on specific languages yield better performance than those pre-trained on specific domains.",
    "authors": [
      "Kelvin Lo",
      "Yuan Jin",
      "Weicong Tan",
      "Ming Liu",
      "Lan Du",
      "Wray Buntine"
    ],
    "published": "2021-10-14T05:26:39+00:00",
    "url": "http://arxiv.org/pdf/2110.07160v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1812.07808v1",
    "title": "Schrieffer-Wolff transformation of Anderson Models",
    "content": "Schrieffer-Wolff transformation of Anderson Models. Schrieffer-Wolff transformation is one of the very important transformations in the study of quantum many body physics. It is used to arrive at the low energy effective hamiltonian of Quantum many-body hamiltonians, which are not generally analytically tractable. In this paper we give a pedagogical review of this transformation for Anderson Impurity model(SIAM) and its lattice generlaization called, the Periodic Anderson Model(PAM).",
    "authors": [
      "Rukhsan Ul Haq",
      "Sachin Satish Bharadwaj"
    ],
    "published": "2018-12-19T08:38:07+00:00",
    "url": "http://arxiv.org/pdf/1812.07808v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2003.05683v1",
    "title": "Identification in a Fully Nonparametric Transformation Model with Heteroscedasticity",
    "content": "Identification in a Fully Nonparametric Transformation Model with Heteroscedasticity. The so far most general identification result in the context of nonparametric transformation models is proven. The result is constructive in the sense that it provides an explicit expression of the transformation function.",
    "authors": [
      "Nick Kloodt"
    ],
    "published": "2020-03-12T09:50:35+00:00",
    "url": "http://arxiv.org/pdf/2003.05683v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2503.00608v1",
    "title": "Real-Time Personalization with Simple Transformers",
    "content": "Real-Time Personalization with Simple Transformers. Real-time personalization has advanced significantly in recent years, with platforms utilizing machine learning models to predict user preferences based on rich behavioral data on each individual user. Traditional approaches usually rely on embedding-based machine learning models to capture user preferences, and then reduce the final optimization task to nearest-neighbors, which can be performed extremely fast. However, these models struggle to capture complex user behaviors, which are essential for making accurate recommendations. Transformer-based models, on the other hand, are known for their practical ability to model sequential behaviors, and hence have been intensively used in personalization recently to overcome these limitations. However, optimizing recommendations under transformer-based models is challenging due to their complicated architectures. In this paper, we address this challenge by considering a specific class of transformers, showing its ability to represent complex user preferences, and developing efficient algorithms for real-time personalization.   We focus on a particular set of transformers, called simple transformers, which contain a single self-attention layer. We show that simple transformers are capable of capturing complex user preferences. We then develop an algorithm that enables fast optimization of recommendation tasks based on simple transformers. Our algorithm achieves near-optimal performance in sub-linear time. Finally, we demonstrate the effectiveness of our approach through an empirical study on datasets from Spotify and Trivago. Our experiment results show that (1) simple transformers can model/predict user preferences substantially more accurately than non-transformer models and nearly as accurately as more complex transformers, and (2) our algorithm completes simple-transformer-based recommendation tasks quickly and effectively.",
    "authors": [
      "Lin An",
      "Andrew A. Li",
      "Vaisnavi Nemala",
      "Gabriel Visotsky"
    ],
    "published": "2025-03-01T20:29:33+00:00",
    "url": "http://arxiv.org/pdf/2503.00608v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2106.02277v1",
    "title": "Glance-and-Gaze Vision Transformer",
    "content": "Glance-and-Gaze Vision Transformer. Recently, there emerges a series of vision Transformers, which show superior performance with a more compact model size than conventional convolutional neural networks, thanks to the strong ability of Transformers to model long-range dependencies. However, the advantages of vision Transformers also come with a price: Self-attention, the core part of Transformer, has a quadratic complexity to the input sequence length. This leads to a dramatic increase of computation and memory cost with the increase of sequence length, thus introducing difficulties when applying Transformers to the vision tasks that require dense predictions based on high-resolution feature maps. In this paper, we propose a new vision Transformer, named Glance-and-Gaze Transformer (GG-Transformer), to address the aforementioned issues. It is motivated by the Glance and Gaze behavior of human beings when recognizing objects in natural scenes, with the ability to efficiently model both long-range dependencies and local context. In GG-Transformer, the Glance and Gaze behavior is realized by two parallel branches: The Glance branch is achieved by performing self-attention on the adaptively-dilated partitions of the input, which leads to a linear complexity while still enjoying a global receptive field; The Gaze branch is implemented by a simple depth-wise convolutional layer, which compensates local image context to the features obtained by the Glance mechanism. We empirically demonstrate our method achieves consistently superior performance over previous state-of-the-art Transformers on various vision tasks and benchmarks. The codes and models will be made available at https://github.com/yucornetto/GG-Transformer.",
    "authors": [
      "Qihang Yu",
      "Yingda Xia",
      "Yutong Bai",
      "Yongyi Lu",
      "Alan Yuille",
      "Wei Shen"
    ],
    "published": "2021-06-04T06:13:47+00:00",
    "url": "http://arxiv.org/pdf/2106.02277v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2405.04515v2",
    "title": "A Transformer with Stack Attention",
    "content": "A Transformer with Stack Attention. Natural languages are believed to be (mildly) context-sensitive. Despite underpinning remarkably capable large language models, transformers are unable to model many context-free language tasks. In an attempt to address this limitation in the modeling power of transformer-based language models, we propose augmenting them with a differentiable, stack-based attention mechanism. Our stack-based attention mechanism can be incorporated into any transformer-based language model and adds a level of interpretability to the model. We show that the addition of our stack-based attention mechanism enables the transformer to model some, but not all, deterministic context-free languages.",
    "authors": [
      "Jiaoda Li",
      "Jennifer C. White",
      "Mrinmaya Sachan",
      "Ryan Cotterell"
    ],
    "published": "2024-05-07T17:47:57+00:00",
    "url": "http://arxiv.org/pdf/2405.04515v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2111.00830v2",
    "title": "Deep Learning Transformer Architecture for Named Entity Recognition on Low Resourced Languages: State of the art results",
    "content": "Deep Learning Transformer Architecture for Named Entity Recognition on Low Resourced Languages: State of the art results. This paper reports on the evaluation of Deep Learning (DL) transformer architecture models for Named-Entity Recognition (NER) on ten low-resourced South African (SA) languages. In addition, these DL transformer models were compared to other Neural Network and Machine Learning (ML) NER models. The findings show that transformer models substantially improve performance when applying discrete fine-tuning parameters per language. Furthermore, fine-tuned transformer models outperform other neural network and machine learning models on NER with the low-resourced SA languages. For example, the transformer models obtained the highest F-scores for six of the ten SA languages and the highest average F-score surpassing the Conditional Random Fields ML model. Practical implications include developing high-performance NER capability with less effort and resource costs, potentially improving downstream NLP tasks such as Machine Translation (MT). Therefore, the application of DL transformer architecture models for NLP NER sequence tagging tasks on low-resourced SA languages is viable. Additional research could evaluate the more recent transformer architecture models on other Natural Language Processing tasks and applications, such as Phrase chunking, MT, and Part-of-Speech tagging.",
    "authors": [
      "Ridewaan Hanslo"
    ],
    "published": "2021-11-01T11:02:01+00:00",
    "url": "http://arxiv.org/pdf/2111.00830v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1310.6079v1",
    "title": "Synchrosqueezed Curvelet Transform for 2D Mode Decomposition",
    "content": "Synchrosqueezed Curvelet Transform for 2D Mode Decomposition. This paper introduces the synchrosqueezed curvelet transform as an optimal tool for 2D mode decomposition of wavefronts or banded wave-like components. The synchrosqueezed curvelet transform consists of a generalized curvelet transform with application dependent geometric scaling parameters, and a synchrosqueezing technique for a sharpened phase space representation. In the case of a superposition of banded wave-like components with well-separated wave-vectors, it is proved that the synchrosqueezed curvelet transform is capable of recognizing each component and precisely estimating local wave-vectors. A discrete analogue of the continuous transform and several clustering models for decomposition are proposed in detail. Some numerical examples with synthetic and real data are provided to demonstrate the above properties of the proposed transform.",
    "authors": [
      "Haizhao Yang",
      "Lexing Ying"
    ],
    "published": "2013-10-22T23:37:39+00:00",
    "url": "http://arxiv.org/pdf/1310.6079v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1703.07388v1",
    "title": "Segal-Bargmann transform: the $q$-deformation",
    "content": "Segal-Bargmann transform: the $q$-deformation. We give identifications of the $q$-deformed Segal-Bargmann transform and define the Segal-Bargmann transform on mixed $q$-Gaussian variables. We prove that, when defined on the random matrix model of \\'Sniady for the $q$-Gaussian variable, the classical Segal-Bargmann transform converges to the $q$-deformed Segal-Bargmann transform in the large $N$ limit. We also show that the $q$-deformed Segal-Bargmann transform can be recovered as a limit of a mixture of classical and free Segal-Bargmann transform.",
    "authors": [
      "Guillaume Cébron",
      "Ching-Wei Ho"
    ],
    "published": "2017-03-21T18:51:20+00:00",
    "url": "http://arxiv.org/pdf/1703.07388v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2208.02034v1",
    "title": "SSformer: A Lightweight Transformer for Semantic Segmentation",
    "content": "SSformer: A Lightweight Transformer for Semantic Segmentation. It is well believed that Transformer performs better in semantic segmentation compared to convolutional neural networks. Nevertheless, the original Vision Transformer may lack of inductive biases of local neighborhoods and possess a high time complexity. Recently, Swin Transformer sets a new record in various vision tasks by using hierarchical architecture and shifted windows while being more efficient. However, as Swin Transformer is specifically designed for image classification, it may achieve suboptimal performance on dense prediction-based segmentation task. Further, simply combing Swin Transformer with existing methods would lead to the boost of model size and parameters for the final segmentation model. In this paper, we rethink the Swin Transformer for semantic segmentation, and design a lightweight yet effective transformer model, called SSformer. In this model, considering the inherent hierarchical design of Swin Transformer, we propose a decoder to aggregate information from different layers, thus obtaining both local and global attentions. Experimental results show the proposed SSformer yields comparable mIoU performance with state-of-the-art models, while maintaining a smaller model size and lower compute.",
    "authors": [
      "Wentao Shi",
      "Jing Xu",
      "Pan Gao"
    ],
    "published": "2022-08-03T12:57:00+00:00",
    "url": "http://arxiv.org/pdf/2208.02034v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2309.11400v1",
    "title": "Transformers versus LSTMs for electronic trading",
    "content": "Transformers versus LSTMs for electronic trading. With the rapid development of artificial intelligence, long short term memory (LSTM), one kind of recurrent neural network (RNN), has been widely applied in time series prediction.   Like RNN, Transformer is designed to handle the sequential data. As Transformer achieved great success in Natural Language Processing (NLP), researchers got interested in Transformer's performance on time series prediction, and plenty of Transformer-based solutions on long time series forecasting have come out recently. However, when it comes to financial time series prediction, LSTM is still a dominant architecture. Therefore, the question this study wants to answer is: whether the Transformer-based model can be applied in financial time series prediction and beat LSTM.   To answer this question, various LSTM-based and Transformer-based models are compared on multiple financial prediction tasks based on high-frequency limit order book data. A new LSTM-based model called DLSTM is built and new architecture for the Transformer-based model is designed to adapt for financial prediction. The experiment result reflects that the Transformer-based model only has the limited advantage in absolute price sequence prediction. The LSTM-based models show better and more robust performance on difference sequence prediction, such as price difference and price movement.",
    "authors": [
      "Paul Bilokon",
      "Yitao Qiu"
    ],
    "published": "2023-09-20T15:25:43+00:00",
    "url": "http://arxiv.org/pdf/2309.11400v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2502.01533v1",
    "title": "Transformers trained on proteins can learn to attend to Euclidean distance",
    "content": "Transformers trained on proteins can learn to attend to Euclidean distance. While conventional Transformers generally operate on sequence data, they can be used in conjunction with structure models, typically SE(3)-invariant or equivariant graph neural networks (GNNs), for 3D applications such as protein structure modelling. These hybrids typically involve either (1) preprocessing/tokenizing structural features as input for Transformers or (2) taking Transformer embeddings and processing them within a structural representation. However, there is evidence that Transformers can learn to process structural information on their own, such as the AlphaFold3 structural diffusion model. In this work we show that Transformers can function independently as structure models when passed linear embeddings of coordinates. We first provide a theoretical explanation for how Transformers can learn to filter attention as a 3D Gaussian with learned variance. We then validate this theory using both simulated 3D points and in the context of masked token prediction for proteins. Finally, we show that pre-training protein Transformer encoders with structure improves performance on a downstream task, yielding better performance than custom structural models. Together, this work provides a basis for using standard Transformers as hybrid structure-language models.",
    "authors": [
      "Isaac Ellmen",
      "Constantin Schneider",
      "Matthew I. J. Raybould",
      "Charlotte M. Deane"
    ],
    "published": "2025-02-03T17:12:44+00:00",
    "url": "http://arxiv.org/pdf/2502.01533v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1709.09354v1",
    "title": "Generative Adversarial Networks with Inverse Transformation Unit",
    "content": "Generative Adversarial Networks with Inverse Transformation Unit. In this paper we introduce a new structure to Generative Adversarial Networks by adding an inverse transformation unit behind the generator. We present two theorems to claim the convergence of the model, and two conjectures to nonideal situations when the transformation is not bijection. A general survey on models with different transformations was done on the MNIST dataset and the Fashion-MNIST dataset, which shows the transformation does not necessarily need to be bijection. Also, with certain transformations that blurs an image, our model successfully learned to sharpen the images and recover blurred images, which was additionally verified by our measurement of sharpness.",
    "authors": [
      "Zhifeng Kong",
      "Shuo Ding"
    ],
    "published": "2017-09-27T06:38:30+00:00",
    "url": "http://arxiv.org/pdf/1709.09354v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2311.07184v3",
    "title": "Cross-Axis Transformer with 3D Rotary Positional Embeddings",
    "content": "Cross-Axis Transformer with 3D Rotary Positional Embeddings. Despite lagging behind their modal cousins in many respects, Vision Transformers have provided an interesting opportunity to bridge the gap between sequence modeling and image modeling. Up until now however, vision transformers have largely been held back, due to both computational inefficiency, and lack of proper handling of spatial dimensions. In this paper, we introduce the Cross-Axis Transformer. CAT is a model inspired by both Axial Transformers, and Microsoft's recent Retentive Network, that drastically reduces the required number of floating point operations required to process an image, while simultaneously converging faster and more accurately than the Vision Transformers it replaces.",
    "authors": [
      "Lily Erickson"
    ],
    "published": "2023-11-13T09:19:14+00:00",
    "url": "http://arxiv.org/pdf/2311.07184v3"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1908.04303v1",
    "title": "Generalised P and CP transformations in the 3-Higgs-doublet model",
    "content": "Generalised P and CP transformations in the 3-Higgs-doublet model. We study generalised P and CP transformations in the three-Higgs-doublet model (3HDM) with Higgs and gauge fields only. We find that there are two equivalence classes, with respect to flavour transformations, of generalised P transformations and there is only one class of CP transformations. We discuss the conditions the potential has to satisfy in order to be invariant under these transformations. We apply the method of bilinears which we briefly review. We discuss the relation to the conventional basis, where the potential is written in terms of scalar products of the doublet fields. In particular we reproduce the known result that a potential is invariant under CP transformations if and only if there is a conventional basis where all parameters are real. Eventually we study standard P and CP transformations in the $n$-Higgs-doublet model (nHDM). We show that for the bilinears of the nHDM the standard CP transformation corresponds to a diagonal linear transformation with only $\\pm 1$ as diagonal elements. We give this matrix explicitly for arbitrary $n$.",
    "authors": [
      "M. Maniatis",
      "O. Nachtmann"
    ],
    "published": "2019-08-12T18:00:01+00:00",
    "url": "http://arxiv.org/pdf/1908.04303v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2305.14858v2",
    "title": "Pre-RMSNorm and Pre-CRMSNorm Transformers: Equivalent and Efficient Pre-LN Transformers",
    "content": "Pre-RMSNorm and Pre-CRMSNorm Transformers: Equivalent and Efficient Pre-LN Transformers. Transformers have achieved great success in machine learning applications. Normalization techniques, such as Layer Normalization (LayerNorm, LN) and Root Mean Square Normalization (RMSNorm), play a critical role in accelerating and stabilizing the training of Transformers. While LayerNorm recenters and rescales input vectors, RMSNorm only rescales the vectors by their RMS value. Despite being more computationally efficient, RMSNorm may compromise the representation ability of Transformers. There is currently no consensus regarding the preferred normalization technique, as some models employ LayerNorm while others utilize RMSNorm, especially in recent large language models. It is challenging to convert Transformers with one normalization to the other type. While there is an ongoing disagreement between the two normalization types, we propose a solution to unify two mainstream Transformer architectures, Pre-LN and Pre-RMSNorm Transformers. By removing the inherent redundant mean information in the main branch of Pre-LN Transformers, we can reduce LayerNorm to RMSNorm, achieving higher efficiency. We further propose the Compressed RMSNorm (CRMSNorm) and Pre-CRMSNorm Transformer based on a lossless compression of the zero-mean vectors. We formally establish the equivalence of Pre-LN, Pre-RMSNorm, and Pre-CRMSNorm Transformer variants in both training and inference. It implies that Pre-LN Transformers can be substituted with Pre-(C)RMSNorm counterparts at almost no cost, offering the same arithmetic functionality along with free efficiency improvement. Experiments demonstrate that we can reduce the training and inference time of Pre-LN Transformers by 1% - 10%.",
    "authors": [
      "Zixuan Jiang",
      "Jiaqi Gu",
      "Hanqing Zhu",
      "David Z. Pan"
    ],
    "published": "2023-05-24T08:08:26+00:00",
    "url": "http://arxiv.org/pdf/2305.14858v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2411.05189v1",
    "title": "Adversarial Robustness of In-Context Learning in Transformers for Linear Regression",
    "content": "Adversarial Robustness of In-Context Learning in Transformers for Linear Regression. Transformers have demonstrated remarkable in-context learning capabilities across various domains, including statistical learning tasks. While previous work has shown that transformers can implement common learning algorithms, the adversarial robustness of these learned algorithms remains unexplored. This work investigates the vulnerability of in-context learning in transformers to \\textit{hijacking attacks} focusing on the setting of linear regression tasks. Hijacking attacks are prompt-manipulation attacks in which the adversary's goal is to manipulate the prompt to force the transformer to generate a specific output. We first prove that single-layer linear transformers, known to implement gradient descent in-context, are non-robust and can be manipulated to output arbitrary predictions by perturbing a single example in the in-context training set. While our experiments show these attacks succeed on linear transformers, we find they do not transfer to more complex transformers with GPT-2 architectures. Nonetheless, we show that these transformers can be hijacked using gradient-based adversarial attacks. We then demonstrate that adversarial training enhances transformers' robustness against hijacking attacks, even when just applied during finetuning. Additionally, we find that in some settings, adversarial training against a weaker attack model can lead to robustness to a stronger attack model. Lastly, we investigate the transferability of hijacking attacks across transformers of varying scales and initialization seeds, as well as between transformers and ordinary least squares (OLS). We find that while attacks transfer effectively between small-scale transformers, they show poor transferability in other scenarios (small-to-large scale, large-to-large scale, and between transformers and OLS).",
    "authors": [
      "Usman Anwar",
      "Johannes Von Oswald",
      "Louis Kirsch",
      "David Krueger",
      "Spencer Frei"
    ],
    "published": "2024-11-07T21:25:58+00:00",
    "url": "http://arxiv.org/pdf/2411.05189v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2004.01923v1",
    "title": "Estimation of the Transformation Function in Fully Nonparametric Transformation Models with Heteroscedasticity",
    "content": "Estimation of the Transformation Function in Fully Nonparametric Transformation Models with Heteroscedasticity. Completely nonparametric transformation models with heteroscedastic errors are considered. Despite their flexibility, such models have rarely been used so far, since estimators of the model components have been missing and even identification of such models has not been clear until very recently. The results of Kloodt (2020) are used to construct the first two estimators of the transformation function in these models. While the first estimator converges to the true transformation function at a parametric rate, the second estimator can be obtained by an explicit formula and is less computationally demanding. Finally, a simulation study is followed by some concluding remarks. Assumptions and proofs can be found in the appendix.",
    "authors": [
      "Nick Kloodt"
    ],
    "published": "2020-04-04T12:50:43+00:00",
    "url": "http://arxiv.org/pdf/2004.01923v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2209.07634v2",
    "title": "Stateful Memory-Augmented Transformers for Efficient Dialogue Modeling",
    "content": "Stateful Memory-Augmented Transformers for Efficient Dialogue Modeling. Transformer encoder-decoder models have achieved great performance in dialogue generation tasks, however, their inability to process long dialogue history often leads to truncation of the context To address this problem, we propose a novel memory-augmented transformer that is compatible with existing pre-trained encoder-decoder models and enables efficient preservation of the dialogue history information. By incorporating a separate memory module alongside the pre-trained transformer, the model can effectively interchange information between the memory states and the current input context. We evaluate our model on three dialogue datasets and two language modeling datasets. Experimental results show that our method has achieved superior efficiency and performance compared to other pre-trained Transformer baselines.",
    "authors": [
      "Qingyang Wu",
      "Zhou Yu"
    ],
    "published": "2022-09-15T22:37:22+00:00",
    "url": "http://arxiv.org/pdf/2209.07634v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2212.14678v1",
    "title": "Exploring Transformer Backbones for Image Diffusion Models",
    "content": "Exploring Transformer Backbones for Image Diffusion Models. We present an end-to-end Transformer based Latent Diffusion model for image synthesis. On the ImageNet class conditioned generation task we show that a Transformer based Latent Diffusion model achieves a 14.1FID which is comparable to the 13.1FID score of a UNet based architecture. In addition to showing the application of Transformer models for Diffusion based image synthesis this simplification in architecture allows easy fusion and modeling of text and image data. The multi-head attention mechanism of Transformers enables simplified interaction between the image and text features which removes the requirement for crossattention mechanism in UNet based Diffusion models.",
    "authors": [
      "Princy Chahal"
    ],
    "published": "2022-12-27T07:05:14+00:00",
    "url": "http://arxiv.org/pdf/2212.14678v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1312.3482v1",
    "title": "Bayesian transformation family selection: moving towards a transformed Gaussian universe",
    "content": "Bayesian transformation family selection: moving towards a transformed Gaussian universe. The problem of transformation selection is thoroughly treated from a Bayesian perspective. Several families of transformations are considered with a view to achieving normality: the Box-Cox, the Modulus, the Yeo & Johnson and the Dual transformation. Markov chain Monte Carlo algorithms have been constructed in order to sample from the posterior distribution of the transformation parameter $\\lambda_T$ associated with each competing family $T$. We investigate different approaches to constructing compatible prior distributions for $\\lambda_T$ over alternative transformation families, using a unit-information power-prior approach and an alternative normal prior with approximate unit-information interpretation. Selection and discrimination between different transformation families is attained via posterior model probabilities. We demonstrate the efficiency of our approach using a variety of simulated datasets. Although there is no choice of transformation family that can be universally applied to all problems, empirical evidence suggests that some particular data structures are best treated by specific transformation families. For example, skewness is associated with the Box-Cox family while fat-tailed distributions are efficiently treated using the Modulus transformation.",
    "authors": [
      "Efstratia Charitidou",
      "Dimitris Fouskakis",
      "Ioannis Ntzoufras"
    ],
    "published": "2013-12-12T13:47:05+00:00",
    "url": "http://arxiv.org/pdf/1312.3482v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2309.14700v1",
    "title": "Structure Invariant Transformation for better Adversarial Transferability",
    "content": "Structure Invariant Transformation for better Adversarial Transferability. Given the severe vulnerability of Deep Neural Networks (DNNs) against adversarial examples, there is an urgent need for an effective adversarial attack to identify the deficiencies of DNNs in security-sensitive applications. As one of the prevalent black-box adversarial attacks, the existing transfer-based attacks still cannot achieve comparable performance with the white-box attacks. Among these, input transformation based attacks have shown remarkable effectiveness in boosting transferability. In this work, we find that the existing input transformation based attacks transform the input image globally, resulting in limited diversity of the transformed images. We postulate that the more diverse transformed images result in better transferability. Thus, we investigate how to locally apply various transformations onto the input image to improve such diversity while preserving the structure of image. To this end, we propose a novel input transformation based attack, called Structure Invariant Attack (SIA), which applies a random image transformation onto each image block to craft a set of diverse images for gradient calculation. Extensive experiments on the standard ImageNet dataset demonstrate that SIA exhibits much better transferability than the existing SOTA input transformation based attacks on CNN-based and transformer-based models, showing its generality and superiority in boosting transferability. Code is available at https://github.com/xiaosen-wang/SIT.",
    "authors": [
      "Xiaosen Wang",
      "Zeliang Zhang",
      "Jianping Zhang"
    ],
    "published": "2023-09-26T06:31:32+00:00",
    "url": "http://arxiv.org/pdf/2309.14700v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2310.16065v1",
    "title": "The Hyperdimensional Transform: a Holographic Representation of Functions",
    "content": "The Hyperdimensional Transform: a Holographic Representation of Functions. Integral transforms are invaluable mathematical tools to map functions into spaces where they are easier to characterize. We introduce the hyperdimensional transform as a new kind of integral transform. It converts square-integrable functions into noise-robust, holographic, high-dimensional representations called hyperdimensional vectors. The central idea is to approximate a function by a linear combination of random functions. We formally introduce a set of stochastic, orthogonal basis functions and define the hyperdimensional transform and its inverse. We discuss general transform-related properties such as its uniqueness, approximation properties of the inverse transform, and the representation of integrals and derivatives. The hyperdimensional transform offers a powerful, flexible framework that connects closely with other integral transforms, such as the Fourier, Laplace, and fuzzy transforms. Moreover, it provides theoretical foundations and new insights for the field of hyperdimensional computing, a computing paradigm that is rapidly gaining attention for efficient and explainable machine learning algorithms, with potential applications in statistical modelling and machine learning. In addition, we provide straightforward and easily understandable code, which can function as a tutorial and allows for the reproduction of the demonstrated examples, from computing the transform to solving differential equations.",
    "authors": [
      "Pieter Dewulf",
      "Michiel Stock",
      "Bernard De Baets"
    ],
    "published": "2023-10-24T11:33:39+00:00",
    "url": "http://arxiv.org/pdf/2310.16065v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2302.09108v1",
    "title": "ViTA: A Vision Transformer Inference Accelerator for Edge Applications",
    "content": "ViTA: A Vision Transformer Inference Accelerator for Edge Applications. Vision Transformer models, such as ViT, Swin Transformer, and Transformer-in-Transformer, have recently gained significant traction in computer vision tasks due to their ability to capture the global relation between features which leads to superior performance. However, they are compute-heavy and difficult to deploy in resource-constrained edge devices. Existing hardware accelerators, including those for the closely-related BERT transformer models, do not target highly resource-constrained environments. In this paper, we address this gap and propose ViTA - a configurable hardware accelerator for inference of vision transformer models, targeting resource-constrained edge computing devices and avoiding repeated off-chip memory accesses. We employ a head-level pipeline and inter-layer MLP optimizations, and can support several commonly used vision transformer models with changes solely in our control logic. We achieve nearly 90% hardware utilization efficiency on most vision transformer models, report a power of 0.88W when synthesised with a clock of 150 MHz, and get reasonable frame rates - all of which makes ViTA suitable for edge applications.",
    "authors": [
      "Shashank Nag",
      "Gourav Datta",
      "Souvik Kundu",
      "Nitin Chandrachoodan",
      "Peter A. Beerel"
    ],
    "published": "2023-02-17T19:35:36+00:00",
    "url": "http://arxiv.org/pdf/2302.09108v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2405.19629v1",
    "title": "YotoR-You Only Transform One Representation",
    "content": "YotoR-You Only Transform One Representation. This paper introduces YotoR (You Only Transform One Representation), a novel deep learning model for object detection that combines Swin Transformers and YoloR architectures. Transformers, a revolutionary technology in natural language processing, have also significantly impacted computer vision, offering the potential to enhance accuracy and computational efficiency. YotoR combines the robust Swin Transformer backbone with the YoloR neck and head. In our experiments, YotoR models TP5 and BP4 consistently outperform YoloR P6 and Swin Transformers in various evaluations, delivering improved object detection performance and faster inference speeds than Swin Transformer models. These results highlight the potential for further model combinations and improvements in real-time object detection with Transformers. The paper concludes by emphasizing the broader implications of YotoR, including its potential to enhance transformer-based models for image-related tasks.",
    "authors": [
      "José Ignacio Díaz Villa",
      "Patricio Loncomilla",
      "Javier Ruiz-del-Solar"
    ],
    "published": "2024-05-30T02:27:56+00:00",
    "url": "http://arxiv.org/pdf/2405.19629v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2406.04089v1",
    "title": "On Limitation of Transformer for Learning HMMs",
    "content": "On Limitation of Transformer for Learning HMMs. Despite the remarkable success of Transformer-based architectures in various sequential modeling tasks, such as natural language processing, computer vision, and robotics, their ability to learn basic sequential models, like Hidden Markov Models (HMMs), is still unclear. This paper investigates the performance of Transformers in learning HMMs and their variants through extensive experimentation and compares them to Recurrent Neural Networks (RNNs). We show that Transformers consistently underperform RNNs in both training speed and testing accuracy across all tested HMM models. There are even challenging HMM instances where Transformers struggle to learn, while RNNs can successfully do so. Our experiments further reveal the relation between the depth of Transformers and the longest sequence length it can effectively learn, based on the types and the complexity of HMMs. To address the limitation of transformers in modeling HMMs, we demonstrate that a variant of the Chain-of-Thought (CoT), called $\\textit{block CoT}$ in the training phase, can help transformers to reduce the evaluation error and to learn longer sequences at a cost of increasing the training time. Finally, we complement our empirical findings by theoretical results proving the expressiveness of transformers in approximating HMMs with logarithmic depth.",
    "authors": [
      "Jiachen Hu",
      "Qinghua Liu",
      "Chi Jin"
    ],
    "published": "2024-06-06T13:59:51+00:00",
    "url": "http://arxiv.org/pdf/2406.04089v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2503.02130v1",
    "title": "Forgetting Transformer: Softmax Attention with a Forget Gate",
    "content": "Forgetting Transformer: Softmax Attention with a Forget Gate. An essential component of modern recurrent sequence models is the forget gate. While Transformers do not have an explicit recurrent form, we show that a forget gate can be naturally incorporated into Transformers by down-weighting the unnormalized attention scores in a data-dependent way. We name this attention mechanism the Forgetting Attention and the resulting model the Forgetting Transformer (FoX). We show that FoX outperforms the Transformer on long-context language modeling, length extrapolation, and short-context downstream tasks, while performing on par with the Transformer on long-context downstream tasks. Moreover, it is compatible with the FlashAttention algorithm and does not require any positional embeddings. Several analyses, including the needle-in-the-haystack test, show that FoX also retains the Transformer's superior long-context capabilities over recurrent sequence models such as Mamba-2, HGRN2, and DeltaNet. We also introduce a \"Pro\" block design that incorporates some common architectural components in recurrent sequence models and find it significantly improves the performance of both FoX and the Transformer. Our code is available at https://github.com/zhixuan-lin/forgetting-transformer.",
    "authors": [
      "Zhixuan Lin",
      "Evgenii Nikishin",
      "Xu Owen He",
      "Aaron Courville"
    ],
    "published": "2025-03-03T23:35:23+00:00",
    "url": "http://arxiv.org/pdf/2503.02130v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1706.08269v2",
    "title": "Top-down Transformation Choice",
    "content": "Top-down Transformation Choice. Simple models are preferred over complex models, but over-simplistic models could lead to erroneous interpretations. The classical approach is to start with a simple model, whose shortcomings are assessed in residual-based model diagnostics. Eventually, one increases the complexity of this initial overly simple model and obtains a better-fitting model. I illustrate how transformation analysis can be used as an alternative approach to model choice. Instead of adding complexity to simple models, step-wise complexity reduction is used to help identify simpler and better-interpretable models. As an example, body mass index distributions in Switzerland are modelled by means of transformation models to understand the impact of sex, age, smoking and other lifestyle factors on a person's body mass index. In this process, I searched for a compromise between model fit and model interpretability. Special emphasis is given to the understanding of the connections between transformation models of increasing complexity. The models used in this analysis ranged from evergreens, such as the normal linear regression model with constant variance, to novel models with extremely flexible conditional distribution functions, such as transformation trees and transformation forests.",
    "authors": [
      "Torsten Hothorn"
    ],
    "published": "2017-06-26T08:08:01+00:00",
    "url": "http://arxiv.org/pdf/1706.08269v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/cond-mat/0004005v1",
    "title": "Rigorous Schrieffer-Wolff transformation for the Anderson impurity model",
    "content": "Rigorous Schrieffer-Wolff transformation for the Anderson impurity model. With the help of computer algebra, I devise an exact unitary transformation for the Anderson impurity model which allows to kill the hybridization term in the slightly simplified case of zero chemical potential. Then I compute explicitly the outcome of this transformation. This is a rigorous version of the well known Schrieffer-Wolff transformation. It should be possible to treat the general case at the price of increased computation time.",
    "authors": [
      "Gilles Poirot"
    ],
    "published": "2000-04-01T13:06:42+00:00",
    "url": "http://arxiv.org/pdf/cond-mat/0004005v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/cond-mat/0308405v1",
    "title": "Exact Unitary Transformation of the One-Dimensional Periodic Anderson Model",
    "content": "Exact Unitary Transformation of the One-Dimensional Periodic Anderson Model. An effective hamiltonian is derived exactly for the one-dimensional periodic Anderson model via a canonical transformation. The canonical transformation has been calculated up to infinite order, thus an exact transformation was performed in the strict mathematical sense. We also discuss briefly the impact of the obtained result on understanding the magnetic properties of several Kondo lattice compounds.",
    "authors": [
      "Raymond Chan",
      "Miklos Gulacsi"
    ],
    "published": "2003-08-20T15:15:12+00:00",
    "url": "http://arxiv.org/pdf/cond-mat/0308405v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1312.0349v1",
    "title": "Solving the Class Diagram Restructuring Transformation Case with FunnyQT",
    "content": "Solving the Class Diagram Restructuring Transformation Case with FunnyQT. FunnyQT is a model querying and model transformation library for the functional Lisp-dialect Clojure providing a rich and efficient querying and transformation API.   This paper describes the FunnyQT solution to the TTC 2013 Class Diagram Restructuring Transformation Case. This solution and the GROOVE solution share the best overall solution award for this case.",
    "authors": [
      "Tassilo Horn"
    ],
    "published": "2013-12-02T07:00:16+00:00",
    "url": "http://arxiv.org/pdf/1312.0349v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1312.0351v1",
    "title": "Solving the Petri-Nets to Statecharts Transformation Case with FunnyQT",
    "content": "Solving the Petri-Nets to Statecharts Transformation Case with FunnyQT. FunnyQT is a model querying and model transformation library for the functional Lisp-dialect Clojure providing a rich and efficient querying and transformation API.   This paper describes the FunnyQT solution to the TTC 2013 Petri-Nets to Statcharts Transformation Case. This solution has won the best overall solution award and the best efficiency award for this case.",
    "authors": [
      "Tassilo Horn"
    ],
    "published": "2013-12-02T07:00:41+00:00",
    "url": "http://arxiv.org/pdf/1312.0351v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1611.03858v1",
    "title": "Few quantum mechanical models in higher dimension-framed with Elzaki transform",
    "content": "Few quantum mechanical models in higher dimension-framed with Elzaki transform. Very first time Elzaki transform is used in non relativistic quantum mechanics to solve $N$-dimensional Schr\\\"{o}dinger equation in a closed form for different solvable potential models. A universal transformation scheme is introduced and a formula based approach is developed which shows how to apply Elzaki transform to differential equation with non-constant coefficients that generally appear in solving quantum mechanical initial value problems specially for multidimensional Schr\\\"{o}dinger equation.",
    "authors": [
      "Tapas Das"
    ],
    "published": "2016-11-11T16:34:58+00:00",
    "url": "http://arxiv.org/pdf/1611.03858v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2410.00038v1",
    "title": "A Novel Spinor-Based Embedding Model for Transformers",
    "content": "A Novel Spinor-Based Embedding Model for Transformers. This paper proposes a novel approach to word embeddings in Transformer models by utilizing spinors from geometric algebra. Spinors offer a rich mathematical framework capable of capturing complex relationships and transformations in high-dimensional spaces. By encoding words as spinors, we aim to enhance the expressiveness and robustness of language representations. We present the theoretical foundations of spinors, detail their integration into Transformer architectures, and discuss potential advantages and challenges.",
    "authors": [
      "Rick White"
    ],
    "published": "2024-09-26T01:18:45+00:00",
    "url": "http://arxiv.org/pdf/2410.00038v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2007.06257v2",
    "title": "Rewiring the Transformer with Depth-Wise LSTMs",
    "content": "Rewiring the Transformer with Depth-Wise LSTMs. Stacking non-linear layers allows deep neural networks to model complicated functions, and including residual connections in Transformer layers is beneficial for convergence and performance. However, residual connections may make the model \"forget\" distant layers and fail to fuse information from previous layers effectively. Selectively managing the representation aggregation of Transformer layers may lead to better performance. In this paper, we present a Transformer with depth-wise LSTMs connecting cascading Transformer layers and sub-layers. We show that layer normalization and feed-forward computation within a Transformer layer can be absorbed into depth-wise LSTMs connecting pure Transformer attention layers. Our experiments with the 6-layer Transformer show significant BLEU improvements in both WMT 14 English-German / French tasks and the OPUS-100 many-to-many multilingual NMT task, and our deep Transformer experiments demonstrate the effectiveness of depth-wise LSTM on the convergence and performance of deep Transformers.",
    "authors": [
      "Hongfei Xu",
      "Yang Song",
      "Qiuhui Liu",
      "Josef van Genabith",
      "Deyi Xiong"
    ],
    "published": "2020-07-13T09:19:34+00:00",
    "url": "http://arxiv.org/pdf/2007.06257v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2110.03921v2",
    "title": "ViDT: An Efficient and Effective Fully Transformer-based Object Detector",
    "content": "ViDT: An Efficient and Effective Fully Transformer-based Object Detector. Transformers are transforming the landscape of computer vision, especially for recognition tasks. Detection transformers are the first fully end-to-end learning systems for object detection, while vision transformers are the first fully transformer-based architecture for image classification. In this paper, we integrate Vision and Detection Transformers (ViDT) to build an effective and efficient object detector. ViDT introduces a reconfigured attention module to extend the recent Swin Transformer to be a standalone object detector, followed by a computationally efficient transformer decoder that exploits multi-scale features and auxiliary techniques essential to boost the detection performance without much increase in computational load. Extensive evaluation results on the Microsoft COCO benchmark dataset demonstrate that ViDT obtains the best AP and latency trade-off among existing fully transformer-based object detectors, and achieves 49.2AP owing to its high scalability for large models. We will release the code and trained models at https://github.com/naver-ai/vidt",
    "authors": [
      "Hwanjun Song",
      "Deqing Sun",
      "Sanghyuk Chun",
      "Varun Jampani",
      "Dongyoon Han",
      "Byeongho Heo",
      "Wonjae Kim",
      "Ming-Hsuan Yang"
    ],
    "published": "2021-10-08T06:32:05+00:00",
    "url": "http://arxiv.org/pdf/2110.03921v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2303.04774v1",
    "title": "Projective cluster-additive transformation for quantum lattice models",
    "content": "Projective cluster-additive transformation for quantum lattice models. We construct a projection-based cluster-additive transformation that block-diagonalizes wide classes of lattice Hamiltonians $\\mathcal{H}=\\mathcal{H}_0 +V$. Its cluster additivity is an essential ingredient to set up perturbative or non-perturbative linked-cluster expansions for degenerate excitation subspaces of $\\mathcal{H}_0$. Our transformation generalizes the minimal transformation known amongst others under the names Takahashi's transformation, Schrieffer-Wolff transformation, des Cloiseaux effective Hamiltonian, canonical van Vleck effective Hamiltonian or two-block orthogonalization method. The effective cluster-additive Hamiltonian and the transformation for a given subspace of $\\mathcal{H}$, that is adiabatically connected to the eigenspace of $\\mathcal{H}_0$ with eigenvalue $e_0^n$, solely depends on the eigenspaces of $\\mathcal{H}$ connected to $e_0^m$ with $e_0^m\\leq e_0^n$. In contrast, other cluster-additive transformations like the multi-block orthognalization method or perturbative continuous unitary transformations need a larger basis. This can be exploited to implement the transformation efficiently both perturbatively and non-perturbatively. As a benchmark, we perform perturbative and non-perturbative linked-cluster expansions in the low-field ordered phase of the transverse-field Ising model on the square lattice for single spin-flips and two spin-flip bound-states.",
    "authors": [
      "M. Hörmann",
      "K. P. Schmidt"
    ],
    "published": "2023-03-08T18:13:26+00:00",
    "url": "http://arxiv.org/pdf/2303.04774v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2307.09402v1",
    "title": "Study of Vision Transformers for Covid-19 Detection from Chest X-rays",
    "content": "Study of Vision Transformers for Covid-19 Detection from Chest X-rays. The COVID-19 pandemic has led to a global health crisis, highlighting the need for rapid and accurate virus detection. This research paper examines transfer learning with vision transformers for COVID-19 detection, known for its excellent performance in image recognition tasks. We leverage the capability of Vision Transformers to capture global context and learn complex patterns from chest X-ray images. In this work, we explored the recent state-of-art transformer models to detect Covid-19 using CXR images such as vision transformer (ViT), Swin-transformer, Max vision transformer (MViT), and Pyramid Vision transformer (PVT). Through the utilization of transfer learning with IMAGENET weights, the models achieved an impressive accuracy range of 98.75% to 99.5%. Our experiments demonstrate that Vision Transformers achieve state-of-the-art performance in COVID-19 detection, outperforming traditional methods and even Convolutional Neural Networks (CNNs). The results highlight the potential of Vision Transformers as a powerful tool for COVID-19 detection, with implications for improving the efficiency and accuracy of screening and diagnosis in clinical settings.",
    "authors": [
      "Sandeep Angara",
      "Sharath Thirunagaru"
    ],
    "published": "2023-07-17T14:06:07+00:00",
    "url": "http://arxiv.org/pdf/2307.09402v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1111.4739v1",
    "title": "HelloWorld! An Instructive Case for the Transformation Tool Contest",
    "content": "HelloWorld! An Instructive Case for the Transformation Tool Contest. This case comprises several primitive tasks that can be solved straight away with most transformation tools. The aim is to cover the most important kinds of primitive operations on models, i.e. create, read, update and delete (CRUD). To this end, tasks such as a constant transformation, a model-to-text transformation, a very basic migration transformation or diverse simple queries or in-place operations on graphs have to be solved.   The motivation for this case is that the results expectedly will be very instructive for beginners. Also, it is really hard to compare transformation languages along complex cases, because the complexity of the respective case might hide the basic language concepts and constructs.",
    "authors": [
      "Steffen Mazanek"
    ],
    "published": "2011-11-21T05:24:36+00:00",
    "url": "http://arxiv.org/pdf/1111.4739v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2101.07918v1",
    "title": "PGT: Pseudo Relevance Feedback Using a Graph-Based Transformer",
    "content": "PGT: Pseudo Relevance Feedback Using a Graph-Based Transformer. Most research on pseudo relevance feedback (PRF) has been done in vector space and probabilistic retrieval models. This paper shows that Transformer-based rerankers can also benefit from the extra context that PRF provides. It presents PGT, a graph-based Transformer that sparsifies attention between graph nodes to enable PRF while avoiding the high computational complexity of most Transformer architectures. Experiments show that PGT improves upon non-PRF Transformer reranker, and it is at least as accurate as Transformer PRF models that use full attention, but with lower computational costs.",
    "authors": [
      "HongChien Yu",
      "Zhuyun Dai",
      "Jamie Callan"
    ],
    "published": "2021-01-20T01:07:47+00:00",
    "url": "http://arxiv.org/pdf/2101.07918v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1312.0344v1",
    "title": "An NMF solution for the Flowgraphs case at the TTC 2013",
    "content": "An NMF solution for the Flowgraphs case at the TTC 2013. Software systems are getting more and more complex. Model-driven engineering (MDE) offers ways to handle such increased complexity by lifting development to a higher level of abstraction. A key part in MDE are transformations that transform any given model into another. These transformations are used to generate all kinds of software artifacts from models. However, there is little consensus about the transformation tools. Thus, the Transformation Tool Contest (TTC) 2013 aims to compare different transformation engines. This is achieved through three different cases that have to be tackled. One of these cases is the Flowgraphs case. A solution has to transform a Java code model into a simplified version and has to derive control and data flow. This paper presents the solution for this case using NMF Transformations as transformation engine.",
    "authors": [
      "Georg Hinkel",
      "Thomas Goldschmidt",
      "Lucia Happe"
    ],
    "published": "2013-12-02T06:59:30+00:00",
    "url": "http://arxiv.org/pdf/1312.0344v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2111.12873v1",
    "title": "Quantised Transforming Auto-Encoders: Achieving Equivariance to Arbitrary Transformations in Deep Networks",
    "content": "Quantised Transforming Auto-Encoders: Achieving Equivariance to Arbitrary Transformations in Deep Networks. In this work we investigate how to achieve equivariance to input transformations in deep networks, purely from data, without being given a model of those transformations. Convolutional Neural Networks (CNNs), for example, are equivariant to image translation, a transformation that can be easily modelled (by shifting the pixels vertically or horizontally). Other transformations, such as out-of-plane rotations, do not admit a simple analytic model. We propose an auto-encoder architecture whose embedding obeys an arbitrary set of equivariance relations simultaneously, such as translation, rotation, colour changes, and many others. This means that it can take an input image, and produce versions transformed by a given amount that were not observed before (e.g. a different point of view of the same object, or a colour variation). Despite extending to many (even non-geometric) transformations, our model reduces exactly to a CNN in the special case of translation-equivariance. Equivariances are important for the interpretability and robustness of deep networks, and we demonstrate results of successful re-rendering of transformed versions of input images on several synthetic and real datasets, as well as results on object pose estimation.",
    "authors": [
      "Jianbo Jiao",
      "João F. Henriques"
    ],
    "published": "2021-11-25T02:26:38+00:00",
    "url": "http://arxiv.org/pdf/2111.12873v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2110.00685v2",
    "title": "Fast Multi-Resolution Transformer Fine-tuning for Extreme Multi-label Text Classification",
    "content": "Fast Multi-Resolution Transformer Fine-tuning for Extreme Multi-label Text Classification. Extreme multi-label text classification (XMC) seeks to find relevant labels from an extreme large label collection for a given text input. Many real-world applications can be formulated as XMC problems, such as recommendation systems, document tagging and semantic search. Recently, transformer based XMC methods, such as X-Transformer and LightXML, have shown significant improvement over other XMC methods. Despite leveraging pre-trained transformer models for text representation, the fine-tuning procedure of transformer models on large label space still has lengthy computational time even with powerful GPUs. In this paper, we propose a novel recursive approach, XR-Transformer to accelerate the procedure through recursively fine-tuning transformer models on a series of multi-resolution objectives related to the original XMC objective function. Empirical results show that XR-Transformer takes significantly less training time compared to other transformer-based XMC models while yielding better state-of-the-art results. In particular, on the public Amazon-3M dataset with 3 million labels, XR-Transformer is not only 20x faster than X-Transformer but also improves the Precision@1 from 51% to 54%.",
    "authors": [
      "Jiong Zhang",
      "Wei-cheng Chang",
      "Hsiang-fu Yu",
      "Inderjit S. Dhillon"
    ],
    "published": "2021-10-01T23:43:29+00:00",
    "url": "http://arxiv.org/pdf/2110.00685v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2203.12944v1",
    "title": "Transformers Meet Visual Learning Understanding: A Comprehensive Review",
    "content": "Transformers Meet Visual Learning Understanding: A Comprehensive Review. Dynamic attention mechanism and global modeling ability make Transformer show strong feature learning ability. In recent years, Transformer has become comparable to CNNs methods in computer vision. This review mainly investigates the current research progress of Transformer in image and video applications, which makes a comprehensive overview of Transformer in visual learning understanding. First, the attention mechanism is reviewed, which plays an essential part in Transformer. And then, the visual Transformer model and the principle of each module are introduced. Thirdly, the existing Transformer-based models are investigated, and their performance is compared in visual learning understanding applications. Three image tasks and two video tasks of computer vision are investigated. The former mainly includes image classification, object detection, and image segmentation. The latter contains object tracking and video classification. It is significant for comparing different models' performance in various tasks on several public benchmark data sets. Finally, ten general problems are summarized, and the developing prospects of the visual Transformer are given in this review.",
    "authors": [
      "Yuting Yang",
      "Licheng Jiao",
      "Xu Liu",
      "Fang Liu",
      "Shuyuan Yang",
      "Zhixi Feng",
      "Xu Tang"
    ],
    "published": "2022-03-24T09:09:00+00:00",
    "url": "http://arxiv.org/pdf/2203.12944v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2211.12316v2",
    "title": "Simplicity Bias in Transformers and their Ability to Learn Sparse Boolean Functions",
    "content": "Simplicity Bias in Transformers and their Ability to Learn Sparse Boolean Functions. Despite the widespread success of Transformers on NLP tasks, recent works have found that they struggle to model several formal languages when compared to recurrent models. This raises the question of why Transformers perform well in practice and whether they have any properties that enable them to generalize better than recurrent models. In this work, we conduct an extensive empirical study on Boolean functions to demonstrate the following: (i) Random Transformers are relatively more biased towards functions of low sensitivity. (ii) When trained on Boolean functions, both Transformers and LSTMs prioritize learning functions of low sensitivity, with Transformers ultimately converging to functions of lower sensitivity. (iii) On sparse Boolean functions which have low sensitivity, we find that Transformers generalize near perfectly even in the presence of noisy labels whereas LSTMs overfit and achieve poor generalization accuracy. Overall, our results provide strong quantifiable evidence that suggests differences in the inductive biases of Transformers and recurrent models which may help explain Transformer's effective generalization performance despite relatively limited expressiveness.",
    "authors": [
      "Satwik Bhattamishra",
      "Arkil Patel",
      "Varun Kanade",
      "Phil Blunsom"
    ],
    "published": "2022-11-22T15:10:48+00:00",
    "url": "http://arxiv.org/pdf/2211.12316v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2303.00957v1",
    "title": "Preference Transformer: Modeling Human Preferences using Transformers for RL",
    "content": "Preference Transformer: Modeling Human Preferences using Transformers for RL. Preference-based reinforcement learning (RL) provides a framework to train agents using human preferences between two behaviors. However, preference-based RL has been challenging to scale since it requires a large amount of human feedback to learn a reward function aligned with human intent. In this paper, we present Preference Transformer, a neural architecture that models human preferences using transformers. Unlike prior approaches assuming human judgment is based on the Markovian rewards which contribute to the decision equally, we introduce a new preference model based on the weighted sum of non-Markovian rewards. We then design the proposed preference model using a transformer architecture that stacks causal and bidirectional self-attention layers. We demonstrate that Preference Transformer can solve a variety of control tasks using real human preferences, while prior approaches fail to work. We also show that Preference Transformer can induce a well-specified reward and attend to critical events in the trajectory by automatically capturing the temporal dependencies in human decision-making. Code is available on the project website: https://sites.google.com/view/preference-transformer.",
    "authors": [
      "Changyeon Kim",
      "Jongjin Park",
      "Jinwoo Shin",
      "Honglak Lee",
      "Pieter Abbeel",
      "Kimin Lee"
    ],
    "published": "2023-03-02T04:24:29+00:00",
    "url": "http://arxiv.org/pdf/2303.00957v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1002.3078v1",
    "title": "Using ATL to define advanced and flexible constraint model transformations",
    "content": "Using ATL to define advanced and flexible constraint model transformations. Transforming constraint models is an important task in re- cent constraint programming systems. User-understandable models are defined during the modeling phase but rewriting or tuning them is manda- tory to get solving-efficient models. We propose a new architecture al- lowing to define bridges between any (modeling or solver) languages and to implement model optimizations. This architecture follows a model- driven approach where the constraint modeling process is seen as a set of model transformations. Among others, an interesting feature is the def- inition of transformations as concept-oriented rules, i.e. based on types of model elements where the types are organized into a hierarchy called a metamodel.",
    "authors": [
      "Raphael Chenouard",
      "Laurent Granvilliers",
      "Ricardo Soto"
    ],
    "published": "2010-02-16T13:09:07+00:00",
    "url": "http://arxiv.org/pdf/1002.3078v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1810.08323v1",
    "title": "Learning Multi-Layer Transform Models",
    "content": "Learning Multi-Layer Transform Models. Learned data models based on sparsity are widely used in signal processing and imaging applications. A variety of methods for learning synthesis dictionaries, sparsifying transforms, etc., have been proposed in recent years, often imposing useful structures or properties on the models. In this work, we focus on sparsifying transform learning, which enjoys a number of advantages. We consider multi-layer or nested extensions of the transform model, and propose efficient learning algorithms. Numerical experiments with image data illustrate the behavior of the multi-layer transform learning algorithm and its usefulness for image denoising. Multi-layer models provide better denoising quality than single layer schemes.",
    "authors": [
      "Saiprasad Ravishankar",
      "Brendt Wohlberg"
    ],
    "published": "2018-10-19T00:56:42+00:00",
    "url": "http://arxiv.org/pdf/1810.08323v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2110.04337v1",
    "title": "Adversarial Token Attacks on Vision Transformers",
    "content": "Adversarial Token Attacks on Vision Transformers. Vision transformers rely on a patch token based self attention mechanism, in contrast to convolutional networks. We investigate fundamental differences between these two families of models, by designing a block sparsity based adversarial token attack. We probe and analyze transformer as well as convolutional models with token attacks of varying patch sizes. We infer that transformer models are more sensitive to token attacks than convolutional models, with ResNets outperforming Transformer models by up to $\\sim30\\%$ in robust accuracy for single token attacks.",
    "authors": [
      "Ameya Joshi",
      "Gauri Jagatap",
      "Chinmay Hegde"
    ],
    "published": "2021-10-08T19:00:16+00:00",
    "url": "http://arxiv.org/pdf/2110.04337v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2105.00827v2",
    "title": "AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models",
    "content": "AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models. Transformer-based pretrained language models (PLMs) have started a new era in modern natural language processing (NLP). These models combine the power of transformers, transfer learning, and self-supervised learning (SSL). Following the success of these models in the general domain, the biomedical research community has developed various in-domain PLMs starting from BioBERT to the latest BioELECTRA and BioALBERT models. We strongly believe there is a need for a survey paper that can provide a comprehensive survey of various transformer-based biomedical pretrained language models (BPLMs). In this survey, we start with a brief overview of foundational concepts like self-supervised learning, embedding layer and transformer encoder layers. We discuss core concepts of transformer-based PLMs like pretraining methods, pretraining tasks, fine-tuning methods, and various embedding types specific to biomedical domain. We introduce a taxonomy for transformer-based BPLMs and then discuss all the models. We discuss various challenges and present possible solutions. We conclude by highlighting some of the open issues which will drive the research community to further improve transformer-based BPLMs.",
    "authors": [
      "Katikapalli Subramanyam Kalyan",
      "Ajit Rajasekharan",
      "Sivanesan Sangeetha"
    ],
    "published": "2021-04-16T18:09:51+00:00",
    "url": "http://arxiv.org/pdf/2105.00827v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2108.02598v1",
    "title": "Knowledge Distillation from BERT Transformer to Speech Transformer for Intent Classification",
    "content": "Knowledge Distillation from BERT Transformer to Speech Transformer for Intent Classification. End-to-end intent classification using speech has numerous advantages compared to the conventional pipeline approach using automatic speech recognition (ASR), followed by natural language processing modules. It attempts to predict intent from speech without using an intermediate ASR module. However, such end-to-end framework suffers from the unavailability of large speech resources with higher acoustic variation in spoken language understanding. In this work, we exploit the scope of the transformer distillation method that is specifically designed for knowledge distillation from a transformer based language model to a transformer based speech model. In this regard, we leverage the reliable and widely used bidirectional encoder representations from transformers (BERT) model as a language model and transfer the knowledge to build an acoustic model for intent classification using the speech. In particular, a multilevel transformer based teacher-student model is designed, and knowledge distillation is performed across attention and hidden sub-layers of different transformer layers of the student and teacher models. We achieve an intent classification accuracy of 99.10% and 88.79% for Fluent speech corpus and ATIS database, respectively. Further, the proposed method demonstrates better performance and robustness in acoustically degraded condition compared to the baseline method.",
    "authors": [
      "Yidi Jiang",
      "Bidisha Sharma",
      "Maulik Madhavi",
      "Haizhou Li"
    ],
    "published": "2021-08-05T13:08:13+00:00",
    "url": "http://arxiv.org/pdf/2108.02598v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2210.12599v2",
    "title": "Transformers For Recognition In Overhead Imagery: A Reality Check",
    "content": "Transformers For Recognition In Overhead Imagery: A Reality Check. There is evidence that transformers offer state-of-the-art recognition performance on tasks involving overhead imagery (e.g., satellite imagery). However, it is difficult to make unbiased empirical comparisons between competing deep learning models, making it unclear whether, and to what extent, transformer-based models are beneficial. In this paper we systematically compare the impact of adding transformer structures into state-of-the-art segmentation models for overhead imagery. Each model is given a similar budget of free parameters, and their hyperparameters are optimized using Bayesian Optimization with a fixed quantity of data and computation time. We conduct our experiments with a large and diverse dataset comprising two large public benchmarks: Inria and DeepGlobe. We perform additional ablation studies to explore the impact of specific transformer-based modeling choices. Our results suggest that transformers provide consistent, but modest, performance improvements. We only observe this advantage however in hybrid models that combine convolutional and transformer-based structures, while fully transformer-based models achieve relatively poor performance.",
    "authors": [
      "Francesco Luzi",
      "Aneesh Gupta",
      "Leslie Collins",
      "Kyle Bradbury",
      "Jordan Malof"
    ],
    "published": "2022-10-23T02:17:31+00:00",
    "url": "http://arxiv.org/pdf/2210.12599v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2211.14655v1",
    "title": "How Crucial is Transformer in Decision Transformer?",
    "content": "How Crucial is Transformer in Decision Transformer?. Decision Transformer (DT) is a recently proposed architecture for Reinforcement Learning that frames the decision-making process as an auto-regressive sequence modeling problem and uses a Transformer model to predict the next action in a sequence of states, actions, and rewards. In this paper, we analyze how crucial the Transformer model is in the complete DT architecture on continuous control tasks. Namely, we replace the Transformer by an LSTM model while keeping the other parts unchanged to obtain what we call a Decision LSTM model. We compare it to DT on continuous control tasks, including pendulum swing-up and stabilization, in simulation and on physical hardware. Our experiments show that DT struggles with continuous control problems, such as inverted pendulum and Furuta pendulum stabilization. On the other hand, the proposed Decision LSTM is able to achieve expert-level performance on these tasks, in addition to learning a swing-up controller on the real system. These results suggest that the strength of the Decision Transformer for continuous control tasks may lie in the overall sequential modeling architecture and not in the Transformer per se.",
    "authors": [
      "Max Siebenborn",
      "Boris Belousov",
      "Junning Huang",
      "Jan Peters"
    ],
    "published": "2022-11-26T20:13:22+00:00",
    "url": "http://arxiv.org/pdf/2211.14655v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2305.16554v1",
    "title": "Emergent Agentic Transformer from Chain of Hindsight Experience",
    "content": "Emergent Agentic Transformer from Chain of Hindsight Experience. Large transformer models powered by diverse data and model scale have dominated natural language modeling and computer vision and pushed the frontier of multiple AI areas. In reinforcement learning (RL), despite many efforts into transformer-based policies, a key limitation, however, is that current transformer-based policies cannot learn by directly combining information from multiple sub-optimal trials. In this work, we address this issue using recently proposed chain of hindsight to relabel experience, where we train a transformer on a sequence of trajectory experience ascending sorted according to their total rewards. Our method consists of relabelling target return of each trajectory to the maximum total reward among in sequence of trajectories and training an autoregressive model to predict actions conditioning on past states, actions, rewards, target returns, and task completion tokens, the resulting model, Agentic Transformer (AT), can learn to improve upon itself both at training and test time. As we show on D4RL and ExoRL benchmarks, to the best our knowledge, this is the first time that a simple transformer-based model performs competitively with both temporal-difference and imitation-learning-based approaches, even from sub-optimal data. Our Agentic Transformer also shows a promising scaling trend that bigger models consistently improve results.",
    "authors": [
      "Hao Liu",
      "Pieter Abbeel"
    ],
    "published": "2023-05-26T00:43:02+00:00",
    "url": "http://arxiv.org/pdf/2305.16554v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2405.02353v1",
    "title": "Early Transformers: A study on Efficient Training of Transformer Models through Early-Bird Lottery Tickets",
    "content": "Early Transformers: A study on Efficient Training of Transformer Models through Early-Bird Lottery Tickets. The training of Transformer models has revolutionized natural language processing and computer vision, but it remains a resource-intensive and time-consuming process. This paper investigates the applicability of the early-bird ticket hypothesis to optimize the training efficiency of Transformer models. We propose a methodology that combines iterative pruning, masked distance calculation, and selective retraining to identify early-bird tickets in various Transformer architectures, including ViT, Swin-T, GPT-2, and RoBERTa. Our experimental results demonstrate that early-bird tickets can be consistently found within the first few epochs of training or fine-tuning, enabling significant resource optimization without compromising performance. The pruned models obtained from early-bird tickets achieve comparable or even superior accuracy to their unpruned counterparts while substantially reducing memory usage. Furthermore, our comparative analysis highlights the generalizability of the early-bird ticket phenomenon across different Transformer models and tasks. This research contributes to the development of efficient training strategies for Transformer models, making them more accessible and resource-friendly. By leveraging early-bird tickets, practitioners can accelerate the progress of natural language processing and computer vision applications while reducing the computational burden associated with training Transformer models.",
    "authors": [
      "Shravan Cheekati"
    ],
    "published": "2024-05-02T23:03:45+00:00",
    "url": "http://arxiv.org/pdf/2405.02353v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2407.17406v1",
    "title": "Dependency Transformer Grammars: Integrating Dependency Structures into Transformer Language Models",
    "content": "Dependency Transformer Grammars: Integrating Dependency Structures into Transformer Language Models. Syntactic Transformer language models aim to achieve better generalization through simultaneously modeling syntax trees and sentences. While prior work has been focusing on adding constituency-based structures to Transformers, we introduce Dependency Transformer Grammars (DTGs), a new class of Transformer language model with explicit dependency-based inductive bias. DTGs simulate dependency transition systems with constrained attention patterns by modifying attention masks, incorporate the stack information through relative positional encoding, and augment dependency arc representation with a combination of token embeddings and operation embeddings. When trained on a dataset of sentences annotated with dependency trees, DTGs achieve better generalization while maintaining comparable perplexity with Transformer language model baselines. DTGs also outperform recent constituency-based models, showing that dependency can better guide Transformer language models. Our code is released at https://github.com/zhaoyd1/Dep_Transformer_Grammars.",
    "authors": [
      "Yida Zhao",
      "Chao Lou",
      "Kewei Tu"
    ],
    "published": "2024-07-24T16:38:38+00:00",
    "url": "http://arxiv.org/pdf/2407.17406v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2411.16993v1",
    "title": "Tree Transformers are an Ineffective Model of Syntactic Constituency",
    "content": "Tree Transformers are an Ineffective Model of Syntactic Constituency. Linguists have long held that a key aspect of natural language syntax is the recursive organization of language units into constituent structures, and research has suggested that current state-of-the-art language models lack an inherent bias towards this feature. A number of alternative models have been proposed to provide inductive biases towards constituency, including the Tree Transformer, which utilizes a modified attention mechanism to organize tokens into constituents.   We investigate Tree Transformers to study whether they utilize meaningful and/or useful constituent structures. We pretrain a large Tree Transformer on language modeling in order to investigate the learned constituent tree representations of sentences, finding little evidence for meaningful structures. Next, we evaluate Tree Transformers with similar transformer models on error detection tasks requiring constituent structure. We find that while the Tree Transformer models may slightly outperform at these tasks, there is little evidence to suggest a meaningful improvement. In general, we conclude that there is little evidence to support Tree Transformer as an effective model of syntactic constituency.",
    "authors": [
      "Michael Ginn"
    ],
    "published": "2024-11-25T23:53:46+00:00",
    "url": "http://arxiv.org/pdf/2411.16993v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2011.05431v1",
    "title": "E.T.: Entity-Transformers. Coreference augmented Neural Language Model for richer mention representations via Entity-Transformer blocks",
    "content": "E.T.: Entity-Transformers. Coreference augmented Neural Language Model for richer mention representations via Entity-Transformer blocks. In the last decade, the field of Neural Language Modelling has witnessed enormous changes, with the development of novel models through the use of Transformer architectures. However, even these models struggle to model long sequences due to memory constraints and increasing computational complexity. Coreference annotations over the training data can provide context far beyond the modelling limitations of such language models. In this paper we present an extension over the Transformer-block architecture used in neural language models, specifically in GPT2, in order to incorporate entity annotations during training. Our model, GPT2E, extends the Transformer layers architecture of GPT2 to Entity-Transformers, an architecture designed to handle coreference information when present. To that end, we achieve richer representations for entity mentions, with insignificant training cost. We show the comparative model performance between GPT2 and GPT2E in terms of Perplexity on the CoNLL 2012 and LAMBADA datasets as well as the key differences in the entity representations and their effects in downstream tasks such as Named Entity Recognition. Furthermore, our approach can be adopted by the majority of Transformer-based language models.",
    "authors": [
      "Nikolaos Stylianou",
      "Ioannis Vlahavas"
    ],
    "published": "2020-11-10T22:28:00+00:00",
    "url": "http://arxiv.org/pdf/2011.05431v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2010.15583v3",
    "title": "Probabilistic Transformers",
    "content": "Probabilistic Transformers. We show that Transformers are Maximum Posterior Probability estimators for Mixtures of Gaussian Models. This brings a probabilistic point of view to Transformers and suggests extensions to other probabilistic cases.",
    "authors": [
      "Javier R. Movellan",
      "Prasad Gabbur"
    ],
    "published": "2020-10-15T01:44:59+00:00",
    "url": "http://arxiv.org/pdf/2010.15583v3"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2412.11428v1",
    "title": "View Transformation Robustness for Multi-View 3D Object Reconstruction with Reconstruction Error-Guided View Selection",
    "content": "View Transformation Robustness for Multi-View 3D Object Reconstruction with Reconstruction Error-Guided View Selection. View transformation robustness (VTR) is critical for deep-learning-based multi-view 3D object reconstruction models, which indicates the methods' stability under inputs with various view transformations. However, existing research seldom focused on view transformation robustness in multi-view 3D object reconstruction. One direct way to improve the models' VTR is to produce data with more view transformations and add them to model training. Recent progress on large vision models, particularly Stable Diffusion models, has provided great potential for generating 3D models or synthesizing novel view images with only a single image input. Directly deploying these models at inference consumes heavy computation resources and their robustness to view transformations is not guaranteed either. To fully utilize the power of Stable Diffusion models without extra inference computation burdens, we propose to generate novel views with Stable Diffusion models for better view transformation robustness. Instead of synthesizing random views, we propose a reconstruction error-guided view selection method, which considers the reconstruction errors' spatial distribution of the 3D predictions and chooses the views that could cover the reconstruction errors as much as possible. The methods are trained and tested on sets with large view transformations to validate the 3D reconstruction models' robustness to view transformations. Extensive experiments demonstrate that the proposed method can outperform state-of-the-art 3D reconstruction methods and other view transformation robustness comparison methods.",
    "authors": [
      "Qi Zhang",
      "Zhouhang Luo",
      "Tao Yu",
      "Hui Huang"
    ],
    "published": "2024-12-16T03:54:08+00:00",
    "url": "http://arxiv.org/pdf/2412.11428v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2110.08774v1",
    "title": "Nonlinear Transform Induced Tensor Nuclear Norm for Tensor Completion",
    "content": "Nonlinear Transform Induced Tensor Nuclear Norm for Tensor Completion. The linear transform-based tensor nuclear norm (TNN) methods have recently obtained promising results for tensor completion. The main idea of this type of methods is exploiting the low-rank structure of frontal slices of the targeted tensor under the linear transform along the third mode. However, the low-rankness of frontal slices is not significant under linear transforms family. To better pursue the low-rank approximation, we propose a nonlinear transform-based TNN (NTTNN). More concretely, the proposed nonlinear transform is a composite transform consisting of the linear semi-orthogonal transform along the third mode and the element-wise nonlinear transform on frontal slices of the tensor under the linear semi-orthogonal transform, which are indispensable and complementary in the composite transform to fully exploit the underlying low-rankness. Based on the suggested low-rankness metric, i.e., NTTNN, we propose a low-rank tensor completion (LRTC) model. To tackle the resulting nonlinear and nonconvex optimization model, we elaborately design the proximal alternating minimization (PAM) algorithm and establish the theoretical convergence guarantee of the PAM algorithm. Extensive experimental results on hyperspectral images, multispectral images, and videos show that the our method outperforms linear transform-based state-of-the-art LRTC methods qualitatively and quantitatively.",
    "authors": [
      "Ben-Zheng Li",
      "Xi-Le Zhao",
      "Teng-Yu Ji",
      "Xiong-Jun Zhang",
      "Ting-Zhu Huang"
    ],
    "published": "2021-10-17T09:25:37+00:00",
    "url": "http://arxiv.org/pdf/2110.08774v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1907.01223v2",
    "title": "Specification testing in semi-parametric transformation models",
    "content": "Specification testing in semi-parametric transformation models. In transformation regression models the response is transformed before fitting a regression model to covariates and transformed response. We assume such a model where the errors are independent from the covariates and the regression function is modeled nonparametrically. We suggest a test for goodness-of-fit of a parametric transformation class based on a distance between a nonparametric transformation estimator and the parametric class. We present asymptotic theory under the null hypothesis of validity of the semi-parametric model and under local alternatives. A bootstrap algorithm is suggested in order to apply the test. We also consider relevant hypotheses to distinguish between large and small distances of the parametric transformation class to the `true' transformation.",
    "authors": [
      "Nick Kloodt",
      "Natalie Neumeyer",
      "Ingrid Van Keilegom"
    ],
    "published": "2019-07-02T08:13:13+00:00",
    "url": "http://arxiv.org/pdf/1907.01223v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2004.00464v1",
    "title": "Deep transformation models: Tackling complex regression problems with neural network based transformation models",
    "content": "Deep transformation models: Tackling complex regression problems with neural network based transformation models. We present a deep transformation model for probabilistic regression. Deep learning is known for outstandingly accurate predictions on complex data but in regression tasks, it is predominantly used to just predict a single number. This ignores the non-deterministic character of most tasks. Especially if crucial decisions are based on the predictions, like in medical applications, it is essential to quantify the prediction uncertainty. The presented deep learning transformation model estimates the whole conditional probability distribution, which is the most thorough way to capture uncertainty about the outcome. We combine ideas from a statistical transformation model (most likely transformation) with recent transformation models from deep learning (normalizing flows) to predict complex outcome distributions. The core of the method is a parameterized transformation function which can be trained with the usual maximum likelihood framework using gradient descent. The method can be combined with existing deep learning architectures. For small machine learning benchmark datasets, we report state of the art performance for most dataset and partly even outperform it. Our method works for complex input data, which we demonstrate by employing a CNN architecture on image data.",
    "authors": [
      "Beate Sick",
      "Torsten Hothorn",
      "Oliver Dürr"
    ],
    "published": "2020-04-01T14:23:12+00:00",
    "url": "http://arxiv.org/pdf/2004.00464v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2210.05781v1",
    "title": "Transforming RDF-star to Property Graphs: A Preliminary Analysis of Transformation Approaches -- extended version",
    "content": "Transforming RDF-star to Property Graphs: A Preliminary Analysis of Transformation Approaches -- extended version. RDF and property graph models have many similarities, such as using basic graph concepts like nodes and edges. However, such models differ in their modeling approach, expressivity, serialization, and the nature of applications. RDF is the de-facto standard model for knowledge graphs on the Semantic Web and supported by a rich ecosystem for inference and processing. The property graph model, in contrast, provides advantages in scalable graph analytical tasks, such as graph matching, path analysis, and graph traversal. RDF-star extends RDF and allows capturing metadata as a first-class citizen. To tap on the advantages of alternative models, the literature proposes different ways of transforming knowledge graphs between property graphs and RDF. However, most of these approaches cannot provide complete transformations for RDF-star graphs. Hence, this paper provides a step towards transforming RDF-star graphs into property graphs. In particular, we identify different cases to evaluate transformation approaches from RDF-star to property graphs. Specifically, we categorize two classes of transformation approaches and analyze them based on the test cases. The obtained insights will form the foundation for building complete transformation approaches in the future.",
    "authors": [
      "Ghadeer Abuoda",
      "Daniele Dell'Aglio",
      "Arthur Keen",
      "Katja Hose"
    ],
    "published": "2022-10-11T20:51:58+00:00",
    "url": "http://arxiv.org/pdf/2210.05781v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2406.05564v1",
    "title": "Automata Extraction from Transformers",
    "content": "Automata Extraction from Transformers. In modern machine (ML) learning systems, Transformer-based architectures have achieved milestone success across a broad spectrum of tasks, yet understanding their operational mechanisms remains an open problem. To improve the transparency of ML systems, automata extraction methods, which interpret stateful ML models as automata typically through formal languages, have proven effective for explaining the mechanism of recurrent neural networks (RNNs). However, few works have been applied to this paradigm to Transformer models. In particular, understanding their processing of formal languages and identifying their limitations in this area remains unexplored. In this paper, we propose an automata extraction algorithm specifically designed for Transformer models. Treating the Transformer model as a black-box system, we track the model through the transformation process of their internal latent representations during their operations, and then use classical pedagogical approaches like L* algorithm to interpret them as deterministic finite-state automata (DFA). Overall, our study reveals how the Transformer model comprehends the structure of formal languages, which not only enhances the interpretability of the Transformer-based ML systems but also marks a crucial step toward a deeper understanding of how ML systems process formal languages. Code and data are available at https://github.com/Zhang-Yihao/Transfomer2DFA.",
    "authors": [
      "Yihao Zhang",
      "Zeming Wei",
      "Meng Sun"
    ],
    "published": "2024-06-08T20:07:24+00:00",
    "url": "http://arxiv.org/pdf/2406.05564v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2410.07407v1",
    "title": "Optimized Spatial Architecture Mapping Flow for Transformer Accelerators",
    "content": "Optimized Spatial Architecture Mapping Flow for Transformer Accelerators. Recent innovations in Transformer-based large language models have significantly advanced the field of general-purpose neural language understanding and generation. With billions of trainable parameters, deployment of these large models relies on high-performance hardware accelerators to efficiently deliver the required computation. Spatial architectures, such as TPUs, offer a promising solution to accelerating computation-intensive workloads. However, the design process for existing spatial architectures is predominantly manual, and it often involves time-consuming redesigns for new applications and new problem dimensions, which greatly limits the development of optimally designed accelerators for Transformer models. To address these challenges, we propose SAMT (Spatial Architecture Mapping for Transformers), a comprehensive framework designed to optimize the dataflow mapping of Transformer inference workloads onto spatial accelerators. We demonstrate the effectiveness of SAMT in improving the performance of spatial accelerators for Transformer models. We propose and leverage the dynamic operator fusion schemes for the Transformer models and co-search the optimal dataflow mapping strategies for spatial accelerators. SAMT significantly reduces inference latency by 12% to 91% and energy consumption by 3% to 23% for evaluated Transformer models compared to traditional spatial accelerator designs among edge, mobile and cloud settings.",
    "authors": [
      "Haocheng Xu",
      "Faraz Tahmasebi",
      "Ye Qiao",
      "Hongzheng Tian",
      "Hyoukjun Kwon",
      "Sitao Huang"
    ],
    "published": "2024-10-09T20:14:24+00:00",
    "url": "http://arxiv.org/pdf/2410.07407v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1510.08981v1",
    "title": "Generating Domain-Specific Transformation Languages for Component & Connector Architecture Descriptions",
    "content": "Generating Domain-Specific Transformation Languages for Component & Connector Architecture Descriptions. Component-based software engineering (CBSE) decomposes complex systems into reusable components. Model-driven engineering (MDE) aims to abstract from complexities by lifting abstract models to primary development artifacts. Component and connector architecture description languages (ADLs) combine CBSE and MDE to describe software systems as hierarchies of component models. Using models as development artifacts is accompanied with the need to evolve, maintain and refactor those models, which can be achieved by model transformations. Domain-specific transformation languages (DSTLs) are tailored to a specific modeling language as the modeling language's concrete syntax is used to describe transformations. To automate the development of DSTLs for ADLs, we present a framework to systematically derive such languages from domain-specific C&C language grammars. These DSTLs enable to describe such model transformations concisely in vocabulary of the underlying ADL. These domain-specific transformations are better comprehensible to ADL experts than generic transformations.",
    "authors": [
      "Lars Hermerschmidt",
      "Katrin Hölldobler",
      "Bernhard Rumpe",
      "Andreas Wortmann"
    ],
    "published": "2015-10-30T06:33:09+00:00",
    "url": "http://arxiv.org/pdf/1510.08981v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2402.13934v2",
    "title": "Do Efficient Transformers Really Save Computation?",
    "content": "Do Efficient Transformers Really Save Computation?. As transformer-based language models are trained on increasingly large datasets and with vast numbers of parameters, finding more efficient alternatives to the standard Transformer has become very valuable. While many efficient Transformers and Transformer alternatives have been proposed, none provide theoretical guarantees that they are a suitable replacement for the standard Transformer. This makes it challenging to identify when to use a specific model and what directions to prioritize for further investigation. In this paper, we aim to understand the capabilities and limitations of efficient Transformers, specifically the Sparse Transformer and the Linear Transformer. We focus on their reasoning capability as exhibited by Chain-of-Thought (CoT) prompts and follow previous works to model them as Dynamic Programming (DP) problems. Our results show that while these models are expressive enough to solve general DP tasks, contrary to expectations, they require a model size that scales with the problem size. Nonetheless, we identify a class of DP problems for which these models can be more efficient than the standard Transformer. We confirm our theoretical results through experiments on representative DP tasks, adding to the understanding of efficient Transformers' practical strengths and weaknesses.",
    "authors": [
      "Kai Yang",
      "Jan Ackermann",
      "Zhenyu He",
      "Guhao Feng",
      "Bohang Zhang",
      "Yunzhen Feng",
      "Qiwei Ye",
      "Di He",
      "Liwei Wang"
    ],
    "published": "2024-02-21T17:00:56+00:00",
    "url": "http://arxiv.org/pdf/2402.13934v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1312.0596v1",
    "title": "An NMF solution for the Petri Nets to State Charts case study at the TTC 2013",
    "content": "An NMF solution for the Petri Nets to State Charts case study at the TTC 2013. Software systems are getting more and more complex. Model-driven engineering (MDE) offers ways to handle such increased complexity by lifting development to a higher level of abstraction. A key part in MDE are transformations that transform any given model into another. These transformations are used to generate all kinds of software artifacts from models. However, there is little consensus about the transformation tools. Thus, the Transformation Tool Contest (TTC) 2013 aims to compare different transformation engines. This is achieved through three different cases that have to be tackled. One of these cases is the Petri Net to State Chart case. A solution has to transform a Petri Net to a State Chart and has to derive a hierarchical structure within the State Chart. This paper presents the solution for this case using NMF Transformations as transformation engine.",
    "authors": [
      "Georg Hinkel",
      "Thomas Goldschmidt",
      "Lucia Happe"
    ],
    "published": "2013-12-02T07:00:51+00:00",
    "url": "http://arxiv.org/pdf/1312.0596v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2112.00578v1",
    "title": "Systematic Generalization with Edge Transformers",
    "content": "Systematic Generalization with Edge Transformers. Recent research suggests that systematic generalization in natural language understanding remains a challenge for state-of-the-art neural models such as Transformers and Graph Neural Networks. To tackle this challenge, we propose Edge Transformer, a new model that combines inspiration from Transformers and rule-based symbolic AI. The first key idea in Edge Transformers is to associate vector states with every edge, that is, with every pair of input nodes -- as opposed to just every node, as it is done in the Transformer model. The second major innovation is a triangular attention mechanism that updates edge representations in a way that is inspired by unification from logic programming. We evaluate Edge Transformer on compositional generalization benchmarks in relational reasoning, semantic parsing, and dependency parsing. In all three settings, the Edge Transformer outperforms Relation-aware, Universal and classical Transformer baselines.",
    "authors": [
      "Leon Bergen",
      "Timothy J. O'Donnell",
      "Dzmitry Bahdanau"
    ],
    "published": "2021-12-01T15:50:45+00:00",
    "url": "http://arxiv.org/pdf/2112.00578v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2008.04057v5",
    "title": "The Chess Transformer: Mastering Play using Generative Language Models",
    "content": "The Chess Transformer: Mastering Play using Generative Language Models. This work demonstrates that natural language transformers can support more generic strategic modeling, particularly for text-archived games. In addition to learning natural language skills, the abstract transformer architecture can generate meaningful moves on a chessboard. With further fine-tuning, the transformer learns complex gameplay by training on 2.8 million chess games in Portable Game Notation. After 30,000 training steps, OpenAI's Generative Pre-trained Transformer (GPT-2) optimizes weights for 774 million parameters. This fine-tuned Chess Transformer generates plausible strategies and displays game formations identifiable as classic openings, such as English or the Slav Exchange. Finally, in live play, the novel model demonstrates a human-to-transformer interface that correctly filters illegal moves and provides a novel method to challenge the transformer's chess strategies. We anticipate future work will build on this transformer's promise, particularly in other strategy games where features can capture the underlying complex rule syntax from simple but expressive player annotations.",
    "authors": [
      "David Noever",
      "Matt Ciolino",
      "Josh Kalin"
    ],
    "published": "2020-08-02T18:04:36+00:00",
    "url": "http://arxiv.org/pdf/2008.04057v5"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2004.06318v3",
    "title": "Relationship between covariance of Wigner functions and transformation noncontextuality",
    "content": "Relationship between covariance of Wigner functions and transformation noncontextuality. We investigate the relationship between two properties of quantum transformations often studied in popular subtheories of quantum theory: covariance of the Wigner representation of the theory and the existence of a transformation noncontextual ontological model of the theory. We consider subtheories of quantum theory specified by a set of states, measurements and transformations, defined specifying a group of unitaries, that map between states (and measurements) within the subtheory. We show that if there exists a Wigner representation of the subtheory which is covariant under the group of unitaries defining the set of transformations then the subtheory admits of a transformation noncontextual ontological model. We provide some concrete arguments to conjecture that the converse statement also holds provided that the underlying ontological model is the one given by the Wigner representation. In addition, we investigate the relationships of covariance and transformation noncontextuality with the existence of a quasiprobability distribution for the theory that represents the transformations as positivity preserving maps. We conclude that covariance implies transformation noncontextuality, which implies positivity preservation.",
    "authors": [
      "Lorenzo Catani"
    ],
    "published": "2020-04-14T06:36:31+00:00",
    "url": "http://arxiv.org/pdf/2004.06318v3"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2204.00423v1",
    "title": "Transformers for 1D Signals in Parkinson's Disease Detection from Gait",
    "content": "Transformers for 1D Signals in Parkinson's Disease Detection from Gait. This paper focuses on the detection of Parkinson's disease based on the analysis of a patient's gait. The growing popularity and success of Transformer networks in natural language processing and image recognition motivated us to develop a novel method for this problem based on an automatic features extraction via Transformers. The use of Transformers in 1D signal is not really widespread yet, but we show in this paper that they are effective in extracting relevant features from 1D signals. As Transformers require a lot of memory, we decoupled temporal and spatial information to make the model smaller. Our architecture used temporal Transformers, dimension reduction layers to reduce the dimension of the data, a spatial Transformer, two fully connected layers and an output layer for the final prediction. Our model outperforms the current state-of-the-art algorithm with 95.2\\% accuracy in distinguishing a Parkinsonian patient from a healthy one on the Physionet dataset. A key learning from this work is that Transformers allow for greater stability in results. The source code and pre-trained models are released in https://github.com/DucMinhDimitriNguyen/Transformers-for-1D-signals-in-Parkinson-s-disease-detection-from-gait.git",
    "authors": [
      "Duc Minh Dimitri Nguyen",
      "Mehdi Miah",
      "Guillaume-Alexandre Bilodeau",
      "Wassim Bouachir"
    ],
    "published": "2022-04-01T13:30:52+00:00",
    "url": "http://arxiv.org/pdf/2204.00423v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2303.00579v1",
    "title": "Are More Layers Beneficial to Graph Transformers?",
    "content": "Are More Layers Beneficial to Graph Transformers?. Despite that going deep has proven successful in many neural architectures, the existing graph transformers are relatively shallow. In this work, we explore whether more layers are beneficial to graph transformers, and find that current graph transformers suffer from the bottleneck of improving performance by increasing depth. Our further analysis reveals the reason is that deep graph transformers are limited by the vanishing capacity of global attention, restricting the graph transformer from focusing on the critical substructure and obtaining expressive features. To this end, we propose a novel graph transformer model named DeepGraph that explicitly employs substructure tokens in the encoded representation, and applies local attention on related nodes to obtain substructure based attention encoding. Our model enhances the ability of the global attention to focus on substructures and promotes the expressiveness of the representations, addressing the limitation of self-attention as the graph transformer deepens. Experiments show that our method unblocks the depth limitation of graph transformers and results in state-of-the-art performance across various graph benchmarks with deeper models.",
    "authors": [
      "Haiteng Zhao",
      "Shuming Ma",
      "Dongdong Zhang",
      "Zhi-Hong Deng",
      "Furu Wei"
    ],
    "published": "2023-03-01T15:22:40+00:00",
    "url": "http://arxiv.org/pdf/2303.00579v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2402.08975v1",
    "title": "Research and application of Transformer based anomaly detection model: A literature review",
    "content": "Research and application of Transformer based anomaly detection model: A literature review. Transformer, as one of the most advanced neural network models in Natural Language Processing (NLP), exhibits diverse applications in the field of anomaly detection. To inspire research on Transformer-based anomaly detection, this review offers a fresh perspective on the concept of anomaly detection. We explore the current challenges of anomaly detection and provide detailed insights into the operating principles of Transformer and its variants in anomaly detection tasks. Additionally, we delineate various application scenarios for Transformer-based anomaly detection models and discuss the datasets and evaluation metrics employed. Furthermore, this review highlights the key challenges in Transformer-based anomaly detection research and conducts a comprehensive analysis of future research trends in this domain. The review includes an extensive compilation of over 100 core references related to Transformer-based anomaly detection. To the best of our knowledge, this is the first comprehensive review that focuses on the research related to Transformer in the context of anomaly detection. We hope that this paper can provide detailed technical information to researchers interested in Transformer-based anomaly detection tasks.",
    "authors": [
      "Mingrui Ma",
      "Lansheng Han",
      "Chunjie Zhou"
    ],
    "published": "2024-02-14T06:39:54+00:00",
    "url": "http://arxiv.org/pdf/2402.08975v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2502.06167v1",
    "title": "Universal Approximation of Visual Autoregressive Transformers",
    "content": "Universal Approximation of Visual Autoregressive Transformers. We investigate the fundamental limits of transformer-based foundation models, extending our analysis to include Visual Autoregressive (VAR) transformers. VAR represents a big step toward generating images using a novel, scalable, coarse-to-fine ``next-scale prediction'' framework. These models set a new quality bar, outperforming all previous methods, including Diffusion Transformers, while having state-of-the-art performance for image synthesis tasks. Our primary contributions establish that, for single-head VAR transformers with a single self-attention layer and single interpolation layer, the VAR Transformer is universal. From the statistical perspective, we prove that such simple VAR transformers are universal approximators for any image-to-image Lipschitz functions. Furthermore, we demonstrate that flow-based autoregressive transformers inherit similar approximation capabilities. Our results provide important design principles for effective and computationally efficient VAR Transformer strategies that can be used to extend their utility to more sophisticated VAR models in image generation and other related areas.",
    "authors": [
      "Yifang Chen",
      "Xiaoyu Li",
      "Yingyu Liang",
      "Zhenmei Shi",
      "Zhao Song"
    ],
    "published": "2025-02-10T05:36:30+00:00",
    "url": "http://arxiv.org/pdf/2502.06167v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2211.14705v1",
    "title": "Semantic-Aware Local-Global Vision Transformer",
    "content": "Semantic-Aware Local-Global Vision Transformer. Vision Transformers have achieved remarkable progresses, among which Swin Transformer has demonstrated the tremendous potential of Transformer for vision tasks. It surmounts the key challenge of high computational complexity by performing local self-attention within shifted windows. In this work we propose the Semantic-Aware Local-Global Vision Transformer (SALG), to further investigate two potential improvements towards Swin Transformer. First, unlike Swin Transformer that performs uniform partition to produce equal size of regular windows for local self-attention, our SALG performs semantic segmentation in an unsupervised way to explore the underlying semantic priors in the image. As a result, each segmented region can correspond to a semantically meaningful part in the image, potentially leading to more effective features within each of segmented regions. Second, instead of only performing local self-attention within local windows as Swin Transformer does, the proposed SALG performs both 1) local intra-region self-attention for learning fine-grained features within each region and 2) global inter-region feature propagation for modeling global dependencies among all regions. Consequently, our model is able to obtain the global view when learning features for each token, which is the essential advantage of Transformer. Owing to the explicit modeling of the semantic priors and the proposed local-global modeling mechanism, our SALG is particularly advantageous for small-scale models when the modeling capacity is not sufficient for other models to learn semantics implicitly. Extensive experiments across various vision tasks demonstrates the merit of our model over other vision Transformers, especially in the small-scale modeling scenarios.",
    "authors": [
      "Jiatong Zhang",
      "Zengwei Yao",
      "Fanglin Chen",
      "Guangming Lu",
      "Wenjie Pei"
    ],
    "published": "2022-11-27T03:16:00+00:00",
    "url": "http://arxiv.org/pdf/2211.14705v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1410.8269v2",
    "title": "Estimation and Prediction in Transformed Nested Error Regression Models",
    "content": "Estimation and Prediction in Transformed Nested Error Regression Models. This paper suggests parametrically transformed nested error regression models (TNERM), which transform the data flexibly to follow the normal linear mixed regression. We provide a procedure for estimating consistently the parameters of the proposed model and a predictor based on the consistent estimators. Then, in order to calibrate uncertainty of the transformed empirical best linear unbiased predictor, we derive prediction intervals with second-order accuracy based on the parametric bootstrap method. The proposed methods are investigated through simulation and empirical studies.",
    "authors": [
      "Shonosuke Sugasawa",
      "Tatsuya Kubokawa"
    ],
    "published": "2014-10-30T07:04:50+00:00",
    "url": "http://arxiv.org/pdf/1410.8269v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2110.04832v2",
    "title": "Higher-Rank Radon Transforms on Constant Curvature Spaces",
    "content": "Higher-Rank Radon Transforms on Constant Curvature Spaces. We study higher-rank Radon transforms that take functions on $j$-dimensional totally geodesic submanifolds in the $n$-dimensional real constant curvature space to functions on similar submanifolds of dimension $k >j$. The corresponding dual transforms are also considered. The transforms are explored the Euclidean case (affine Grassmannian bundles), the elliptic case (compact Grassmannians), and the hyperbolic case (the hyperboloid model, the Beltrami-Klein model, and the projective model). The main objectives are sharp conditions for the existence and injectivity of the Radon transforms in Lebesgue spaces, transition from one model to another, support theorems, and inversion formulas. Conjectures and open problems are discussed.",
    "authors": [
      "Boris Rubin"
    ],
    "published": "2021-10-10T16:00:57+00:00",
    "url": "http://arxiv.org/pdf/2110.04832v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2310.05884v2",
    "title": "A Meta-Learning Perspective on Transformers for Causal Language Modeling",
    "content": "A Meta-Learning Perspective on Transformers for Causal Language Modeling. The Transformer architecture has become prominent in developing large causal language models. However, mechanisms to explain its capabilities are not well understood. Focused on the training process, here we establish a meta-learning view of the Transformer architecture when trained for the causal language modeling task, by explicating an inner optimization process within the Transformer. Further, within the inner optimization, we discover and theoretically analyze a special characteristic of the norms of learned token representations within Transformer-based causal language models. Our analysis is supported by experiments in various settings.",
    "authors": [
      "Xinbo Wu",
      "Lav R. Varshney"
    ],
    "published": "2023-10-09T17:27:36+00:00",
    "url": "http://arxiv.org/pdf/2310.05884v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2302.12021v2",
    "title": "Derivative-Free Optimization with Transformed Objective Functions (DFOTO) and the Algorithm Based on the Least Frobenius Norm Updating Quadratic Model",
    "content": "Derivative-Free Optimization with Transformed Objective Functions (DFOTO) and the Algorithm Based on the Least Frobenius Norm Updating Quadratic Model. Derivative-free optimization problems are optimization problems where derivative information is unavailable. The least Frobenius norm updating quadratic interpolation model function is one of the essential under-determined model functions for model-based derivative-free trust-region methods. This article proposes derivative-free optimization with transformed objective functions and gives a trust-region method with the least Frobenius norm model. The model updating formula is based on Powell's formula. The method shares the same framework with those for problems without transformations, and its query scheme is given. We propose the definitions related to optimality-preserving transformations to understand the interpolation model in our method. We prove the existence of model optimality-preserving transformations beyond translation transformation. The necessary and sufficient condition for such transformations is given. The affine transformation with a positive multiplication coefficient is not model optimality-preserving. We also analyze the corresponding least Frobenius norm updating model and its interpolation error when the objective function is affinely transformed. Convergence property of a provable algorithmic framework containing our model is given. Numerical results of solving test problems and a real-world problem with the implementation NEWUOA-Trans show that our method can successfully solve most problems with objective optimality-preserving transformations, even though such transformations will change the optimality of the model function. To our best knowledge, this is the first work providing the model-based derivative-free algorithm and analysis for transformed problems with the function evaluation oracle (not the function-value comparison oracle). This article also proposes the \"moving-target\" optimization problem.",
    "authors": [
      "Pengcheng Xie",
      "Ya-xiang Yuan"
    ],
    "published": "2023-02-19T15:42:50+00:00",
    "url": "http://arxiv.org/pdf/2302.12021v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2110.08678v2",
    "title": "Improving Transformers with Probabilistic Attention Keys",
    "content": "Improving Transformers with Probabilistic Attention Keys. Multi-head attention is a driving force behind state-of-the-art transformers, which achieve remarkable performance across a variety of natural language processing (NLP) and computer vision tasks. It has been observed that for many applications, those attention heads learn redundant embedding, and most of them can be removed without degrading the performance of the model. Inspired by this observation, we propose Transformer with a Mixture of Gaussian Keys (Transformer-MGK), a novel transformer architecture that replaces redundant heads in transformers with a mixture of keys at each head. These mixtures of keys follow a Gaussian mixture model and allow each attention head to focus on different parts of the input sequence efficiently. Compared to its conventional transformer counterpart, Transformer-MGK accelerates training and inference, has fewer parameters, and requires fewer FLOPs to compute while achieving comparable or better accuracy across tasks. Transformer-MGK can also be easily extended to use with linear attention. We empirically demonstrate the advantage of Transformer-MGK in a range of practical applications, including language modeling and tasks that involve very long sequences. On the Wikitext-103 and Long Range Arena benchmark, Transformer-MGKs with 4 heads attain comparable or better performance to the baseline transformers with 8 heads.",
    "authors": [
      "Tam Nguyen",
      "Tan M. Nguyen",
      "Dung D. Le",
      "Duy Khuong Nguyen",
      "Viet-Anh Tran",
      "Richard G. Baraniuk",
      "Nhat Ho",
      "Stanley J. Osher"
    ],
    "published": "2021-10-16T23:43:24+00:00",
    "url": "http://arxiv.org/pdf/2110.08678v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2306.01128v2",
    "title": "Learning Transformer Programs",
    "content": "Learning Transformer Programs. Recent research in mechanistic interpretability has attempted to reverse-engineer Transformer models by carefully inspecting network weights and activations. However, these approaches require considerable manual effort and still fall short of providing complete, faithful descriptions of the underlying algorithms. In this work, we introduce a procedure for training Transformers that are mechanistically interpretable by design. We build on RASP [Weiss et al., 2021], a programming language that can be compiled into Transformer weights. Instead of compiling human-written programs into Transformers, we design a modified Transformer that can be trained using gradient-based optimization and then automatically converted into a discrete, human-readable program. We refer to these models as Transformer Programs. To validate our approach, we learn Transformer Programs for a variety of problems, including an in-context learning task, a suite of algorithmic problems (e.g. sorting, recognizing Dyck languages), and NLP tasks including named entity recognition and text classification. The Transformer Programs can automatically find reasonable solutions, performing on par with standard Transformers of comparable size; and, more importantly, they are easy to interpret. To demonstrate these advantages, we convert Transformers into Python programs and use off-the-shelf code analysis tools to debug model errors and identify the \"circuits\" used to solve different sub-problems. We hope that Transformer Programs open a new path toward the goal of intrinsically interpretable machine learning.",
    "authors": [
      "Dan Friedman",
      "Alexander Wettig",
      "Danqi Chen"
    ],
    "published": "2023-06-01T20:27:01+00:00",
    "url": "http://arxiv.org/pdf/2306.01128v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2007.11199v1",
    "title": "Romeo: A Design Tool for Embedding Transformable Parts in 3D Models to Robotically Augment Default Functionalities",
    "content": "Romeo: A Design Tool for Embedding Transformable Parts in 3D Models to Robotically Augment Default Functionalities. Reconfiguring shapes of objects enables transforming existing passive objects with robotic functionalities, e.g., a transformable coffee cup holder can be attached to a chair's armrest, a piggy bank can reach out an arm to 'steal' coins. Despite the advance in end-user 3D design and fabrication, it remains challenging for non-experts to create such 'transformables' using existing tools due to the requirement of specific engineering knowledge such as mechanisms and robotic design.   We present Romeo -- a design tool for creating transformables to robotically augment objects' default functionalities. Romeo allows users to transform an object into a robotic arm by expressing at a high level what type of task is expected. Users can select which part of the object to be transformed, specify motion points in space for the transformed part to follow and the corresponding action to be taken. Romeo then automatically generates a robotic arm embedded in the transformable part ready for fabrication. A design session validated this tool where participants used Romeo to accomplish controlled design tasks and to open-endedly create coin-stealing piggy banks by transforming 3D objects of their own choice.",
    "authors": [
      "Jiahao Li",
      "Meilin Cui",
      "Jeeeun Kim",
      "Xiang 'Anthony' Chen"
    ],
    "published": "2020-07-22T04:54:43+00:00",
    "url": "http://arxiv.org/pdf/2007.11199v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2010.11358v1",
    "title": "N-ODE Transformer: A Depth-Adaptive Variant of the Transformer Using Neural Ordinary Differential Equations",
    "content": "N-ODE Transformer: A Depth-Adaptive Variant of the Transformer Using Neural Ordinary Differential Equations. We use neural ordinary differential equations to formulate a variant of the Transformer that is depth-adaptive in the sense that an input-dependent number of time steps is taken by the ordinary differential equation solver. Our goal in proposing the N-ODE Transformer is to investigate whether its depth-adaptivity may aid in overcoming some specific known theoretical limitations of the Transformer in handling nonlocal effects. Specifically, we consider the simple problem of determining the parity of a binary sequence, for which the standard Transformer has known limitations that can only be overcome by using a sufficiently large number of layers or attention heads. We find, however, that the depth-adaptivity of the N-ODE Transformer does not provide a remedy for the inherently nonlocal nature of the parity problem, and provide explanations for why this is so. Next, we pursue regularization of the N-ODE Transformer by penalizing the arclength of the ODE trajectories, but find that this fails to improve the accuracy or efficiency of the N-ODE Transformer on the challenging parity problem. We suggest future avenues of research for modifications and extensions of the N-ODE Transformer that may lead to improved accuracy and efficiency for sequence modelling tasks such as neural machine translation.",
    "authors": [
      "Aaron Baier-Reinio",
      "Hans De Sterck"
    ],
    "published": "2020-10-22T00:48:24+00:00",
    "url": "http://arxiv.org/pdf/2010.11358v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2106.02242v2",
    "title": "Scalable Transformers for Neural Machine Translation",
    "content": "Scalable Transformers for Neural Machine Translation. Transformer has been widely adopted in Neural Machine Translation (NMT) because of its large capacity and parallel training of sequence generation. However, the deployment of Transformer is challenging because different scenarios require models of different complexities and scales. Naively training multiple Transformers is redundant in terms of both computation and memory. In this paper, we propose a novel Scalable Transformers, which naturally contains sub-Transformers of different scales and have shared parameters. Each sub-Transformer can be easily obtained by cropping the parameters of the largest Transformer. A three-stage training scheme is proposed to tackle the difficulty of training the Scalable Transformers, which introduces additional supervisions from word-level and sequence-level self-distillation. Extensive experiments were conducted on WMT EN-De and En-Fr to validate our proposed Scalable Transformers.",
    "authors": [
      "Peng Gao",
      "Shijie Geng",
      "Yu Qiao",
      "Xiaogang Wang",
      "Jifeng Dai",
      "Hongsheng Li"
    ],
    "published": "2021-06-04T04:04:10+00:00",
    "url": "http://arxiv.org/pdf/2106.02242v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2206.06488v2",
    "title": "Multimodal Learning with Transformers: A Survey",
    "content": "Multimodal Learning with Transformers: A Survey. Transformer is a promising neural network learner, and has achieved great success in various machine learning tasks. Thanks to the recent prevalence of multimodal applications and big data, Transformer-based multimodal learning has become a hot topic in AI research. This paper presents a comprehensive survey of Transformer techniques oriented at multimodal data. The main contents of this survey include: (1) a background of multimodal learning, Transformer ecosystem, and the multimodal big data era, (2) a theoretical review of Vanilla Transformer, Vision Transformer, and multimodal Transformers, from a geometrically topological perspective, (3) a review of multimodal Transformer applications, via two important paradigms, i.e., for multimodal pretraining and for specific multimodal tasks, (4) a summary of the common challenges and designs shared by the multimodal Transformer models and applications, and (5) a discussion of open problems and potential research directions for the community.",
    "authors": [
      "Peng Xu",
      "Xiatian Zhu",
      "David A. Clifton"
    ],
    "published": "2022-06-13T21:36:09+00:00",
    "url": "http://arxiv.org/pdf/2206.06488v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2405.17190v1",
    "title": "SoK: Leveraging Transformers for Malware Analysis",
    "content": "SoK: Leveraging Transformers for Malware Analysis. The introduction of transformers has been an important breakthrough for AI research and application as transformers are the foundation behind Generative AI. A promising application domain for transformers is cybersecurity, in particular the malware domain analysis. The reason is the flexibility of the transformer models in handling long sequential features and understanding contextual relationships. However, as the use of transformers for malware analysis is still in the infancy stage, it is critical to evaluate, systematize, and contextualize existing literature to foster future research. This Systematization of Knowledge (SoK) paper aims to provide a comprehensive analysis of transformer-based approaches designed for malware analysis. Based on our systematic analysis of existing knowledge, we structure and propose taxonomies based on: (a) how different transformers are adapted, organized, and modified across various use cases; and (b) how diverse feature types and their representation capabilities are reflected. We also provide an inventory of datasets used to explore multiple research avenues in the use of transformers for malware analysis and discuss open challenges with future research directions. We believe that this SoK paper will assist the research community in gaining detailed insights from existing work and will serve as a foundational resource for implementing novel research using transformers for malware analysis.",
    "authors": [
      "Pradip Kunwar",
      "Kshitiz Aryal",
      "Maanak Gupta",
      "Mahmoud Abdelsalam",
      "Elisa Bertino"
    ],
    "published": "2024-05-27T14:14:07+00:00",
    "url": "http://arxiv.org/pdf/2405.17190v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2406.08443v2",
    "title": "Transform-Dependent Adversarial Attacks",
    "content": "Transform-Dependent Adversarial Attacks. Deep networks are highly vulnerable to adversarial attacks, yet conventional attack methods utilize static adversarial perturbations that induce fixed mispredictions. In this work, we exploit an overlooked property of adversarial perturbations--their dependence on image transforms--and introduce transform-dependent adversarial attacks. Unlike traditional attacks, our perturbations exhibit metamorphic properties, enabling diverse adversarial effects as a function of transformation parameters. We demonstrate that this transform-dependent vulnerability exists across different architectures (e.g., CNN and transformer), vision tasks (e.g., image classification and object detection), and a wide range of image transforms. Additionally, we show that transform-dependent perturbations can serve as a defense mechanism, preventing sensitive information disclosure when image enhancement transforms pose a risk of revealing private content. Through analysis in blackbox and defended model settings, we show that transform-dependent perturbations achieve high targeted attack success rates, outperforming state-of-the-art transfer attacks by 17-31% in blackbox scenarios. Our work introduces novel, controllable paradigm for adversarial attack deployment, revealing a previously overlooked vulnerability in deep networks.",
    "authors": [
      "Yaoteng Tan",
      "Zikui Cai",
      "M. Salman Asif"
    ],
    "published": "2024-06-12T17:31:36+00:00",
    "url": "http://arxiv.org/pdf/2406.08443v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2410.18217v1",
    "title": "A Methodology for Transformer Ratio Adjustment in Small-Size Rotary Transformers",
    "content": "A Methodology for Transformer Ratio Adjustment in Small-Size Rotary Transformers. This study addresses a neglected challenge that has been hidden in the Rotary Transformer (RT) field: the possibility of a discrepancy between transformer ratio and turn number ratio in small-size transformers. Previous investigations have shown that in the geometry design of RTs, as well as their resonant circuit design, the transformer ratio has been regarded as the same as the turn number ratio. This estimation is logical and true when a large-size RT is investigated. However, in small-size RTs, the magnitudes of leakage and magnetization inductances are significantly close, which leads to a difference between transformer ratio and turn number ratio. Accordingly, the absence of an exact methodology for transformer ratio calculation brought us to conduct this investigation. In this regard, a transformer ratio adjustment is suggested after proposing a low-error magnetic model. Its accuracy is high enough to consider different air gaps and subsequently calculate inductance with reference to 3D finite element analysis (3D-FEA). Finally, we take advantage of a test bench to show the exactness and proficiency of the suggested transformer ratio adjustment.",
    "authors": [
      "Saeed Hajmohammadi",
      "MohammadSadegh KhajueeZadeh",
      "Farid Tootoonchian",
      "Sajjad Mohammadi"
    ],
    "published": "2024-10-23T18:36:01+00:00",
    "url": "http://arxiv.org/pdf/2410.18217v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2412.00984v1",
    "title": "TGTOD: A Global Temporal Graph Transformer for Outlier Detection at Scale",
    "content": "TGTOD: A Global Temporal Graph Transformer for Outlier Detection at Scale. While Transformers have revolutionized machine learning on various data, existing Transformers for temporal graphs face limitations in (1) restricted receptive fields, (2) overhead of subgraph extraction, and (3) suboptimal generalization capability beyond link prediction. In this paper, we rethink temporal graph Transformers and propose TGTOD, a novel end-to-end Temporal Graph Transformer for Outlier Detection. TGTOD employs global attention to model both structural and temporal dependencies within temporal graphs. To tackle scalability, our approach divides large temporal graphs into spatiotemporal patches, which are then processed by a hierarchical Transformer architecture comprising Patch Transformer, Cluster Transformer, and Temporal Transformer. We evaluate TGTOD on three public datasets under two settings, comparing with a wide range of baselines. Our experimental results demonstrate the effectiveness of TGTOD, achieving AP improvement of 61% on Elliptic. Furthermore, our efficiency evaluation shows that TGTOD reduces training time by 44x compared to existing Transformers for temporal graphs. To foster reproducibility, we make our implementation publicly available at https://github.com/kayzliu/tgtod.",
    "authors": [
      "Kay Liu",
      "Jiahao Ding",
      "MohamadAli Torkamani",
      "Philip S. Yu"
    ],
    "published": "2024-12-01T22:24:55+00:00",
    "url": "http://arxiv.org/pdf/2412.00984v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2412.11478v1",
    "title": "Properties preserved by classes of Chu transforms",
    "content": "Properties preserved by classes of Chu transforms. Chu spaces and Chu transforms were first investigated in category theory by Barr and Chu in 1979. In 2000 van Benthem shifted to the model-theoretic point of view by isolating a class of infinitary two-sorted properties, the flow formulas, which are preserved by all Chu transforms. D\\v{z}amonja and V\\\"a\\\"an\\\"anen in 2021 considered a special kind of Chu transforms, satisfying a density condition. These authors used dense Chu transforms to compare abstract logics, in particular showing that such transforms preserve compactness.   This motivates the problem of characterizing which properties are preserved by dense Chu transforms. We solve this problem by isolating the inconsistency-flow formulas, which are flow formulas with an added predicate to express inconsistency. Our main result characterizes the first-order inconsistency-flow formulas exactly as those first-order properties preserved by dense Chu transforms. Furthermore, we consider several instances of Chu transforms in concrete situations such as topological spaces, graphs and ultrafilters. In particular, we prove that Chu transforms between ultrafilters correspond to the Rudin-Keisler ordering.",
    "authors": [
      "Mirna Džamonja",
      "Francesco Parente"
    ],
    "published": "2024-12-16T06:46:15+00:00",
    "url": "http://arxiv.org/pdf/2412.11478v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1409.2309v1",
    "title": "A Domain Specific Transformation Language",
    "content": "A Domain Specific Transformation Language. Domain specific languages (DSLs) allow domain experts to model parts of the system under development in a problem-oriented notation that is well-known in the respective domain. The introduction of a DSL is often accompanied the desire to transform its instances. Although the modeling language is domain specific, the transformation language used to describe modifications, such as model evolution or refactoring operations, on the underlying model, usually is a rather domain independent language nowadays. Most transformation languages use a generic notation of model patterns that is closely related to typed and attributed graphs or to object diagrams (the abstract syntax). A notation that reflects the transformed elements of the original DSL in its own concrete syntax would be strongly preferable, because it would be more comprehensible and easier to learn for domain experts. In this paper we present a transformation language that reuses the concrete syntax of a textual modeling language for hierarchical automata, which allows domain experts to describe models as well as modifications of models in a convenient, yet precise manner. As an outlook, we illustrate a scenario where we generate transformation languages from existing textual languages.",
    "authors": [
      "Bernhard Rumpe",
      "Ingo Weisemöller"
    ],
    "published": "2014-09-08T12:04:22+00:00",
    "url": "http://arxiv.org/pdf/1409.2309v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2110.05722v3",
    "title": "LightSeq2: Accelerated Training for Transformer-based Models on GPUs",
    "content": "LightSeq2: Accelerated Training for Transformer-based Models on GPUs. Transformer-based neural models are used in many AI applications. Training these models is expensive, as it takes huge GPU resources and long duration. It is challenging because typical data like sentences have variable lengths, and Transformer's computation patterns are more complex than convolutional neural networks. Existing systems either only focus on model inference or optimization for only BERT-like encoder models. In this paper, we present LightSeq2, a system to accelerate training for a general family of Transformer models on GPUs. We propose a series of GPU optimization techniques tailored to the specific computation flow and memory access patterns of Transformer models. LightSeq2 supports many model architectures, including BERT (encoder-only), GPT (decoder-only), Transformer (encoder-decoder), and vision Transformer. Our experiments for a variety of models and benchmarks show that LightSeq2 is consistently faster (1.4-3.5x) than previous systems on different GPUs. In particular, it gains 308% training speedup compared with existing systems on a large public machine translation benchmark (WMT14 English-German).",
    "authors": [
      "Xiaohui Wang",
      "Yang Wei",
      "Ying Xiong",
      "Guyue Huang",
      "Xian Qian",
      "Yufei Ding",
      "Mingxuan Wang",
      "Lei Li"
    ],
    "published": "2021-10-12T03:17:03+00:00",
    "url": "http://arxiv.org/pdf/2110.05722v3"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2209.12816v2",
    "title": "Fast-FNet: Accelerating Transformer Encoder Models via Efficient Fourier Layers",
    "content": "Fast-FNet: Accelerating Transformer Encoder Models via Efficient Fourier Layers. Transformer-based language models utilize the attention mechanism for substantial performance improvements in almost all natural language processing (NLP) tasks. Similar attention structures are also extensively studied in several other areas. Although the attention mechanism enhances the model performances significantly, its quadratic complexity prevents efficient processing of long sequences. Recent works focused on eliminating the disadvantages of computational inefficiency and showed that transformer-based models can still reach competitive results without the attention layer. A pioneering study proposed the FNet, which replaces the attention layer with the Fourier Transform (FT) in the transformer encoder architecture. FNet achieves competitive performances concerning the original transformer encoder model while accelerating training process by removing the computational burden of the attention mechanism. However, the FNet model ignores essential properties of the FT from the classical signal processing that can be leveraged to increase model efficiency further. We propose different methods to deploy FT efficiently in transformer encoder models. Our proposed architectures have smaller number of model parameters, shorter training times, less memory usage, and some additional performance improvements. We demonstrate these improvements through extensive experiments on common benchmarks.",
    "authors": [
      "Nurullah Sevim",
      "Ege Ozan Özyedek",
      "Furkan Şahinuç",
      "Aykut Koç"
    ],
    "published": "2022-09-26T16:23:02+00:00",
    "url": "http://arxiv.org/pdf/2209.12816v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2402.00742v2",
    "title": "Transforming and Combining Rewards for Aligning Large Language Models",
    "content": "Transforming and Combining Rewards for Aligning Large Language Models. A common approach for aligning language models to human preferences is to first learn a reward model from preference data, and then use this reward model to update the language model. We study two closely related problems that arise in this approach. First, any monotone transformation of the reward model preserves preference ranking; is there a choice that is ``better'' than others? Second, we often wish to align language models to multiple properties: how should we combine multiple reward models? Using a probabilistic interpretation of the alignment procedure, we identify a natural choice for transformation for (the common case of) rewards learned from Bradley-Terry preference models. The derived transformation is straightforward: we apply a log-sigmoid function to the centered rewards, a method we term ``LSC-transformation'' (log-sigmoid-centered transformation). This transformation has two important properties. First, it emphasizes improving poorly-performing outputs, rather than outputs that already score well. This mitigates both underfitting (where some prompts are not improved) and reward hacking (where the model learns to exploit misspecification of the reward model). Second, it enables principled aggregation of rewards by linking summation to logical conjunction: the sum of transformed rewards corresponds to the probability that the output is ``good'' in all measured properties, in a sense we make precise. Experiments aligning language models to be both helpful and harmless using RLHF show substantial improvements over the baseline (non-transformed) approach.",
    "authors": [
      "Zihao Wang",
      "Chirag Nagpal",
      "Jonathan Berant",
      "Jacob Eisenstein",
      "Alex D'Amour",
      "Sanmi Koyejo",
      "Victor Veitch"
    ],
    "published": "2024-02-01T16:39:28+00:00",
    "url": "http://arxiv.org/pdf/2402.00742v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1111.4750v1",
    "title": "Solving the TTC 2011 Reengineering Case with MOLA and Higher-Order Transformations",
    "content": "Solving the TTC 2011 Reengineering Case with MOLA and Higher-Order Transformations. The Reengineering Case of the Transformation Tool Contest 2011 deals with automatic extraction of state machine from Java source code. The transformation task involves complex, non-local matching of model elements. This paper contains the solution of the task using model transformation language MOLA. The MOLA solution uses higher-order transformations (HOT-s) to generate a part of the required MOLA program. The described HOT approach allows creating reusable, complex model transformation libraries for generic tasks without modifying an implementation of a model transformation language. Thus model transformation users who are not the developers of the language can achieve the desired functionality more easily.",
    "authors": [
      "Agris Sostaks",
      "Elina Kalnina",
      "Audris Kalnins",
      "Edgars Celms",
      "Janis Iraids"
    ],
    "published": "2011-11-21T05:26:06+00:00",
    "url": "http://arxiv.org/pdf/1111.4750v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1906.01787v1",
    "title": "Learning Deep Transformer Models for Machine Translation",
    "content": "Learning Deep Transformer Models for Machine Translation. Transformer is the state-of-the-art model in recent machine translation evaluations. Two strands of research are promising to improve models of this kind: the first uses wide networks (a.k.a. Transformer-Big) and has been the de facto standard for the development of the Transformer system, and the other uses deeper language representation but faces the difficulty arising from learning deep networks. Here, we continue the line of research on the latter. We claim that a truly deep Transformer model can surpass the Transformer-Big counterpart by 1) proper use of layer normalization and 2) a novel way of passing the combination of previous layers to the next. On WMT'16 English- German, NIST OpenMT'12 Chinese-English and larger WMT'18 Chinese-English tasks, our deep system (30/25-layer encoder) outperforms the shallow Transformer-Big/Base baseline (6-layer encoder) by 0.4-2.4 BLEU points. As another bonus, the deep model is 1.6X smaller in size and 3X faster in training than Transformer-Big.",
    "authors": [
      "Qiang Wang",
      "Bei Li",
      "Tong Xiao",
      "Jingbo Zhu",
      "Changliang Li",
      "Derek F. Wong",
      "Lidia S. Chao"
    ],
    "published": "2019-06-05T02:24:12+00:00",
    "url": "http://arxiv.org/pdf/1906.01787v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2408.09523v1",
    "title": "A Unified Framework for Interpretable Transformers Using PDEs and Information Theory",
    "content": "A Unified Framework for Interpretable Transformers Using PDEs and Information Theory. This paper presents a novel unified theoretical framework for understanding Transformer architectures by integrating Partial Differential Equations (PDEs), Neural Information Flow Theory, and Information Bottleneck Theory. We model Transformer information dynamics as a continuous PDE process, encompassing diffusion, self-attention, and nonlinear residual components. Our comprehensive experiments across image and text modalities demonstrate that the PDE model effectively captures key aspects of Transformer behavior, achieving high similarity (cosine similarity > 0.98) with Transformer attention distributions across all layers. While the model excels in replicating general information flow patterns, it shows limitations in fully capturing complex, non-linear transformations. This work provides crucial theoretical insights into Transformer mechanisms, offering a foundation for future optimizations in deep learning architectural design. We discuss the implications of our findings, potential applications in model interpretability and efficiency, and outline directions for enhancing PDE models to better mimic the intricate behaviors observed in Transformers, paving the way for more transparent and optimized AI systems.",
    "authors": [
      "Yukun Zhang"
    ],
    "published": "2024-08-18T16:16:57+00:00",
    "url": "http://arxiv.org/pdf/2408.09523v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2310.11597v1",
    "title": "The Efficacy of Transformer-based Adversarial Attacks in Security Domains",
    "content": "The Efficacy of Transformer-based Adversarial Attacks in Security Domains. Today, the security of many domains rely on the use of Machine Learning to detect threats, identify vulnerabilities, and safeguard systems from attacks. Recently, transformer architectures have improved the state-of-the-art performance on a wide range of tasks such as malware detection and network intrusion detection. But, before abandoning current approaches to transformers, it is crucial to understand their properties and implications on cybersecurity applications. In this paper, we evaluate the robustness of transformers to adversarial samples for system defenders (i.e., resiliency to adversarial perturbations generated on different types of architectures) and their adversarial strength for system attackers (i.e., transferability of adversarial samples generated by transformers to other target models). To that effect, we first fine-tune a set of pre-trained transformer, Convolutional Neural Network (CNN), and hybrid (an ensemble of transformer and CNN) models to solve different downstream image-based tasks. Then, we use an attack algorithm to craft 19,367 adversarial examples on each model for each task. The transferability of these adversarial examples is measured by evaluating each set on other models to determine which models offer more adversarial strength, and consequently, more robustness against these attacks. We find that the adversarial examples crafted on transformers offer the highest transferability rate (i.e., 25.7% higher than the average) onto other models. Similarly, adversarial examples crafted on other models have the lowest rate of transferability (i.e., 56.7% lower than the average) onto transformers. Our work emphasizes the importance of studying transformer architectures for attacking and defending models in security domains, and suggests using them as the primary architecture in transfer attack settings.",
    "authors": [
      "Kunyang Li",
      "Kyle Domico",
      "Jean-Charles Noirot Ferrand",
      "Patrick McDaniel"
    ],
    "published": "2023-10-17T21:45:23+00:00",
    "url": "http://arxiv.org/pdf/2310.11597v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/math/0103079v1",
    "title": "Vertex-IRF transformations and quantization of dynamical r-matrices",
    "content": "Vertex-IRF transformations and quantization of dynamical r-matrices. Motivated by the correspondence between the vertex and IRF models in statistical mechanics, we define and study a notion of vertex-IRF transformation for dynamical twists that generalizes a usual gauge transformation. We use vertex-IRF transformations to quantize all completely degenerate dynamical r-matrices on finite-dimensional Lie algebras.",
    "authors": [
      "Pavel Etingof",
      "Dmitri Nikshych"
    ],
    "published": "2001-03-13T17:05:31+00:00",
    "url": "http://arxiv.org/pdf/math/0103079v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/nlin/0606011v1",
    "title": "The Darboux-Backlund transformation for the static 2-dimensional continuum Heisenberg chain",
    "content": "The Darboux-Backlund transformation for the static 2-dimensional continuum Heisenberg chain. We construct the Darboux-Backlund transformation for the sigma model describing static configurations of the 2-dimensional classical continuum Heisenberg chain. The transformation is characterized by a non-trivial normalization matrix depending on the background solution. In order to obtain the transformation we use a new, more general, spectral problem.",
    "authors": [
      "Jan L. Cieslinski",
      "Joanna Czarnecka"
    ],
    "published": "2006-06-02T13:03:28+00:00",
    "url": "http://arxiv.org/pdf/nlin/0606011v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2412.10377v1",
    "title": "What is the JEFT?",
    "content": "What is the JEFT?. The JEFT is the acronym for the Joint-Eigenspace Fourier Transform defined on a noncompact symmetric space. It is a consequence of a general construction of a Fourier transform modelled on the Harish-Chandra Fourier transform (on a semi-simple Lie group with finite centre) which (on the corresponding symmetric space of the noncompact type) serves as the Poisson-completion of the famous Helgason Fourier transform",
    "authors": [
      "O. O. Oyadare"
    ],
    "published": "2024-11-26T14:26:33+00:00",
    "url": "http://arxiv.org/pdf/2412.10377v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/0807.3669v1",
    "title": "A new probabilistic transformation of belief mass assignment",
    "content": "A new probabilistic transformation of belief mass assignment. In this paper, we propose in Dezert-Smarandache Theory (DSmT) framework, a new probabilistic transformation, called DSmP, in order to build a subjective probability measure from any basic belief assignment defined on any model of the frame of discernment. Several examples are given to show how the DSmP transformation works and we compare it to main existing transformations proposed in the literature so far. We show the advantages of DSmP over classical transformations in term of Probabilistic Information Content (PIC). The direct extension of this transformation for dealing with qualitative belief assignments is also presented.",
    "authors": [
      "Jean Dezert",
      "Florentin Smarandache"
    ],
    "published": "2008-07-23T13:49:30+00:00",
    "url": "http://arxiv.org/pdf/0807.3669v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/0907.3907v1",
    "title": "Classical isotropic two body potentials generating martensitic transformations",
    "content": "Classical isotropic two body potentials generating martensitic transformations. An isotropic interaction potential for classical particles is devised in such a way that the crystalline ground state of the system changes discontinuously when some parameter of the potential is varied. Using this potential we model martensitic transformations, and are able to study in detail the processes that are usually associated with it: shape memory effect, superelasticity, as well as many details concerning the dynamics of the transformation, particularly the characteristics of the martensitic texture obtained as a function of parameters affecting the transformation rate. Here we introduce the interaction potentials and present some basic results about the transformation it describes, for the particular case of two dimensional triangular-rombohedral and triangular-square transformation.",
    "authors": [
      "M. F. Laguna",
      "E. A. Jagla"
    ],
    "published": "2009-07-22T19:20:43+00:00",
    "url": "http://arxiv.org/pdf/0907.3907v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1711.02226v1",
    "title": "Unsupervised Transformation Learning via Convex Relaxations",
    "content": "Unsupervised Transformation Learning via Convex Relaxations. Our goal is to extract meaningful transformations from raw images, such as varying the thickness of lines in handwriting or the lighting in a portrait. We propose an unsupervised approach to learn such transformations by attempting to reconstruct an image from a linear combination of transformations of its nearest neighbors. On handwritten digits and celebrity portraits, we show that even with linear transformations, our method generates visually high-quality modified images. Moreover, since our method is semiparametric and does not model the data distribution, the learned transformations extrapolate off the training data and can be applied to new types of images.",
    "authors": [
      "Tatsunori B. Hashimoto",
      "John C. Duchi",
      "Percy Liang"
    ],
    "published": "2017-11-06T23:56:41+00:00",
    "url": "http://arxiv.org/pdf/1711.02226v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1901.01322v1",
    "title": "Transformed Snapshot Interpolation with High Resolution Transforms",
    "content": "Transformed Snapshot Interpolation with High Resolution Transforms. In the last few years, several methods have been developed to deal with jump singularities in parametric or stochastic hyperbolic PDEs. They typically use some alignment of the jump-sets in physical space before performing well established reduced order modelling techniques such as reduced basis methods, POD or simply interpolation. In the current literature, the transforms are typically of low resolution in space, mostly low order polynomials, Fourier modes or constant shifts. In this paper, we discuss higher resolution transforms in one of the recent methods, the transformed snapshot interpolation (TSI). We introduce a new discretization of the transforms with an appropriate behaviour near singularities and consider their numerical computation via an optimization procedure.",
    "authors": [
      "G. Welper"
    ],
    "published": "2019-01-04T21:48:27+00:00",
    "url": "http://arxiv.org/pdf/1901.01322v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1806.01793v1",
    "title": "Gradient-based Filter Design for the Dual-tree Wavelet Transform",
    "content": "Gradient-based Filter Design for the Dual-tree Wavelet Transform. The wavelet transform has seen success when incorporated into neural network architectures, such as in wavelet scattering networks. More recently, it has been shown that the dual-tree complex wavelet transform can provide better representations than the standard transform. With this in mind, we extend our previous method for learning filters for the 1D and 2D wavelet transforms into the dual-tree domain. We show that with few modifications to our original model, we can learn directional filters that leverage the properties of the dual-tree wavelet transform.",
    "authors": [
      "Daniel Recoskie",
      "Richard Mann"
    ],
    "published": "2018-06-04T16:29:23+00:00",
    "url": "http://arxiv.org/pdf/1806.01793v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2002.06170v1",
    "title": "Transformer on a Diet",
    "content": "Transformer on a Diet. Transformer has been widely used thanks to its ability to capture sequence information in an efficient way. However, recent developments, such as BERT and GPT-2, deliver only heavy architectures with a focus on effectiveness. In this paper, we explore three carefully-designed light Transformer architectures to figure out whether the Transformer with less computations could produce competitive results. Experimental results on language model benchmark datasets hint that such trade-off is promising, and the light Transformer reduces 70% parameters at best, while obtains competitive perplexity compared to standard Transformer. The source code is publicly available.",
    "authors": [
      "Chenguang Wang",
      "Zihao Ye",
      "Aston Zhang",
      "Zheng Zhang",
      "Alexander J. Smola"
    ],
    "published": "2020-02-14T18:41:58+00:00",
    "url": "http://arxiv.org/pdf/2002.06170v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2107.10932v1",
    "title": "FNetAR: Mixing Tokens with Autoregressive Fourier Transforms",
    "content": "FNetAR: Mixing Tokens with Autoregressive Fourier Transforms. In this note we examine the autoregressive generalization of the FNet algorithm, in which self-attention layers from the standard Transformer architecture are substituted with a trivial sparse-uniformsampling procedure based on Fourier transforms. Using the Wikitext-103 benchmark, we demonstratethat FNetAR retains state-of-the-art performance (25.8 ppl) on the task of causal language modelingcompared to a Transformer-XL baseline (24.2 ppl) with only half the number self-attention layers,thus providing further evidence for the superfluity of deep neural networks with heavily compoundedattention mechanisms. The autoregressive Fourier transform could likely be used for parameterreduction on most Transformer-based time-series prediction models.",
    "authors": [
      "Tim Lou",
      "Michael Park",
      "Mohammad Ramezanali",
      "Vincent Tang"
    ],
    "published": "2021-07-22T21:24:02+00:00",
    "url": "http://arxiv.org/pdf/2107.10932v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2004.03761v1",
    "title": "Adaptive Transformers in RL",
    "content": "Adaptive Transformers in RL. Recent developments in Transformers have opened new interesting areas of research in partially observable reinforcement learning tasks. Results from late 2019 showed that Transformers are able to outperform LSTMs on both memory intense and reactive tasks. In this work we first partially replicate the results shown in Stabilizing Transformers in RL on both reactive and memory based environments. We then show performance improvement coupled with reduced computation when adding adaptive attention span to this Stable Transformer on a challenging DMLab30 environment. The code for all our experiments and models is available at https://github.com/jerrodparker20/adaptive-transformers-in-rl.",
    "authors": [
      "Shakti Kumar",
      "Jerrod Parker",
      "Panteha Naderian"
    ],
    "published": "2020-04-08T01:03:10+00:00",
    "url": "http://arxiv.org/pdf/2004.03761v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2205.07056v1",
    "title": "Transformer Scale Gate for Semantic Segmentation",
    "content": "Transformer Scale Gate for Semantic Segmentation. Effectively encoding multi-scale contextual information is crucial for accurate semantic segmentation. Existing transformer-based segmentation models combine features across scales without any selection, where features on sub-optimal scales may degrade segmentation outcomes. Leveraging from the inherent properties of Vision Transformers, we propose a simple yet effective module, Transformer Scale Gate (TSG), to optimally combine multi-scale features.TSG exploits cues in self and cross attentions in Vision Transformers for the scale selection. TSG is a highly flexible plug-and-play module, and can easily be incorporated with any encoder-decoder-based hierarchical vision Transformer architecture. Extensive experiments on the Pascal Context and ADE20K datasets demonstrate that our feature selection strategy achieves consistent gains.",
    "authors": [
      "Hengcan Shi",
      "Munawar Hayat",
      "Jianfei Cai"
    ],
    "published": "2022-05-14T13:11:39+00:00",
    "url": "http://arxiv.org/pdf/2205.07056v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2309.13641v1",
    "title": "Transformations on hypergraph families",
    "content": "Transformations on hypergraph families. We present a new general theory of function-based hypergraph transformations on finite families of finite hypergraphs. A function-based hypergraph transformation formalises the action of structurally modifying hypergraphs from a family in a consistent manner. The mathematical form of the transformations facilitates their analysis and incorporation into larger mathematical structures, and concurs with the function-based nature of modelling in the physical world. Since quotients of hypergraphs afford their simplification and comparison, we also discuss the notion of a quotient hypergraph transformation induced by an equivalence relation on the vertex set of a hypergraph family. Finally, we demonstrate function-based hypergraph transformations with two fundamental classes of examples involving the addition or deletion of hyperedges or hypergraphs.",
    "authors": [
      "Sean Trinity Vittadello"
    ],
    "published": "2023-09-24T14:02:07+00:00",
    "url": "http://arxiv.org/pdf/2309.13641v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2406.16784v1",
    "title": "The Progression of Transformers from Language to Vision to MOT: A Literature Review on Multi-Object Tracking with Transformers",
    "content": "The Progression of Transformers from Language to Vision to MOT: A Literature Review on Multi-Object Tracking with Transformers. The transformer neural network architecture allows for autoregressive sequence-to-sequence modeling through the use of attention layers. It was originally created with the application of machine translation but has revolutionized natural language processing. Recently, transformers have also been applied across a wide variety of pattern recognition tasks, particularly in computer vision. In this literature review, we describe major advances in computer vision utilizing transformers. We then focus specifically on Multi-Object Tracking (MOT) and discuss how transformers are increasingly becoming competitive in state-of-the-art MOT works, yet still lag behind traditional deep learning methods.",
    "authors": [
      "Abhi Kamboj"
    ],
    "published": "2024-06-24T16:45:28+00:00",
    "url": "http://arxiv.org/pdf/2406.16784v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2411.17029v1",
    "title": "$β$ orientation reconstruction and shear deformation calculation in hcp-bcc-hcp phase transformation",
    "content": "$β$ orientation reconstruction and shear deformation calculation in hcp-bcc-hcp phase transformation. We introduce a cluster-based technique to automate pixel-wise reconstruction of $\\beta$ orientations from parent $\\alpha$ orientations over large, indexed regions. This approach provides a valuable tool for analyzing problems that require historical information about current $\\alpha$ microstructures, such as investigating variant selection mechanisms during the $\\alpha \\to \\beta \\to \\alpha$ transformation. Additionally, we present a method for calculating deformation gradient variants associated with phase transformations between hcp ($\\alpha$) and bcc ($\\beta$) phases based upon a series of frame rotations and shape transformations. This method streamlines the integration of transformation kinematics into continuum-based models by enabling convenient computation of the deformation gradient governing the transformation.",
    "authors": [
      "Zhuowen Zhao",
      "Thomas R. Bieler",
      "Philip Eisenlohr"
    ],
    "published": "2024-11-26T01:46:34+00:00",
    "url": "http://arxiv.org/pdf/2411.17029v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2501.17727v1",
    "title": "Sparse Autoencoders Can Interpret Randomly Initialized Transformers",
    "content": "Sparse Autoencoders Can Interpret Randomly Initialized Transformers. Sparse autoencoders (SAEs) are an increasingly popular technique for interpreting the internal representations of transformers. In this paper, we apply SAEs to 'interpret' random transformers, i.e., transformers where the parameters are sampled IID from a Gaussian rather than trained on text data. We find that random and trained transformers produce similarly interpretable SAE latents, and we confirm this finding quantitatively using an open-source auto-interpretability pipeline. Further, we find that SAE quality metrics are broadly similar for random and trained transformers. We find that these results hold across model sizes and layers. We discuss a number of number interesting questions that this work raises for the use of SAEs and auto-interpretability in the context of mechanistic interpretability.",
    "authors": [
      "Thomas Heap",
      "Tim Lawson",
      "Lucy Farnik",
      "Laurence Aitchison"
    ],
    "published": "2025-01-29T16:11:12+00:00",
    "url": "http://arxiv.org/pdf/2501.17727v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2305.09880v4",
    "title": "A survey of the Vision Transformers and their CNN-Transformer based Variants",
    "content": "A survey of the Vision Transformers and their CNN-Transformer based Variants. Vision transformers have become popular as a possible substitute to convolutional neural networks (CNNs) for a variety of computer vision applications. These transformers, with their ability to focus on global relationships in images, offer large learning capacity. However, they may suffer from limited generalization as they do not tend to model local correlation in images. Recently, in vision transformers hybridization of both the convolution operation and self-attention mechanism has emerged, to exploit both the local and global image representations. These hybrid vision transformers, also referred to as CNN-Transformer architectures, have demonstrated remarkable results in vision applications. Given the rapidly growing number of hybrid vision transformers, it has become necessary to provide a taxonomy and explanation of these hybrid architectures. This survey presents a taxonomy of the recent vision transformer architectures and more specifically that of the hybrid vision transformers. Additionally, the key features of these architectures such as the attention mechanisms, positional embeddings, multi-scale processing, and convolution are also discussed. In contrast to the previous survey papers that are primarily focused on individual vision transformer architectures or CNNs, this survey uniquely emphasizes the emerging trend of hybrid vision transformers. By showcasing the potential of hybrid vision transformers to deliver exceptional performance across a range of computer vision tasks, this survey sheds light on the future directions of this rapidly evolving architecture.",
    "authors": [
      "Asifullah Khan",
      "Zunaira Rauf",
      "Anabia Sohail",
      "Abdul Rehman",
      "Hifsa Asif",
      "Aqsa Asif",
      "Umair Farooq"
    ],
    "published": "2023-05-17T01:27:27+00:00",
    "url": "http://arxiv.org/pdf/2305.09880v4"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1703.08113v1",
    "title": "Well-Behaved Model Transformations with Model Subtyping",
    "content": "Well-Behaved Model Transformations with Model Subtyping. In model-driven engineering, models abstract the relevant features of software artefacts and model transformations act on them automating complex tasks of the development process. It is, thus, crucially important to provide pragmatic, reliable methods to verify that model transformations guarantee the correctness of generated models in order to ensure the quality of the final end product. In this paper, we build on an object-oriented algebraic encoding of metamodels and models as defined in the standard Meta-Object Facility and in tools, such as the Eclipse Modeling Framework, to specify a domain-specific language for representing the action part of model transformations. We introduce the big-step operational structural semantics of this language and its type system, which includes a notion of polymorphic model subtyping, showing that well-typed model transformations are well behaved. That is, that metamodel-conformant model transformations never go wrong.",
    "authors": [
      "Artur Boronat"
    ],
    "published": "2017-03-23T15:42:14+00:00",
    "url": "http://arxiv.org/pdf/1703.08113v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2109.05522v1",
    "title": "TEASEL: A Transformer-Based Speech-Prefixed Language Model",
    "content": "TEASEL: A Transformer-Based Speech-Prefixed Language Model. Multimodal language analysis is a burgeoning field of NLP that aims to simultaneously model a speaker's words, acoustical annotations, and facial expressions. In this area, lexicon features usually outperform other modalities because they are pre-trained on large corpora via Transformer-based models. Despite their strong performance, training a new self-supervised learning (SSL) Transformer on any modality is not usually attainable due to insufficient data, which is the case in multimodal language learning. This work proposes a Transformer-Based Speech-Prefixed Language Model called TEASEL to approach the mentioned constraints without training a complete Transformer model. TEASEL model includes speech modality as a dynamic prefix besides the textual modality compared to a conventional language model. This method exploits a conventional pre-trained language model as a cross-modal Transformer model. We evaluated TEASEL for the multimodal sentiment analysis task defined by CMU-MOSI dataset. Extensive experiments show that our model outperforms unimodal baseline language models by 4% and outperforms the current multimodal state-of-the-art (SoTA) model by 1% in F1-score. Additionally, our proposed method is 72% smaller than the SoTA model.",
    "authors": [
      "Mehdi Arjmand",
      "Mohammad Javad Dousti",
      "Hadi Moradi"
    ],
    "published": "2021-09-12T14:08:57+00:00",
    "url": "http://arxiv.org/pdf/2109.05522v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1804.11121v1",
    "title": "Towards the Automation of Metamorphic Testing in Model Transformations",
    "content": "Towards the Automation of Metamorphic Testing in Model Transformations. Model transformations are the cornerstone of Model-Driven Engineering, and provide the essential mechanisms for manipulating and transforming models. Checking whether the output of a model transformation is correct is a manual and error-prone task, this is referred to as the oracle problem in the software testing literature. The correctness of the model transformation program is crucial for the proper generation of its output, so it should be tested. Metamorphic testing is a testing technique to alleviate the oracle problem consisting on exploiting the relations between different inputs and outputs of the program under test, so-called metamorphic relations. In this paper we give an insight into our approach to generically define metamorphic relations for model transformations, which can be automatically instantiated given any specific model transformation.",
    "authors": [
      "Javier Troya",
      "Sergio Segura",
      "Antonio Ruiz-Cortés"
    ],
    "published": "2018-04-30T10:57:51+00:00",
    "url": "http://arxiv.org/pdf/1804.11121v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2010.04759v1",
    "title": "A Generic Approach to Detect Design Patterns in Model Transformations Using a String-Matching Algorithm",
    "content": "A Generic Approach to Detect Design Patterns in Model Transformations Using a String-Matching Algorithm. Maintaining software artifacts is among the hardest tasks an engineer faces. Like any other piece of code, model transformations developed by engineers are also subject to maintenance. To facilitate the comprehension of programs, software engineers rely on many techniques, such as design pattern detection. Therefore, detecting design patterns in model transformation implementations is of tremendous value for developers. In this paper, we propose a generic technique to detect design patterns and their variations in model transformation implementations automatically. It takes as input a set of model transformation rules and the participants of a model transformation design pattern to find occurrences of the latter in the former. The technique also detects certain kinds of degenerate forms of the pattern, thus indicating potential opportunities to improve the model transformation implementation.",
    "authors": [
      "Chihab eddine Mokaddem",
      "Houari Sahraoui",
      "Eugene Syriani"
    ],
    "published": "2020-10-09T18:39:08+00:00",
    "url": "http://arxiv.org/pdf/2010.04759v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2202.09481v2",
    "title": "TransDreamer: Reinforcement Learning with Transformer World Models",
    "content": "TransDreamer: Reinforcement Learning with Transformer World Models. The Dreamer agent provides various benefits of Model-Based Reinforcement Learning (MBRL) such as sample efficiency, reusable knowledge, and safe planning. However, its world model and policy networks inherit the limitations of recurrent neural networks and thus an important question is how an MBRL framework can benefit from the recent advances of transformers and what the challenges are in doing so. In this paper, we propose a transformer-based MBRL agent, called TransDreamer. We first introduce the Transformer State-Space Model, a world model that leverages a transformer for dynamics predictions. We then share this world model with a transformer-based policy network and obtain stability in training a transformer-based RL agent. In experiments, we apply the proposed model to 2D visual RL and 3D first-person visual RL tasks both requiring long-range memory access for memory-based reasoning. We show that the proposed model outperforms Dreamer in these complex tasks.",
    "authors": [
      "Chang Chen",
      "Yi-Fu Wu",
      "Jaesik Yoon",
      "Sungjin Ahn"
    ],
    "published": "2022-02-19T00:30:52+00:00",
    "url": "http://arxiv.org/pdf/2202.09481v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2207.05366v2",
    "title": "Image and Model Transformation with Secret Key for Vision Transformer",
    "content": "Image and Model Transformation with Secret Key for Vision Transformer. In this paper, we propose a combined use of transformed images and vision transformer (ViT) models transformed with a secret key. We show for the first time that models trained with plain images can be directly transformed to models trained with encrypted images on the basis of the ViT architecture, and the performance of the transformed models is the same as models trained with plain images when using test images encrypted with the key. In addition, the proposed scheme does not require any specially prepared data for training models or network modification, so it also allows us to easily update the secret key. In an experiment, the effectiveness of the proposed scheme is evaluated in terms of performance degradation and model protection performance in an image classification task on the CIFAR-10 dataset.",
    "authors": [
      "Hitoshi Kiya",
      "Ryota Iijima",
      "MaungMaung Aprilpyone",
      "Yuma Kinoshita"
    ],
    "published": "2022-07-12T08:02:47+00:00",
    "url": "http://arxiv.org/pdf/2207.05366v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2402.01032v2",
    "title": "Repeat After Me: Transformers are Better than State Space Models at Copying",
    "content": "Repeat After Me: Transformers are Better than State Space Models at Copying. Transformers are the dominant architecture for sequence modeling, but there is growing interest in models that use a fixed-size latent state that does not depend on the sequence length, which we refer to as \"generalized state space models\" (GSSMs). In this paper we show that while GSSMs are promising in terms of inference-time efficiency, they are limited compared to transformer models on tasks that require copying from the input context. We start with a theoretical analysis of the simple task of string copying and prove that a two layer transformer can copy strings of exponential length while GSSMs are fundamentally limited by their fixed-size latent state. Empirically, we find that transformers outperform GSSMs in terms of efficiency and generalization on synthetic tasks that require copying the context. Finally, we evaluate pretrained large language models and find that transformer models dramatically outperform state space models at copying and retrieving information from context. Taken together, these results suggest a fundamental gap between transformers and GSSMs on tasks of practical interest.",
    "authors": [
      "Samy Jelassi",
      "David Brandfonbrener",
      "Sham M. Kakade",
      "Eran Malach"
    ],
    "published": "2024-02-01T21:44:11+00:00",
    "url": "http://arxiv.org/pdf/2402.01032v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2407.18841v2",
    "title": "QT-TDM: Planning With Transformer Dynamics Model and Autoregressive Q-Learning",
    "content": "QT-TDM: Planning With Transformer Dynamics Model and Autoregressive Q-Learning. Inspired by the success of the Transformer architecture in natural language processing and computer vision, we investigate the use of Transformers in Reinforcement Learning (RL), specifically in modeling the environment's dynamics using Transformer Dynamics Models (TDMs). We evaluate the capabilities of TDMs for continuous control in real-time planning scenarios with Model Predictive Control (MPC). While Transformers excel in long-horizon prediction, their tokenization mechanism and autoregressive nature lead to costly planning over long horizons, especially as the environment's dimensionality increases. To alleviate this issue, we use a TDM for short-term planning, and learn an autoregressive discrete Q-function using a separate Q-Transformer (QT) model to estimate a long-term return beyond the short-horizon planning. Our proposed method, QT-TDM, integrates the robust predictive capabilities of Transformers as dynamics models with the efficacy of a model-free Q-Transformer to mitigate the computational burden associated with real-time planning. Experiments in diverse state-based continuous control tasks show that QT-TDM is superior in performance and sample efficiency compared to existing Transformer-based RL models while achieving fast and computationally efficient inference.",
    "authors": [
      "Mostafa Kotb",
      "Cornelius Weber",
      "Muhammad Burhan Hafez",
      "Stefan Wermter"
    ],
    "published": "2024-07-26T16:05:26+00:00",
    "url": "http://arxiv.org/pdf/2407.18841v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2310.12296v1",
    "title": "Understanding Video Transformers for Segmentation: A Survey of Application and Interpretability",
    "content": "Understanding Video Transformers for Segmentation: A Survey of Application and Interpretability. Video segmentation encompasses a wide range of categories of problem formulation, e.g., object, scene, actor-action and multimodal video segmentation, for delineating task-specific scene components with pixel-level masks. Recently, approaches in this research area shifted from concentrating on ConvNet-based to transformer-based models. In addition, various interpretability approaches have appeared for transformer models and video temporal dynamics, motivated by the growing interest in basic scientific understanding, model diagnostics and societal implications of real-world deployment. Previous surveys mainly focused on ConvNet models on a subset of video segmentation tasks or transformers for classification tasks. Moreover, component-wise discussion of transformer-based video segmentation models has not yet received due focus. In addition, previous reviews of interpretability methods focused on transformers for classification, while analysis of video temporal dynamics modelling capabilities of video models received less attention. In this survey, we address the above with a thorough discussion of various categories of video segmentation, a component-wise discussion of the state-of-the-art transformer-based models, and a review of related interpretability methods. We first present an introduction to the different video segmentation task categories, their objectives, specific challenges and benchmark datasets. Next, we provide a component-wise review of recent transformer-based models and document the state of the art on different video segmentation tasks. Subsequently, we discuss post-hoc and ante-hoc interpretability methods for transformer models and interpretability methods for understanding the role of the temporal dimension in video models. Finally, we conclude our discussion with future research directions.",
    "authors": [
      "Rezaul Karim",
      "Richard P. Wildes"
    ],
    "published": "2023-10-18T19:58:25+00:00",
    "url": "http://arxiv.org/pdf/2310.12296v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1312.1019v1",
    "title": "$L^2$ orbital stability of Dirac solitons in the massive Thirring model",
    "content": "$L^2$ orbital stability of Dirac solitons in the massive Thirring model. We prove $L^2$ orbital stability of Dirac solitons in the massive Thirring model. Our analysis uses local well posedness of the massive Thirring model in $L^2$, conservation of the charge functional, and the auto--B\\\"{a}cklund transformation. The latter transformation exists because the massive Thirring model is integrable via the inverse scattering transform method.",
    "authors": [
      "Andres Contreras",
      "Dmitry E. Pelinovsky",
      "Yusuke Shimabukuro"
    ],
    "published": "2013-12-04T04:26:34+00:00",
    "url": "http://arxiv.org/pdf/1312.1019v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1903.07402v2",
    "title": "Neutron: An Implementation of the Transformer Translation Model and its Variants",
    "content": "Neutron: An Implementation of the Transformer Translation Model and its Variants. The Transformer translation model is easier to parallelize and provides better performance compared to recurrent seq2seq models, which makes it popular among industry and research community. We implement the Neutron in this work, including the Transformer model and its several variants from most recent researches. It is highly optimized, easy to modify and provides comparable performance with interesting features while keeping readability.",
    "authors": [
      "Hongfei Xu",
      "Qiuhui Liu"
    ],
    "published": "2019-03-18T12:54:22+00:00",
    "url": "http://arxiv.org/pdf/1903.07402v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1001.1027v5",
    "title": "An Unsupervised Algorithm For Learning Lie Group Transformations",
    "content": "An Unsupervised Algorithm For Learning Lie Group Transformations. We present several theoretical contributions which allow Lie groups to be fit to high dimensional datasets. Transformation operators are represented in their eigen-basis, reducing the computational complexity of parameter estimation to that of training a linear transformation model. A transformation specific \"blurring\" operator is introduced that allows inference to escape local minima via a smoothing of the transformation space. A penalty on traversed manifold distance is added which encourages the discovery of sparse, minimal distance, transformations between states. Both learning and inference are demonstrated using these methods for the full set of affine transformations on natural image patches. Transformation operators are then trained on natural video sequences. It is shown that the learned video transformations provide a better description of inter-frame differences than the standard motion model based on rigid translation.",
    "authors": [
      "Jascha Sohl-Dickstein",
      "Ching Ming Wang",
      "Bruno A. Olshausen"
    ],
    "published": "2010-01-07T06:22:56+00:00",
    "url": "http://arxiv.org/pdf/1001.1027v5"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2008.03452v2",
    "title": "Partitioning signal classes using transport transforms for data analysis and machine learning",
    "content": "Partitioning signal classes using transport transforms for data analysis and machine learning. A relatively new set of transport-based transforms (CDT, R-CDT, LOT) have shown their strength and great potential in various image and data processing tasks such as parametric signal estimation, classification, cancer detection among many others. It is hence worthwhile to elucidate some of the mathematical properties that explain the successes of these transforms when they are used as tools in data analysis, signal processing or data classification. In particular, we give conditions under which classes of signals that are created by algebraic generative models are transformed into convex sets by the transport transforms. Such convexification of the classes simplify the classification and other data analysis and processing problems when viewed in the transform domain. More specifically, we study the extent and limitation of the convexification ability of these transforms under an algebraic generative modeling framework. We hope that this paper will serve as an introduction to these transforms and will encourage mathematicians and other researchers to further explore the theoretical underpinnings and algorithmic tools that will help understand the successes of these transforms and lay the groundwork for further successful applications.",
    "authors": [
      "Akram Aldroubi",
      "Shiying Li",
      "Gustavo K. Rohde"
    ],
    "published": "2020-08-08T06:12:10+00:00",
    "url": "http://arxiv.org/pdf/2008.03452v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2105.12043v1",
    "title": "Temporal Action Proposal Generation with Transformers",
    "content": "Temporal Action Proposal Generation with Transformers. Transformer networks are effective at modeling long-range contextual information and have recently demonstrated exemplary performance in the natural language processing domain. Conventionally, the temporal action proposal generation (TAPG) task is divided into two main sub-tasks: boundary prediction and proposal confidence prediction, which rely on the frame-level dependencies and proposal-level relationships separately. To capture the dependencies at different levels of granularity, this paper intuitively presents a unified temporal action proposal generation framework with original Transformers, called TAPG Transformer, which consists of a Boundary Transformer and a Proposal Transformer. Specifically, the Boundary Transformer captures long-term temporal dependencies to predict precise boundary information and the Proposal Transformer learns the rich inter-proposal relationships for reliable confidence evaluation. Extensive experiments are conducted on two popular benchmarks: ActivityNet-1.3 and THUMOS14, and the results demonstrate that TAPG Transformer outperforms state-of-the-art methods. Equipped with the existing action classifier, our method achieves remarkable performance on the temporal action localization task. Codes and models will be available.",
    "authors": [
      "Lining Wang",
      "Haosen Yang",
      "Wenhao Wu",
      "Hongxun Yao",
      "Hujie Huang"
    ],
    "published": "2021-05-25T16:22:12+00:00",
    "url": "http://arxiv.org/pdf/2105.12043v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2208.11790v2",
    "title": "Addressing Token Uniformity in Transformers via Singular Value Transformation",
    "content": "Addressing Token Uniformity in Transformers via Singular Value Transformation. Token uniformity is commonly observed in transformer-based models, in which different tokens share a large proportion of similar information after going through stacked multiple self-attention layers in a transformer. In this paper, we propose to use the distribution of singular values of outputs of each transformer layer to characterise the phenomenon of token uniformity and empirically illustrate that a less skewed singular value distribution can alleviate the `token uniformity' problem. Base on our observations, we define several desirable properties of singular value distributions and propose a novel transformation function for updating the singular values. We show that apart from alleviating token uniformity, the transformation function should preserve the local neighbourhood structure in the original embedding space. Our proposed singular value transformation function is applied to a range of transformer-based language models such as BERT, ALBERT, RoBERTa and DistilBERT, and improved performance is observed in semantic textual similarity evaluation and a range of GLUE tasks. Our source code is available at https://github.com/hanqi-qi/tokenUni.git.",
    "authors": [
      "Hanqi Yan",
      "Lin Gui",
      "Wenjie Li",
      "Yulan He"
    ],
    "published": "2022-08-24T22:44:09+00:00",
    "url": "http://arxiv.org/pdf/2208.11790v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2406.19249v1",
    "title": "NTFormer: A Composite Node Tokenized Graph Transformer for Node Classification",
    "content": "NTFormer: A Composite Node Tokenized Graph Transformer for Node Classification. Recently, the emerging graph Transformers have made significant advancements for node classification on graphs. In most graph Transformers, a crucial step involves transforming the input graph into token sequences as the model input, enabling Transformer to effectively learn the node representations. However, we observe that existing methods only express partial graph information of nodes through single-type token generation. Consequently, they require tailored strategies to encode additional graph-specific features into the Transformer to ensure the quality of node representation learning, limiting the model flexibility to handle diverse graphs. To this end, we propose a new graph Transformer called NTFormer to address this issue. NTFormer introduces a novel token generator called Node2Par, which constructs various token sequences using different token elements for each node. This flexibility allows Node2Par to generate valuable token sequences from different perspectives, ensuring comprehensive expression of rich graph features. Benefiting from the merits of Node2Par, NTFormer only leverages a Transformer-based backbone without graph-specific modifications to learn node representations, eliminating the need for graph-specific modifications. Extensive experiments conducted on various benchmark datasets containing homophily and heterophily graphs with different scales demonstrate the superiority of NTFormer over representative graph Transformers and graph neural networks for node classification.",
    "authors": [
      "Jinsong Chen",
      "Siyu Jiang",
      "Kun He"
    ],
    "published": "2024-06-27T15:16:00+00:00",
    "url": "http://arxiv.org/pdf/2406.19249v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2407.07745v1",
    "title": "Quantization of KLT Matrices via GMRF Modeling of Image Blocks for Adaptive Transform Coding",
    "content": "Quantization of KLT Matrices via GMRF Modeling of Image Blocks for Adaptive Transform Coding. Forward adaptive transform coding of images requires a codebook of transform matrices from which the best transform can be chosen for each macroblock. Codebook construction is a problem of designing a quantizer for Karhunen-L\\'{o}eve transform (KLT) matrices estimated from sample image blocks. We present a novel method for KLT matrix quantization based on a finite-lattice non-causal homogeneous Gauss-Markov random field (GMRF) model with asymmetric Neumann boundary conditions for blocks in natural images. The matrix quantization problem is solved in the GMRF parameter space, simplifying the harder problem of quantizing a large matrix subject to an orthonormality constraint to a low-dimensional vector quantization problem. Typically used GMRF parameter estimation methods such as maximum-likelihood (ML) do not necessarily maximize the coding performance of the resulting transform matrices. To this end we propose a method for GMRF parameter estimation from sample image data, which maximizes the high-rate transform coding gain. We also investigate the application of GMRF-based transforms to variable block-size adaptive transform coding.",
    "authors": [
      "Rashmi Boragolla",
      "Pradeepa Yahampath"
    ],
    "published": "2024-06-24T22:56:31+00:00",
    "url": "http://arxiv.org/pdf/2407.07745v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2402.13388v3",
    "title": "Transformer tricks: Precomputing the first layer",
    "content": "Transformer tricks: Precomputing the first layer. This micro-paper describes a trick to speed up inference of transformers with RoPE (such as LLaMA, Mistral, PaLM, and Gemma). For these models, a large portion of the first transformer layer can be precomputed, which results in slightly lower latency and lower cost-per-token. Because this trick optimizes only one layer, the relative savings depend on the total number of layers. For example, the maximum savings for a model with only 4 layers (such as Whisper tiny) is limited to 25%, while a 32-layer model is limited to 3% savings. See https://github.com/OpenMachine-ai/transformer-tricks for code and more transformer tricks.",
    "authors": [
      "Nils Graef"
    ],
    "published": "2024-02-20T21:34:56+00:00",
    "url": "http://arxiv.org/pdf/2402.13388v3"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2406.16741v2",
    "title": "Extracting thin film structures of energy materials using transformers",
    "content": "Extracting thin film structures of energy materials using transformers. Neutron-Transformer Reflectometry and Advanced Computation Engine (N-TRACE ), a neural network model using transformer architecture, is introduced for neutron reflectometry data analysis. It offers fast, accurate initial parameter estimations and efficient refinements, improving efficiency and precision for real-time data analysis of lithium-mediated nitrogen reduction for electrochemical ammonia synthesis, with relevance to other chemical transformations and batteries. Despite limitations in generalizing across systems, it shows promises for the use of transformers as the basis for models that could replace trial-and-error approaches to modeling reflectometry data.",
    "authors": [
      "Chen Zhang",
      "Valerie A. Niemann",
      "Peter Benedek",
      "Thomas F. Jaramillo",
      "Mathieu Doucet"
    ],
    "published": "2024-06-24T15:48:19+00:00",
    "url": "http://arxiv.org/pdf/2406.16741v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2503.09791v1",
    "title": "Minimal Time Series Transformer",
    "content": "Minimal Time Series Transformer. Transformer is the state-of-the-art model for many natural language processing, computer vision, and audio analysis problems. Transformer effectively combines information from the past input and output samples in auto-regressive manner so that each sample becomes aware of all inputs and outputs. In sequence-to-sequence (Seq2Seq) modeling, the transformer processed samples become effective in predicting the next output. Time series forecasting is a Seq2Seq problem. The original architecture is defined for discrete input and output sequence tokens, but to adopt it for time series, the model must be adapted for continuous data. This work introduces minimal adaptations to make the original transformer architecture suitable for continuous value time series data.",
    "authors": [
      "Joni-Kristian Kämäräinen"
    ],
    "published": "2025-03-12T19:48:37+00:00",
    "url": "http://arxiv.org/pdf/2503.09791v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1911.07004v1",
    "title": "AETv2: AutoEncoding Transformations for Self-Supervised Representation Learning by Minimizing Geodesic Distances in Lie Groups",
    "content": "AETv2: AutoEncoding Transformations for Self-Supervised Representation Learning by Minimizing Geodesic Distances in Lie Groups. Self-supervised learning by predicting transformations has demonstrated outstanding performances in both unsupervised and (semi-)supervised tasks. Among the state-of-the-art methods is the AutoEncoding Transformations (AET) by decoding transformations from the learned representations of original and transformed images. Both deterministic and probabilistic AETs rely on the Euclidean distance to measure the deviation of estimated transformations from their groundtruth counterparts. However, this assumption is questionable as a group of transformations often reside on a curved manifold rather staying in a flat Euclidean space. For this reason, we should use the geodesic to characterize how an image transform along the manifold of a transformation group, and adopt its length to measure the deviation between transformations. Particularly, we present to autoencode a Lie group of homography transformations PG(2) to learn image representations. For this, we make an estimate of the intractable Riemannian logarithm by projecting PG(2) to a subgroup of rotation transformations SO(3) that allows the closed-form expression of geodesic distances. Experiments demonstrate the proposed AETv2 model outperforms the previous version as well as the other state-of-the-art self-supervised models in multiple tasks.",
    "authors": [
      "Feng Lin",
      "Haohang Xu",
      "Houqiang Li",
      "Hongkai Xiong",
      "Guo-Jun Qi"
    ],
    "published": "2019-11-16T09:58:58+00:00",
    "url": "http://arxiv.org/pdf/1911.07004v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1903.10863v3",
    "title": "AVT: Unsupervised Learning of Transformation Equivariant Representations by Autoencoding Variational Transformations",
    "content": "AVT: Unsupervised Learning of Transformation Equivariant Representations by Autoencoding Variational Transformations. The learning of Transformation-Equivariant Representations (TERs), which is introduced by Hinton et al. \\cite{hinton2011transforming}, has been considered as a principle to reveal visual structures under various transformations. It contains the celebrated Convolutional Neural Networks (CNNs) as a special case that only equivary to the translations. In contrast, we seek to train TERs for a generic class of transformations and train them in an {\\em unsupervised} fashion. To this end, we present a novel principled method by Autoencoding Variational Transformations (AVT), compared with the conventional approach to autoencoding data. Formally, given transformed images, the AVT seeks to train the networks by maximizing the mutual information between the transformations and representations. This ensures the resultant TERs of individual images contain the {\\em intrinsic} information about their visual structures that would equivary {\\em extricably} under various transformations in a generalized {\\em nonlinear} case. Technically, we show that the resultant optimization problem can be efficiently solved by maximizing a variational lower-bound of the mutual information. This variational approach introduces a transformation decoder to approximate the intractable posterior of transformations, resulting in an autoencoding architecture with a pair of the representation encoder and the transformation decoder. Experiments demonstrate the proposed AVT model sets a new record for the performances on unsupervised tasks, greatly closing the performance gap to the supervised models.",
    "authors": [
      "Guo-Jun Qi",
      "Liheng Zhang",
      "Chang Wen Chen",
      "Qi Tian"
    ],
    "published": "2019-03-23T21:31:10+00:00",
    "url": "http://arxiv.org/pdf/1903.10863v3"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1306.6734v1",
    "title": "A novel ER model to relational model transformation algorithm for semantically clear high quality database design",
    "content": "A novel ER model to relational model transformation algorithm for semantically clear high quality database design. Conceptual modelling using the entity relationship (ER) model has been widely used for database design for a long period of time. However, studies indicate that creating a satisfactory relational model representation from an ER model is uncertain due to the insufficiencies both in the transformation methods used and in the relational model itself. In an effort to solve the issue the original ER notation has been modified, and accordingly, a new transformation algorithm has been developed. This paper presents the proposed transformation algorithm. Using a real world example it shows how the algorithm can be applied in practice. The paper also discusses how to validate the resulted database and reclaim the information that it represents.",
    "authors": [
      "Dhammika Pieris"
    ],
    "published": "2013-06-28T07:12:23+00:00",
    "url": "http://arxiv.org/pdf/1306.6734v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2002.02337v1",
    "title": "The Generalized Crofoot Transform",
    "content": "The Generalized Crofoot Transform. We introduce a generalized Crofoot transform between the model spaces corresponding to matrix-valued inner functions. As an application, we obtain results about matrix-valued truncated Toeplitz operators.",
    "authors": [
      "Rewayat Khan"
    ],
    "published": "2020-01-23T17:50:19+00:00",
    "url": "http://arxiv.org/pdf/2002.02337v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2009.02070v2",
    "title": "AutoTrans: Automating Transformer Design via Reinforced Architecture Search",
    "content": "AutoTrans: Automating Transformer Design via Reinforced Architecture Search. Though the transformer architectures have shown dominance in many natural language understanding tasks, there are still unsolved issues for the training of transformer models, especially the need for a principled way of warm-up which has shown importance for stable training of a transformer, as well as whether the task at hand prefer to scale the attention product or not. In this paper, we empirically explore automating the design choices in the transformer model, i.e., how to set layer-norm, whether to scale, number of layers, number of heads, activation function, etc, so that one can obtain a transformer architecture that better suits the tasks at hand. RL is employed to navigate along search space, and special parameter sharing strategies are designed to accelerate the search. It is shown that sampling a proportion of training data per epoch during search help to improve the search quality. Experiments on the CoNLL03, Multi-30k, IWSLT14 and WMT-14 shows that the searched transformer model can outperform the standard transformers. In particular, we show that our learned model can be trained more robustly with large learning rates without warm-up.",
    "authors": [
      "Wei Zhu",
      "Xiaoling Wang",
      "Xipeng Qiu",
      "Yuan Ni",
      "Guotong Xie"
    ],
    "published": "2020-09-04T08:46:22+00:00",
    "url": "http://arxiv.org/pdf/2009.02070v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2103.14803v2",
    "title": "Face Transformer for Recognition",
    "content": "Face Transformer for Recognition. Recently there has been a growing interest in Transformer not only in NLP but also in computer vision. We wonder if transformer can be used in face recognition and whether it is better than CNNs. Therefore, we investigate the performance of Transformer models in face recognition. Considering the original Transformer may neglect the inter-patch information, we modify the patch generation process and make the tokens with sliding patches which overlaps with each others. The models are trained on CASIA-WebFace and MS-Celeb-1M databases, and evaluated on several mainstream benchmarks, including LFW, SLLFW, CALFW, CPLFW, TALFW, CFP-FP, AGEDB and IJB-C databases. We demonstrate that Face Transformer models trained on a large-scale database, MS-Celeb-1M, achieve comparable performance as CNN with similar number of parameters and MACs. To facilitate further researches, Face Transformer models and codes are available at https://github.com/zhongyy/Face-Transformer.",
    "authors": [
      "Yaoyao Zhong",
      "Weihong Deng"
    ],
    "published": "2021-03-27T03:53:29+00:00",
    "url": "http://arxiv.org/pdf/2103.14803v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2111.11870v1",
    "title": "DBIA: Data-free Backdoor Injection Attack against Transformer Networks",
    "content": "DBIA: Data-free Backdoor Injection Attack against Transformer Networks. Recently, transformer architecture has demonstrated its significance in both Natural Language Processing (NLP) and Computer Vision (CV) tasks. Though other network models are known to be vulnerable to the backdoor attack, which embeds triggers in the model and controls the model behavior when the triggers are presented, little is known whether such an attack is still valid on the transformer models and if so, whether it can be done in a more cost-efficient manner. In this paper, we propose DBIA, a novel data-free backdoor attack against the CV-oriented transformer networks, leveraging the inherent attention mechanism of transformers to generate triggers and injecting the backdoor using the poisoned surrogate dataset. We conducted extensive experiments based on three benchmark transformers, i.e., ViT, DeiT and Swin Transformer, on two mainstream image classification tasks, i.e., CIFAR10 and ImageNet. The evaluation results demonstrate that, consuming fewer resources, our approach can embed backdoors with a high success rate and a low impact on the performance of the victim transformers. Our code is available at https://anonymous.4open.science/r/DBIA-825D.",
    "authors": [
      "Peizhuo Lv",
      "Hualong Ma",
      "Jiachen Zhou",
      "Ruigang Liang",
      "Kai Chen",
      "Shengzhi Zhang",
      "Yunfei Yang"
    ],
    "published": "2021-11-22T08:13:51+00:00",
    "url": "http://arxiv.org/pdf/2111.11870v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2106.06981v2",
    "title": "Thinking Like Transformers",
    "content": "Thinking Like Transformers. What is the computational model behind a Transformer? Where recurrent neural networks have direct parallels in finite state machines, allowing clear discussion and thought around architecture variants or trained models, Transformers have no such familiar parallel. In this paper we aim to change that, proposing a computational model for the transformer-encoder in the form of a programming language. We map the basic components of a transformer-encoder -- attention and feed-forward computation -- into simple primitives, around which we form a programming language: the Restricted Access Sequence Processing Language (RASP). We show how RASP can be used to program solutions to tasks that could conceivably be learned by a Transformer, and how a Transformer can be trained to mimic a RASP solution. In particular, we provide RASP programs for histograms, sorting, and Dyck-languages. We further use our model to relate their difficulty in terms of the number of required layers and attention heads: analyzing a RASP program implies a maximum number of heads and layers necessary to encode a task in a transformer. Finally, we see how insights gained from our abstraction might be used to explain phenomena seen in recent works.",
    "authors": [
      "Gail Weiss",
      "Yoav Goldberg",
      "Eran Yahav"
    ],
    "published": "2021-06-13T13:04:46+00:00",
    "url": "http://arxiv.org/pdf/2106.06981v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2110.05069v3",
    "title": "Efficient Training of Audio Transformers with Patchout",
    "content": "Efficient Training of Audio Transformers with Patchout. The great success of transformer-based models in natural language processing (NLP) has led to various attempts at adapting these architectures to other domains such as vision and audio. Recent work has shown that transformers can outperform Convolutional Neural Networks (CNNs) on vision and audio tasks. However, one of the main shortcomings of transformer models, compared to the well-established CNNs, is the computational complexity. In transformers, the compute and memory complexity is known to grow quadratically with the input length. Therefore, there has been extensive work on optimizing transformers, but often at the cost of degrading predictive performance. In this work, we propose a novel method to optimize and regularize transformers on audio spectrograms. Our proposed models achieve a new state-of-the-art performance on Audioset and can be trained on a single consumer-grade GPU. Furthermore, we propose a transformer model that outperforms CNNs in terms of both performance and training speed. Source code: https://github.com/kkoutini/PaSST",
    "authors": [
      "Khaled Koutini",
      "Jan Schlüter",
      "Hamid Eghbal-zadeh",
      "Gerhard Widmer"
    ],
    "published": "2021-10-11T08:07:50+00:00",
    "url": "http://arxiv.org/pdf/2110.05069v3"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2201.04905v1",
    "title": "A Formal Category Theoretical Framework for Multi-model Data Transformations",
    "content": "A Formal Category Theoretical Framework for Multi-model Data Transformations. Data integration and migration processes in polystores and multi-model database management systems highly benefit from data and schema transformations. Rigorous modeling of transformations is a complex problem. The data and schema transformation field is scattered with multiple different transformation frameworks, tools, and mappings. These are usually domain-specific and lack solid theoretical foundations. Our first goal is to define category theoretical foundations for relational, graph, and hierarchical data models and instances. Each data instance is represented as a category theoretical mapping called a functor. We formalize data and schema transformations as Kan lifts utilizing the functorial representation for the instances. A Kan lift is a category theoretical construction consisting of two mappings satisfying a certain universal property. In this work, the two mappings correspond to schema transformation and data transformation.",
    "authors": [
      "Valter Uotila",
      "Jiaheng Lu"
    ],
    "published": "2022-01-13T11:49:52+00:00",
    "url": "http://arxiv.org/pdf/2201.04905v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2208.01575v2",
    "title": "ferret: a Framework for Benchmarking Explainers on Transformers",
    "content": "ferret: a Framework for Benchmarking Explainers on Transformers. As Transformers are increasingly relied upon to solve complex NLP problems, there is an increased need for their decisions to be humanly interpretable. While several explainable AI (XAI) techniques for interpreting the outputs of transformer-based models have been proposed, there is still a lack of easy access to using and comparing them. We introduce ferret, a Python library to simplify the use and comparisons of XAI methods on transformer-based classifiers. With ferret, users can visualize and compare transformers-based models output explanations using state-of-the-art XAI methods on any free-text or existing XAI corpora. Moreover, users can also evaluate ad-hoc XAI metrics to select the most faithful and plausible explanations. To align with the recently consolidated process of sharing and using transformers-based models from Hugging Face, ferret interfaces directly with its Python library. In this paper, we showcase ferret to benchmark XAI methods used on transformers for sentiment analysis and hate speech detection. We show how specific methods provide consistently better explanations and are preferable in the context of transformer models.",
    "authors": [
      "Giuseppe Attanasio",
      "Eliana Pastor",
      "Chiara Di Bonaventura",
      "Debora Nozza"
    ],
    "published": "2022-08-02T16:21:42+00:00",
    "url": "http://arxiv.org/pdf/2208.01575v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2212.02791v1",
    "title": "Event-based Monocular Dense Depth Estimation with Recurrent Transformers",
    "content": "Event-based Monocular Dense Depth Estimation with Recurrent Transformers. Event cameras, offering high temporal resolutions and high dynamic ranges, have brought a new perspective to address common challenges (e.g., motion blur and low light) in monocular depth estimation. However, how to effectively exploit the sparse spatial information and rich temporal cues from asynchronous events remains a challenging endeavor. To this end, we propose a novel event-based monocular depth estimator with recurrent transformers, namely EReFormer, which is the first pure transformer with a recursive mechanism to process continuous event streams. Technically, for spatial modeling, a novel transformer-based encoder-decoder with a spatial transformer fusion module is presented, having better global context information modeling capabilities than CNN-based methods. For temporal modeling, we design a gate recurrent vision transformer unit that introduces a recursive mechanism into transformers, improving temporal modeling capabilities while alleviating the expensive GPU memory cost. The experimental results show that our EReFormer outperforms state-of-the-art methods by a margin on both synthetic and real-world datasets. We hope that our work will attract further research to develop stunning transformers in the event-based vision community. Our open-source code can be found in the supplemental material.",
    "authors": [
      "Xu Liu",
      "Jianing Li",
      "Xiaopeng Fan",
      "Yonghong Tian"
    ],
    "published": "2022-12-06T07:06:59+00:00",
    "url": "http://arxiv.org/pdf/2212.02791v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2304.06274v1",
    "title": "EWT: Efficient Wavelet-Transformer for Single Image Denoising",
    "content": "EWT: Efficient Wavelet-Transformer for Single Image Denoising. Transformer-based image denoising methods have achieved encouraging results in the past year. However, it must uses linear operations to model long-range dependencies, which greatly increases model inference time and consumes GPU storage space. Compared with convolutional neural network-based methods, current Transformer-based image denoising methods cannot achieve a balance between performance improvement and resource consumption. In this paper, we propose an Efficient Wavelet Transformer (EWT) for image denoising. Specifically, we use Discrete Wavelet Transform (DWT) and Inverse Wavelet Transform (IWT) for downsampling and upsampling, respectively. This method can fully preserve the image features while reducing the image resolution, thereby greatly reducing the device resource consumption of the Transformer model. Furthermore, we propose a novel Dual-stream Feature Extraction Block (DFEB) to extract image features at different levels, which can further reduce model inference time and GPU memory usage. Experiments show that our method speeds up the original Transformer by more than 80%, reduces GPU memory usage by more than 60%, and achieves excellent denoising results. All code will be public.",
    "authors": [
      "Juncheng Li",
      "Bodong Cheng",
      "Ying Chen",
      "Guangwei Gao",
      "Tieyong Zeng"
    ],
    "published": "2023-04-13T05:17:54+00:00",
    "url": "http://arxiv.org/pdf/2304.06274v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2311.08362v1",
    "title": "Transformers can optimally learn regression mixture models",
    "content": "Transformers can optimally learn regression mixture models. Mixture models arise in many regression problems, but most methods have seen limited adoption partly due to these algorithms' highly-tailored and model-specific nature. On the other hand, transformers are flexible, neural sequence models that present the intriguing possibility of providing general-purpose prediction methods, even in this mixture setting. In this work, we investigate the hypothesis that transformers can learn an optimal predictor for mixtures of regressions. We construct a generative process for a mixture of linear regressions for which the decision-theoretic optimal procedure is given by data-driven exponential weights on a finite set of parameters. We observe that transformers achieve low mean-squared error on data generated via this process. By probing the transformer's output at inference time, we also show that transformers typically make predictions that are close to the optimal predictor. Our experiments also demonstrate that transformers can learn mixtures of regressions in a sample-efficient fashion and are somewhat robust to distribution shifts. We complement our experimental observations by proving constructively that the decision-theoretic optimal procedure is indeed implementable by a transformer.",
    "authors": [
      "Reese Pathak",
      "Rajat Sen",
      "Weihao Kong",
      "Abhimanyu Das"
    ],
    "published": "2023-11-14T18:09:15+00:00",
    "url": "http://arxiv.org/pdf/2311.08362v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2410.05258v1",
    "title": "Differential Transformer",
    "content": "Differential Transformer. Transformer tends to overallocate attention to irrelevant context. In this work, we introduce Diff Transformer, which amplifies attention to the relevant context while canceling noise. Specifically, the differential attention mechanism calculates attention scores as the difference between two separate softmax attention maps. The subtraction cancels noise, promoting the emergence of sparse attention patterns. Experimental results on language modeling show that Diff Transformer outperforms Transformer in various settings of scaling up model size and training tokens. More intriguingly, it offers notable advantages in practical applications, such as long-context modeling, key information retrieval, hallucination mitigation, in-context learning, and reduction of activation outliers. By being less distracted by irrelevant context, Diff Transformer can mitigate hallucination in question answering and text summarization. For in-context learning, Diff Transformer not only enhances accuracy but is also more robust to order permutation, which was considered as a chronic robustness issue. The results position Diff Transformer as a highly effective and promising architecture to advance large language models.",
    "authors": [
      "Tianzhu Ye",
      "Li Dong",
      "Yuqing Xia",
      "Yutao Sun",
      "Yi Zhu",
      "Gao Huang",
      "Furu Wei"
    ],
    "published": "2024-10-07T17:57:38+00:00",
    "url": "http://arxiv.org/pdf/2410.05258v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2410.06848v1",
    "title": "Forgetting Through Transforming: Enabling Federated Unlearning via Class-Aware Representation Transformation",
    "content": "Forgetting Through Transforming: Enabling Federated Unlearning via Class-Aware Representation Transformation. Federated Unlearning (FU) enables clients to selectively remove the influence of specific data from a trained federated learning model, addressing privacy concerns and regulatory requirements. However, existing FU methods often struggle to balance effective erasure with model utility preservation, especially for class-level unlearning in non-IID settings. We propose Federated Unlearning via Class-aware Representation Transformation (FUCRT), a novel method that achieves unlearning through class-aware representation transformation. FUCRT employs two key components: (1) a transformation class selection strategy to identify optimal forgetting directions, and (2) a transformation alignment technique using dual class-aware contrastive learning to ensure consistent transformations across clients. Extensive experiments on four datasets demonstrate FUCRT's superior performance in terms of erasure guarantee, model utility preservation, and efficiency. FUCRT achieves complete (100\\%) erasure of unlearning classes while maintaining or improving performance on remaining classes, outperforming state-of-the-art baselines across both IID and Non-IID settings. Analysis of the representation space reveals FUCRT's ability to effectively merge unlearning class representations with the transformation class from remaining classes, closely mimicking the model retrained from scratch.",
    "authors": [
      "Qi Guo",
      "Zhen Tian",
      "Minghao Yao",
      "Yong Qi",
      "Saiyu Qi",
      "Yun Li",
      "Jin Song Dong"
    ],
    "published": "2024-10-09T13:08:14+00:00",
    "url": "http://arxiv.org/pdf/2410.06848v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1902.07761v1",
    "title": "Formalizing Cyber--Physical System Model Transformation via Abstract Interpretation",
    "content": "Formalizing Cyber--Physical System Model Transformation via Abstract Interpretation. Model transformation tools assist system designers by reducing the labor--intensive task of creating and updating models of various aspects of systems, ensuring that modeling assumptions remain consistent across every model of a system, and identifying constraints on system design imposed by these modeling assumptions. We have proposed a model transformation approach based on abstract interpretation, a static program analysis technique. Abstract interpretation allows us to define transformations that are provably correct and specific. This work develops the foundations of this approach to model transformation. We define model transformation in terms of abstract interpretation and prove the soundness of our approach. Furthermore, we develop formalisms useful for encoding model properties. This work provides a methodology for relating models of different aspects of a system and for applying modeling techniques from one system domain, such as smart power grids, to other domains, such as water distribution networks.",
    "authors": [
      "Natasha Jarus",
      "Sahra Sedigh Sarvestani",
      "Ali Hurson"
    ],
    "published": "2019-02-20T20:11:12+00:00",
    "url": "http://arxiv.org/pdf/1902.07761v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/0809.4710v3",
    "title": "Generalized Transformation for Decorated Spin Models",
    "content": "Generalized Transformation for Decorated Spin Models. The paper discusses the transformation of decorated Ising models into an effective \\textit{undecorated} spin models, using the most general Hamiltonian for interacting Ising models including a long range and high order interactions. The inverse of a Vandermonde matrix with equidistant nodes $[-s,s]$ is used to obtain an analytical expression of the transformation. This kind of transformation is very useful to obtain the partition function of decorated systems. The method presented by Fisher is also extended, in order to obtain the correlation functions of the decorated Ising models transforming into an effective undecorated Ising models. We apply this transformation to a particular mixed spin-(1/2,1) and (1/2,2) square lattice with only nearest site interaction. This model could be transformed into an effective uniform spin-$S$ square lattice with nearest and next-nearest interaction, furthermore the effective Hamiltonian also include combinations of three-body and four-body interactions, particularly we considered spin 1 and 2.",
    "authors": [
      "Onofre Rojas",
      "J. S. Valverde",
      "S. M. de Souza"
    ],
    "published": "2008-09-26T20:44:19+00:00",
    "url": "http://arxiv.org/pdf/0809.4710v3"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2002.00212v3",
    "title": "Pop Music Transformer: Beat-based Modeling and Generation of Expressive Pop Piano Compositions",
    "content": "Pop Music Transformer: Beat-based Modeling and Generation of Expressive Pop Piano Compositions. A great number of deep learning based models have been recently proposed for automatic music composition. Among these models, the Transformer stands out as a prominent approach for generating expressive classical piano performance with a coherent structure of up to one minute. The model is powerful in that it learns abstractions of data on its own, without much human-imposed domain knowledge or constraints. In contrast with this general approach, this paper shows that Transformers can do even better for music modeling, when we improve the way a musical score is converted into the data fed to a Transformer model. In particular, we seek to impose a metrical structure in the input data, so that Transformers can be more easily aware of the beat-bar-phrase hierarchical structure in music. The new data representation maintains the flexibility of local tempo changes, and provides hurdles to control the rhythmic and harmonic structure of music. With this approach, we build a Pop Music Transformer that composes Pop piano music with better rhythmic structure than existing Transformer models.",
    "authors": [
      "Yu-Siang Huang",
      "Yi-Hsuan Yang"
    ],
    "published": "2020-02-01T14:12:35+00:00",
    "url": "http://arxiv.org/pdf/2002.00212v3"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2012.00363v1",
    "title": "Modifying Memories in Transformer Models",
    "content": "Modifying Memories in Transformer Models. Large Transformer models have achieved impressive performance in many natural language tasks. In particular, Transformer based language models have been shown to have great capabilities in encoding factual knowledge in their vast amount of parameters. While the tasks of improving the memorization and generalization of Transformers have been widely studied, it is not well known how to make transformers forget specific old facts and memorize new ones. In this paper, we propose a new task of \\emph{explicitly modifying specific factual knowledge in Transformer models while ensuring the model performance does not degrade on the unmodified facts}. This task is useful in many scenarios, such as updating stale knowledge, protecting privacy, and eliminating unintended biases stored in the models. We benchmarked several approaches that provide natural baseline performances on this task. This leads to the discovery of key components of a Transformer model that are especially effective for knowledge modifications. The work also provides insights into the role that different training phases (such as pretraining and fine-tuning) play towards memorization and knowledge modification.",
    "authors": [
      "Chen Zhu",
      "Ankit Singh Rawat",
      "Manzil Zaheer",
      "Srinadh Bhojanapalli",
      "Daliang Li",
      "Felix Yu",
      "Sanjiv Kumar"
    ],
    "published": "2020-12-01T09:39:13+00:00",
    "url": "http://arxiv.org/pdf/2012.00363v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2003.07000v1",
    "title": "TRANS-BLSTM: Transformer with Bidirectional LSTM for Language Understanding",
    "content": "TRANS-BLSTM: Transformer with Bidirectional LSTM for Language Understanding. Bidirectional Encoder Representations from Transformers (BERT) has recently achieved state-of-the-art performance on a broad range of NLP tasks including sentence classification, machine translation, and question answering. The BERT model architecture is derived primarily from the transformer. Prior to the transformer era, bidirectional Long Short-Term Memory (BLSTM) has been the dominant modeling architecture for neural machine translation and question answering. In this paper, we investigate how these two modeling techniques can be combined to create a more powerful model architecture. We propose a new architecture denoted as Transformer with BLSTM (TRANS-BLSTM) which has a BLSTM layer integrated to each transformer block, leading to a joint modeling framework for transformer and BLSTM. We show that TRANS-BLSTM models consistently lead to improvements in accuracy compared to BERT baselines in GLUE and SQuAD 1.1 experiments. Our TRANS-BLSTM model obtains an F1 score of 94.01% on the SQuAD 1.1 development dataset, which is comparable to the state-of-the-art result.",
    "authors": [
      "Zhiheng Huang",
      "Peng Xu",
      "Davis Liang",
      "Ajay Mishra",
      "Bing Xiang"
    ],
    "published": "2020-03-16T03:38:51+00:00",
    "url": "http://arxiv.org/pdf/2003.07000v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2010.12780v1",
    "title": "Open-Domain Dialogue Generation Based on Pre-trained Language Models",
    "content": "Open-Domain Dialogue Generation Based on Pre-trained Language Models. Pre-trained language models have been successfully used in response generation for open-domain dialogue. Four main frameworks have been proposed: (1) Transformer-ED using Transformer encoder and decoder separately for source and target sentences; (2) Transformer-Dec using Transformer decoder for both source and target sentences; (3) Transformer-MLM using Transformer decoder that applies bi-directional attention on the source side and left-to-right attention on the target side with masked language model objective; and (4) Transformer-AR that uses auto-regressive objective instead. In this study, we compare these frameworks on 3 datasets, and our comparison reveals that the best framework uses bidirectional attention on the source side and does not separate encoder and decoder. We also examine model discrepancy, and our experiments confirm that the performance of a model is directly impacted by the underlying discrepancies. We then propose two correction methods to reduce the discrepancies, and both improve the model performance. These results show that discrepancies is an important factor to consider when we use a pre-trained model, and a reduction in discrepancies can lead to improved performance.",
    "authors": [
      "Yan Zeng",
      "Jian-Yun Nie"
    ],
    "published": "2020-10-24T04:52:28+00:00",
    "url": "http://arxiv.org/pdf/2010.12780v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2106.01345v2",
    "title": "Decision Transformer: Reinforcement Learning via Sequence Modeling",
    "content": "Decision Transformer: Reinforcement Learning via Sequence Modeling. We introduce a framework that abstracts Reinforcement Learning (RL) as a sequence modeling problem. This allows us to draw upon the simplicity and scalability of the Transformer architecture, and associated advances in language modeling such as GPT-x and BERT. In particular, we present Decision Transformer, an architecture that casts the problem of RL as conditional sequence modeling. Unlike prior approaches to RL that fit value functions or compute policy gradients, Decision Transformer simply outputs the optimal actions by leveraging a causally masked Transformer. By conditioning an autoregressive model on the desired return (reward), past states, and actions, our Decision Transformer model can generate future actions that achieve the desired return. Despite its simplicity, Decision Transformer matches or exceeds the performance of state-of-the-art model-free offline RL baselines on Atari, OpenAI Gym, and Key-to-Door tasks.",
    "authors": [
      "Lili Chen",
      "Kevin Lu",
      "Aravind Rajeswaran",
      "Kimin Lee",
      "Aditya Grover",
      "Michael Laskin",
      "Pieter Abbeel",
      "Aravind Srinivas",
      "Igor Mordatch"
    ],
    "published": "2021-06-02T17:53:39+00:00",
    "url": "http://arxiv.org/pdf/2106.01345v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2205.08594v1",
    "title": "Bayesian Discrete Conditional Transformation Models",
    "content": "Bayesian Discrete Conditional Transformation Models. We propose a novel Bayesian model framework for discrete ordinal and count data based on conditional transformations of the responses. The conditional transformation function is estimated from the data in conjunction with an a priori chosen reference distribution. For count responses, the resulting transformation model is novel in the sense that it is a Bayesian fully parametric yet distribution-free approach that can additionally account for excess zeros with additive transformation function specifications. For ordinal categoric responses, our cumulative link transformation model allows the inclusion of linear and nonlinear covariate effects that can additionally be made category-specific, resulting in (non-)proportional odds or hazards models and more, depending on the choice of the reference distribution. Inference is conducted by a generic modular Markov chain Monte Carlo algorithm where multivariate Gaussian priors enforce specific properties such as smoothness on the functional effects. To illustrate the versatility of Bayesian discrete conditional transformation models, applications to counts of patent citations in the presence of excess zeros and on treating forest health categories in a discrete partial proportional odds model are presented.",
    "authors": [
      "Manuel Carlan",
      "Thomas Kneib"
    ],
    "published": "2022-05-17T19:26:43+00:00",
    "url": "http://arxiv.org/pdf/2205.08594v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2302.08639v2",
    "title": "Improving Transformer-based Networks With Locality For Automatic Speaker Verification",
    "content": "Improving Transformer-based Networks With Locality For Automatic Speaker Verification. Recently, Transformer-based architectures have been explored for speaker embedding extraction. Although the Transformer employs the self-attention mechanism to efficiently model the global interaction between token embeddings, it is inadequate for capturing short-range local context, which is essential for the accurate extraction of speaker information. In this study, we enhance the Transformer with the enhanced locality modeling in two directions. First, we propose the Locality-Enhanced Conformer (LE-Confomer) by introducing depth-wise convolution and channel-wise attention into the Conformer blocks. Second, we present the Speaker Swin Transformer (SST) by adapting the Swin Transformer, originally proposed for vision tasks, into speaker embedding network. We evaluate the proposed approaches on the VoxCeleb datasets and a large-scale Microsoft internal multilingual (MS-internal) dataset. The proposed models achieve 0.75% EER on VoxCeleb 1 test set, outperforming the previously proposed Transformer-based models and CNN-based models, such as ResNet34 and ECAPA-TDNN. When trained on the MS-internal dataset, the proposed models achieve promising results with 14.6% relative reduction in EER over the Res2Net50 model.",
    "authors": [
      "Mufan Sang",
      "Yong Zhao",
      "Gang Liu",
      "John H. L. Hansen",
      "Jian Wu"
    ],
    "published": "2023-02-17T01:04:51+00:00",
    "url": "http://arxiv.org/pdf/2302.08639v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2308.13680v1",
    "title": "ACC-UNet: A Completely Convolutional UNet model for the 2020s",
    "content": "ACC-UNet: A Completely Convolutional UNet model for the 2020s. This decade is marked by the introduction of Vision Transformer, a radical paradigm shift in broad computer vision. A similar trend is followed in medical imaging, UNet, one of the most influential architectures, has been redesigned with transformers. Recently, the efficacy of convolutional models in vision is being reinvestigated by seminal works such as ConvNext, which elevates a ResNet to Swin Transformer level. Deriving inspiration from this, we aim to improve a purely convolutional UNet model so that it can be on par with the transformer-based models, e.g, Swin-Unet or UCTransNet. We examined several advantages of the transformer-based UNet models, primarily long-range dependencies and cross-level skip connections. We attempted to emulate them through convolution operations and thus propose, ACC-UNet, a completely convolutional UNet model that brings the best of both worlds, the inherent inductive biases of convnets with the design decisions of transformers. ACC-UNet was evaluated on 5 different medical image segmentation benchmarks and consistently outperformed convnets, transformers, and their hybrids. Notably, ACC-UNet outperforms state-of-the-art models Swin-Unet and UCTransNet by $2.64 \\pm 2.54\\%$ and $0.45 \\pm 1.61\\%$ in terms of dice score, respectively, while using a fraction of their parameters ($59.26\\%$ and $24.24\\%$). Our codes are available at https://github.com/kiharalab/ACC-UNet.",
    "authors": [
      "Nabil Ibtehaz",
      "Daisuke Kihara"
    ],
    "published": "2023-08-25T21:39:43+00:00",
    "url": "http://arxiv.org/pdf/2308.13680v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2111.05529v2",
    "title": "Understanding the Generalization Benefit of Model Invariance from a Data Perspective",
    "content": "Understanding the Generalization Benefit of Model Invariance from a Data Perspective. Machine learning models that are developed with invariance to certain types of data transformations have demonstrated superior generalization performance in practice. However, the underlying mechanism that explains why invariance leads to better generalization is not well-understood, limiting our ability to select appropriate data transformations for a given dataset. This paper studies the generalization benefit of model invariance by introducing the sample cover induced by transformations, i.e., a representative subset of a dataset that can approximately recover the whole dataset using transformations. Based on this notion, we refine the generalization bound for invariant models and characterize the suitability of a set of data transformations by the sample covering number induced by transformations, i.e., the smallest size of its induced sample covers. We show that the generalization bound can be tightened for suitable transformations that have a small sample covering number. Moreover, our proposed sample covering number can be empirically evaluated, providing a practical guide for selecting transformations to develop model invariance for better generalization. We evaluate the sample covering numbers for commonly used transformations on multiple datasets and demonstrate that the smaller sample covering number for a set of transformations indicates a smaller gap between the test and training error for invariant models, thus validating our propositions.",
    "authors": [
      "Sicheng Zhu",
      "Bang An",
      "Furong Huang"
    ],
    "published": "2021-11-10T04:53:07+00:00",
    "url": "http://arxiv.org/pdf/2111.05529v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2306.12384v1",
    "title": "Probing the limit of hydrologic predictability with the Transformer network",
    "content": "Probing the limit of hydrologic predictability with the Transformer network. For a number of years since its introduction to hydrology, recurrent neural networks like long short-term memory (LSTM) have proven remarkably difficult to surpass in terms of daily hydrograph metrics on known, comparable benchmarks. Outside of hydrology, Transformers have now become the model of choice for sequential prediction tasks, making it a curious architecture to investigate. Here, we first show that a vanilla Transformer architecture is not competitive against LSTM on the widely benchmarked CAMELS dataset, and lagged especially for the high-flow metrics due to short-term processes. However, a recurrence-free variant of Transformer can obtain mixed comparisons with LSTM, producing the same Kling-Gupta efficiency coefficient (KGE), along with other metrics. The lack of advantages for the Transformer is linked to the Markovian nature of the hydrologic prediction problem. Similar to LSTM, the Transformer can also merge multiple forcing dataset to improve model performance. While the Transformer results are not higher than current state-of-the-art, we still learned some valuable lessons: (1) the vanilla Transformer architecture is not suitable for hydrologic modeling; (2) the proposed recurrence-free modification can improve Transformer performance so future work can continue to test more of such modifications; and (3) the prediction limits on the dataset should be close to the current state-of-the-art model. As a non-recurrent model, the Transformer may bear scale advantages for learning from bigger datasets and storing knowledge. This work serves as a reference point for future modifications of the model.",
    "authors": [
      "Jiangtao Liu",
      "Yuchen Bian",
      "Chaopeng Shen"
    ],
    "published": "2023-06-21T17:06:54+00:00",
    "url": "http://arxiv.org/pdf/2306.12384v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2405.17821v2",
    "title": "RITUAL: Random Image Transformations as a Universal Anti-hallucination Lever in Large Vision Language Models",
    "content": "RITUAL: Random Image Transformations as a Universal Anti-hallucination Lever in Large Vision Language Models. Recent advancements in Large Vision Language Models (LVLMs) have revolutionized how machines understand and generate textual responses based on visual inputs, yet they often produce \"hallucinatory\" outputs that misinterpret visual information, posing challenges in reliability and trustworthiness. We propose RITUAL, a simple decoding method that reduces hallucinations by leveraging randomly transformed images as complementary inputs during decoding, adjusting the output probability distribution without additional training or external models. Our key insight is that random transformations expose the model to diverse visual perspectives, enabling it to correct misinterpretations that lead to hallucinations. Specifically, when a model hallucinates based on the original image, the transformed images -- altered in aspects such as orientation, scale, or color -- provide alternative viewpoints that help recalibrate the model's predictions. By integrating the probability distributions from both the original and transformed images, RITUAL effectively reduces hallucinations. To further improve reliability and address potential instability from arbitrary transformations, we introduce RITUAL+, an extension that selects image transformations based on self-feedback from the LVLM. Instead of applying transformations randomly, RITUAL+ uses the LVLM to evaluate and choose transformations that are most beneficial for reducing hallucinations in a given context. This self-adaptive approach mitigates the potential negative impact of certain transformations on specific tasks, ensuring more consistent performance across different scenarios. Experiments demonstrate that RITUAL and RITUAL+ significantly reduce hallucinations across several object hallucination benchmarks.",
    "authors": [
      "Sangmin Woo",
      "Jaehyuk Jang",
      "Donguk Kim",
      "Yubin Choi",
      "Changick Kim"
    ],
    "published": "2024-05-28T04:41:02+00:00",
    "url": "http://arxiv.org/pdf/2405.17821v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2306.02074v2",
    "title": "A Conditional Generative Chatbot using Transformer Model",
    "content": "A Conditional Generative Chatbot using Transformer Model. A Chatbot serves as a communication tool between a human user and a machine to achieve an appropriate answer based on the human input. In more recent approaches, a combination of Natural Language Processing and sequential models are used to build a generative Chatbot. The main challenge of these models is their sequential nature, which leads to less accurate results. To tackle this challenge, in this paper, a novel architecture is proposed using conditional Wasserstein Generative Adversarial Networks and a transformer model for answer generation in Chatbots. While the generator of the proposed model consists of a full transformer model to generate an answer, the discriminator includes only the encoder part of a transformer model followed by a classifier. To the best of our knowledge, this is the first time that a generative Chatbot is proposed using the embedded transformer in both generator and discriminator models. Relying on the parallel computing of the transformer model, the results of the proposed model on the Cornell Movie-Dialog corpus and the Chit-Chat datasets confirm the superiority of the proposed model compared to state-of-the-art alternatives using different evaluation metrics.",
    "authors": [
      "Nura Esfandiari",
      "Kourosh Kiani",
      "Razieh Rastgoo"
    ],
    "published": "2023-06-03T10:35:04+00:00",
    "url": "http://arxiv.org/pdf/2306.02074v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2203.11210v1",
    "title": "Disentangling Patterns and Transformations from One Sequence of Images with Shape-invariant Lie Group Transformer",
    "content": "Disentangling Patterns and Transformations from One Sequence of Images with Shape-invariant Lie Group Transformer. An effective way to model the complex real world is to view the world as a composition of basic components of objects and transformations. Although humans through development understand the compositionality of the real world, it is extremely difficult to equip robots with such a learning mechanism. In recent years, there has been significant research on autonomously learning representations of the world using the deep learning; however, most studies have taken a statistical approach, which requires a large number of training data. Contrary to such existing methods, we take a novel algebraic approach for representation learning based on a simpler and more intuitive formulation that the observed world is the combination of multiple independent patterns and transformations that are invariant to the shape of patterns. Since the shape of patterns can be viewed as the invariant features against symmetric transformations such as translation or rotation, we can expect that the patterns can naturally be extracted by expressing transformations with symmetric Lie group transformers and attempting to reconstruct the scene with them. Based on this idea, we propose a model that disentangles the scenes into the minimum number of basic components of patterns and Lie transformations from only one sequence of images, by introducing the learnable shape-invariant Lie group transformers as transformation components. Experiments show that given one sequence of images in which two objects are moving independently, the proposed model can discover the hidden distinct objects and multiple shape-invariant transformations that constitute the scenes.",
    "authors": [
      "T. Takada",
      "W. Shimaya",
      "Y. Ohmura",
      "Y. Kuniyoshi"
    ],
    "published": "2022-03-21T11:55:13+00:00",
    "url": "http://arxiv.org/pdf/2203.11210v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1606.00941v1",
    "title": "An Exact Linearization Method for OLTC of Transformer in Branch Flow Model",
    "content": "An Exact Linearization Method for OLTC of Transformer in Branch Flow Model. The branch flow based optimal power flow(OPF) problem in radianlly operated distribution networks can be exactly relazed to a second order cone programming (SOCP) model without considering transformers. However, the introdution of nonlinear transformer models will make the OPF model non-convex. This paper presents an exact linearized transformer's OLTC model to keep the OPF model convex via binary expanstion scheme and big-M method. Validity of the proposed method is verified using IEEE 33-bus test system.",
    "authors": [
      "Wenchuan Wu",
      "Zhuang Tian",
      "Boming Zhang"
    ],
    "published": "2016-06-03T01:27:01+00:00",
    "url": "http://arxiv.org/pdf/1606.00941v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2404.09976v1",
    "title": "Diffscaler: Enhancing the Generative Prowess of Diffusion Transformers",
    "content": "Diffscaler: Enhancing the Generative Prowess of Diffusion Transformers. Recently, diffusion transformers have gained wide attention with its excellent performance in text-to-image and text-to-vidoe models, emphasizing the need for transformers as backbone for diffusion models. Transformer-based models have shown better generalization capability compared to CNN-based models for general vision tasks. However, much less has been explored in the existing literature regarding the capabilities of transformer-based diffusion backbones and expanding their generative prowess to other datasets. This paper focuses on enabling a single pre-trained diffusion transformer model to scale across multiple datasets swiftly, allowing for the completion of diverse generative tasks using just one model. To this end, we propose DiffScaler, an efficient scaling strategy for diffusion models where we train a minimal amount of parameters to adapt to different tasks. In particular, we learn task-specific transformations at each layer by incorporating the ability to utilize the learned subspaces of the pre-trained model, as well as the ability to learn additional task-specific subspaces, which may be absent in the pre-training dataset. As these parameters are independent, a single diffusion model with these task-specific parameters can be used to perform multiple tasks simultaneously. Moreover, we find that transformer-based diffusion models significantly outperform CNN-based diffusion models methods while performing fine-tuning over smaller datasets. We perform experiments on four unconditional image generation datasets. We show that using our proposed method, a single pre-trained model can scale up to perform these conditional and unconditional tasks, respectively, with minimal parameter tuning while performing as close as fine-tuning an entire diffusion model for that particular task.",
    "authors": [
      "Nithin Gopalakrishnan Nair",
      "Jeya Maria Jose Valanarasu",
      "Vishal M. Patel"
    ],
    "published": "2024-04-15T17:55:43+00:00",
    "url": "http://arxiv.org/pdf/2404.09976v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2409.09239v3",
    "title": "Autoregressive + Chain of Thought = Recurrent: Recurrence's Role in Language Models' Computability and a Revisit of Recurrent Transformer",
    "content": "Autoregressive + Chain of Thought = Recurrent: Recurrence's Role in Language Models' Computability and a Revisit of Recurrent Transformer. The Transformer architecture excels in a variety of language modeling tasks, outperforming traditional neural architectures such as RNN and LSTM. This is partially due to its elimination of recurrent connections, which allows for parallel training and a smoother flow of gradients. However, this move away from recurrent structures places the Transformer model at the lower end of Chomsky's computational hierarchy, imposing limitations on its computational abilities. Consequently, even advanced Transformer-based models face considerable difficulties in tasks like counting, string reversal, and multiplication. These tasks, though seemingly elementary, require a level of computational complexity that exceeds the capabilities of the Transformer architecture. Concurrently, the emergence of ``Chain of Thought\" (CoT) prompting has enabled Transformer-based language models to tackle tasks that were previously impossible or poorly executed. In this work, we thoroughly investigate the influence of recurrent structures in neural models on their reasoning abilities and computability, contrasting the role autoregression plays in the neural models' computational power. We then shed light on how the CoT approach can mimic recurrent computation and act as a bridge between autoregression and recurrence in the context of language models. It is this approximated recurrence that notably improves the model's performance and computational capacity. Moreover, we revisit recent recurrent-based Transformer model designs, focusing on their computational abilities through our proposed concept of ``recurrence-completeness\" and identify key theoretical limitations in models like Linear Transformer and RWKV. Through this, we aim to provide insight into the neural model architectures and prompt better model design.",
    "authors": [
      "Xiang Zhang",
      "Muhammad Abdul-Mageed",
      "Laks V. S. Lakshmanan"
    ],
    "published": "2024-09-14T00:30:57+00:00",
    "url": "http://arxiv.org/pdf/2409.09239v3"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2502.09029v2",
    "title": "MTDP: A Modulated Transformer based Diffusion Policy Model",
    "content": "MTDP: A Modulated Transformer based Diffusion Policy Model. Recent research on robot manipulation based on Behavior Cloning (BC) has made significant progress. By combining diffusion models with BC, diffusion policiy has been proposed, enabling robots to quickly learn manipulation tasks with high success rates. However, integrating diffusion policy with high-capacity Transformer presents challenges, traditional Transformer architectures struggle to effectively integrate guiding conditions, resulting in poor performance in manipulation tasks when using Transformer-based models. In this paper, we investigate key architectural designs of Transformers and improve the traditional Transformer architecture by proposing the Modulated Transformer Diffusion Policy (MTDP) model for diffusion policy. The core of this model is the Modulated Attention module we proposed, which more effectively integrates the guiding conditions with the main input, improving the generative model's output quality and, consequently, increasing the robot's task success rate. In six experimental tasks, MTDP outperformed existing Transformer model architectures, particularly in the Toolhang experiment, where the success rate increased by 12\\%. To verify the generality of Modulated Attention, we applied it to the UNet architecture to construct Modulated UNet Diffusion Policy model (MUDP), which also achieved higher success rates than existing UNet architectures across all six experiments. The Diffusion Policy uses Denoising Diffusion Probabilistic Models (DDPM) as the diffusion model. Building on this, we also explored Denoising Diffusion Implicit Models (DDIM) as the diffusion model, constructing the MTDP-I and MUDP-I model, which nearly doubled the generation speed while maintaining performance.",
    "authors": [
      "Qianhao Wang",
      "Yinqian Sun",
      "Enmeng Lu",
      "Qian Zhang",
      "Yi Zeng"
    ],
    "published": "2025-02-13T07:35:03+00:00",
    "url": "http://arxiv.org/pdf/2502.09029v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2302.14017v1",
    "title": "Full Stack Optimization of Transformer Inference: a Survey",
    "content": "Full Stack Optimization of Transformer Inference: a Survey. Recent advances in state-of-the-art DNN architecture design have been moving toward Transformer models. These models achieve superior accuracy across a wide range of applications. This trend has been consistent over the past several years since Transformer models were originally introduced. However, the amount of compute and bandwidth required for inference of recent Transformer models is growing at a significant rate, and this has made their deployment in latency-sensitive applications challenging. As such, there has been an increased focus on making Transformer models more efficient, with methods that range from changing the architecture design, all the way to developing dedicated domain-specific accelerators. In this work, we survey different approaches for efficient Transformer inference, including: (i) analysis and profiling of the bottlenecks in existing Transformer architectures and their similarities and differences with previous convolutional models; (ii) implications of Transformer architecture on hardware, including the impact of non-linear operations such as Layer Normalization, Softmax, and GELU, as well as linear operations, on hardware design; (iii) approaches for optimizing a fixed Transformer architecture; (iv) challenges in finding the right mapping and scheduling of operations for Transformer models; and (v) approaches for optimizing Transformer models by adapting the architecture using neural architecture search. Finally, we perform a case study by applying the surveyed optimizations on Gemmini, the open-source, full-stack DNN accelerator generator, and we show how each of these approaches can yield improvements, compared to previous benchmark results on Gemmini. Among other things, we find that a full-stack co-design approach with the aforementioned methods can result in up to 88.7x speedup with a minimal performance degradation for Transformer inference.",
    "authors": [
      "Sehoon Kim",
      "Coleman Hooper",
      "Thanakul Wattanawong",
      "Minwoo Kang",
      "Ruohan Yan",
      "Hasan Genc",
      "Grace Dinh",
      "Qijing Huang",
      "Kurt Keutzer",
      "Michael W. Mahoney",
      "Yakun Sophia Shao",
      "Amir Gholami"
    ],
    "published": "2023-02-27T18:18:13+00:00",
    "url": "http://arxiv.org/pdf/2302.14017v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1611.09890v1",
    "title": "An Affine-Invariant Bayesian Cluster Process",
    "content": "An Affine-Invariant Bayesian Cluster Process. In order to identify clusters of objects with features transformed by unknown affine transformations, we develop a Bayesian cluster process which is invariant with respect to certain linear transformations of the feature space and able to cluster data without knowing the number of clusters in advance. Specifically, our proposed method can identify clusters invariant to orthogonal transformations under model I, invariant to scaling-coordinate orthogonal transformations under model II, or invariant to arbitrary non-singular linear transformations under model III. The proposed split-merge algorithm leads to an irreducible and aperiodic Markov chain, which is also efficient at identifying clusters reasonably well for various applications. We illustrate the applications of our approach to both synthetic and real data such as leukemia gene expression data for model I; wine data and two half-moons benchmark data for model II; three-dimensional Denmark road geographic coordinate system data and an arbitrary non-singular transformed two half-moons data for model III. These examples show that the proposed method could be widely applied in many fields, especially for finding the number of clusters and identifying clusters of samples of interest in aerial photography and genomic data.",
    "authors": [
      "Hsin-Hsiung Huang",
      "Jie Yang"
    ],
    "published": "2016-11-29T21:25:54+00:00",
    "url": "http://arxiv.org/pdf/1611.09890v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2105.03824v4",
    "title": "FNet: Mixing Tokens with Fourier Transforms",
    "content": "FNet: Mixing Tokens with Fourier Transforms. We show that Transformer encoder architectures can be sped up, with limited accuracy costs, by replacing the self-attention sublayers with simple linear transformations that \"mix\" input tokens. These linear mixers, along with standard nonlinearities in feed-forward layers, prove competent at modeling semantic relationships in several text classification tasks. Most surprisingly, we find that replacing the self-attention sublayer in a Transformer encoder with a standard, unparameterized Fourier Transform achieves 92-97% of the accuracy of BERT counterparts on the GLUE benchmark, but trains 80% faster on GPUs and 70% faster on TPUs at standard 512 input lengths. At longer input lengths, our FNet model is significantly faster: when compared to the \"efficient\" Transformers on the Long Range Arena benchmark, FNet matches the accuracy of the most accurate models, while outpacing the fastest models across all sequence lengths on GPUs (and across relatively shorter lengths on TPUs). Finally, FNet has a light memory footprint and is particularly efficient at smaller model sizes; for a fixed speed and accuracy budget, small FNet models outperform Transformer counterparts.",
    "authors": [
      "James Lee-Thorp",
      "Joshua Ainslie",
      "Ilya Eckstein",
      "Santiago Ontanon"
    ],
    "published": "2021-05-09T03:32:48+00:00",
    "url": "http://arxiv.org/pdf/2105.03824v4"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2010.11395v3",
    "title": "Developing Real-time Streaming Transformer Transducer for Speech Recognition on Large-scale Dataset",
    "content": "Developing Real-time Streaming Transformer Transducer for Speech Recognition on Large-scale Dataset. Recently, Transformer based end-to-end models have achieved great success in many areas including speech recognition. However, compared to LSTM models, the heavy computational cost of the Transformer during inference is a key issue to prevent their applications. In this work, we explored the potential of Transformer Transducer (T-T) models for the fist pass decoding with low latency and fast speed on a large-scale dataset. We combine the idea of Transformer-XL and chunk-wise streaming processing to design a streamable Transformer Transducer model. We demonstrate that T-T outperforms the hybrid model, RNN Transducer (RNN-T), and streamable Transformer attention-based encoder-decoder model in the streaming scenario. Furthermore, the runtime cost and latency can be optimized with a relatively small look-ahead.",
    "authors": [
      "Xie Chen",
      "Yu Wu",
      "Zhenghao Wang",
      "Shujie Liu",
      "Jinyu Li"
    ],
    "published": "2020-10-22T03:01:21+00:00",
    "url": "http://arxiv.org/pdf/2010.11395v3"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2106.13230v1",
    "title": "Video Swin Transformer",
    "content": "Video Swin Transformer. The vision community is witnessing a modeling shift from CNNs to Transformers, where pure Transformer architectures have attained top accuracy on the major video recognition benchmarks. These video models are all built on Transformer layers that globally connect patches across the spatial and temporal dimensions. In this paper, we instead advocate an inductive bias of locality in video Transformers, which leads to a better speed-accuracy trade-off compared to previous approaches which compute self-attention globally even with spatial-temporal factorization. The locality of the proposed video architecture is realized by adapting the Swin Transformer designed for the image domain, while continuing to leverage the power of pre-trained image models. Our approach achieves state-of-the-art accuracy on a broad range of video recognition benchmarks, including on action recognition (84.9 top-1 accuracy on Kinetics-400 and 86.1 top-1 accuracy on Kinetics-600 with ~20x less pre-training data and ~3x smaller model size) and temporal modeling (69.6 top-1 accuracy on Something-Something v2). The code and models will be made publicly available at https://github.com/SwinTransformer/Video-Swin-Transformer.",
    "authors": [
      "Ze Liu",
      "Jia Ning",
      "Yue Cao",
      "Yixuan Wei",
      "Zheng Zhang",
      "Stephen Lin",
      "Han Hu"
    ],
    "published": "2021-06-24T17:59:46+00:00",
    "url": "http://arxiv.org/pdf/2106.13230v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2202.12166v1",
    "title": "Attention Enables Zero Approximation Error",
    "content": "Attention Enables Zero Approximation Error. Deep learning models have been widely applied in various aspects of daily life. Many variant models based on deep learning structures have achieved even better performances. Attention-based architectures have become almost ubiquitous in deep learning structures. Especially, the transformer model has now defeated the convolutional neural network in image classification tasks to become the most widely used tool. However, the theoretical properties of attention-based models are seldom considered. In this work, we show that with suitable adaptations, the single-head self-attention transformer with a fixed number of transformer encoder blocks and free parameters is able to generate any desired polynomial of the input with no error. The number of transformer encoder blocks is the same as the degree of the target polynomial. Even more exciting, we find that these transformer encoder blocks in this model do not need to be trained. As a direct consequence, we show that the single-head self-attention transformer with increasing numbers of free parameters is universal. These surprising theoretical results clearly explain the outstanding performances of the transformer model and may shed light on future modifications in real applications. We also provide some experiments to verify our theoretical result.",
    "authors": [
      "Zhiying Fang",
      "Yidong Ouyang",
      "Ding-Xuan Zhou",
      "Guang Cheng"
    ],
    "published": "2022-02-24T16:06:01+00:00",
    "url": "http://arxiv.org/pdf/2202.12166v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2204.05454v1",
    "title": "Are Multimodal Transformers Robust to Missing Modality?",
    "content": "Are Multimodal Transformers Robust to Missing Modality?. Multimodal data collected from the real world are often imperfect due to missing modalities. Therefore multimodal models that are robust against modal-incomplete data are highly preferred. Recently, Transformer models have shown great success in processing multimodal data. However, existing work has been limited to either architecture designs or pre-training strategies; whether Transformer models are naturally robust against missing-modal data has rarely been investigated. In this paper, we present the first-of-its-kind work to comprehensively investigate the behavior of Transformers in the presence of modal-incomplete data. Unsurprising, we find Transformer models are sensitive to missing modalities while different modal fusion strategies will significantly affect the robustness. What surprised us is that the optimal fusion strategy is dataset dependent even for the same Transformer model; there does not exist a universal strategy that works in general cases. Based on these findings, we propose a principle method to improve the robustness of Transformer models by automatically searching for an optimal fusion strategy regarding input data. Experimental validations on three benchmarks support the superior performance of the proposed method.",
    "authors": [
      "Mengmeng Ma",
      "Jian Ren",
      "Long Zhao",
      "Davide Testuggine",
      "Xi Peng"
    ],
    "published": "2022-04-12T00:21:31+00:00",
    "url": "http://arxiv.org/pdf/2204.05454v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2302.04869v1",
    "title": "Reversible Vision Transformers",
    "content": "Reversible Vision Transformers. We present Reversible Vision Transformers, a memory efficient architecture design for visual recognition. By decoupling the GPU memory requirement from the depth of the model, Reversible Vision Transformers enable scaling up architectures with efficient memory usage. We adapt two popular models, namely Vision Transformer and Multiscale Vision Transformers, to reversible variants and benchmark extensively across both model sizes and tasks of image classification, object detection and video classification. Reversible Vision Transformers achieve a reduced memory footprint of up to 15.5x at roughly identical model complexity, parameters and accuracy, demonstrating the promise of reversible vision transformers as an efficient backbone for hardware resource limited training regimes. Finally, we find that the additional computational burden of recomputing activations is more than overcome for deeper models, where throughput can increase up to 2.3x over their non-reversible counterparts. Full code and trained models are available at https://github.com/facebookresearch/slowfast. A simpler, easy to understand and modify version is also available at https://github.com/karttikeya/minREV",
    "authors": [
      "Karttikeya Mangalam",
      "Haoqi Fan",
      "Yanghao Li",
      "Chao-Yuan Wu",
      "Bo Xiong",
      "Christoph Feichtenhofer",
      "Jitendra Malik"
    ],
    "published": "2023-02-09T18:59:54+00:00",
    "url": "http://arxiv.org/pdf/2302.04869v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2306.09539v4",
    "title": "Block-State Transformers",
    "content": "Block-State Transformers. State space models (SSMs) have shown impressive results on tasks that require modeling long-range dependencies and efficiently scale to long sequences owing to their subquadratic runtime complexity. Originally designed for continuous signals, SSMs have shown superior performance on a plethora of tasks, in vision and audio; however, SSMs still lag Transformer performance in Language Modeling tasks. In this work, we propose a hybrid layer named Block-State Transformer (BST), that internally combines an SSM sublayer for long-range contextualization, and a Block Transformer sublayer for short-term representation of sequences. We study three different, and completely parallelizable, variants that integrate SSMs and block-wise attention. We show that our model outperforms similar Transformer-based architectures on language modeling perplexity and generalizes to longer sequences. In addition, the Block-State Transformer demonstrates more than tenfold increase in speed at the layer level compared to the Block-Recurrent Transformer when model parallelization is employed.",
    "authors": [
      "Mahan Fathi",
      "Jonathan Pilault",
      "Orhan Firat",
      "Christopher Pal",
      "Pierre-Luc Bacon",
      "Ross Goroshin"
    ],
    "published": "2023-06-15T22:48:08+00:00",
    "url": "http://arxiv.org/pdf/2306.09539v4"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2406.19623v1",
    "title": "FRA-DiagSys: A Transformer Winding Fault Diagnosis System for Identifying Fault Types and degrees Using Frequency Response Analysis",
    "content": "FRA-DiagSys: A Transformer Winding Fault Diagnosis System for Identifying Fault Types and degrees Using Frequency Response Analysis. The electric power transformer is a critical component in electrical distribution networks, and the diagnosis of faults in transformers is an important research area. Frequency Response Analysis (FRA) methods are widely used for analyzing winding faults in transformers, particularly in Chinese power stations. However, the current approach relies on manual expertise to interpret FRA curves, which can be both skill-intensive and lacks precision. This study presents a novel approach using a Multilayer perceptron model to directly model and analyze FRA data, simulating various winding fault types and degrees in 12-disc winding and 10-disc winding transformers with different connection configurations, resulting in three distinct datasets. Six different Multilayer perceptron architectures were developed, with optimal models achieving recognition accuracies of over 99.7% for diagnosing fault degrees and more than 90% for fault types. Hence, this paper has yielded a model architecture that exhibits commendable performance in diagnosing various fault types and their severities in different models of transformers when utilizing different FRA connection methods. Additionally, a specialized diagnostic system called FRA-DiagSys with two-stage model utilization was developed, achieving 100% accuracy in diagnosing fault types and degrees for a specific winding-10 power transformer, surpassing other diagnostic methods and strategies.",
    "authors": [
      "Guohao Wang"
    ],
    "published": "2024-06-28T03:12:21+00:00",
    "url": "http://arxiv.org/pdf/2406.19623v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2501.09588v1",
    "title": "Atleus: Accelerating Transformers on the Edge Enabled by 3D Heterogeneous Manycore Architectures",
    "content": "Atleus: Accelerating Transformers on the Edge Enabled by 3D Heterogeneous Manycore Architectures. Transformer architectures have become the standard neural network model for various machine learning applications including natural language processing and computer vision. However, the compute and memory requirements introduced by transformer models make them challenging to adopt for edge applications. Furthermore, fine-tuning pre-trained transformers (e.g., foundation models) is a common task to enhance the model's predictive performance on specific tasks/applications. Existing transformer accelerators are oblivious to complexities introduced by fine-tuning. In this paper, we propose the design of a three-dimensional (3D) heterogeneous architecture referred to as Atleus that incorporates heterogeneous computing resources specifically optimized to accelerate transformer models for the dual purposes of fine-tuning and inference. Specifically, Atleus utilizes non-volatile memory and systolic array for accelerating transformer computational kernels using an integrated 3D platform. Moreover, we design a suitable NoC to achieve high performance and energy efficiency. Finally, Atleus adopts an effective quantization scheme to support model compression. Experimental results demonstrate that Atleus outperforms existing state-of-the-art by up to 56x and 64.5x in terms of performance and energy efficiency respectively",
    "authors": [
      "Pratyush Dhingra",
      "Janardhan Rao Doppa",
      "Partha Pratim Pande"
    ],
    "published": "2025-01-16T15:11:33+00:00",
    "url": "http://arxiv.org/pdf/2501.09588v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1003.0746v1",
    "title": "Automatically Discovering Hidden Transformation Chaining Constraints",
    "content": "Automatically Discovering Hidden Transformation Chaining Constraints. Model transformations operate on models conforming to precisely defined metamodels. Consequently, it often seems relatively easy to chain them: the output of a transformation may be given as input to a second one if metamodels match. However, this simple rule has some obvious limitations. For instance, a transformation may only use a subset of a metamodel. Therefore, chaining transformations appropriately requires more information. We present here an approach that automatically discovers more detailed information about actual chaining constraints by statically analyzing transformations. The objective is to provide developers who decide to chain transformations with more data on which to base their choices. This approach has been successfully applied to the case of a library of endogenous transformations. They all have the same source and target metamodel but have some hidden chaining constraints. In such a case, the simple metamodel matching rule given above does not provide any useful information.",
    "authors": [
      "Raphael Chenouard",
      "Frédéric Jouault"
    ],
    "published": "2010-03-03T08:04:45+00:00",
    "url": "http://arxiv.org/pdf/1003.0746v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1408.2730v3",
    "title": "A Transform Method of a Force Curve Obtained by Surface Force Apparatus to the Density Distribution of a Liquid on a Surface: An Improved Version",
    "content": "A Transform Method of a Force Curve Obtained by Surface Force Apparatus to the Density Distribution of a Liquid on a Surface: An Improved Version. We propose a transform method from a force curve obtained by a surface force apparatus (SFA) to a density distribution of a liquid on a surface of the SFA probe. (We emphasize that the transform method is a theory for the experiment.) In the method, two-body potential between the SFA probe and the solvent sphere is modeled as the soft attractive potential with rigid wall. The model potential is more realistic compared with the rigid potential applied in our earlier work. The introduction of the model potential is the improved point of the present transform method. The transform method is derived based on the statistical mechanics of a simple liquid where the simple liquid is an ensemble of small spheres. To derive the transform method, Kirkwood superposition approximation is used. It is found that the transformation can be done by a sequential computation. It is considered that the solvation structure can be obtained more precisely by using the improved transform method.",
    "authors": [
      "Ken-ichi Amano",
      "Eisuke Tanaka"
    ],
    "published": "2014-08-08T07:59:41+00:00",
    "url": "http://arxiv.org/pdf/1408.2730v3"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2008.00623v2",
    "title": "DeLighT: Deep and Light-weight Transformer",
    "content": "DeLighT: Deep and Light-weight Transformer. We introduce a deep and light-weight transformer, DeLighT, that delivers similar or better performance than standard transformer-based models with significantly fewer parameters. DeLighT more efficiently allocates parameters both (1) within each Transformer block using the DeLighT transformation, a deep and light-weight transformation, and (2) across blocks using block-wise scaling, which allows for shallower and narrower DeLighT blocks near the input and wider and deeper DeLighT blocks near the output. Overall, DeLighT networks are 2.5 to 4 times deeper than standard transformer models and yet have fewer parameters and operations. Experiments on benchmark machine translation and language modeling tasks show that DeLighT matches or improves the performance of baseline Transformers with 2 to 3 times fewer parameters on average. Our source code is available at: \\url{https://github.com/sacmehta/delight}",
    "authors": [
      "Sachin Mehta",
      "Marjan Ghazvininejad",
      "Srinivasan Iyer",
      "Luke Zettlemoyer",
      "Hannaneh Hajishirzi"
    ],
    "published": "2020-08-03T03:08:29+00:00",
    "url": "http://arxiv.org/pdf/2008.00623v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2009.13033v2",
    "title": "Where Does the Robustness Come from? A Study of the Transformation-based Ensemble Defence",
    "content": "Where Does the Robustness Come from? A Study of the Transformation-based Ensemble Defence. This paper aims to provide a thorough study on the effectiveness of the transformation-based ensemble defence for image classification and its reasons. It has been empirically shown that they can enhance the robustness against evasion attacks, while there is little analysis on the reasons. In particular, it is not clear whether the robustness improvement is a result of transformation or ensemble. In this paper, we design two adaptive attacks to better evaluate the transformation-based ensemble defence. We conduct experiments to show that 1) the transferability of adversarial examples exists among the models trained on data records after different reversible transformations; 2) the robustness gained through transformation-based ensemble is limited; 3) this limited robustness is mainly from the irreversible transformations rather than the ensemble of a number of models; and 4) blindly increasing the number of sub-models in a transformation-based ensemble does not bring extra robustness gain.",
    "authors": [
      "Chang Liao",
      "Yao Cheng",
      "Chengfang Fang",
      "Jie Shi"
    ],
    "published": "2020-09-28T02:55:56+00:00",
    "url": "http://arxiv.org/pdf/2009.13033v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2111.15473v1",
    "title": "New Approaches to Long Document Summarization: Fourier Transform Based Attention in a Transformer Model",
    "content": "New Approaches to Long Document Summarization: Fourier Transform Based Attention in a Transformer Model. In this work, we extensively redesign the newly introduced method of token mixing using Fourier Transforms (FNET) to replace the computationally expensive self-attention mechanism in a full transformer implementation on a long document summarization task (> 512 tokens). As a baseline, we also carried out long document summarization using established methods such as Longformer and Big Bird transformer models that are capable of processing over 8000 tokens and are currently the state of the art methods for these type of problems. The original FNET paper implemented this in an encoder only architecture while abstractive summarization requires both an encoder and a decoder. Since such a pretrained transformer model does not currently exist in the public domain, we decided to implement a full transformer based on this Fourier token mixing approach in an encoder/decoder architecture which we trained starting with Glove embeddings for the individual words in the corpus. We investigated a number of different extensions to the original FNET architecture and evaluated them on their Rouge F1-score performance on a summarization task. All modifications showed better performance on the summarization task than when using the original FNET encoder in a transformer architecture.",
    "authors": [
      "Andrew Kiruluta",
      "Andreas Lemos",
      "Eric Lundy"
    ],
    "published": "2021-11-25T18:03:41+00:00",
    "url": "http://arxiv.org/pdf/2111.15473v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2102.01502v1",
    "title": "ADePT: Auto-encoder based Differentially Private Text Transformation",
    "content": "ADePT: Auto-encoder based Differentially Private Text Transformation. Privacy is an important concern when building statistical models on data containing personal information. Differential privacy offers a strong definition of privacy and can be used to solve several privacy concerns (Dwork et al., 2014). Multiple solutions have been proposed for the differentially-private transformation of datasets containing sensitive information. However, such transformation algorithms offer poor utility in Natural Language Processing (NLP) tasks due to noise added in the process. In this paper, we address this issue by providing a utility-preserving differentially private text transformation algorithm using auto-encoders. Our algorithm transforms text to offer robustness against attacks and produces transformations with high semantic quality that perform well on downstream NLP tasks. We prove the theoretical privacy guarantee of our algorithm and assess its privacy leakage under Membership Inference Attacks(MIA) (Shokri et al., 2017) on models trained with transformed data. Our results show that the proposed model performs better against MIA attacks while offering lower to no degradation in the utility of the underlying transformation process compared to existing baselines.",
    "authors": [
      "Satyapriya Krishna",
      "Rahul Gupta",
      "Christophe Dupuy"
    ],
    "published": "2021-01-29T23:15:24+00:00",
    "url": "http://arxiv.org/pdf/2102.01502v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2108.02765v1",
    "title": "Decoupled Transformer for Scalable Inference in Open-domain Question Answering",
    "content": "Decoupled Transformer for Scalable Inference in Open-domain Question Answering. Large transformer models, such as BERT, achieve state-of-the-art results in machine reading comprehension (MRC) for open-domain question answering (QA). However, transformers have a high computational cost for inference which makes them hard to apply to online QA systems for applications like voice assistants. To reduce computational cost and latency, we propose decoupling the transformer MRC model into input-component and cross-component. The decoupling allows for part of the representation computation to be performed offline and cached for online use. To retain the decoupled transformer accuracy, we devised a knowledge distillation objective from a standard transformer model. Moreover, we introduce learned representation compression layers which help reduce by four times the storage requirement for the cache. In experiments on the SQUAD 2.0 dataset, a decoupled transformer reduces the computational cost and latency of open-domain MRC by 30-40% with only 1.2 points worse F1-score compared to a standard transformer.",
    "authors": [
      "Haytham ElFadeel",
      "Stan Peshterliev"
    ],
    "published": "2021-08-05T17:53:40+00:00",
    "url": "http://arxiv.org/pdf/2108.02765v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2203.01186v1",
    "title": "Hybrid Model-based / Data-driven Graph Transform for Image Coding",
    "content": "Hybrid Model-based / Data-driven Graph Transform for Image Coding. Transform coding to sparsify signal representations remains crucial in an image compression pipeline. While the Karhunen-Lo\\`{e}ve transform (KLT) computed from an empirical covariance matrix $\\bar{C}$ is theoretically optimal for a stationary process, in practice, collecting sufficient statistics from a non-stationary image to reliably estimate $\\bar{C}$ can be difficult. In this paper, to encode an intra-prediction residual block, we pursue a hybrid model-based / data-driven approach: the first $K$ eigenvectors of a transform matrix are derived from a statistical model, e.g., the asymmetric discrete sine transform (ADST), for stability, while the remaining $N-K$ are computed from $\\bar{C}$ for performance. The transform computation is posed as a graph learning problem, where we seek a graph Laplacian matrix minimizing a graphical lasso objective inside a convex cone sharing the first $K$ eigenvectors in a Hilbert space of real symmetric matrices. We efficiently solve the problem via augmented Lagrangian relaxation and proximal gradient (PG). Using WebP as a baseline image codec, experimental results show that our hybrid graph transform achieved better energy compaction than default discrete cosine transform (DCT) and better stability than KLT.",
    "authors": [
      "Saghar Bagheri",
      "Tam Thuc Do",
      "Gene Cheung",
      "Antonio Ortega"
    ],
    "published": "2022-03-02T15:36:44+00:00",
    "url": "http://arxiv.org/pdf/2203.01186v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2304.10891v1",
    "title": "Transformer-based models and hardware acceleration analysis in autonomous driving: A survey",
    "content": "Transformer-based models and hardware acceleration analysis in autonomous driving: A survey. Transformer architectures have exhibited promising performance in various autonomous driving applications in recent years. On the other hand, its dedicated hardware acceleration on portable computational platforms has become the next critical step for practical deployment in real autonomous vehicles. This survey paper provides a comprehensive overview, benchmark, and analysis of Transformer-based models specifically tailored for autonomous driving tasks such as lane detection, segmentation, tracking, planning, and decision-making. We review different architectures for organizing Transformer inputs and outputs, such as encoder-decoder and encoder-only structures, and explore their respective advantages and disadvantages. Furthermore, we discuss Transformer-related operators and their hardware acceleration schemes in depth, taking into account key factors such as quantization and runtime. We specifically illustrate the operator level comparison between layers from convolutional neural network, Swin-Transformer, and Transformer with 4D encoder. The paper also highlights the challenges, trends, and current insights in Transformer-based models, addressing their hardware deployment and acceleration issues within the context of long-term autonomous driving applications.",
    "authors": [
      "Juan Zhong",
      "Zheng Liu",
      "Xi Chen"
    ],
    "published": "2023-04-21T11:15:31+00:00",
    "url": "http://arxiv.org/pdf/2304.10891v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2310.01082v2",
    "title": "Linear attention is (maybe) all you need (to understand transformer optimization)",
    "content": "Linear attention is (maybe) all you need (to understand transformer optimization). Transformer training is notoriously difficult, requiring a careful design of optimizers and use of various heuristics. We make progress towards understanding the subtleties of training Transformers by carefully studying a simple yet canonical linearized shallow Transformer model. Specifically, we train linear Transformers to solve regression tasks, inspired by J.~von Oswald et al.~(ICML 2023), and K.~Ahn et al.~(NeurIPS 2023). Most importantly, we observe that our proposed linearized models can reproduce several prominent aspects of Transformer training dynamics. Consequently, the results obtained in this paper suggest that a simple linearized Transformer model could actually be a valuable, realistic abstraction for understanding Transformer optimization.",
    "authors": [
      "Kwangjun Ahn",
      "Xiang Cheng",
      "Minhak Song",
      "Chulhee Yun",
      "Ali Jadbabaie",
      "Suvrit Sra"
    ],
    "published": "2023-10-02T10:48:42+00:00",
    "url": "http://arxiv.org/pdf/2310.01082v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2310.10930v1",
    "title": "Enhanced Transformer Architecture for Natural Language Processing",
    "content": "Enhanced Transformer Architecture for Natural Language Processing. Transformer is a state-of-the-art model in the field of natural language processing (NLP). Current NLP models primarily increase the number of transformers to improve processing performance. However, this technique requires a lot of training resources such as computing capacity. In this paper, a novel structure of Transformer is proposed. It is featured by full layer normalization, weighted residual connection, positional encoding exploiting reinforcement learning, and zero masked self-attention. The proposed Transformer model, which is called Enhanced Transformer, is validated by the bilingual evaluation understudy (BLEU) score obtained with the Multi30k translation dataset. As a result, the Enhanced Transformer achieves 202.96% higher BLEU score as compared to the original transformer with the translation dataset.",
    "authors": [
      "Woohyeon Moon",
      "Taeyoung Kim",
      "Bumgeun Park",
      "Dongsoo Har"
    ],
    "published": "2023-10-17T01:59:07+00:00",
    "url": "http://arxiv.org/pdf/2310.10930v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2402.12691v2",
    "title": "Tree-Planted Transformers: Unidirectional Transformer Language Models with Implicit Syntactic Supervision",
    "content": "Tree-Planted Transformers: Unidirectional Transformer Language Models with Implicit Syntactic Supervision. Syntactic Language Models (SLMs) can be trained efficiently to reach relatively high performance; however, they have trouble with inference efficiency due to the explicit generation of syntactic structures. In this paper, we propose a new method dubbed tree-planting: instead of explicitly generating syntactic structures, we \"plant\" trees into attention weights of unidirectional Transformer LMs to implicitly reflect syntactic structures of natural language. Specifically, unidirectional Transformer LMs trained with tree-planting will be called Tree-Planted Transformers (TPT), which inherit the training efficiency from SLMs without changing the inference efficiency of their underlying Transformer LMs. Targeted syntactic evaluations on the SyntaxGym benchmark demonstrated that TPTs, despite the lack of explicit generation of syntactic structures, significantly outperformed not only vanilla Transformer LMs but also various SLMs that generate hundreds of syntactic structures in parallel. This result suggests that TPTs can learn human-like syntactic knowledge as data-efficiently as SLMs while maintaining the modeling space of Transformer LMs unchanged.",
    "authors": [
      "Ryo Yoshida",
      "Taiga Someya",
      "Yohei Oseki"
    ],
    "published": "2024-02-20T03:37:24+00:00",
    "url": "http://arxiv.org/pdf/2402.12691v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2403.03168v1",
    "title": "Learning Explicitly Conditioned Sparsifying Transforms",
    "content": "Learning Explicitly Conditioned Sparsifying Transforms. Sparsifying transforms became in the last decades widely known tools for finding structured sparse representations of signals in certain transform domains. Despite the popularity of classical transforms such as DCT and Wavelet, learning optimal transforms that guarantee good representations of data into the sparse domain has been recently analyzed in a series of papers. Typically, the conditioning number and representation ability are complementary key features of learning square transforms that may not be explicitly controlled in a given optimization model. Unlike the existing approaches from the literature, in our paper, we consider a new sparsifying transform model that enforces explicit control over the data representation quality and the condition number of the learned transforms. We confirm through numerical experiments that our model presents better numerical behavior than the state-of-the-art.",
    "authors": [
      "Andrei Pătraşcu",
      "Cristian Rusu",
      "Paul Irofti"
    ],
    "published": "2024-03-05T18:03:51+00:00",
    "url": "http://arxiv.org/pdf/2403.03168v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2407.12638v1",
    "title": "ARTEMIS: A Mixed Analog-Stochastic In-DRAM Accelerator for Transformer Neural Networks",
    "content": "ARTEMIS: A Mixed Analog-Stochastic In-DRAM Accelerator for Transformer Neural Networks. Transformers have emerged as a powerful tool for natural language processing (NLP) and computer vision. Through the attention mechanism, these models have exhibited remarkable performance gains when compared to conventional approaches like recurrent neural networks (RNNs) and convolutional neural networks (CNNs). Nevertheless, transformers typically demand substantial execution time due to their extensive computations and large memory footprint. Processing in-memory (PIM) and near-memory computing (NMC) are promising solutions to accelerating transformers as they offer high compute parallelism and memory bandwidth. However, designing PIM/NMC architectures to support the complex operations and massive amounts of data that need to be moved between layers in transformer neural networks remains a challenge. We propose ARTEMIS, a mixed analog-stochastic in-DRAM accelerator for transformer models. Through employing minimal changes to the conventional DRAM arrays, ARTEMIS efficiently alleviates the costs associated with transformer model execution by supporting stochastic computing for multiplications and temporal analog accumulations using a novel in-DRAM metal-on-metal capacitor. Our analysis indicates that ARTEMIS exhibits at least 3.0x speedup, 1.8x lower energy, and 1.9x better energy efficiency compared to GPU, TPU, CPU, and state-of-the-art PIM transformer hardware accelerators.",
    "authors": [
      "Salma Afifi",
      "Ishan Thakkar",
      "Sudeep Pasricha"
    ],
    "published": "2024-07-17T15:08:14+00:00",
    "url": "http://arxiv.org/pdf/2407.12638v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2411.07218v1",
    "title": "TreeCoders: Trees of Transformers",
    "content": "TreeCoders: Trees of Transformers. In this paper, we introduce TreeCoders, a novel family of transformer trees. We moved away from traditional linear transformers to complete k-ary trees. Transformer blocks serve as nodes, and generic classifiers learn to select the best child and route the sequence of tokens to a specific leaf. The selectors, moved outside the transformer blocks, allow for the use of a variety of architecture without further modifications. Furthermore, our proposed architecture supports sparse node activation due to the logarithmic complexity of a tree search. We validate our idea by testing a series of decoder-only tree transformers, achieving competitive results across a diverse range of language datasets. Our study demonstrates that the proposed tree transformer model outperforms a size-equivalent linear transformer model 76\\% of the time over a wide range of tree architectures. Furthermore, our proposed model naturally lends itself to distributed implementation.",
    "authors": [
      "Pierre Colonna D'Istria",
      "Abdulrahman Altahhan"
    ],
    "published": "2024-11-11T18:40:04+00:00",
    "url": "http://arxiv.org/pdf/2411.07218v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2212.14164v2",
    "title": "On Transforming Reinforcement Learning by Transformer: The Development Trajectory",
    "content": "On Transforming Reinforcement Learning by Transformer: The Development Trajectory. Transformer, originally devised for natural language processing, has also attested significant success in computer vision. Thanks to its super expressive power, researchers are investigating ways to deploy transformers to reinforcement learning (RL) and the transformer-based models have manifested their potential in representative RL benchmarks. In this paper, we collect and dissect recent advances on transforming RL by transformer (transformer-based RL or TRL), in order to explore its development trajectory and future trend. We group existing developments in two categories: architecture enhancement and trajectory optimization, and examine the main applications of TRL in robotic manipulation, text-based games, navigation and autonomous driving. For architecture enhancement, these methods consider how to apply the powerful transformer structure to RL problems under the traditional RL framework, which model agents and environments much more precisely than deep RL methods, but they are still limited by the inherent defects of traditional RL algorithms, such as bootstrapping and \"deadly triad\". For trajectory optimization, these methods treat RL problems as sequence modeling and train a joint state-action model over entire trajectories under the behavior cloning framework, which are able to extract policies from static datasets and fully use the long-sequence modeling capability of the transformer. Given these advancements, extensions and challenges in TRL are reviewed and proposals about future direction are discussed. We hope that this survey can provide a detailed introduction to TRL and motivate future research in this rapidly developing field.",
    "authors": [
      "Shengchao Hu",
      "Li Shen",
      "Ya Zhang",
      "Yixin Chen",
      "Dacheng Tao"
    ],
    "published": "2022-12-29T03:15:59+00:00",
    "url": "http://arxiv.org/pdf/2212.14164v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2502.20525v1",
    "title": "Revisiting Kernel Attention with Correlated Gaussian Process Representation",
    "content": "Revisiting Kernel Attention with Correlated Gaussian Process Representation. Transformers have increasingly become the de facto method to model sequential data with state-of-the-art performance. Due to its widespread use, being able to estimate and calibrate its modeling uncertainty is important to understand and design robust transformer models. To achieve this, previous works have used Gaussian processes (GPs) to perform uncertainty calibration for the attention units of transformers and attained notable successes. However, such approaches have to confine the transformers to the space of symmetric attention to ensure the necessary symmetric requirement of their GP's kernel specification, which reduces the representation capacity of the model. To mitigate this restriction, we propose the Correlated Gaussian Process Transformer (CGPT), a new class of transformers whose self-attention units are modeled as cross-covariance between two correlated GPs (CGPs). This allows asymmetries in attention and can enhance the representation capacity of GP-based transformers. We also derive a sparse approximation for CGP to make it scale better. Our empirical studies show that both CGP-based and sparse CGP-based transformers achieve better performance than state-of-the-art GP-based transformers on a variety of benchmark tasks. The code for our experiments is available at https://github.com/MinhLong210/CGP-Transformers.",
    "authors": [
      "Long Minh Bui",
      "Tho Tran Huu",
      "Duy Dinh",
      "Tan Minh Nguyen",
      "Trong Nghia Hoang"
    ],
    "published": "2025-02-27T21:21:48+00:00",
    "url": "http://arxiv.org/pdf/2502.20525v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/hep-th/9301005v1",
    "title": "Duality, Marginal Perturbations and Gauging",
    "content": "Duality, Marginal Perturbations and Gauging. We study duality transformations for two-dimensional sigma models with abelian chiral isometries and prove that generic such transformations are equivalent to integrated marginal perturbations by bilinears in the chiral currents, thus confirming a recent conjecture by Hassan and Sen formulated in the context of Wess-Zumino-Witten models. Specific duality transformations instead give rise to coset models plus free bosons.",
    "authors": [
      "Måns Henningson",
      "Chiara R. Nappi"
    ],
    "published": "1993-01-04T20:56:00+00:00",
    "url": "http://arxiv.org/pdf/hep-th/9301005v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/hep-th/0506188v2",
    "title": "Classical solution of a sigma-model in curved background",
    "content": "Classical solution of a sigma-model in curved background. We have solved a sigma-model in curved background using the fact that the Poisson-Lie T-duality can transform the curved background into the flat one. For finding solution of the flat model we have used transformation of coordinates that makes the metric constant. The T-duality transform was then explicitly performed.",
    "authors": [
      "Ladislav Hlavaty"
    ],
    "published": "2005-06-22T15:01:41+00:00",
    "url": "http://arxiv.org/pdf/hep-th/0506188v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1508.02014v2",
    "title": "On the injectivity of the generalized Radon transform arising in a model of mathematical economics",
    "content": "On the injectivity of the generalized Radon transform arising in a model of mathematical economics. In the present article we consider the uniqueness problem for the generalized Radon transform arising in a mathematical model of production. We prove uniqueness theorems for this transform and for the profit function in the corresponding model of production. Our approach is based on the multidimensional Wiener's approximation theorems.",
    "authors": [
      "A. D. Agaltsov"
    ],
    "published": "2015-08-09T11:54:44+00:00",
    "url": "http://arxiv.org/pdf/1508.02014v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1712.08390v1",
    "title": "Covariant hodograph transformations between nonlocal short pulse models and AKNS$(-1)$ system",
    "content": "Covariant hodograph transformations between nonlocal short pulse models and AKNS$(-1)$ system. The paper presents hodograph transformation between nonlocal short pulse models and the first member in the AKNS negative hierarchy (AKNS($-1$)). We consider real and complex multi-component cases. It is shown that the independent variables of the short pulse models and AKNS($-1$) that are connected via hodograph transformation are covariant in nonlocal reductions.",
    "authors": [
      "Kui Chen",
      "Shimin Liu",
      "Da-jun Zhang"
    ],
    "published": "2017-12-22T10:56:29+00:00",
    "url": "http://arxiv.org/pdf/1712.08390v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2001.01140v1",
    "title": "Transformer-based language modeling and decoding for conversational speech recognition",
    "content": "Transformer-based language modeling and decoding for conversational speech recognition. We propose a way to use a transformer-based language model in conversational speech recognition. Specifically, we focus on decoding efficiently in a weighted finite-state transducer framework. We showcase an approach to lattice re-scoring that allows for longer range history captured by a transfomer-based language model and takes advantage of a transformer's ability to avoid computing sequentially.",
    "authors": [
      "Kareem Nassar"
    ],
    "published": "2020-01-04T23:27:59+00:00",
    "url": "http://arxiv.org/pdf/2001.01140v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1908.03979v2",
    "title": "Mapping the Hubbard model to the t-J model using ground state unitary transformations",
    "content": "Mapping the Hubbard model to the t-J model using ground state unitary transformations. The effective low-energy models of the Hubbard model are usually derived from perturbation theory. Here we derive the effective model of the Hubbard model in spin space and t-J space using a unitary transformation from numerical optimization. We represent the Hamiltonian as Matrix product state(MPO) and represent the unitary transformation using gates according to tensor network methods. We obtain this unitary transformation by optimizing the unitary transformation between the ground state of the Hubbard model and the projection of the Hubbard model ground state into spin space and t-J space. The unitary transformation we get from numerical optimization yields effective models that are in line with perturbation theories. This numerical optimization method starting from ground state provides another approach to analyze effective low-energy models of strongly correlated electron systems.",
    "authors": [
      "Yifan Tian"
    ],
    "published": "2019-08-12T01:26:18+00:00",
    "url": "http://arxiv.org/pdf/1908.03979v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2308.00197v1",
    "title": "Performance Evaluation of Swin Vision Transformer Model using Gradient Accumulation Optimization Technique",
    "content": "Performance Evaluation of Swin Vision Transformer Model using Gradient Accumulation Optimization Technique. Vision Transformers (ViTs) have emerged as a promising approach for visual recognition tasks, revolutionizing the field by leveraging the power of transformer-based architectures. Among the various ViT models, Swin Transformers have gained considerable attention due to their hierarchical design and ability to capture both local and global visual features effectively. This paper evaluates the performance of Swin ViT model using gradient accumulation optimization (GAO) technique. We investigate the impact of gradient accumulation optimization technique on the model's accuracy and training time. Our experiments show that applying the GAO technique leads to a significant decrease in the accuracy of the Swin ViT model, compared to the standard Swin Transformer model. Moreover, we detect a significant increase in the training time of the Swin ViT model when GAO model is applied. These findings suggest that applying the GAO technique may not be suitable for the Swin ViT model, and concern should be undertaken when using GAO technique for other transformer-based models.",
    "authors": [
      "Sanad Aburass",
      "Osama Dorgham"
    ],
    "published": "2023-07-31T23:30:16+00:00",
    "url": "http://arxiv.org/pdf/2308.00197v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2309.10061v1",
    "title": "Transformed-Linear Innovations Algorithm for Modeling and Forecasting of Time Series Extremes",
    "content": "Transformed-Linear Innovations Algorithm for Modeling and Forecasting of Time Series Extremes. The innovations algorithm is a classical recursive forecasting algorithm used in time series analysis. We develop the innovations algorithm for a class of nonnegative regularly varying time series models constructed via transformed-linear arithmetic. In addition to providing the best linear predictor, the algorithm also enables us to estimate parameters of transformed-linear regularly-varying moving average (MA) models, thus providing a tool for modeling.   We first construct an inner product space of transformed-linear combinations of nonnegative regularly-varying random variables and prove its link to a Hilbert space which allows us to employ the projection theorem, from which we develop the transformed-linear innovations algorithm. Turning our attention to the class of transformed linear MA($\\infty$) models, we give results on parameter estimation and also show that this class of models is dense in the class of possible tail pairwise dependence functions (TPDFs). We also develop an extremes analogue of the classical Wold decomposition. Simulation study shows that our class of models captures tail dependence for the GARCH(1,1) model and a Markov time series model, both of which are outside our class of models.",
    "authors": [
      "Nehali Mhatre",
      "Daniel Cooley"
    ],
    "published": "2023-09-18T18:17:07+00:00",
    "url": "http://arxiv.org/pdf/2309.10061v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2403.03737v1",
    "title": "Probabilistic Topic Modelling with Transformer Representations",
    "content": "Probabilistic Topic Modelling with Transformer Representations. Topic modelling was mostly dominated by Bayesian graphical models during the last decade. With the rise of transformers in Natural Language Processing, however, several successful models that rely on straightforward clustering approaches in transformer-based embedding spaces have emerged and consolidated the notion of topics as clusters of embedding vectors. We propose the Transformer-Representation Neural Topic Model (TNTM), which combines the benefits of topic representations in transformer-based embedding spaces and probabilistic modelling. Therefore, this approach unifies the powerful and versatile notion of topics based on transformer embeddings with fully probabilistic modelling, as in models such as Latent Dirichlet Allocation (LDA). We utilize the variational autoencoder (VAE) framework for improved inference speed and modelling flexibility. Experimental results show that our proposed model achieves results on par with various state-of-the-art approaches in terms of embedding coherence while maintaining almost perfect topic diversity. The corresponding source code is available at https://github.com/ArikReuter/TNTM.",
    "authors": [
      "Arik Reuter",
      "Anton Thielmann",
      "Christoph Weisser",
      "Benjamin Säfken",
      "Thomas Kneib"
    ],
    "published": "2024-03-06T14:27:29+00:00",
    "url": "http://arxiv.org/pdf/2403.03737v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2011.04006v1",
    "title": "Long Range Arena: A Benchmark for Efficient Transformers",
    "content": "Long Range Arena: A Benchmark for Efficient Transformers. Transformers do not scale very well to long sequence lengths largely because of quadratic self-attention complexity. In the recent months, a wide spectrum of efficient, fast Transformers have been proposed to tackle this problem, more often than not claiming superior or comparable model quality to vanilla Transformer models. To this date, there is no well-established consensus on how to evaluate this class of models. Moreover, inconsistent benchmarking on a wide spectrum of tasks and datasets makes it difficult to assess relative model quality amongst many models. This paper proposes a systematic and unified benchmark, LRA, specifically focused on evaluating model quality under long-context scenarios. Our benchmark is a suite of tasks consisting of sequences ranging from $1K$ to $16K$ tokens, encompassing a wide range of data types and modalities such as text, natural, synthetic images, and mathematical expressions requiring similarity, structural, and visual-spatial reasoning. We systematically evaluate ten well-established long-range Transformer models (Reformers, Linformers, Linear Transformers, Sinkhorn Transformers, Performers, Synthesizers, Sparse Transformers, and Longformers) on our newly proposed benchmark suite. LRA paves the way towards better understanding this class of efficient Transformer models, facilitates more research in this direction, and presents new challenging tasks to tackle. Our benchmark code will be released at https://github.com/google-research/long-range-arena.",
    "authors": [
      "Yi Tay",
      "Mostafa Dehghani",
      "Samira Abnar",
      "Yikang Shen",
      "Dara Bahri",
      "Philip Pham",
      "Jinfeng Rao",
      "Liu Yang",
      "Sebastian Ruder",
      "Donald Metzler"
    ],
    "published": "2020-11-08T15:53:56+00:00",
    "url": "http://arxiv.org/pdf/2011.04006v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/hep-th/9905051v1",
    "title": "Making Sense of Singular Gauge Transformations in 1+1 and 2+1 Fermion Models",
    "content": "Making Sense of Singular Gauge Transformations in 1+1 and 2+1 Fermion Models. We study the problem of decoupling fermion fields in 1+1 and 2+1 dimensions, in interaction with a gauge field, by performing local transformations of the fermions in the functional integral. This could always be done if singular (large) gauge transformations were allowed, since any gauge field configuration may be represented as a singular pure gauge field. However, the effect of a singular gauge transformation of the fermions is equivalent to the one of a regular transformation with a non-trivial action on the spinorial indices. For example, in the two dimensional case, singular gauge transformations lead naturally to chiral transformations, and hence to the usual decoupling mechanism based on Fujikawa Jacobians. In 2+1 dimensions, using the same procedure, different transformations emerge, which also give rise to Fujikawa Jacobians. We apply this idea to obtain the v.e.v of the fermionic current in a background field, in terms of the Jacobian for an infinitesimal decoupling transformation, finding the parity violating result.",
    "authors": [
      "C. D. Fosco",
      "F. A. Schaposnik"
    ],
    "published": "1999-05-07T12:05:18+00:00",
    "url": "http://arxiv.org/pdf/hep-th/9905051v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1107.4396v2",
    "title": "The IHS Transformations Based Image Fusion",
    "content": "The IHS Transformations Based Image Fusion. The IHS sharpening technique is one of the most commonly used techniques for sharpening. Different transformations have been developed to transfer a color image from the RGB space to the IHS space. Through literature, it appears that, various scientists proposed alternative IHS transformations and many papers have reported good results whereas others show bad ones as will as not those obtained which the formula of IHS transformation were used. In addition to that, many papers show different formulas of transformation matrix such as IHS transformation. This leads to confusion what is the exact formula of the IHS transformation?. Therefore, the main purpose of this work is to explore different IHS transformation techniques and experiment it as IHS based image fusion. The image fusion performance was evaluated, in this study, using various methods to estimate the quality and degree of information improvement of a fused image quantitatively.",
    "authors": [
      "Firouz Abdullah Al-Wassai",
      "N. V. Kalyankar",
      "Ali A. Al-Zuky"
    ],
    "published": "2011-07-19T06:18:56+00:00",
    "url": "http://arxiv.org/pdf/1107.4396v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1110.0061v1",
    "title": "Learning image transformations without training examples",
    "content": "Learning image transformations without training examples. The use of image transformations is essential for efficient modeling and learning of visual data. But the class of relevant transformations is large: affine transformations, projective transformations, elastic deformations, ... the list goes on. Therefore, learning these transformations, rather than hand coding them, is of great conceptual interest. To the best of our knowledge, all the related work so far has been concerned with either supervised or weakly supervised learning (from correlated sequences, video streams, or image-transform pairs). In this paper, on the contrary, we present a simple method for learning affine and elastic transformations when no examples of these transformations are explicitly given, and no prior knowledge of space (such as ordering of pixels) is included either. The system has only access to a moderately large database of natural images arranged in no particular order.",
    "authors": [
      "Sergey Pankov"
    ],
    "published": "2011-10-01T01:07:03+00:00",
    "url": "http://arxiv.org/pdf/1110.0061v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1507.05390v2",
    "title": "Derivative-dependent metric transformation and physical degrees of freedom",
    "content": "Derivative-dependent metric transformation and physical degrees of freedom. We study metric transformations which depend on a scalar field $\\phi$ and its first derivatives and confirm that the number of physical degrees of freedom does not change under such transformations, as long as they are not singular. We perform a Hamiltonian analysis of a simple model in the gauge $\\phi = t$. In addition, we explicitly show that the transformation and the gauge fixing do commute in transforming the action. We then extend the analysis to more general gravitational theories and transformations in general gauges. We verify that the set of all constraints and the constraint algebra are left unchanged by such transformations and conclude that the number of degrees of freedom is not modified by a regular and invertible generic transformation among two metrics. We also discuss the implications on the recently called \"hidden\" constraints and on the case of a singular transformation, a.k.a. mimetic gravity.",
    "authors": [
      "Guillem Domènech",
      "Shinji Mukohyama",
      "Ryo Namba",
      "Atsushi Naruko",
      "Rio Saitou",
      "Yota Watanabe"
    ],
    "published": "2015-07-20T05:58:30+00:00",
    "url": "http://arxiv.org/pdf/1507.05390v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1710.04914v1",
    "title": "Atomic force microscopy study of the tetragonal to monoclinic transformation behaviour of silica doped yttria-stabilized zirconia",
    "content": "Atomic force microscopy study of the tetragonal to monoclinic transformation behaviour of silica doped yttria-stabilized zirconia. The tetragonal to monoclinic phase transformation of zirconia has been the subject of extensive studies over the last 20 years [1-4]. The main features of the transformation have been identified and its martensitic nature is now widely recognised [5-8]. More specifically, the relevance of a nucleation and growth model to describe the transformation is widely accepted. Recent fracture episodes [9] of zirconia hip joint heads were reported, failures related to the t-m transformation degradation. Among the materials solutions considered for decreasing the sensitivity to t-m phase transformation, the possibility of adding silica as a dopant appears as an appealing one. Previous studies have revealed the beneficial effect of silica addition by the formation of a glassy phase at the grain boundaries and triple points. This glassy phase has been proven to reduce the residual stresses level [10], slowing down the transformation kinetics. Preliminary quantitative investigations by XRD have shown these materials are less susceptible to transformation. However, the mechanism by which the transformation propagated has still to be assessed.",
    "authors": [
      "Sylvain Deville",
      "Jérôme Chevalier",
      "Laurent Gremillard"
    ],
    "published": "2017-10-13T13:28:48+00:00",
    "url": "http://arxiv.org/pdf/1710.04914v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1103.6045v1",
    "title": "Elastic cloaking theory",
    "content": "Elastic cloaking theory. The transformation theory of optics and acoustics is developed for the equations of linear anisotropic elasticity. The transformed equations correspond to non-unique material properties that can be varied for a given transformation by selection of the matrix relating displacements in the two descriptions. This gauge matrix can be chosen to make the transformed density isotropic for any transformation although the stress in the transformed material is not generally symmetric. Symmetric stress is obtained only if the gauge matrix is identical to the transformation matrix, in agreement with Milton et al. (2006). The elastic transformation theory is applied to the case of cylindrical anisotropy. The equations of motion for the transformed material with isotropic density are expressed in Stroh format, suitable for modeling cylindrical elastic cloaking. It is shown that there is a preferred approximate material with symmetric stress that could be a useful candidate for making cylindrical elastic cloaking devices.",
    "authors": [
      "A. N. Norris",
      "A. L. Shuvalov"
    ],
    "published": "2011-03-30T20:40:10+00:00",
    "url": "http://arxiv.org/pdf/1103.6045v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2201.09617v1",
    "title": "Leveraging Data and Analytics for Digital Business Transformation through DataOps: An Information Processing Perspective",
    "content": "Leveraging Data and Analytics for Digital Business Transformation through DataOps: An Information Processing Perspective. Digital business transformation has become increasingly important for organizations. Since transforming business digitally is an ongoing process, it requires an integrated and disciplined approach. Data Operations (DataOps), emerging in practice, can provide organizations with such an approach to leverage data and analytics for digital business transformation. This paper proposes a framework that integrates digital business transformation, data analytics, and DataOps through the lens of information processing theory (IPT). The details of this framework explain how organizations can employ DataOps as an integrated and disciplined approach to understand their analytical information needs and develop the analytical information processing capability required for digital business transformation. DataOps-enabled digital business transformation, in turn, improves organizational performance by improving operational efficiency and creating new business models. This research extends current knowledge on digital transformation by bringing in DataOps and analytics through IPT and thereby provide organizations with a novel approach for their digital business transformations.",
    "authors": [
      "Jia Xu",
      "Humza Naseer",
      "Sean Maynard",
      "Justin Fillipou"
    ],
    "published": "2022-01-24T11:49:57+00:00",
    "url": "http://arxiv.org/pdf/2201.09617v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2203.09581v3",
    "title": "SepTr: Separable Transformer for Audio Spectrogram Processing",
    "content": "SepTr: Separable Transformer for Audio Spectrogram Processing. Following the successful application of vision transformers in multiple computer vision tasks, these models have drawn the attention of the signal processing community. This is because signals are often represented as spectrograms (e.g. through Discrete Fourier Transform) which can be directly provided as input to vision transformers. However, naively applying transformers to spectrograms is suboptimal. Since the axes represent distinct dimensions, i.e. frequency and time, we argue that a better approach is to separate the attention dedicated to each axis. To this end, we propose the Separable Transformer (SepTr), an architecture that employs two transformer blocks in a sequential manner, the first attending to tokens within the same time interval, and the second attending to tokens within the same frequency bin. We conduct experiments on three benchmark data sets, showing that our separable architecture outperforms conventional vision transformers and other state-of-the-art methods. Unlike standard transformers, SepTr linearly scales the number of trainable parameters with the input size, thus having a lower memory footprint. Our code is available as open source at https://github.com/ristea/septr.",
    "authors": [
      "Nicolae-Catalin Ristea",
      "Radu Tudor Ionescu",
      "Fahad Shahbaz Khan"
    ],
    "published": "2022-03-17T19:48:43+00:00",
    "url": "http://arxiv.org/pdf/2203.09581v3"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2210.10461v2",
    "title": "Geostatistics in the presence of multivariate complexities: comparison of multi-Gaussian transforms",
    "content": "Geostatistics in the presence of multivariate complexities: comparison of multi-Gaussian transforms. One of the most challenging aspects of multivariate geostatistics is dealing with complex relationships between variables. Geostatistical co-simulation and spatial decorrelation methods, commonly used for modelling multiple variables, are ineffective in the presence of multivariate complexities. On the other hand, multi-Gaussian transforms are designed to deal with complex multivariate relationships, such as non-linearity, heteroscedasticity and geological constraints. These methods transform the variables into independent multi-Gaussian factors that can be individually simulated. This study compares the performance of the following multi-Gaussian transforms: rotation based iterative Gaussianisation, projection pursuit multivariate transform and flow transformation. Case studies with bivariate complexities are used to evaluate and compare the realisations of the transformed values. For this purpose, commonly used geostatistical validation metrics are applied, including multivariate normality tests, reproduction of bivariate relationships, and histogram and variogram validation. Based on most of the metrics, all three methods produced results of similar quality. The most obvious difference is the execution speed for forward and back transformation, for which flow transformation is much slower.",
    "authors": [
      "Sultan Abulkhair",
      "Peter A. Dowd",
      "Chaoshui Xu"
    ],
    "published": "2022-10-19T10:54:42+00:00",
    "url": "http://arxiv.org/pdf/2210.10461v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2303.15105v1",
    "title": "Vision Transformer with Quadrangle Attention",
    "content": "Vision Transformer with Quadrangle Attention. Window-based attention has become a popular choice in vision transformers due to its superior performance, lower computational complexity, and less memory footprint. However, the design of hand-crafted windows, which is data-agnostic, constrains the flexibility of transformers to adapt to objects of varying sizes, shapes, and orientations. To address this issue, we propose a novel quadrangle attention (QA) method that extends the window-based attention to a general quadrangle formulation. Our method employs an end-to-end learnable quadrangle regression module that predicts a transformation matrix to transform default windows into target quadrangles for token sampling and attention calculation, enabling the network to model various targets with different shapes and orientations and capture rich context information. We integrate QA into plain and hierarchical vision transformers to create a new architecture named QFormer, which offers minor code modifications and negligible extra computational cost. Extensive experiments on public benchmarks demonstrate that QFormer outperforms existing representative vision transformers on various vision tasks, including classification, object detection, semantic segmentation, and pose estimation. The code will be made publicly available at \\href{https://github.com/ViTAE-Transformer/QFormer}{QFormer}.",
    "authors": [
      "Qiming Zhang",
      "Jing Zhang",
      "Yufei Xu",
      "Dacheng Tao"
    ],
    "published": "2023-03-27T11:13:50+00:00",
    "url": "http://arxiv.org/pdf/2303.15105v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2304.04225v1",
    "title": "Transformer Utilization in Medical Image Segmentation Networks",
    "content": "Transformer Utilization in Medical Image Segmentation Networks. Owing to success in the data-rich domain of natural images, Transformers have recently become popular in medical image segmentation. However, the pairing of Transformers with convolutional blocks in varying architectural permutations leaves their relative effectiveness to open interpretation. We introduce Transformer Ablations that replace the Transformer blocks with plain linear operators to quantify this effectiveness. With experiments on 8 models on 2 medical image segmentation tasks, we explore -- 1) the replaceable nature of Transformer-learnt representations, 2) Transformer capacity alone cannot prevent representational replaceability and works in tandem with effective design, 3) The mere existence of explicit feature hierarchies in transformer blocks is more beneficial than accompanying self-attention modules, 4) Major spatial downsampling before Transformer modules should be used with caution.",
    "authors": [
      "Saikat Roy",
      "Gregor Koehler",
      "Michael Baumgartner",
      "Constantin Ulrich",
      "Jens Petersen",
      "Fabian Isensee",
      "Klaus Maier-Hein"
    ],
    "published": "2023-04-09T12:35:22+00:00",
    "url": "http://arxiv.org/pdf/2304.04225v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2410.08806v1",
    "title": "Don't Transform the Code, Code the Transforms: Towards Precise Code Rewriting using LLMs",
    "content": "Don't Transform the Code, Code the Transforms: Towards Precise Code Rewriting using LLMs. Tools for rewriting, refactoring and optimizing code should be fast and correct. Large language models (LLMs), by their nature, possess neither of these qualities. Yet, there remains tremendous opportunity in using LLMs to improve code.   We explore the use of LLMs not to transform code, but to code transforms. We propose a chain-of-thought approach to synthesizing code transformations from a small number of input/output code examples that incorporates execution and feedback. Unlike the direct rewrite approach, LLM-generated transformations are easy to inspect, debug, and validate. The logic of the rewrite is explicitly coded and easy to adapt. The compute required to run code transformations is minute compared to that of LLM rewriting.   We test our approach on 16 Python code transformations and find that LLM- generated transforms are perfectly precise for 7 of them and less imprecise than direct LLM rewriting on the others. We hope to encourage further research to improving the precision of LLM code rewriting.",
    "authors": [
      "Chris Cummins",
      "Volker Seeker",
      "Jordi Armengol-Estapé",
      "Aram H. Markosyan",
      "Gabriel Synnaeve",
      "Hugh Leather"
    ],
    "published": "2024-10-11T13:45:16+00:00",
    "url": "http://arxiv.org/pdf/2410.08806v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2410.09122v1",
    "title": "Study on (r,s)- Generalised Transformation Graphs, A Novel Perspective Based on Transformation Graphs",
    "content": "Study on (r,s)- Generalised Transformation Graphs, A Novel Perspective Based on Transformation Graphs. For a graph $\\mathbb{Q}=(\\mathbb{V},\\mathbb{E})$, the transformation graphs are defined as graphs with vertex set being $\\mathbb{V(Q)} \\cup \\mathbb{E(Q)}$ and edge set is described following certain conditions. In comparison to the structure descriptor of the original graph $\\mathbb{Q}$, the topological descriptor of its transformation graphs displays distinct characteristics related to structure. Thus, a compound's transformation graphs descriptors can be used to model a variety of structural features of the underlying chemical. In this work, the concept of transformation graphs are extended giving rise to novel class of graphs, the $(r,s)$- generalised transformation graphs, whose vertex set is union of $r$ copies of $\\mathbb{V(Q)}$ and $s$ copies of $\\mathbb{E(Q)}$, where, $r, s \\in N$ and the edge set are defined under certain conditions. Further, these class of graphs are analysed with the help of first Zagreb index. Mainly, there are eight transformation graphs based on the criteria for edge set, but under the concept of $(r,s)$- generalised transformation graphs, infinite number of graphs can be described and analysed.",
    "authors": [
      "Parvez Ali",
      "Annmaria Baby",
      "D. Antony Xavier",
      "Theertha Nair A",
      "Haidar Ali",
      "Syed Ajaz K. Kirmani"
    ],
    "published": "2024-10-11T05:52:45+00:00",
    "url": "http://arxiv.org/pdf/2410.09122v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2206.01136v3",
    "title": "Transforming medical imaging with Transformers? A comparative review of key properties, current progresses, and future perspectives",
    "content": "Transforming medical imaging with Transformers? A comparative review of key properties, current progresses, and future perspectives. Transformer, the latest technological advance of deep learning, has gained prevalence in natural language processing or computer vision. Since medical imaging bear some resemblance to computer vision, it is natural to inquire about the status quo of Transformers in medical imaging and ask the question: can the Transformer models transform medical imaging? In this paper, we attempt to make a response to the inquiry. After a brief introduction of the fundamentals of Transformers, especially in comparison with convolutional neural networks (CNNs), and highlighting key defining properties that characterize the Transformers, we offer a comprehensive review of the state-of-the-art Transformer-based approaches for medical imaging and exhibit current research progresses made in the areas of medical image segmentation, recognition, detection, registration, reconstruction, enhancement, etc. In particular, what distinguishes our review lies in its organization based on the Transformer's key defining properties, which are mostly derived from comparing the Transformer and CNN, and its type of architecture, which specifies the manner in which the Transformer and CNN are combined, all helping the readers to best understand the rationale behind the reviewed approaches. We conclude with discussions of future perspectives.",
    "authors": [
      "Jun Li",
      "Junyu Chen",
      "Yucheng Tang",
      "Ce Wang",
      "Bennett A. Landman",
      "S. Kevin Zhou"
    ],
    "published": "2022-06-02T16:38:31+00:00",
    "url": "http://arxiv.org/pdf/2206.01136v3"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/math/0511063v2",
    "title": "Birational transformations of weighted graphs",
    "content": "Birational transformations of weighted graphs. We introduce the notion of a standard weighted graph and show that every weighted graph has an essentially unique standard model. Moreover we classify birational transformations between such models. Our central result shows that these are composed of elementary transformations. The latter ones are defined similarly to the well known elementary transformations of ruled surfaces.   In a forthcoming paper, we apply these results in the geometric setup to obtain standard equivariant completions of affine surfaces with an action of certain algebraic groups. We show that these completions are unique up to equivariant elementary transformations.",
    "authors": [
      "Hubert Flenner",
      "Shulim Kaliman",
      "Mikhail Zaidenberg"
    ],
    "published": "2005-11-02T20:47:12+00:00",
    "url": "http://arxiv.org/pdf/math/0511063v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1010.0049v3",
    "title": "Avrami exponent under transient and heterogeneous nucleation transformation conditions",
    "content": "Avrami exponent under transient and heterogeneous nucleation transformation conditions. The Kolmogorov-Johnson-Mehl-Avrami model for isothermal transformation kinetics is universal under specific assumptions. However, the experimental Avrami exponent deviates from the universal value. In this context, we study the effect of transient heterogeneous nucleation on the Avrami exponent for bulk materials and also for transformations leading to nanostructured materials. All transformations are assumed to be polymorphic. A discrete version of the KJMA model is modified for this purpose. Scaling relations for transformations under different conditions are reported.",
    "authors": [
      "I. Sinha",
      "R. K. Mandal"
    ],
    "published": "2010-10-01T01:58:39+00:00",
    "url": "http://arxiv.org/pdf/1010.0049v3"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1410.4778v1",
    "title": "Parametric Transformed Fay-Herriot Model for Small Area Estimation",
    "content": "Parametric Transformed Fay-Herriot Model for Small Area Estimation. In this paper, we consider parametric transformed Fay-Herriot models, and clarify conditions on transformations under which the estimator of the transformation is consistent. It is shown that the dual power transformation satisfies the conditions. Based on asymptotic properties for estimators of parameters, we derive a second-order approximation of the prediction error of the empirical best linear unbiased predictors (EBLUP) and obtain a second-order unbiased estimator of the prediction error. Finally, performances of the proposed procedures are investigated through simulation and empirical studies.",
    "authors": [
      "Shonosuke Sugasawa",
      "Tatsuya Kubokawa"
    ],
    "published": "2014-10-17T15:57:56+00:00",
    "url": "http://arxiv.org/pdf/1410.4778v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1806.08887v2",
    "title": "The Sparse Manifold Transform",
    "content": "The Sparse Manifold Transform. We present a signal representation framework called the sparse manifold transform that combines key ideas from sparse coding, manifold learning, and slow feature analysis. It turns non-linear transformations in the primary sensory signal space into linear interpolations in a representational embedding space while maintaining approximate invertibility. The sparse manifold transform is an unsupervised and generative framework that explicitly and simultaneously models the sparse discreteness and low-dimensional manifold structure found in natural scenes. When stacked, it also models hierarchical composition. We provide a theoretical description of the transform and demonstrate properties of the learned representation on both synthetic data and natural videos.",
    "authors": [
      "Yubei Chen",
      "Dylan M. Paiton",
      "Bruno A. Olshausen"
    ],
    "published": "2018-06-23T01:44:50+00:00",
    "url": "http://arxiv.org/pdf/1806.08887v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2107.04735v1",
    "title": "Local-to-Global Self-Attention in Vision Transformers",
    "content": "Local-to-Global Self-Attention in Vision Transformers. Transformers have demonstrated great potential in computer vision tasks. To avoid dense computations of self-attentions in high-resolution visual data, some recent Transformer models adopt a hierarchical design, where self-attentions are only computed within local windows. This design significantly improves the efficiency but lacks global feature reasoning in early stages. In this work, we design a multi-path structure of the Transformer, which enables local-to-global reasoning at multiple granularities in each stage. The proposed framework is computationally efficient and highly effective. With a marginal increasement in computational overhead, our model achieves notable improvements in both image classification and semantic segmentation. Code is available at https://github.com/ljpadam/LG-Transformer",
    "authors": [
      "Jinpeng Li",
      "Yichao Yan",
      "Shengcai Liao",
      "Xiaokang Yang",
      "Ling Shao"
    ],
    "published": "2021-07-10T02:34:55+00:00",
    "url": "http://arxiv.org/pdf/2107.04735v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2110.08037v1",
    "title": "Tensor-to-Image: Image-to-Image Translation with Vision Transformers",
    "content": "Tensor-to-Image: Image-to-Image Translation with Vision Transformers. Transformers gain huge attention since they are first introduced and have a wide range of applications. Transformers start to take over all areas of deep learning and the Vision transformers paper also proved that they can be used for computer vision tasks. In this paper, we utilized a vision transformer-based custom-designed model, tensor-to-image, for the image to image translation. With the help of self-attention, our model was able to generalize and apply to different problems without a single modification.",
    "authors": [
      "Yiğit Gündüç"
    ],
    "published": "2021-10-06T17:57:45+00:00",
    "url": "http://arxiv.org/pdf/2110.08037v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2210.05193v2",
    "title": "Viterbi Decoding of Directed Acyclic Transformer for Non-Autoregressive Machine Translation",
    "content": "Viterbi Decoding of Directed Acyclic Transformer for Non-Autoregressive Machine Translation. Non-autoregressive models achieve significant decoding speedup in neural machine translation but lack the ability to capture sequential dependency. Directed Acyclic Transformer (DA-Transformer) was recently proposed to model sequential dependency with a directed acyclic graph. Consequently, it has to apply a sequential decision process at inference time, which harms the global translation accuracy. In this paper, we present a Viterbi decoding framework for DA-Transformer, which guarantees to find the joint optimal solution for the translation and decoding path under any length constraint. Experimental results demonstrate that our approach consistently improves the performance of DA-Transformer while maintaining a similar decoding speedup.",
    "authors": [
      "Chenze Shao",
      "Zhengrui Ma",
      "Yang Feng"
    ],
    "published": "2022-10-11T06:53:34+00:00",
    "url": "http://arxiv.org/pdf/2210.05193v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2303.12914v1",
    "title": "TRON: Transformer Neural Network Acceleration with Non-Coherent Silicon Photonics",
    "content": "TRON: Transformer Neural Network Acceleration with Non-Coherent Silicon Photonics. Transformer neural networks are rapidly being integrated into state-of-the-art solutions for natural language processing (NLP) and computer vision. However, the complex structure of these models creates challenges for accelerating their execution on conventional electronic platforms. We propose the first silicon photonic hardware neural network accelerator called TRON for transformer-based models such as BERT, and Vision Transformers. Our analysis demonstrates that TRON exhibits at least 14x better throughput and 8x better energy efficiency, in comparison to state-of-the-art transformer accelerators.",
    "authors": [
      "Salma Afifi",
      "Febin Sunny",
      "Mahdi Nikdast",
      "Sudeep Pasricha"
    ],
    "published": "2023-03-22T21:09:49+00:00",
    "url": "http://arxiv.org/pdf/2303.12914v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2305.01280v1",
    "title": "AxWin Transformer: A Context-Aware Vision Transformer Backbone with Axial Windows",
    "content": "AxWin Transformer: A Context-Aware Vision Transformer Backbone with Axial Windows. Recently Transformer has shown good performance in several vision tasks due to its powerful modeling capabilities. To reduce the quadratic complexity caused by the attention, some outstanding work restricts attention to local regions or extends axial interactions. However, these methos often lack the interaction of local and global information, balancing coarse and fine-grained information. To address this problem, we propose AxWin Attention, which models context information in both local windows and axial views. Based on the AxWin Attention, we develop a context-aware vision transformer backbone, named AxWin Transformer, which outperforming the state-of-the-art methods in both classification and downstream segmentation and detection tasks.",
    "authors": [
      "Fangjian Lin",
      "Yizhe Ma",
      "Sitong Wu",
      "Long Yu",
      "Shengwei Tian"
    ],
    "published": "2023-05-02T09:33:11+00:00",
    "url": "http://arxiv.org/pdf/2305.01280v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2310.02066v3",
    "title": "De Novo Drug Design with Joint Transformers",
    "content": "De Novo Drug Design with Joint Transformers. De novo drug design requires simultaneously generating novel molecules outside of training data and predicting their target properties, making it a hard task for generative models. To address this, we propose Joint Transformer that combines a Transformer decoder, Transformer encoder, and a predictor in a joint generative model with shared weights. We formulate a probabilistic black-box optimization algorithm that employs Joint Transformer to generate novel molecules with improved target properties and outperforms other SMILES-based optimization methods in de novo drug design.",
    "authors": [
      "Adam Izdebski",
      "Ewelina Weglarz-Tomczak",
      "Ewa Szczurek",
      "Jakub M. Tomczak"
    ],
    "published": "2023-10-03T14:09:15+00:00",
    "url": "http://arxiv.org/pdf/2310.02066v3"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2408.03855v1",
    "title": "Why transformers are obviously good models of language",
    "content": "Why transformers are obviously good models of language. Nobody knows how language works, but many theories abound. Transformers are a class of neural networks that process language automatically with more success than alternatives, both those based on neural computations and those that rely on other (e.g. more symbolic) mechanisms. Here, I highlight direct connections between the transformer architecture and certain theoretical perspectives on language. The empirical success of transformers relative to alternative models provides circumstantial evidence that the linguistic approaches that transformers embody should be, at least, evaluated with greater scrutiny by the linguistics community and, at best, considered to be the currently best available theories.",
    "authors": [
      "Felix Hill"
    ],
    "published": "2024-08-07T15:52:46+00:00",
    "url": "http://arxiv.org/pdf/2408.03855v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2110.10957v1",
    "title": "Vis-TOP: Visual Transformer Overlay Processor",
    "content": "Vis-TOP: Visual Transformer Overlay Processor. In recent years, Transformer has achieved good results in Natural Language Processing (NLP) and has also started to expand into Computer Vision (CV). Excellent models such as the Vision Transformer and Swin Transformer have emerged. At the same time, the platform for Transformer models was extended to embedded devices to meet some resource-sensitive application scenarios. However, due to the large number of parameters, the complex computational flow and the many different structural variants of Transformer models, there are a number of issues that need to be addressed in its hardware design. This is both an opportunity and a challenge. We propose Vis-TOP (Visual Transformer Overlay Processor), an overlay processor for various visual Transformer models. It differs from coarse-grained overlay processors such as CPU, GPU, NPE, and from fine-grained customized designs for a specific model. Vis-TOP summarizes the characteristics of all visual Transformer models and implements a three-layer and two-level transformation structure that allows the model to be switched or changed freely without changing the hardware architecture. At the same time, the corresponding instruction bundle and hardware architecture are designed in three-layer and two-level transformation structure. After quantization of Swin Transformer tiny model using 8-bit fixed points (fix_8), we implemented an overlay processor on the ZCU102. Compared to GPU, the TOP throughput is 1.5x higher. Compared to the existing Transformer accelerators, our throughput per DSP is between 2.2x and 11.7x higher than others. In a word, the approach in this paper meets the requirements of real-time AI in terms of both resource consumption and inference speed. Vis-TOP provides a cost-effective and power-effective solution based on reconfigurable devices for computer vision at the edge.",
    "authors": [
      "Wei Hu",
      "Dian Xu",
      "Zimeng Fan",
      "Fang Liu",
      "Yanxiang He"
    ],
    "published": "2021-10-21T08:11:12+00:00",
    "url": "http://arxiv.org/pdf/2110.10957v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2208.12259v3",
    "title": "Pix4Point: Image Pretrained Standard Transformers for 3D Point Cloud Understanding",
    "content": "Pix4Point: Image Pretrained Standard Transformers for 3D Point Cloud Understanding. While Transformers have achieved impressive success in natural language processing and computer vision, their performance on 3D point clouds is relatively poor. This is mainly due to the limitation of Transformers: a demanding need for extensive training data. Unfortunately, in the realm of 3D point clouds, the availability of large datasets is a challenge, exacerbating the issue of training Transformers for 3D tasks. In this work, we solve the data issue of point cloud Transformers from two perspectives: (i) introducing more inductive bias to reduce the dependency of Transformers on data, and (ii) relying on cross-modality pretraining. More specifically, we first present Progressive Point Patch Embedding and present a new point cloud Transformer model namely PViT. PViT shares the same backbone as Transformer but is shown to be less hungry for data, enabling Transformer to achieve performance comparable to the state-of-the-art. Second, we formulate a simple yet effective pipeline dubbed \"Pix4Point\" that allows harnessing Transformers pretrained in the image domain to enhance downstream point cloud understanding. This is achieved through a modality-agnostic Transformer backbone with the help of a tokenizer and decoder specialized in the different domains. Pretrained on a large number of widely available images, significant gains of PViT are observed in the tasks of 3D point cloud classification, part segmentation, and semantic segmentation on ScanObjectNN, ShapeNetPart, and S3DIS, respectively. Our code and models are available at https://github.com/guochengqian/Pix4Point .",
    "authors": [
      "Guocheng Qian",
      "Abdullah Hamdi",
      "Xingdi Zhang",
      "Bernard Ghanem"
    ],
    "published": "2022-08-25T17:59:29+00:00",
    "url": "http://arxiv.org/pdf/2208.12259v3"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2209.08167v2",
    "title": "Quantum Vision Transformers",
    "content": "Quantum Vision Transformers. In this work, quantum transformers are designed and analysed in detail by extending the state-of-the-art classical transformer neural network architectures known to be very performant in natural language processing and image analysis. Building upon the previous work, which uses parametrised quantum circuits for data loading and orthogonal neural layers, we introduce three types of quantum transformers for training and inference, including a quantum transformer based on compound matrices, which guarantees a theoretical advantage of the quantum attention mechanism compared to their classical counterpart both in terms of asymptotic run time and the number of model parameters. These quantum architectures can be built using shallow quantum circuits and produce qualitatively different classification models. The three proposed quantum attention layers vary on the spectrum between closely following the classical transformers and exhibiting more quantum characteristics. As building blocks of the quantum transformer, we propose a novel method for loading a matrix as quantum states as well as two new trainable quantum orthogonal layers adaptable to different levels of connectivity and quality of quantum computers. We performed extensive simulations of the quantum transformers on standard medical image datasets that showed competitively, and at times better performance compared to the classical benchmarks, including the best-in-class classical vision transformers. The quantum transformers we trained on these small-scale datasets require fewer parameters compared to standard classical benchmarks. Finally, we implemented our quantum transformers on superconducting quantum computers and obtained encouraging results for up to six qubit experiments.",
    "authors": [
      "El Amine Cherrat",
      "Iordanis Kerenidis",
      "Natansh Mathur",
      "Jonas Landman",
      "Martin Strahm",
      "Yun Yvonna Li"
    ],
    "published": "2022-09-16T20:51:23+00:00",
    "url": "http://arxiv.org/pdf/2209.08167v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2211.06755v3",
    "title": "The chiPower transformation: a valid alternative to logratio transformations in compositional data analysis",
    "content": "The chiPower transformation: a valid alternative to logratio transformations in compositional data analysis. The approach to analysing compositional data has been dominated by the use of logratio transformations, to ensure exact subcompositional coherence and, in some situations, exact isometry as well. A problem with this approach is that data zeros, found in most applications, have to be replaced to allow the logarithmic transformation. An alternative new approach, called the `chiPower' transformation, which allows data zeros, is to combine the standardization inherent in the chi-square distance in correspondence analysis, with the essential elements of the Box-Cox power transformation. The chiPower transformation is justified because it} defines between-sample distances that tend to logratio distances for strictly positive data as the power parameter tends to zero, and are then equivalent to transforming to logratios. For data with zeros, a value of the power can be identified that brings the chiPower transformation as close as possible to a logratio transformation, without having to substitute the zeros. Especially in the area of high-dimensional data, this alternative approach can present such a high level of coherence and isometry as to be a valid approach to the analysis of compositional data. Furthermore, in a supervised learning context, if the compositional variables serve as predictors of a response in a modelling framework, for example generalized linear models, then the power can be used as a tuning parameter in optimizing the accuracy of prediction through cross-validation. The chiPower-transformed variables have a straightforward interpretation, since they are each identified with single compositional parts, not ratios.",
    "authors": [
      "Michael Greenacre"
    ],
    "published": "2022-11-12T22:10:46+00:00",
    "url": "http://arxiv.org/pdf/2211.06755v3"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2405.16563v2",
    "title": "Higher-Order Transformer Derivative Estimates for Explicit Pathwise Learning Guarantees",
    "content": "Higher-Order Transformer Derivative Estimates for Explicit Pathwise Learning Guarantees. An inherent challenge in computing fully-explicit generalization bounds for transformers involves obtaining covering number estimates for the given transformer class $T$. Crude estimates rely on a uniform upper bound on the local-Lipschitz constants of transformers in $T$, and finer estimates require an analysis of their higher-order partial derivatives. Unfortunately, these precise higher-order derivative estimates for (realistic) transformer models are not currently available in the literature as they are combinatorially delicate due to the intricate compositional structure of transformer blocks.   This paper fills this gap by precisely estimating all the higher-order derivatives of all orders for the transformer model. We consider realistic transformers with multiple (non-linearized) attention heads per block and layer normalization. We obtain fully-explicit estimates of all constants in terms of the number of attention heads, the depth and width of each transformer block, and the number of normalization layers. Further, we explicitly analyze the impact of various standard activation function choices (e.g. SWISH and GeLU). As an application, we obtain explicit pathwise generalization bounds for transformers on a single trajectory of an exponentially-ergodic Markov process valid at a fixed future time horizon. We conclude that real-world transformers can learn from $N$ (non-i.i.d.) samples of a single Markov process's trajectory at a rate of ${O}(\\operatorname{polylog}(N)/\\sqrt{N})$.",
    "authors": [
      "Yannick Limmer",
      "Anastasis Kratsios",
      "Xuwei Yang",
      "Raeid Saqur",
      "Blanka Horvath"
    ],
    "published": "2024-05-26T13:19:32+00:00",
    "url": "http://arxiv.org/pdf/2405.16563v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2407.09093v2",
    "title": "On Exact Bit-level Reversible Transformers Without Changing Architectures",
    "content": "On Exact Bit-level Reversible Transformers Without Changing Architectures. Various reversible deep neural networks (DNN) models have been proposed to reduce memory consumption in the training process. However, almost all existing reversible DNNs either require special non-standard architectures or are constructed by modifying existing DNN architectures considerably to enable reversibility. In this work we present the BDIA-transformer, which is an exact bit-level reversible transformer that uses an unchanged standard architecture for inference. The basic idea is to first treat each transformer block as the Euler integration approximation for solving an ordinary differential equation (ODE) and then incorporate the technique of bidirectional integration approximation (BDIA) into the neural architecture, together with activation quantization to make it exactly bit-level reversible. In the training process, we let a hyper-parameter $\\gamma$ in BDIA-transformer randomly take one of the two values $\\{0.5, -0.5\\}$ per training sample per transformer block for averaging every two consecutive integration approximations. As a result, BDIA-transformer can be viewed as training an ensemble of ODE solvers parameterized by a set of binary random variables, which regularizes the model and results in improved validation accuracy. Lightweight side information per transformer block is required to be stored in the forward process to account for binary quantization loss to enable exact bit-level reversibility. In the inference procedure, the expectation $\\mathbb{E}(\\gamma)=0$ is taken to make the resulting architectures of BDIA-transformer identical to transformers up to activation quantization. Our experiments in both image classification and language translation show that BDIA-transformers outperform their conventional counterparts significantly in terms of validation performance while also requiring considerably less training memory.",
    "authors": [
      "Guoqiang Zhang",
      "J. P. Lewis",
      "W. B. Kleijn"
    ],
    "published": "2024-07-12T08:42:58+00:00",
    "url": "http://arxiv.org/pdf/2407.09093v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1102.2200v3",
    "title": "Direct algebraic mapping transformation for decorated spin models",
    "content": "Direct algebraic mapping transformation for decorated spin models. In this article we propose a general transformation for decorated spin models. The advantage of this transformation is to perform a direct mapping of a decorated spin model onto another effective spin thus simplifying algebraic computations by avoiding the proliferation of unnecessary iterative transformations and parameters that might otherwise lead to transcendental equations. Direct mapping transformation is discussed in detail for decorated Ising spin models as well as for decorated Ising-Heisenberg spin models, with arbitrary coordination number and with some constrained Hamiltonian's parameter for systems with coordination number larger than 4 (3) with (without) spin inversion symmetry respectively. In order to illustrate this transformation we give several examples of this mapping transformation, where most of them were not explored yet.",
    "authors": [
      "Onofre Rojas",
      "S. M. de Souza"
    ],
    "published": "2011-02-10T18:50:47+00:00",
    "url": "http://arxiv.org/pdf/1102.2200v3"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1608.05368v1",
    "title": "Scaling Bounded Model Checking By Transforming Programs With Arrays",
    "content": "Scaling Bounded Model Checking By Transforming Programs With Arrays. Bounded Model Checking is one the most successful techniques for finding bugs in program. However, for programs with loops iterating over large-sized arrays, bounded model checkers often exceed the limit of resources available to them. We present a transformation that enables bounded model checkers to verify a certain class of array properties. Our technique transforms an array-manipulating program in ANSI-C to an array-free and loop-free program. The transformed program can efficiently be verified by an off-the-shelf bounded model checker. Though the transformed program is, in general, an abstraction of the original program, we formally characterize the properties for which the transformation is precise. We demonstrate the applicability and usefulness of our technique on both industry code as well as academic benchmarks.",
    "authors": [
      "Anushri Jana",
      "Uday P. Khedker",
      "Advaita Datar",
      "R Venkatesh",
      "C Niyas"
    ],
    "published": "2016-08-17T17:35:49+00:00",
    "url": "http://arxiv.org/pdf/1608.05368v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2111.09641v2",
    "title": "Evaluating Transformers for Lightweight Action Recognition",
    "content": "Evaluating Transformers for Lightweight Action Recognition. In video action recognition, transformers consistently reach state-of-the-art accuracy. However, many models are too heavyweight for the average researcher with limited hardware resources. In this work, we explore the limitations of video transformers for lightweight action recognition. We benchmark 13 video transformers and baselines across 3 large-scale datasets and 10 hardware devices. Our study is the first to evaluate the efficiency of action recognition models in depth across multiple devices and train a wide range of video transformers under the same conditions. We categorize current methods into three classes and show that composite transformers that augment convolutional backbones are best at lightweight action recognition, despite lacking accuracy. Meanwhile, attention-only models need more motion modeling capabilities and stand-alone attention block models currently incur too much latency overhead. Our experiments conclude that current video transformers are not yet capable of lightweight action recognition on par with traditional convolutional baselines, and that the previously mentioned shortcomings need to be addressed to bridge this gap. Code to reproduce our experiments will be made publicly available.",
    "authors": [
      "Raivo Koot",
      "Markus Hennerbichler",
      "Haiping Lu"
    ],
    "published": "2021-11-18T11:45:42+00:00",
    "url": "http://arxiv.org/pdf/2111.09641v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2003.12738v1",
    "title": "Variational Transformers for Diverse Response Generation",
    "content": "Variational Transformers for Diverse Response Generation. Despite the great promise of Transformers in many sequence modeling tasks (e.g., machine translation), their deterministic nature hinders them from generalizing to high entropy tasks such as dialogue response generation. Previous work proposes to capture the variability of dialogue responses with a recurrent neural network (RNN)-based conditional variational autoencoder (CVAE). However, the autoregressive computation of the RNN limits the training efficiency. Therefore, we propose the Variational Transformer (VT), a variational self-attentive feed-forward sequence model. The VT combines the parallelizability and global receptive field of the Transformer with the variational nature of the CVAE by incorporating stochastic latent variables into Transformers. We explore two types of the VT: 1) modeling the discourse-level diversity with a global latent variable; and 2) augmenting the Transformer decoder with a sequence of fine-grained latent variables. Then, the proposed models are evaluated on three conversational datasets with both automatic metric and human evaluation. The experimental results show that our models improve standard Transformers and other baselines in terms of diversity, semantic relevance, and human judgment.",
    "authors": [
      "Zhaojiang Lin",
      "Genta Indra Winata",
      "Peng Xu",
      "Zihan Liu",
      "Pascale Fung"
    ],
    "published": "2020-03-28T07:48:02+00:00",
    "url": "http://arxiv.org/pdf/2003.12738v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2106.00479v1",
    "title": "DoT: An efficient Double Transformer for NLP tasks with tables",
    "content": "DoT: An efficient Double Transformer for NLP tasks with tables. Transformer-based approaches have been successfully used to obtain state-of-the-art accuracy on natural language processing (NLP) tasks with semi-structured tables. These model architectures are typically deep, resulting in slow training and inference, especially for long inputs. To improve efficiency while maintaining a high accuracy, we propose a new architecture, DoT, a double transformer model, that decomposes the problem into two sub-tasks: A shallow pruning transformer that selects the top-K tokens, followed by a deep task-specific transformer that takes as input those K tokens. Additionally, we modify the task-specific attention to incorporate the pruning scores. The two transformers are jointly trained by optimizing the task-specific loss. We run experiments on three benchmarks, including entailment and question-answering. We show that for a small drop of accuracy, DoT improves training and inference time by at least 50%. We also show that the pruning transformer effectively selects relevant tokens enabling the end-to-end model to maintain similar accuracy as slower baseline models. Finally, we analyse the pruning and give some insight into its impact on the task model.",
    "authors": [
      "Syrine Krichene",
      "Thomas Müller",
      "Julian Martin Eisenschlos"
    ],
    "published": "2021-06-01T13:33:53+00:00",
    "url": "http://arxiv.org/pdf/2106.00479v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2205.13891v2",
    "title": "Transformers from an Optimization Perspective",
    "content": "Transformers from an Optimization Perspective. Deep learning models such as the Transformer are often constructed by heuristics and experience. To provide a complementary foundation, in this work we study the following problem: Is it possible to find an energy function underlying the Transformer model, such that descent steps along this energy correspond with the Transformer forward pass? By finding such a function, we can view Transformers as the unfolding of an interpretable optimization process across iterations. This unfolding perspective has been frequently adopted in the past to elucidate more straightforward deep models such as MLPs and CNNs; however, it has thus far remained elusive obtaining a similar equivalence for more complex models with self-attention mechanisms like the Transformer. To this end, we first outline several major obstacles before providing companion techniques to at least partially address them, demonstrating for the first time a close association between energy function minimization and deep layers with self-attention. This interpretation contributes to our intuition and understanding of Transformers, while potentially laying the ground-work for new model designs.",
    "authors": [
      "Yongyi Yang",
      "Zengfeng Huang",
      "David Wipf"
    ],
    "published": "2022-05-27T10:45:15+00:00",
    "url": "http://arxiv.org/pdf/2205.13891v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2306.09305v2",
    "title": "Fast Training of Diffusion Models with Masked Transformers",
    "content": "Fast Training of Diffusion Models with Masked Transformers. We propose an efficient approach to train large diffusion models with masked transformers. While masked transformers have been extensively explored for representation learning, their application to generative learning is less explored in the vision domain. Our work is the first to exploit masked training to reduce the training cost of diffusion models significantly. Specifically, we randomly mask out a high proportion (e.g., 50%) of patches in diffused input images during training. For masked training, we introduce an asymmetric encoder-decoder architecture consisting of a transformer encoder that operates only on unmasked patches and a lightweight transformer decoder on full patches. To promote a long-range understanding of full patches, we add an auxiliary task of reconstructing masked patches to the denoising score matching objective that learns the score of unmasked patches. Experiments on ImageNet-256x256 and ImageNet-512x512 show that our approach achieves competitive and even better generative performance than the state-of-the-art Diffusion Transformer (DiT) model, using only around 30% of its original training time. Thus, our method shows a promising way of efficiently training large transformer-based diffusion models without sacrificing the generative performance.",
    "authors": [
      "Hongkai Zheng",
      "Weili Nie",
      "Arash Vahdat",
      "Anima Anandkumar"
    ],
    "published": "2023-06-15T17:38:48+00:00",
    "url": "http://arxiv.org/pdf/2306.09305v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2407.06404v2",
    "title": "Design-Specific Transformations in Visualization",
    "content": "Design-Specific Transformations in Visualization. In visualization, the process of transforming raw data into visually comprehensible representations is pivotal. While existing models like the Information Visualization Reference Model describe the data-to-visual mapping process, they often overlook a crucial intermediary step: design-specific transformations. This process, occurring after data transformation but before visual-data mapping, further derives data, such as groupings, layout, and statistics, that are essential to properly render the visualization. In this paper, we advocate for a deeper exploration of design-specific transformations, highlighting their importance in understanding visualization properties, particularly in relation to user tasks. We incorporate design-specific transformations into the Information Visualization Reference Model and propose a new formalism that encompasses the user task as a function over data. The resulting formalism offers three key benefits over existing visualization models: (1) describing task as compositions of functions, (2) enabling analysis of data transformations for visual-data mapping, and (3) empowering reasoning about visualization correctness and effectiveness. We further discuss the potential implications of this model on visualization theory and visualization experiment design.",
    "authors": [
      "Eugene Wu",
      "Remco Chang"
    ],
    "published": "2024-07-08T21:22:58+00:00",
    "url": "http://arxiv.org/pdf/2407.06404v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2412.13419v1",
    "title": "Exploring Transformer-Augmented LSTM for Temporal and Spatial Feature Learning in Trajectory Prediction",
    "content": "Exploring Transformer-Augmented LSTM for Temporal and Spatial Feature Learning in Trajectory Prediction. Accurate vehicle trajectory prediction is crucial for ensuring safe and efficient autonomous driving. This work explores the integration of Transformer based model with Long Short-Term Memory (LSTM) based technique to enhance spatial and temporal feature learning in vehicle trajectory prediction. Here, a hybrid model that combines LSTMs for temporal encoding with a Transformer encoder for capturing complex interactions between vehicles is proposed. Spatial trajectory features of the neighboring vehicles are processed and goes through a masked scatter mechanism in a grid based environment, which is then combined with temporal trajectory of the vehicles. This combined trajectory data are learned by sequential LSTM encoding and Transformer based attention layers. The proposed model is benchmarked against predecessor LSTM based methods, including STA-LSTM, SA-LSTM, CS-LSTM, and NaiveLSTM. Our results, while not outperforming it's predecessor, demonstrate the potential of integrating Transformers with LSTM based technique to build interpretable trajectory prediction model. Future work will explore alternative architectures using Transformer applications to further enhance performance. This study provides a promising direction for improving trajectory prediction models by leveraging transformer based architectures, paving the way for more robust and interpretable vehicle trajectory prediction system.",
    "authors": [
      "Chandra Raskoti",
      "Weizi Li"
    ],
    "published": "2024-12-18T01:31:08+00:00",
    "url": "http://arxiv.org/pdf/2412.13419v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2501.18401v2",
    "title": "MatIR: A Hybrid Mamba-Transformer Image Restoration Model",
    "content": "MatIR: A Hybrid Mamba-Transformer Image Restoration Model. In recent years, Transformers-based models have made significant progress in the field of image restoration by leveraging their inherent ability to capture complex contextual features. Recently, Mamba models have made a splash in the field of computer vision due to their ability to handle long-range dependencies and their significant computational efficiency compared to Transformers. However, Mamba currently lags behind Transformers in contextual learning capabilities. To overcome the limitations of these two models, we propose a Mamba-Transformer hybrid image restoration model called MatIR. Specifically, MatIR cross-cycles the blocks of the Transformer layer and the Mamba layer to extract features, thereby taking full advantage of the advantages of the two architectures. In the Mamba module, we introduce the Image Inpainting State Space (IRSS) module, which traverses along four scan paths to achieve efficient processing of long sequence data. In the Transformer module, we combine triangular window-based local attention with channel-based global attention to effectively activate the attention mechanism over a wider range of image pixels. Extensive experimental results and ablation studies demonstrate the effectiveness of our approach.",
    "authors": [
      "Juan Wen",
      "Weiyan Hou",
      "Luc Van Gool",
      "Radu Timofte"
    ],
    "published": "2025-01-30T14:55:40+00:00",
    "url": "http://arxiv.org/pdf/2501.18401v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2107.08957v2",
    "title": "Clinical Relation Extraction Using Transformer-based Models",
    "content": "Clinical Relation Extraction Using Transformer-based Models. The newly emerged transformer technology has a tremendous impact on NLP research. In the general English domain, transformer-based models have achieved state-of-the-art performances on various NLP benchmarks. In the clinical domain, researchers also have investigated transformer models for clinical applications. The goal of this study is to systematically explore three widely used transformer-based models (i.e., BERT, RoBERTa, and XLNet) for clinical relation extraction and develop an open-source package with clinical pre-trained transformer-based models to facilitate information extraction in the clinical domain. We developed a series of clinical RE models based on three transformer architectures, namely BERT, RoBERTa, and XLNet. We evaluated these models using 2 publicly available datasets from 2018 MADE1.0 and 2018 n2c2 challenges. We compared two classification strategies (binary vs. multi-class classification) and investigated two approaches to generate candidate relations in different experimental settings. In this study, we compared three transformer-based (BERT, RoBERTa, and XLNet) models for relation extraction. We demonstrated that the RoBERTa-clinical RE model achieved the best performance on the 2018 MADE1.0 dataset with an F1-score of 0.8958. On the 2018 n2c2 dataset, the XLNet-clinical model achieved the best F1-score of 0.9610. Our results indicated that the binary classification strategy consistently outperformed the multi-class classification strategy for clinical relation extraction. Our methods and models are publicly available at https://github.com/uf-hobi-informatics-lab/ClinicalTransformerRelationExtraction. We believe this work will improve current practice on clinical relation extraction and other related NLP tasks in the biomedical domain.",
    "authors": [
      "Xi Yang",
      "Zehao Yu",
      "Yi Guo",
      "Jiang Bian",
      "Yonghui Wu"
    ],
    "published": "2021-07-19T15:15:51+00:00",
    "url": "http://arxiv.org/pdf/2107.08957v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2307.01189v2",
    "title": "Trainable Transformer in Transformer",
    "content": "Trainable Transformer in Transformer. Recent works attribute the capability of in-context learning (ICL) in large pre-trained language models to implicitly simulating and fine-tuning an internal model (e.g., linear or 2-layer MLP) during inference. However, such constructions require large memory overhead, which makes simulation of more sophisticated internal models intractable. In this work, we propose an efficient construction, Transformer in Transformer (in short, TinT), that allows a transformer to simulate and fine-tune complex models internally during inference (e.g., pre-trained language models). In particular, we introduce innovative approximation techniques that allow a TinT model with less than 2 billion parameters to simulate and fine-tune a 125 million parameter transformer model within a single forward pass. TinT accommodates many common transformer variants and its design ideas also improve the efficiency of past instantiations of simple models inside transformers. We conduct end-to-end experiments to validate the internal fine-tuning procedure of TinT on various language modeling and downstream tasks. For example, even with a limited one-step budget, we observe TinT for a OPT-125M model improves performance by 4-16% absolute on average compared to OPT-125M. These findings suggest that large pre-trained language models are capable of performing intricate subroutines. To facilitate further work, a modular and extensible codebase for TinT is included.",
    "authors": [
      "Abhishek Panigrahi",
      "Sadhika Malladi",
      "Mengzhou Xia",
      "Sanjeev Arora"
    ],
    "published": "2023-07-03T17:53:39+00:00",
    "url": "http://arxiv.org/pdf/2307.01189v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2405.06443v2",
    "title": "Residual-based Attention Physics-informed Neural Networks for Spatio-Temporal Ageing Assessment of Transformers Operated in Renewable Power Plants",
    "content": "Residual-based Attention Physics-informed Neural Networks for Spatio-Temporal Ageing Assessment of Transformers Operated in Renewable Power Plants. Transformers are crucial for reliable and efficient power system operations, particularly in supporting the integration of renewable energy. Effective monitoring of transformer health is critical to maintain grid stability and performance. Thermal insulation ageing is a key transformer failure mode, which is generally tracked by monitoring the hotspot temperature (HST). However, HST measurement is complex, costly, and often estimated from indirect measurements. Existing HST models focus on space-agnostic thermal models, providing worst-case HST estimates. This article introduces a spatio-temporal model for transformer winding temperature and ageing estimation, which leverages physics-based partial differential equations (PDEs) with data-driven Neural Networks (NN) in a Physics Informed Neural Networks (PINNs) configuration to improve prediction accuracy and acquire spatio-temporal resolution. The computational accuracy of the PINN model is improved through the implementation of the Residual-Based Attention (PINN-RBA) scheme that accelerates the PINN model convergence. The PINN-RBA model is benchmarked against self-adaptive attention schemes and classical vanilla PINN configurations. For the first time, PINN based oil temperature predictions are used to estimate spatio-temporal transformer winding temperature values, validated through PDE numerical solution and fiber optic sensor measurements. Furthermore, the spatio-temporal transformer ageing model is inferred, which supports transformer health management decision-making. Results are validated with a distribution transformer operating on a floating photovoltaic power plant.",
    "authors": [
      "Ibai Ramirez",
      "Joel Pino",
      "David Pardo",
      "Mikel Sanz",
      "Luis del Rio",
      "Alvaro Ortiz",
      "Kateryna Morozovska",
      "Jose I. Aizpurua"
    ],
    "published": "2024-05-10T12:48:57+00:00",
    "url": "http://arxiv.org/pdf/2405.06443v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/hep-th/9210121v1",
    "title": "Marginal Deformations of WZNW and Coset Models from O(d,d) Transformation",
    "content": "Marginal Deformations of WZNW and Coset Models from O(d,d) Transformation. We show that O(2,2) transformation of SU(2) WZNW model gives rise to marginal deformation of this model by the operator $\\int d^2 z J(z)\\bar J(\\bar z)$ where $J$, $\\bar J$ are U(1) currents in the Cartan subalgebra. Generalization of this result to other WZNW theories is discussed. We also consider O(3,3) transformation of the product of an SU(2) WZNW model and a gauged SU(2) WZNW model. The three parameter set of models obtained after the transformation is shown to be the result of first deforming the product of two SU(2) WZNW theories by marginal operators of the form $\\sum_{i,j=1}^2 C_{ij} J_i \\bar J_j$, and then gauging an appropriate U(1) subgroup of the theory. Our analysis leads to a general conjecture that O(d,d) transformation of any WZNW model corresponds to marginal deformation of the WZNW theory by combination of appropriate left and right moving currents belonging to the Cartan subalgebra; and O(d,d) transformation of a gauged WZNW model can be identified to the gauged version of such marginally deformed WZNW models.",
    "authors": [
      "S. F. Hassan",
      "Ashoke Sen"
    ],
    "published": "1992-10-22T14:29:00+00:00",
    "url": "http://arxiv.org/pdf/hep-th/9210121v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2305.01883v2",
    "title": "A Lightweight CNN-Transformer Model for Learning Traveling Salesman Problems",
    "content": "A Lightweight CNN-Transformer Model for Learning Traveling Salesman Problems. Several studies have attempted to solve traveling salesman problems (TSPs) using various deep learning techniques. Among them, Transformer-based models show state-of-the-art performance even for large-scale Traveling Salesman Problems (TSPs). However, they are based on fully-connected attention models and suffer from large computational complexity and GPU memory usage. Our work is the first CNN-Transformer model based on a CNN embedding layer and partial self-attention for TSP. Our CNN-Transformer model is able to better learn spatial features from input data using a CNN embedding layer compared with the standard Transformer-based models. It also removes considerable redundancy in fully-connected attention models using the proposed partial self-attention. Experimental results show that the proposed CNN embedding layer and partial self-attention are very effective in improving performance and computational complexity. The proposed model exhibits the best performance in real-world datasets and outperforms other existing state-of-the-art (SOTA) Transformer-based models in various aspects. Our code is publicly available at https://github.com/cm8908/CNN_Transformer3.",
    "authors": [
      "Minseop Jung",
      "Jaeseung Lee",
      "Jibum Kim"
    ],
    "published": "2023-05-03T04:28:10+00:00",
    "url": "http://arxiv.org/pdf/2305.01883v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2402.05964v2",
    "title": "A Survey on Transformer Compression",
    "content": "A Survey on Transformer Compression. Transformer plays a vital role in the realms of natural language processing (NLP) and computer vision (CV), specially for constructing large language models (LLM) and large vision models (LVM). Model compression methods reduce the memory and computational cost of Transformer, which is a necessary step to implement large language/vision models on practical devices. Given the unique architecture of Transformer, featuring alternative attention and feedforward neural network (FFN) modules, specific compression techniques are usually required. The efficiency of these compression methods is also paramount, as retraining large models on the entire training dataset is usually impractical. This survey provides a comprehensive review of recent compression methods, with a specific focus on their application to Transformer-based models. The compression methods are primarily categorized into pruning, quantization, knowledge distillation, and efficient architecture design (Mamba, RetNet, RWKV, etc.). In each category, we discuss compression methods for both language and vision tasks, highlighting common underlying principles. Finally, we delve into the relation between various compression methods, and discuss further directions in this domain.",
    "authors": [
      "Yehui Tang",
      "Yunhe Wang",
      "Jianyuan Guo",
      "Zhijun Tu",
      "Kai Han",
      "Hailin Hu",
      "Dacheng Tao"
    ],
    "published": "2024-02-05T12:16:28+00:00",
    "url": "http://arxiv.org/pdf/2402.05964v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2411.09953v2",
    "title": "Brain-inspired Action Generation with Spiking Transformer Diffusion Policy Model",
    "content": "Brain-inspired Action Generation with Spiking Transformer Diffusion Policy Model. Spiking Neural Networks (SNNs) has the ability to extract spatio-temporal features due to their spiking sequence. While previous research has primarily foucus on the classification of image and reinforcement learning. In our paper, we put forward novel diffusion policy model based on Spiking Transformer Neural Networks and Denoising Diffusion Probabilistic Model (DDPM): Spiking Transformer Modulate Diffusion Policy Model (STMDP), a new brain-inspired model for generating robot action trajectories. In order to improve the performance of this model, we develop a novel decoder module: Spiking Modulate De coder (SMD), which replaces the traditional Decoder module within the Transformer architecture. Additionally, we explored the substitution of DDPM with Denoising Diffusion Implicit Models (DDIM) in our frame work. We conducted experiments across four robotic manipulation tasks and performed ablation studies on the modulate block. Our model consistently outperforms existing Transformer-based diffusion policy method. Especially in Can task, we achieved an improvement of 8%. The proposed STMDP method integrates SNNs, dffusion model and Transformer architecture, which offers new perspectives and promising directions for exploration in brain-inspired robotics.",
    "authors": [
      "Qianhao Wang",
      "Yinqian Sun",
      "Enmeng Lu",
      "Qian Zhang",
      "Yi Zeng"
    ],
    "published": "2024-11-15T05:11:28+00:00",
    "url": "http://arxiv.org/pdf/2411.09953v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2503.04416v1",
    "title": "Learning Transformer-based World Models with Contrastive Predictive Coding",
    "content": "Learning Transformer-based World Models with Contrastive Predictive Coding. The DreamerV3 algorithm recently obtained remarkable performance across diverse environment domains by learning an accurate world model based on Recurrent Neural Networks (RNNs). Following the success of model-based reinforcement learning algorithms and the rapid adoption of the Transformer architecture for its superior training efficiency and favorable scaling properties, recent works such as STORM have proposed replacing RNN-based world models with Transformer-based world models using masked self-attention. However, despite the improved training efficiency of these methods, their impact on performance remains limited compared to the Dreamer algorithm, struggling to learn competitive Transformer-based world models. In this work, we show that the next state prediction objective adopted in previous approaches is insufficient to fully exploit the representation capabilities of Transformers. We propose to extend world model predictions to longer time horizons by introducing TWISTER (Transformer-based World model wIth contraSTivE Representations), a world model using action-conditioned Contrastive Predictive Coding to learn high-level temporal feature representations and improve the agent performance. TWISTER achieves a human-normalized mean score of 162% on the Atari 100k benchmark, setting a new record among state-of-the-art methods that do not employ look-ahead search.",
    "authors": [
      "Maxime Burchi",
      "Radu Timofte"
    ],
    "published": "2025-03-06T13:18:37+00:00",
    "url": "http://arxiv.org/pdf/2503.04416v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2103.08124v1",
    "title": "The Watanabe-Strogatz transform and constant of motion functionals for kinetic vector models",
    "content": "The Watanabe-Strogatz transform and constant of motion functionals for kinetic vector models. We present a kinetic version of the Watanabe-Strogatz(WS) transform for vector models in this paper. From the generalized WS-transform, we obtain the cross-ratio type constant of motion functionals for kinetic vector models under suitable conditions. We present the sufficient and necessary conditions for the existence of the suggested constant of motion functional. As an application of the constant of motion functional, we provide the instability of bipolar states of the kinetic swarm sphere model. We also provide the WS-transform and constant of motion functional for non-identical kinetic vector models.",
    "authors": [
      "Hansol Park"
    ],
    "published": "2021-03-15T03:56:26+00:00",
    "url": "http://arxiv.org/pdf/2103.08124v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2001.00501v3",
    "title": "EEG based Continuous Speech Recognition using Transformers",
    "content": "EEG based Continuous Speech Recognition using Transformers. In this paper we investigate continuous speech recognition using electroencephalography (EEG) features using recently introduced end-to-end transformer based automatic speech recognition (ASR) model. Our results demonstrate that transformer based model demonstrate faster training compared to recurrent neural network (RNN) based sequence-to-sequence EEG models and better performance during inference time for smaller test set vocabulary but as we increase the vocabulary size, the performance of the RNN based models were better than transformer based model on a limited English vocabulary.",
    "authors": [
      "Gautam Krishna",
      "Co Tran",
      "Mason Carnahan",
      "Ahmed H Tewfik"
    ],
    "published": "2019-12-31T08:36:59+00:00",
    "url": "http://arxiv.org/pdf/2001.00501v3"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2011.03803v1",
    "title": "Rethinking the Value of Transformer Components",
    "content": "Rethinking the Value of Transformer Components. Transformer becomes the state-of-the-art translation model, while it is not well studied how each intermediate component contributes to the model performance, which poses significant challenges for designing optimal architectures. In this work, we bridge this gap by evaluating the impact of individual component (sub-layer) in trained Transformer models from different perspectives. Experimental results across language pairs, training strategies, and model capacities show that certain components are consistently more important than the others. We also report a number of interesting findings that might help humans better analyze, understand and improve Transformer models. Based on these observations, we further propose a new training strategy that can improves translation performance by distinguishing the unimportant components in training.",
    "authors": [
      "Wenxuan Wang",
      "Zhaopeng Tu"
    ],
    "published": "2020-11-07T16:31:45+00:00",
    "url": "http://arxiv.org/pdf/2011.03803v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2106.08367v1",
    "title": "What Context Features Can Transformer Language Models Use?",
    "content": "What Context Features Can Transformer Language Models Use?. Transformer-based language models benefit from conditioning on contexts of hundreds to thousands of previous tokens. What aspects of these contexts contribute to accurate model prediction? We describe a series of experiments that measure usable information by selectively ablating lexical and structural information in transformer language models trained on English Wikipedia. In both mid- and long-range contexts, we find that several extremely destructive context manipulations -- including shuffling word order within sentences and deleting all words other than nouns -- remove less than 15% of the usable information. Our results suggest that long contexts, but not their detailed syntactic and propositional content, are important for the low perplexity of current transformer language models.",
    "authors": [
      "Joe O'Connor",
      "Jacob Andreas"
    ],
    "published": "2021-06-15T18:38:57+00:00",
    "url": "http://arxiv.org/pdf/2106.08367v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2405.21060v1",
    "title": "Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality",
    "content": "Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality. While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.",
    "authors": [
      "Tri Dao",
      "Albert Gu"
    ],
    "published": "2024-05-31T17:50:01+00:00",
    "url": "http://arxiv.org/pdf/2405.21060v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2012.09958v1",
    "title": "Toward Transformer-Based Object Detection",
    "content": "Toward Transformer-Based Object Detection. Transformers have become the dominant model in natural language processing, owing to their ability to pretrain on massive amounts of data, then transfer to smaller, more specific tasks via fine-tuning. The Vision Transformer was the first major attempt to apply a pure transformer model directly to images as input, demonstrating that as compared to convolutional networks, transformer-based architectures can achieve competitive results on benchmark classification tasks. However, the computational complexity of the attention operator means that we are limited to low-resolution inputs. For more complex tasks such as detection or segmentation, maintaining a high input resolution is crucial to ensure that models can properly identify and reflect fine details in their output. This naturally raises the question of whether or not transformer-based architectures such as the Vision Transformer are capable of performing tasks other than classification. In this paper, we determine that Vision Transformers can be used as a backbone by a common detection task head to produce competitive COCO results. The model that we propose, ViT-FRCNN, demonstrates several known properties associated with transformers, including large pretraining capacity and fast fine-tuning performance. We also investigate improvements over a standard detection backbone, including superior performance on out-of-domain images, better performance on large objects, and a lessened reliance on non-maximum suppression. We view ViT-FRCNN as an important stepping stone toward a pure-transformer solution of complex vision tasks such as object detection.",
    "authors": [
      "Josh Beal",
      "Eric Kim",
      "Eric Tzeng",
      "Dong Huk Park",
      "Andrew Zhai",
      "Dmitry Kislyuk"
    ],
    "published": "2020-12-17T22:33:14+00:00",
    "url": "http://arxiv.org/pdf/2012.09958v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2005.04162v2",
    "title": "Graph Consistency as a Graduated Property: Consistency-Sustaining and -Improving Graph Transformations",
    "content": "Graph Consistency as a Graduated Property: Consistency-Sustaining and -Improving Graph Transformations. Where graphs are used for modelling and specifying systems, consistency is an important concern. To be a valid model of a system, the graph structure must satisfy a number of constraints. To date, consistency has primarily been viewed as a binary property: a graph either is or is not consistent with respect to a set of graph constraints. This has enabled the definition of notions such as constraint-preserving and constraint-guaranteeing graph transformations. Many practical applications - for example model repair or evolutionary search - implicitly assume a more graduated notion of consistency, but without an explicit formalisation only limited analysis of these applications is possible. In this paper, we introduce an explicit notion of consistency as a graduated property, depending on the number of constraint violations in a graph. We present two new characterisations of transformations (and transformation rules) enabling reasoning about the gradual introduction of consistency: while consistency-sustaining transformations do not decrease the consistency level, consistency-improving transformations strictly reduce the number of constraint violations. We show how these new definitions refine the existing concepts of constraint-preserving and constraint-guaranteeing transformations. To support a static analysis based on our characterisations, we present criteria for deciding which form of consistency ensuring transformations is induced by the application of a transformation rule. We illustrate our contributions in the context of an example from search-based model engineering.",
    "authors": [
      "Jens Kosiol",
      "Daniel Strüber",
      "Gabriele Taentzer",
      "Steffen Zschaler"
    ],
    "published": "2020-05-08T17:01:24+00:00",
    "url": "http://arxiv.org/pdf/2005.04162v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2211.04963v1",
    "title": "Pure Transformer with Integrated Experts for Scene Text Recognition",
    "content": "Pure Transformer with Integrated Experts for Scene Text Recognition. Scene text recognition (STR) involves the task of reading text in cropped images of natural scenes. Conventional models in STR employ convolutional neural network (CNN) followed by recurrent neural network in an encoder-decoder framework. In recent times, the transformer architecture is being widely adopted in STR as it shows strong capability in capturing long-term dependency which appears to be prominent in scene text images. Many researchers utilized transformer as part of a hybrid CNN-transformer encoder, often followed by a transformer decoder. However, such methods only make use of the long-term dependency mid-way through the encoding process. Although the vision transformer (ViT) is able to capture such dependency at an early stage, its utilization remains largely unexploited in STR. This work proposes the use of a transformer-only model as a simple baseline which outperforms hybrid CNN-transformer models. Furthermore, two key areas for improvement were identified. Firstly, the first decoded character has the lowest prediction accuracy. Secondly, images of different original aspect ratios react differently to the patch resolutions while ViT only employ one fixed patch resolution. To explore these areas, Pure Transformer with Integrated Experts (PTIE) is proposed. PTIE is a transformer model that can process multiple patch resolutions and decode in both the original and reverse character orders. It is examined on 7 commonly used benchmarks and compared with over 20 state-of-the-art methods. The experimental results show that the proposed method outperforms them and obtains state-of-the-art results in most benchmarks.",
    "authors": [
      "Yew Lee Tan",
      "Adams Wai-kin Kong",
      "Jung-Jae Kim"
    ],
    "published": "2022-11-09T15:26:59+00:00",
    "url": "http://arxiv.org/pdf/2211.04963v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1703.10895v1",
    "title": "A Module-System Discipline for Model-Driven Software Development",
    "content": "A Module-System Discipline for Model-Driven Software Development. Model-driven development is a pragmatic approach to software development that embraces domain-specific languages (DSLs), where models correspond to DSL programs. A distinguishing feature of model-driven development is that clients of a model can select from an open set of alternative semantics of the model by applying different model transformation. However, in existing model-driven frameworks, dependencies between models, model transformations, and generated code artifacts are either implicit or globally declared in build scripts, which impedes modular reasoning, separate compilation, and programmability in general. We propose the design of a new module system that incorporates models and model transformations as modules. A programmer can apply transformations in import statements, thus declaring a dependency on generated code artifacts. Our design enables modular reasoning and separate compilation by preventing hidden dependencies, and it supports mixing modeling artifacts with conventional code artifacts as well as higher-order transformations. We have formalized our design and the aforementioned properties and have validated it by an implementation and case studies that show that our module system successfully integrates model-driven development into conventional programming languages.",
    "authors": [
      "Sebastian Erdweg",
      "Klaus Ostermann"
    ],
    "published": "2017-03-31T13:54:32+00:00",
    "url": "http://arxiv.org/pdf/1703.10895v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/0803.2911v1",
    "title": "The impact of bound states on similarity renormalization group transformations",
    "content": "The impact of bound states on similarity renormalization group transformations. We study a simple class of unitary renormalization group transformations governed by a parameter f in the range [0,1]. For f=0, the transformation is one introduced by Wegner in condensed matter physics, and for f=1 it is a simpler transformation that is being used in nuclear theory. The transformation with f=0 diagonalizes the Hamiltonian but in the transformations with f near 1 divergent couplings arise as bound state thresholds emerge. To illustrate and diagnose this behavior, we numerically study Hamiltonian flows in two simple models with bound states: one with asymptotic freedom and a related one with a limit cycle. The f=0 transformation places bound-state eigenvalues on the diagonal at their natural scale, after which the bound states decouple from the dynamics at much smaller momentum scales. At the other extreme, the f=1 transformation tries to move bound-state eigenvalues to the part of the diagonal corresponding to the lowest momentum scales available and inevitably diverges when this scale is taken to zero. We analyze the shift mechanism analytically in a 3x3 matrix model, which displays the essense of this RG behavior.",
    "authors": [
      "Stanislaw D. Glazek",
      "Robert J. Perry"
    ],
    "published": "2008-03-20T00:48:23+00:00",
    "url": "http://arxiv.org/pdf/0803.2911v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1010.0935v1",
    "title": "Geometric picture of generalized-CP and Higgs-family transformations in the two-Higgs-doublet model",
    "content": "Geometric picture of generalized-CP and Higgs-family transformations in the two-Higgs-doublet model. In the two-Higgs-doublet model (THDM), generalized-CP transformations (phi_i--> X_{ij} phi_j^* where X is unitary) and unitary Higgs-family transformations (phi_i--> U_{ij} phi_j) have recently been examined in a series of papers. In terms of gauge-invariant bilinear functions of the Higgs fields phi_i, the Higgs-family transformations and the generalized-CP transformations possess a simple geometric description. Namely, these transformations correspond in the space of scalar-field bilinears to proper and improper rotations, respectively. In this formalism, recent results relating generalized CP transformations with Higgs-family transformations have a clear geometric interpretation.",
    "authors": [
      "P. M. Ferreira",
      "Howard E. Haber",
      "M. Maniatis",
      "O. Nachtmann",
      "Joao P. Silva"
    ],
    "published": "2010-10-05T16:39:34+00:00",
    "url": "http://arxiv.org/pdf/1010.0935v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1102.4162v1",
    "title": "Comparative Study on DFD to UML Diagrams Transformations",
    "content": "Comparative Study on DFD to UML Diagrams Transformations. Most of legacy systems use nowadays were modeled and documented using structured approach. Expansion of these systems in terms of functionality and maintainability requires shift towards object-oriented documentation and design, which has been widely accepted by the industry. In this paper, we present a survey of the existing Data Flow Diagram (DFD) to Unified Modeling language (UML) transformation techniques. We analyze transformation techniques using a set of parameters, identified in the survey. Based on identified parameters, we present an analysis matrix, which describes the strengths and weaknesses of transformation techniques. It is observed that most of the transformation approaches are rule based, which are incomplete and defined at abstract level that does not cover in depth transformation and automation issues. Transformation approaches are data centric, which focuses on data-store for class diagram generation. Very few of the transformation techniques have been applied on case study as a proof of concept, which are not comprehensive and majority of them are partially automated.",
    "authors": [
      "Atif A. A. Jilani",
      "Muhammad Usman",
      "Aamer Nadeem"
    ],
    "published": "2011-02-21T08:38:49+00:00",
    "url": "http://arxiv.org/pdf/1102.4162v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1911.03179v2",
    "title": "Lipschitz Constrained Parameter Initialization for Deep Transformers",
    "content": "Lipschitz Constrained Parameter Initialization for Deep Transformers. The Transformer translation model employs residual connection and layer normalization to ease the optimization difficulties caused by its multi-layer encoder/decoder structure. Previous research shows that even with residual connection and layer normalization, deep Transformers still have difficulty in training, and particularly Transformer models with more than 12 encoder/decoder layers fail to converge. In this paper, we first empirically demonstrate that a simple modification made in the official implementation, which changes the computation order of residual connection and layer normalization, can significantly ease the optimization of deep Transformers. We then compare the subtle differences in computation order in considerable detail, and present a parameter initialization method that leverages the Lipschitz constraint on the initialization of Transformer parameters that effectively ensures training convergence. In contrast to findings in previous research we further demonstrate that with Lipschitz parameter initialization, deep Transformers with the original computation order can converge, and obtain significant BLEU improvements with up to 24 layers. In contrast to previous research which focuses on deep encoders, our approach additionally enables Transformers to also benefit from deep decoders.",
    "authors": [
      "Hongfei Xu",
      "Qiuhui Liu",
      "Josef van Genabith",
      "Deyi Xiong",
      "Jingyi Zhang"
    ],
    "published": "2019-11-08T10:52:43+00:00",
    "url": "http://arxiv.org/pdf/1911.03179v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2112.14000v1",
    "title": "Pale Transformer: A General Vision Transformer Backbone with Pale-Shaped Attention",
    "content": "Pale Transformer: A General Vision Transformer Backbone with Pale-Shaped Attention. Recently, Transformers have shown promising performance in various vision tasks. To reduce the quadratic computation complexity caused by the global self-attention, various methods constrain the range of attention within a local region to improve its efficiency. Consequently, their receptive fields in a single attention layer are not large enough, resulting in insufficient context modeling. To address this issue, we propose a Pale-Shaped self-Attention (PS-Attention), which performs self-attention within a pale-shaped region. Compared to the global self-attention, PS-Attention can reduce the computation and memory costs significantly. Meanwhile, it can capture richer contextual information under the similar computation complexity with previous local self-attention mechanisms. Based on the PS-Attention, we develop a general Vision Transformer backbone with a hierarchical architecture, named Pale Transformer, which achieves 83.4%, 84.3%, and 84.9% Top-1 accuracy with the model size of 22M, 48M, and 85M respectively for 224 ImageNet-1K classification, outperforming the previous Vision Transformer backbones. For downstream tasks, our Pale Transformer backbone performs better than the recent state-of-the-art CSWin Transformer by a large margin on ADE20K semantic segmentation and COCO object detection & instance segmentation. The code will be released on https://github.com/BR-IDL/PaddleViT.",
    "authors": [
      "Sitong Wu",
      "Tianyi Wu",
      "Haoru Tan",
      "Guodong Guo"
    ],
    "published": "2021-12-28T05:37:24+00:00",
    "url": "http://arxiv.org/pdf/2112.14000v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2105.11689v2",
    "title": "Self-Supervised Graph Representation Learning via Topology Transformations",
    "content": "Self-Supervised Graph Representation Learning via Topology Transformations. We present the Topology Transformation Equivariant Representation learning, a general paradigm of self-supervised learning for node representations of graph data to enable the wide applicability of Graph Convolutional Neural Networks (GCNNs). We formalize the proposed model from an information-theoretic perspective, by maximizing the mutual information between topology transformations and node representations before and after the transformations. We derive that maximizing such mutual information can be relaxed to minimizing the cross entropy between the applied topology transformation and its estimation from node representations. In particular, we seek to sample a subset of node pairs from the original graph and flip the edge connectivity between each pair to transform the graph topology. Then, we self-train a representation encoder to learn node representations by reconstructing the topology transformations from the feature representations of the original and transformed graphs. In experiments, we apply the proposed model to the downstream node classification, graph classification and link prediction tasks, and results show that the proposed method outperforms the state-of-the-art unsupervised approaches.",
    "authors": [
      "Xiang Gao",
      "Wei Hu",
      "Guo-Jun Qi"
    ],
    "published": "2021-05-25T06:11:03+00:00",
    "url": "http://arxiv.org/pdf/2105.11689v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2107.05687v2",
    "title": "Revisiting Uncertainty-based Query Strategies for Active Learning with Transformers",
    "content": "Revisiting Uncertainty-based Query Strategies for Active Learning with Transformers. Active learning is the iterative construction of a classification model through targeted labeling, enabling significant labeling cost savings. As most research on active learning has been carried out before transformer-based language models (\"transformers\") became popular, despite its practical importance, comparably few papers have investigated how transformers can be combined with active learning to date. This can be attributed to the fact that using state-of-the-art query strategies for transformers induces a prohibitive runtime overhead, which effectively nullifies, or even outweighs the desired cost savings. For this reason, we revisit uncertainty-based query strategies, which had been largely outperformed before, but are particularly suited in the context of fine-tuning transformers. In an extensive evaluation, we connect transformers to experiments from previous research, assessing their performance on five widely used text classification benchmarks. For active learning with transformers, several other uncertainty-based approaches outperform the well-known prediction entropy query strategy, thereby challenging its status as most popular uncertainty baseline in active learning for text classification.",
    "authors": [
      "Christopher Schröder",
      "Andreas Niekler",
      "Martin Potthast"
    ],
    "published": "2021-07-12T18:56:04+00:00",
    "url": "http://arxiv.org/pdf/2107.05687v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1705.04136v3",
    "title": "Adaptively Transformed Mixed Model Prediction of General Finite Population Parameters",
    "content": "Adaptively Transformed Mixed Model Prediction of General Finite Population Parameters. For estimating area-specific parameters (quantities) in a finite population, a mixed model prediction approach is attractive. However, this approach strongly depends on the normality assumption of the response values although we often encounter a non-normal case in practice. In such a case, transforming observations to make them suitable for normality assumption is a useful tool, but the problem of selecting suitable transformation still remains open. To overcome the difficulty, we here propose a new empirical best predicting method by using a parametric family of transformations to estimate a suitable transformation based on the data. We suggest a simple estimating method for transformation parameters based on the profile likelihood function, which achieves consistency under some conditions on transformation functions. For measuring variability of point prediction, we construct an empirical Bayes confidence interval of the population parameter of interest. Through simulation studies, we investigate numerical performance of the proposed methods. Finally, we apply the proposed method to synthetic income data in Spanish provinces in which the resulting estimates indicate that the commonly used log-transformation would not be appropriate.",
    "authors": [
      "Shonosuke Sugasawa",
      "Tatsuya Kubokawa"
    ],
    "published": "2017-05-11T12:31:57+00:00",
    "url": "http://arxiv.org/pdf/1705.04136v3"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2106.04108v3",
    "title": "Fully Transformer Networks for Semantic Image Segmentation",
    "content": "Fully Transformer Networks for Semantic Image Segmentation. Transformers have shown impressive performance in various natural language processing and computer vision tasks, due to the capability of modeling long-range dependencies. Recent progress has demonstrated that combining such Transformers with CNN-based semantic image segmentation models is very promising. However, it is not well studied yet on how well a pure Transformer based approach can achieve for image segmentation. In this work, we explore a novel framework for semantic image segmentation, which is encoder-decoder based Fully Transformer Networks (FTN). Specifically, we first propose a Pyramid Group Transformer (PGT) as the encoder for progressively learning hierarchical features, meanwhile reducing the computation complexity of the standard Visual Transformer (ViT). Then, we propose a Feature Pyramid Transformer (FPT) to fuse semantic-level and spatial-level information from multiple levels of the PGT encoder for semantic image segmentation. Surprisingly, this simple baseline can achieve better results on multiple challenging semantic segmentation and face parsing benchmarks, including PASCAL Context, ADE20K, COCOStuff, and CelebAMask-HQ. The source code will be released on https://github.com/BR-IDL/PaddleViT.",
    "authors": [
      "Sitong Wu",
      "Tianyi Wu",
      "Fangjian Lin",
      "Shengwei Tian",
      "Guodong Guo"
    ],
    "published": "2021-06-08T05:15:28+00:00",
    "url": "http://arxiv.org/pdf/2106.04108v3"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2203.12644v4",
    "title": "Linearizing Transformer with Key-Value Memory",
    "content": "Linearizing Transformer with Key-Value Memory. Efficient transformer variants with linear time complexity have been developed to mitigate the quadratic computational overhead of the vanilla transformer. Among them are low-rank projection methods such as Linformer and kernel-based Transformers. Despite their unique merits, they usually suffer from a performance drop comparing with the vanilla transformer on many sequence generation tasks, and often fail to obtain computation gain when the generation is short. We propose MemSizer, an approach towards closing the performance gap while improving the efficiency even with short generation. It projects the source sequences into lower dimension representations like Linformer, while enjoying efficient recurrent-style incremental computation similar to kernel-based transformers. This yields linear computation time and constant memory complexity at inference time. MemSizer also employs a lightweight multi-head mechanism which renders the computation as light as a single-head model. We demonstrate that MemSizer provides an improved balance between efficiency and accuracy over the vanilla transformer and other efficient transformer variants in three typical sequence generation tasks, including machine translation, abstractive text summarization, and language modeling.",
    "authors": [
      "Yizhe Zhang",
      "Deng Cai"
    ],
    "published": "2022-03-23T18:10:18+00:00",
    "url": "http://arxiv.org/pdf/2203.12644v4"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2303.00086v3",
    "title": "Applying Plain Transformers to Real-World Point Clouds",
    "content": "Applying Plain Transformers to Real-World Point Clouds. To apply transformer-based models to point cloud understanding, many previous works modify the architecture of transformers by using, e.g., local attention and down-sampling. Although they have achieved promising results, earlier works on transformers for point clouds have two issues. First, the power of plain transformers is still under-explored. Second, they focus on simple and small point clouds instead of complex real-world ones. This work revisits the plain transformers in real-world point cloud understanding. We first take a closer look at some fundamental components of plain transformers, e.g., patchifier and positional embedding, for both efficiency and performance. To close the performance gap due to the lack of inductive bias and annotated data, we investigate self-supervised pre-training with masked autoencoder (MAE). Specifically, we propose drop patch, which prevents information leakage and significantly improves the effectiveness of MAE. Our models achieve SOTA results in semantic segmentation on the S3DIS dataset and object detection on the ScanNet dataset with lower computational costs. Our work provides a new baseline for future research on transformers for point clouds.",
    "authors": [
      "Lanxiao Li",
      "Michael Heizmann"
    ],
    "published": "2023-02-28T21:06:36+00:00",
    "url": "http://arxiv.org/pdf/2303.00086v3"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2307.02913v3",
    "title": "Numerical Methods with Coordinate Transforms for Efficient Brownian Dynamics Simulations",
    "content": "Numerical Methods with Coordinate Transforms for Efficient Brownian Dynamics Simulations. Many stochastic processes in the physical and biological sciences can be modelled as Brownian dynamics with multiplicative noise. However, numerical integrators for these processes can lose accuracy or even fail to converge when the diffusion term is configuration-dependent. One remedy is to construct a transform to a constant-diffusion process and sample the transformed process instead. In this work, we explain how coordinate-based and time-rescaling-based transforms can be used either individually or in combination to map a general class of variable-diffusion Brownian motion processes into constant-diffusion ones. The transforms are invertible, thus allowing recovery of the original dynamics. We motivate our methodology using examples in one dimension before then considering multivariate diffusion processes. We illustrate the benefits of the transforms through numerical simulations, demonstrating how the right combination of integrator and transform can improve computational efficiency and the order of convergence to the invariant distribution. Notably, the transforms that we derive are applicable to a class of multibody, anisotropic Stokes-Einstein diffusion that has applications in biophysical modelling.",
    "authors": [
      "Dominic Phillips",
      "Charles Matthews",
      "Benedict Leimkuhler"
    ],
    "published": "2023-07-06T10:56:20+00:00",
    "url": "http://arxiv.org/pdf/2307.02913v3"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2309.12578v1",
    "title": "SPION: Layer-Wise Sparse Training of Transformer via Convolutional Flood Filling",
    "content": "SPION: Layer-Wise Sparse Training of Transformer via Convolutional Flood Filling. Sparsifying the Transformer has garnered considerable interest, as training the Transformer is very computationally demanding. Prior efforts to sparsify the Transformer have either used a fixed pattern or data-driven approach to reduce the number of operations involving the computation of multi-head attention, which is the main bottleneck of the Transformer. However, existing methods suffer from inevitable problems, such as the potential loss of essential sequence features due to the uniform fixed pattern applied across all layers, and an increase in the model size resulting from the use of additional parameters to learn sparsity patterns in attention operations. In this paper, we propose a novel sparsification scheme for the Transformer that integrates convolution filters and the flood filling method to efficiently capture the layer-wise sparse pattern in attention operations. Our sparsification approach reduces the computational complexity and memory footprint of the Transformer during training. Efficient implementations of the layer-wise sparsified attention algorithm on GPUs are developed, demonstrating a new SPION that achieves up to 3.08X speedup over existing state-of-the-art sparse Transformer models, with better evaluation quality.",
    "authors": [
      "Bokyeong Yoon",
      "Yoonsang Han",
      "Gordon Euhyun Moon"
    ],
    "published": "2023-09-22T02:14:46+00:00",
    "url": "http://arxiv.org/pdf/2309.12578v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2311.01429v1",
    "title": "Efficient Vision Transformer for Accurate Traffic Sign Detection",
    "content": "Efficient Vision Transformer for Accurate Traffic Sign Detection. This research paper addresses the challenges associated with traffic sign detection in self-driving vehicles and driver assistance systems. The development of reliable and highly accurate algorithms is crucial for the widespread adoption of traffic sign recognition and detection (TSRD) in diverse real-life scenarios. However, this task is complicated by suboptimal traffic images affected by factors such as camera movement, adverse weather conditions, and inadequate lighting. This study specifically focuses on traffic sign detection methods and introduces the application of the Transformer model, particularly the Vision Transformer variants, to tackle this task. The Transformer's attention mechanism, originally designed for natural language processing, offers improved parallel efficiency. Vision Transformers have demonstrated success in various domains, including autonomous driving, object detection, healthcare, and defense-related applications. To enhance the efficiency of the Transformer model, the research proposes a novel strategy that integrates a locality inductive bias and a transformer module. This includes the introduction of the Efficient Convolution Block and the Local Transformer Block, which effectively capture short-term and long-term dependency information, thereby improving both detection speed and accuracy. Experimental evaluations demonstrate the significant advancements achieved by this approach, particularly when applied to the GTSDB dataset.",
    "authors": [
      "Javad Mirzapour Kaleybar",
      "Hooman Khaloo",
      "Avaz Naghipour"
    ],
    "published": "2023-11-02T17:44:32+00:00",
    "url": "http://arxiv.org/pdf/2311.01429v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2402.02005v2",
    "title": "Topology-Informed Graph Transformer",
    "content": "Topology-Informed Graph Transformer. Transformers have revolutionized performance in Natural Language Processing and Vision, paving the way for their integration with Graph Neural Networks (GNNs). One key challenge in enhancing graph transformers is strengthening the discriminative power of distinguishing isomorphisms of graphs, which plays a crucial role in boosting their predictive performances. To address this challenge, we introduce 'Topology-Informed Graph Transformer (TIGT)', a novel transformer enhancing both discriminative power in detecting graph isomorphisms and the overall performance of Graph Transformers. TIGT consists of four components: A topological positional embedding layer using non-isomorphic universal covers based on cyclic subgraphs of graphs to ensure unique graph representation: A dual-path message-passing layer to explicitly encode topological characteristics throughout the encoder layers: A global attention mechanism: And a graph information layer to recalibrate channel-wise graph features for better feature representation. TIGT outperforms previous Graph Transformers in classifying synthetic dataset aimed at distinguishing isomorphism classes of graphs. Additionally, mathematical analysis and empirical evaluations highlight our model's competitive edge over state-of-the-art Graph Transformers across various benchmark datasets.",
    "authors": [
      "Yun Young Choi",
      "Sun Woo Park",
      "Minho Lee",
      "Youngho Woo"
    ],
    "published": "2024-02-03T03:17:44+00:00",
    "url": "http://arxiv.org/pdf/2402.02005v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2405.19779v2",
    "title": "Automatic Graph Topology-Aware Transformer",
    "content": "Automatic Graph Topology-Aware Transformer. Existing efforts are dedicated to designing many topologies and graph-aware strategies for the graph Transformer, which greatly improve the model's representation capabilities. However, manually determining the suitable Transformer architecture for a specific graph dataset or task requires extensive expert knowledge and laborious trials. This paper proposes an evolutionary graph Transformer architecture search framework (EGTAS) to automate the construction of strong graph Transformers. We build a comprehensive graph Transformer search space with the micro-level and macro-level designs. EGTAS evolves graph Transformer topologies at the macro level and graph-aware strategies at the micro level. Furthermore, a surrogate model based on generic architectural coding is proposed to directly predict the performance of graph Transformers, substantially reducing the evaluation cost of evolutionary search. We demonstrate the efficacy of EGTAS across a range of graph-level and node-level tasks, encompassing both small-scale and large-scale graph datasets. Experimental results and ablation studies show that EGTAS can construct high-performance architectures that rival state-of-the-art manual and automated baselines.",
    "authors": [
      "Chao Wang",
      "Jiaxuan Zhao",
      "Lingling Li",
      "Licheng Jiao",
      "Fang Liu",
      "Shuyuan Yang"
    ],
    "published": "2024-05-30T07:44:31+00:00",
    "url": "http://arxiv.org/pdf/2405.19779v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2407.01602v1",
    "title": "Clustering in pure-attention hardmax transformers and its role in sentiment analysis",
    "content": "Clustering in pure-attention hardmax transformers and its role in sentiment analysis. Transformers are extremely successful machine learning models whose mathematical properties remain poorly understood. Here, we rigorously characterize the behavior of transformers with hardmax self-attention and normalization sublayers as the number of layers tends to infinity. By viewing such transformers as discrete-time dynamical systems describing the evolution of points in a Euclidean space, and thanks to a geometric interpretation of the self-attention mechanism based on hyperplane separation, we show that the transformer inputs asymptotically converge to a clustered equilibrium determined by special points called leaders. We then leverage this theoretical understanding to solve sentiment analysis problems from language processing using a fully interpretable transformer model, which effectively captures `context' by clustering meaningless words around leader words carrying the most meaning. Finally, we outline remaining challenges to bridge the gap between the mathematical analysis of transformers and their real-life implementation.",
    "authors": [
      "Albert Alcalde",
      "Giovanni Fantuzzi",
      "Enrique Zuazua"
    ],
    "published": "2024-06-26T16:13:35+00:00",
    "url": "http://arxiv.org/pdf/2407.01602v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2411.17110v1",
    "title": "TabulaX: Leveraging Large Language Models for Multi-Class Table Transformations",
    "content": "TabulaX: Leveraging Large Language Models for Multi-Class Table Transformations. The integration of tabular data from diverse sources is often hindered by inconsistencies in formatting and representation, posing significant challenges for data analysts and personal digital assistants. Existing methods for automating tabular data transformations are limited in scope, often focusing on specific types of transformations or lacking interpretability. In this paper, we introduce TabulaX, a novel framework that leverages Large Language Models (LLMs) for multi-class tabular transformations. TabulaX first classifies input tables into four transformation classes (string-based, numerical, algorithmic, and general) and then applies tailored methods to generate human-interpretable transformation functions, such as numeric formulas or programming code. This approach enhances transparency and allows users to understand and modify the mappings. Through extensive experiments on real-world datasets from various domains, we demonstrate that TabulaX outperforms existing state-of-the-art approaches in terms of accuracy, supports a broader class of transformations, and generates interpretable transformations that can be efficiently applied.",
    "authors": [
      "Arash Dargahi Nobari",
      "Davood Rafiei"
    ],
    "published": "2024-11-26T05:00:23+00:00",
    "url": "http://arxiv.org/pdf/2411.17110v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2502.16533v2",
    "title": "A Survey of Graph Transformers: Architectures, Theories and Applications",
    "content": "A Survey of Graph Transformers: Architectures, Theories and Applications. Graph Transformers (GTs) have demonstrated a strong capability in modeling graph structures by addressing the intrinsic limitations of graph neural networks (GNNs), such as over-smoothing and over-squashing. Recent studies have proposed diverse architectures, enhanced explainability, and practical applications for Graph Transformers. In light of these rapid developments, we conduct a comprehensive review of Graph Transformers, covering aspects such as their architectures, theoretical foundations, and applications within this survey. We categorize the architecture of Graph Transformers according to their strategies for processing structural information, including graph tokenization, positional encoding, structure-aware attention and model ensemble. Furthermore, from the theoretical perspective, we examine the expressivity of Graph Transformers in various discussed architectures and contrast them with other advanced graph learning algorithms to discover the connections. Furthermore, we provide a summary of the practical applications where Graph Transformers have been utilized, such as molecule, protein, language, vision, traffic, brain and material data. At the end of this survey, we will discuss the current challenges and prospective directions in Graph Transformers for potential future research.",
    "authors": [
      "Chaohao Yuan",
      "Kangfei Zhao",
      "Ercan Engin Kuruoglu",
      "Liang Wang",
      "Tingyang Xu",
      "Wenbing Huang",
      "Deli Zhao",
      "Hong Cheng",
      "Yu Rong"
    ],
    "published": "2025-02-23T10:55:19+00:00",
    "url": "http://arxiv.org/pdf/2502.16533v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2503.01835v1",
    "title": "Primus: Enforcing Attention Usage for 3D Medical Image Segmentation",
    "content": "Primus: Enforcing Attention Usage for 3D Medical Image Segmentation. Transformers have achieved remarkable success across multiple fields, yet their impact on 3D medical image segmentation remains limited with convolutional networks still dominating major benchmarks. In this work, we a) analyze current Transformer-based segmentation models and identify critical shortcomings, particularly their over-reliance on convolutional blocks. Further, we demonstrate that in some architectures, performance is unaffected by the absence of the Transformer, thereby demonstrating their limited effectiveness. To address these challenges, we move away from hybrid architectures and b) introduce a fully Transformer-based segmentation architecture, termed Primus. Primus leverages high-resolution tokens, combined with advances in positional embeddings and block design, to maximally leverage its Transformer blocks. Through these adaptations Primus surpasses current Transformer-based methods and competes with state-of-the-art convolutional models on multiple public datasets. By doing so, we create the first pure Transformer architecture and take a significant step towards making Transformers state-of-the-art for 3D medical image segmentation.",
    "authors": [
      "Tassilo Wald",
      "Saikat Roy",
      "Fabian Isensee",
      "Constantin Ulrich",
      "Sebastian Ziegler",
      "Dasha Trofimova",
      "Raphael Stock",
      "Michael Baumgartner",
      "Gregor Köhler",
      "Klaus Maier-Hein"
    ],
    "published": "2025-03-03T18:56:29+00:00",
    "url": "http://arxiv.org/pdf/2503.01835v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1605.08613v2",
    "title": "Quantum decoration transformation for spin models",
    "content": "Quantum decoration transformation for spin models. It is quite relevant the extension of decoration transformation for quantum spin models since most of the real materials could be well described by Heisenberg type models. Here we propose an exact quantum decoration transformation and also showing interesting properties such as the persistence of symmetry and the symmetry breaking during this transformation. Although the proposed transformation, in principle, cannot be used to map exactly a quantum spin lattice model into another quantum spin lattice model, since the operators are non-commutative. However, it is possible the mapping in the \"classical\" limit, establishing an equivalence between both quantum spin lattice models. To study the validity of this approach for quantum spin lattice model, we use the Zassenhaus formula, and we verify how the correction could influence the decoration transformation. But this correction could be useless to improve the quantum decoration transformation because it involves the second-nearest-neighbor and further nearest neighbor couplings, which leads into a cumbersome task to establish the equivalence between both lattice models. This correction also gives us valuable information about its contribution, for most of the Heisenberg type models, this correction could be irrelevant at least up to the third order term of Zassenhaus formula. This transformation is applied to a finite size Heisenberg chain, comparing with the exact numerical results, our result is consistent for weak xy-anisotropy coupling. We also apply to bond-alternating Ising-Heisenberg chain model, obtaining an accurate result in the limit of the quasi-Ising chain.",
    "authors": [
      "F. F. Braz",
      "F. C. Rodrigues",
      "S. M. de Souza",
      "Onofre Rojas"
    ],
    "published": "2016-05-27T12:48:12+00:00",
    "url": "http://arxiv.org/pdf/1605.08613v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2204.08023v1",
    "title": "VDTR: Video Deblurring with Transformer",
    "content": "VDTR: Video Deblurring with Transformer. Video deblurring is still an unsolved problem due to the challenging spatio-temporal modeling process. While existing convolutional neural network-based methods show a limited capacity for effective spatial and temporal modeling for video deblurring. This paper presents VDTR, an effective Transformer-based model that makes the first attempt to adapt Transformer for video deblurring. VDTR exploits the superior long-range and relation modeling capabilities of Transformer for both spatial and temporal modeling. However, it is challenging to design an appropriate Transformer-based model for video deblurring due to the complicated non-uniform blurs, misalignment across multiple frames and the high computational costs for high-resolution spatial modeling. To address these problems, VDTR advocates performing attention within non-overlapping windows and exploiting the hierarchical structure for long-range dependencies modeling. For frame-level spatial modeling, we propose an encoder-decoder Transformer that utilizes multi-scale features for deblurring. For multi-frame temporal modeling, we adapt Transformer to fuse multiple spatial features efficiently. Compared with CNN-based methods, the proposed method achieves highly competitive results on both synthetic and real-world video deblurring benchmarks, including DVD, GOPRO, REDS and BSD. We hope such a Transformer-based architecture can serve as a powerful alternative baseline for video deblurring and other video restoration tasks. The source code will be available at \\url{https://github.com/ljzycmd/VDTR}.",
    "authors": [
      "Mingdeng Cao",
      "Yanbo Fan",
      "Yong Zhang",
      "Jue Wang",
      "Yujiu Yang"
    ],
    "published": "2022-04-17T14:22:14+00:00",
    "url": "http://arxiv.org/pdf/2204.08023v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2305.15099v1",
    "title": "Fourier Transformer: Fast Long Range Modeling by Removing Sequence Redundancy with FFT Operator",
    "content": "Fourier Transformer: Fast Long Range Modeling by Removing Sequence Redundancy with FFT Operator. The transformer model is known to be computationally demanding, and prohibitively costly for long sequences, as the self-attention module uses a quadratic time and space complexity with respect to sequence length. Many researchers have focused on designing new forms of self-attention or introducing new parameters to overcome this limitation, however a large portion of them prohibits the model to inherit weights from large pretrained models. In this work, the transformer's inefficiency has been taken care of from another perspective. We propose Fourier Transformer, a simple yet effective approach by progressively removing redundancies in hidden sequence using the ready-made Fast Fourier Transform (FFT) operator to perform Discrete Cosine Transformation (DCT). Fourier Transformer is able to significantly reduce computational costs while retain the ability to inherit from various large pretrained models. Experiments show that our model achieves state-of-the-art performances among all transformer-based models on the long-range modeling benchmark LRA with significant improvement in both speed and space. For generative seq-to-seq tasks including CNN/DailyMail and ELI5, by inheriting the BART weights our model outperforms the standard BART and other efficient models. \\footnote{Our code is publicly available at \\url{https://github.com/LUMIA-Group/FourierTransformer}}",
    "authors": [
      "Ziwei He",
      "Meng Yang",
      "Minwei Feng",
      "Jingcheng Yin",
      "Xinbing Wang",
      "Jingwen Leng",
      "Zhouhan Lin"
    ],
    "published": "2023-05-24T12:33:06+00:00",
    "url": "http://arxiv.org/pdf/2305.15099v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2112.09300v1",
    "title": "Towards End-to-End Image Compression and Analysis with Transformers",
    "content": "Towards End-to-End Image Compression and Analysis with Transformers. We propose an end-to-end image compression and analysis model with Transformers, targeting to the cloud-based image classification application. Instead of placing an existing Transformer-based image classification model directly after an image codec, we aim to redesign the Vision Transformer (ViT) model to perform image classification from the compressed features and facilitate image compression with the long-term information from the Transformer. Specifically, we first replace the patchify stem (i.e., image splitting and embedding) of the ViT model with a lightweight image encoder modelled by a convolutional neural network. The compressed features generated by the image encoder are injected convolutional inductive bias and are fed to the Transformer for image classification bypassing image reconstruction. Meanwhile, we propose a feature aggregation module to fuse the compressed features with the selected intermediate features of the Transformer, and feed the aggregated features to a deconvolutional neural network for image reconstruction. The aggregated features can obtain the long-term information from the self-attention mechanism of the Transformer and improve the compression performance. The rate-distortion-accuracy optimization problem is finally solved by a two-step training strategy. Experimental results demonstrate the effectiveness of the proposed model in both the image compression and the classification tasks.",
    "authors": [
      "Yuanchao Bai",
      "Xu Yang",
      "Xianming Liu",
      "Junjun Jiang",
      "Yaowei Wang",
      "Xiangyang Ji",
      "Wen Gao"
    ],
    "published": "2021-12-17T03:28:14+00:00",
    "url": "http://arxiv.org/pdf/2112.09300v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2406.06484v6",
    "title": "Parallelizing Linear Transformers with the Delta Rule over Sequence Length",
    "content": "Parallelizing Linear Transformers with the Delta Rule over Sequence Length. Transformers with linear attention (i.e., linear transformers) and state-space models have recently been suggested as a viable linear-time alternative to transformers with softmax attention. However, these models still underperform transformers especially on tasks that require in-context retrieval. While more expressive variants of linear transformers which replace the additive update in linear transformers with the delta rule (DeltaNet) have been found to be more effective at associative recall, existing algorithms for training such models do not parallelize over sequence length and are thus inefficient to train on modern hardware. This work describes a hardware-efficient algorithm for training linear transformers with the delta rule, which exploits a memory-efficient representation for computing products of Householder matrices. This algorithm allows us to scale up DeltaNet to standard language modeling settings. We train a 1.3B model for 100B tokens and find that it outperforms recent linear-time baselines such as Mamba and GLA in terms of perplexity and zero-shot performance on downstream tasks. We also experiment with two hybrid models which combine DeltaNet layers with (1) sliding-window attention layers every other layer or (2) two global attention layers, and find that these hybrids outperform strong transformer baselines.",
    "authors": [
      "Songlin Yang",
      "Bailin Wang",
      "Yu Zhang",
      "Yikang Shen",
      "Yoon Kim"
    ],
    "published": "2024-06-10T17:24:42+00:00",
    "url": "http://arxiv.org/pdf/2406.06484v6"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2407.15360v1",
    "title": "Dissecting Multiplication in Transformers: Insights into LLMs",
    "content": "Dissecting Multiplication in Transformers: Insights into LLMs. Transformer-based large language models have achieved remarkable performance across various natural language processing tasks. However, they often struggle with seemingly easy tasks like arithmetic despite their vast capabilities. This stark disparity raise human's concerns about their safe and ethical use, hinder their widespread adoption.In this paper, we focus on a typical arithmetic task, integer multiplication, to explore and explain the imperfection of transformers in this domain. We provide comprehensive analysis of a vanilla transformer trained to perform n-digit integer multiplication. Our observations indicate that the model decomposes multiplication task into multiple parallel subtasks, sequentially optimizing each subtask for each digit to complete the final multiplication. Based on observation and analysis, we infer the reasons of transformers deficiencies in multiplication tasks lies in their difficulty in calculating successive carryovers and caching intermediate results, and confirmed this inference through experiments. Guided by these findings, we propose improvements to enhance transformers performance on multiplication tasks. These enhancements are validated through rigorous testing and mathematical modeling, not only enhance transformer's interpretability, but also improve its performance, e.g., we achieve over 99.9% accuracy on 5-digit integer multiplication with a tiny transformer, outperform LLMs GPT-4. Our method contributes to the broader fields of model understanding and interpretability, paving the way for analyzing more complex tasks and Transformer models. This work underscores the importance of explainable AI, helping to build trust in large language models and promoting their adoption in critical applications.",
    "authors": [
      "Luyu Qiu",
      "Jianing Li",
      "Chi Su",
      "Chen Jason Zhang",
      "Lei Chen"
    ],
    "published": "2024-07-22T04:07:26+00:00",
    "url": "http://arxiv.org/pdf/2407.15360v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1904.09408v2",
    "title": "Language Models with Transformers",
    "content": "Language Models with Transformers. The Transformer architecture is superior to RNN-based models in computational efficiency. Recently, GPT and BERT demonstrate the efficacy of Transformer models on various NLP tasks using pre-trained language models on large-scale corpora. Surprisingly, these Transformer architectures are suboptimal for language model itself. Neither self-attention nor the positional encoding in the Transformer is able to efficiently incorporate the word-level sequential context crucial to language modeling.   In this paper, we explore effective Transformer architectures for language model, including adding additional LSTM layers to better capture the sequential context while still keeping the computation efficient. We propose Coordinate Architecture Search (CAS) to find an effective architecture through iterative refinement of the model. Experimental results on the PTB, WikiText-2, and WikiText-103 show that CAS achieves perplexities between 20.42 and 34.11 on all problems, i.e. on average an improvement of 12.0 perplexity units compared to state-of-the-art LSTMs. The source code is publicly available.",
    "authors": [
      "Chenguang Wang",
      "Mu Li",
      "Alexander J. Smola"
    ],
    "published": "2019-04-20T06:43:14+00:00",
    "url": "http://arxiv.org/pdf/1904.09408v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2005.08100v1",
    "title": "Conformer: Convolution-augmented Transformer for Speech Recognition",
    "content": "Conformer: Convolution-augmented Transformer for Speech Recognition. Recently Transformer and Convolution neural network (CNN) based models have shown promising results in Automatic Speech Recognition (ASR), outperforming Recurrent neural networks (RNNs). Transformer models are good at capturing content-based global interactions, while CNNs exploit local features effectively. In this work, we achieve the best of both worlds by studying how to combine convolution neural networks and transformers to model both local and global dependencies of an audio sequence in a parameter-efficient way. To this regard, we propose the convolution-augmented transformer for speech recognition, named Conformer. Conformer significantly outperforms the previous Transformer and CNN based models achieving state-of-the-art accuracies. On the widely used LibriSpeech benchmark, our model achieves WER of 2.1%/4.3% without using a language model and 1.9%/3.9% with an external language model on test/testother. We also observe competitive performance of 2.7%/6.3% with a small model of only 10M parameters.",
    "authors": [
      "Anmol Gulati",
      "James Qin",
      "Chung-Cheng Chiu",
      "Niki Parmar",
      "Yu Zhang",
      "Jiahui Yu",
      "Wei Han",
      "Shibo Wang",
      "Zhengdong Zhang",
      "Yonghui Wu",
      "Ruoming Pang"
    ],
    "published": "2020-05-16T20:56:25+00:00",
    "url": "http://arxiv.org/pdf/2005.08100v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2010.02744v1",
    "title": "Stepwise Extractive Summarization and Planning with Structured Transformers",
    "content": "Stepwise Extractive Summarization and Planning with Structured Transformers. We propose encoder-centric stepwise models for extractive summarization using structured transformers -- HiBERT and Extended Transformers. We enable stepwise summarization by injecting the previously generated summary into the structured transformer as an auxiliary sub-structure. Our models are not only efficient in modeling the structure of long inputs, but they also do not rely on task-specific redundancy-aware modeling, making them a general purpose extractive content planner for different tasks. When evaluated on CNN/DailyMail extractive summarization, stepwise models achieve state-of-the-art performance in terms of Rouge without any redundancy aware modeling or sentence filtering. This also holds true for Rotowire table-to-text generation, where our models surpass previously reported metrics for content selection, planning and ordering, highlighting the strength of stepwise modeling. Amongst the two structured transformers we test, stepwise Extended Transformers provides the best performance across both datasets and sets a new standard for these challenges.",
    "authors": [
      "Shashi Narayan",
      "Joshua Maynez",
      "Jakub Adamek",
      "Daniele Pighin",
      "Blaž Bratanič",
      "Ryan McDonald"
    ],
    "published": "2020-10-06T14:12:58+00:00",
    "url": "http://arxiv.org/pdf/2010.02744v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2209.10966v2",
    "title": "Adaptation of domain-specific transformer models with text oversampling for sentiment analysis of social media posts on Covid-19 vaccines",
    "content": "Adaptation of domain-specific transformer models with text oversampling for sentiment analysis of social media posts on Covid-19 vaccines. Covid-19 has spread across the world and several vaccines have been developed to counter its surge. To identify the correct sentiments associated with the vaccines from social media posts, we fine-tune various state-of-the-art pre-trained transformer models on tweets associated with Covid-19 vaccines. Specifically, we use the recently introduced state-of-the-art pre-trained transformer models RoBERTa, XLNet and BERT, and the domain-specific transformer models CT-BERT and BERTweet that are pre-trained on Covid-19 tweets. We further explore the option of text augmentation by oversampling using Language Model based Oversampling Technique (LMOTE) to improve the accuracies of these models, specifically, for small sample datasets where there is an imbalanced class distribution among the positive, negative and neutral sentiment classes. Our results summarize our findings on the suitability of text oversampling for imbalanced small sample datasets that are used to fine-tune state-of-the-art pre-trained transformer models, and the utility of domain-specific transformer models for the classification task.",
    "authors": [
      "Anmol Bansal",
      "Arjun Choudhry",
      "Anubhav Sharma",
      "Seba Susan"
    ],
    "published": "2022-09-22T12:36:40+00:00",
    "url": "http://arxiv.org/pdf/2209.10966v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2210.17393v1",
    "title": "Probabilistic Decomposition Transformer for Time Series Forecasting",
    "content": "Probabilistic Decomposition Transformer for Time Series Forecasting. Time series forecasting is crucial for many fields, such as disaster warning, weather prediction, and energy consumption. The Transformer-based models are considered to have revolutionized the field of sequence modeling. However, the complex temporal patterns of the time series hinder the model from mining reliable temporal dependencies. Furthermore, the autoregressive form of the Transformer introduces cumulative errors in the inference step. In this paper, we propose the probabilistic decomposition Transformer model that combines the Transformer with a conditional generative model, which provides hierarchical and interpretable probabilistic forecasts for intricate time series. The Transformer is employed to learn temporal patterns and implement primary probabilistic forecasts, while the conditional generative model is used to achieve non-autoregressive hierarchical probabilistic forecasts by introducing latent space feature representations. In addition, the conditional generative model reconstructs typical features of the series, such as seasonality and trend terms, from probability distributions in the latent space to enable complex pattern separation and provide interpretable forecasts. Extensive experiments on several datasets demonstrate the effectiveness and robustness of the proposed model, indicating that it compares favorably with the state of the art.",
    "authors": [
      "Junlong Tong",
      "Liping Xie",
      "Wankou Yang",
      "Kanjian Zhang"
    ],
    "published": "2022-10-31T15:22:50+00:00",
    "url": "http://arxiv.org/pdf/2210.17393v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2304.07434v1",
    "title": "Masked Pre-Training of Transformers for Histology Image Analysis",
    "content": "Masked Pre-Training of Transformers for Histology Image Analysis. In digital pathology, whole slide images (WSIs) are widely used for applications such as cancer diagnosis and prognosis prediction. Visual transformer models have recently emerged as a promising method for encoding large regions of WSIs while preserving spatial relationships among patches. However, due to the large number of model parameters and limited labeled data, applying transformer models to WSIs remains challenging. Inspired by masked language models, we propose a pretext task for training the transformer model without labeled data to address this problem. Our model, MaskHIT, uses the transformer output to reconstruct masked patches and learn representative histological features based on their positions and visual features. The experimental results demonstrate that MaskHIT surpasses various multiple instance learning approaches by 3% and 2% on survival prediction and cancer subtype classification tasks, respectively. Furthermore, MaskHIT also outperforms two of the most recent state-of-the-art transformer-based methods. Finally, a comparison between the attention maps generated by the MaskHIT model with pathologist's annotations indicates that the model can accurately identify clinically relevant histological structures in each task.",
    "authors": [
      "Shuai Jiang",
      "Liesbeth Hondelink",
      "Arief A. Suriawinata",
      "Saeed Hassanpour"
    ],
    "published": "2023-04-14T23:56:49+00:00",
    "url": "http://arxiv.org/pdf/2304.07434v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2306.01076v2",
    "title": "Quantization-Aware and Tensor-Compressed Training of Transformers for Natural Language Understanding",
    "content": "Quantization-Aware and Tensor-Compressed Training of Transformers for Natural Language Understanding. Fine-tuned transformer models have shown superior performances in many natural language tasks. However, the large model size prohibits deploying high-performance transformer models on resource-constrained devices. This paper proposes a quantization-aware tensor-compressed training approach to reduce the model size, arithmetic operations, and ultimately runtime latency of transformer-based models. We compress the embedding and linear layers of transformers into small low-rank tensor cores, which significantly reduces model parameters. A quantization-aware training with learnable scale factors is used to further obtain low-precision representations of the tensor-compressed models. The developed approach can be used for both end-to-end training and distillation-based training. To improve the convergence, a layer-by-layer distillation is applied to distill a quantized and tensor-compressed student model from a pre-trained transformer. The performance is demonstrated in two natural language understanding tasks, showing up to $63\\times$ compression ratio, little accuracy loss and remarkable inference and training speedup.",
    "authors": [
      "Zi Yang",
      "Samridhi Choudhary",
      "Siegfried Kunzmann",
      "Zheng Zhang"
    ],
    "published": "2023-06-01T18:32:08+00:00",
    "url": "http://arxiv.org/pdf/2306.01076v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2410.00273v1",
    "title": "Comprehensive Performance Modeling and System Design Insights for Foundation Models",
    "content": "Comprehensive Performance Modeling and System Design Insights for Foundation Models. Generative AI, in particular large transformer models, are increasingly driving HPC system design in science and industry. We analyze performance characteristics of such transformer models and discuss their sensitivity to the transformer type, parallelization strategy, and HPC system features (accelerators and interconnects). We utilize a performance model that allows us to explore this complex design space and highlight its key components. We find that different transformer types demand different parallelism and system characteristics at different training regimes. Large Language Models are performant with 3D parallelism and amplify network needs only at pre-training scales with reduced dependence on accelerator capacity and bandwidth. On the other hand, long-sequence transformers, representative of scientific foundation models, place a more uniform dependence on network and capacity with necessary 4D parallelism. Our analysis emphasizes the need for closer performance modeling of different transformer types keeping system features in mind and demonstrates a path towards this. Our code is available as open-source.",
    "authors": [
      "Shashank Subramanian",
      "Ermal Rrapaj",
      "Peter Harrington",
      "Smeet Chheda",
      "Steven Farrell",
      "Brian Austin",
      "Samuel Williams",
      "Nicholas Wright",
      "Wahid Bhimji"
    ],
    "published": "2024-09-30T22:56:42+00:00",
    "url": "http://arxiv.org/pdf/2410.00273v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2412.03606v1",
    "title": "Advanced Risk Prediction and Stability Assessment of Banks Using Time Series Transformer Models",
    "content": "Advanced Risk Prediction and Stability Assessment of Banks Using Time Series Transformer Models. This paper aims to study the prediction of the bank stability index based on the Time Series Transformer model. The bank stability index is an important indicator to measure the health status and risk resistance of financial institutions. Traditional prediction methods are difficult to adapt to complex market changes because they rely on single-dimensional macroeconomic data. This paper proposes a prediction framework based on the Time Series Transformer, which uses the self-attention mechanism of the model to capture the complex temporal dependencies and nonlinear relationships in financial data. Through experiments, we compare the model with LSTM, GRU, CNN, TCN and RNN-Transformer models. The experimental results show that the Time Series Transformer model outperforms other models in both mean square error (MSE) and mean absolute error (MAE) evaluation indicators, showing strong prediction ability. This shows that the Time Series Transformer model can better handle multidimensional time series data in bank stability prediction, providing new technical approaches and solutions for financial risk management.",
    "authors": [
      "Wenying Sun",
      "Zhen Xu",
      "Wenqing Zhang",
      "Kunyuan Ma",
      "You Wu",
      "Mengfang Sun"
    ],
    "published": "2024-12-04T08:15:27+00:00",
    "url": "http://arxiv.org/pdf/2412.03606v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2104.01572v1",
    "title": "TransfoRNN: Capturing the Sequential Information in Self-Attention Representations for Language Modeling",
    "content": "TransfoRNN: Capturing the Sequential Information in Self-Attention Representations for Language Modeling. In this paper, we describe the use of recurrent neural networks to capture sequential information from the self-attention representations to improve the Transformers. Although self-attention mechanism provides a means to exploit long context, the sequential information, i.e. the arrangement of tokens, is not explicitly captured. We propose to cascade the recurrent neural networks to the Transformers, which referred to as the TransfoRNN model, to capture the sequential information. We found that the TransfoRNN models which consists of only shallow Transformers stack is suffice to give comparable, if not better, performance than a deeper Transformer model. Evaluated on the Penn Treebank and WikiText-2 corpora, the proposed TransfoRNN model has shown lower model perplexities with fewer number of model parameters. On the Penn Treebank corpus, the model perplexities were reduced up to 5.5% with the model size reduced up to 10.5%. On the WikiText-2 corpus, the model perplexity was reduced up to 2.2% with a 27.7% smaller model. Also, the TransfoRNN model was applied on the LibriSpeech speech recognition task and has shown comparable results with the Transformer models.",
    "authors": [
      "Tze Yuang Chong",
      "Xuyang Wang",
      "Lin Yang",
      "Junjie Wang"
    ],
    "published": "2021-04-04T09:31:18+00:00",
    "url": "http://arxiv.org/pdf/2104.01572v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/hep-ph/9404222v1",
    "title": "Topologically non--trivial chiral transformations and their representations in a finite model space",
    "content": "Topologically non--trivial chiral transformations and their representations in a finite model space. The role of chiral transformations in effective theories modeling Quantum Chromo Dynamics is reviewed. In the context of the Nambu--Jona--Lasinio model the hidden gauge and massive Yang--Mills approaches to vector mesons are demonstrated to be linked by a special chiral transformation which removes the chiral field from the scalar--pseudoscalar sector. The role of this transformation in the presence of a topologically non--trivial chiral field is illuminated. The fermion determinant for such a field configuration is evaluated by summing the discretized eigenvalues of the Dirac Hamiltonian. This discretization is accomplished by demanding certain boundary conditions on the quark fields leaving a finite model space. The properties of two sets of boundary conditions are compared. When the topologically non-trivial chiral transformation is applied to the meson fields the associated transformation of the boundary conditions is shown to be indispensable. A constructive procedure for transforming the boundary conditions is developed.",
    "authors": [
      "R. Alkofer",
      "H. Reinhardt",
      "J. Schlienz",
      "H. Weigel"
    ],
    "published": "1994-04-06T12:01:24+00:00",
    "url": "http://arxiv.org/pdf/hep-ph/9404222v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/hep-th/9312154v1",
    "title": "Exact O(d,d) Transformations in WZW Models",
    "content": "Exact O(d,d) Transformations in WZW Models. Using the algebraic Hamiltonian approach, we derive the exact to all orders O(d,d) transformations of the metric and the dilaton field in WZW and WZW coset models for both compact and non-compact groups. It is shown that under the exact $O(d)\\times O(d)$ transformation only the leading order of the inverse metric $G^{-1}$ is transformed. The quantity $\\sqrt{G}\\exp(\\Phi)$ is the same in all the dual models and in particular is independent of k. We also show that the exact metric and dilaton field that correspond to G/U(1)^d WZW can be obtained by applying the exact O(d,d) transformations on the (ungauged) WZW, a result that was known to one loop order only. As an example we give the O(2,2) transformations in the $SL(2,R)$ WZW that transform to its dual exact models. These include also the exact 3D black string and the exact 2D black hole with an extra $U(1)$ free field.",
    "authors": [
      "David Gershon"
    ],
    "published": "1993-12-17T17:16:11+00:00",
    "url": "http://arxiv.org/pdf/hep-th/9312154v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1503.02344v1",
    "title": "Selection of the optimal Box-Cox transformation parameter for modelling and forecasting age-specific fertility",
    "content": "Selection of the optimal Box-Cox transformation parameter for modelling and forecasting age-specific fertility. The Box-Cox transformation can sometimes yield noticeable improvements in model simplicity, variance homogeneity and precision of estimation, such as in modelling and forecasting age-specific fertility. Despite its importance, there have been few studies focusing on the optimal selection of Box-Cox transformation parameters in demographic forecasting. A simple method is proposed for selecting the optimal Box-Cox transformation parameter, along with an algorithm based on an in-sample forecast error measure. Illustrated by Australian age-specific fertility, the out-of-sample accuracy of a forecasting method can be improved with the selected Box-Cox transformation parameter. Furthermore, the log transformation is not adequate for modelling and forecasting age-specific fertility. The Box-Cox transformation parameter should be embedded in statistical analysis of age-specific demographic data, in order to fully capture forecast uncertainties.",
    "authors": [
      "Han Lin Shang"
    ],
    "published": "2015-03-08T23:37:39+00:00",
    "url": "http://arxiv.org/pdf/1503.02344v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1704.06135v1",
    "title": "Modelling social-ecological transformations: an adaptive network proposal",
    "content": "Modelling social-ecological transformations: an adaptive network proposal. Transformations to create more sustainable social-ecological systems are urgently needed. Structural change is a feature of transformations of social-ecological systems that is of critical importance but is little understood. Here, we propose a framework for conceptualising and modelling sustainability transformations based on adaptive networks. Adaptive networks focus attention on the interplay between the structure of a social-ecological system and the dynamics of individual entities. Adaptive networks could progress transformations research by: 1) focusing research on changes in structure; 2) providing a conceptual framework that clarifies the temporal dynamics of social-ecological transformations compared to the most commonly used heuristic in resilience studies, the ball-and-cup diagram; 3) providing quantitative modelling tools in an area of study dominated by qualitative methods. We illustrate the potential application of adaptive networks to social-ecological transformations using a case study of illegal fishing in the Southern Ocean and a theoretical model of socially networked resource users.",
    "authors": [
      "Steven J. Lade",
      "Örjan Bodin",
      "Jonathan F. Donges",
      "Elin Enfors Kautsky",
      "Diego Galafassi",
      "Per Olsson",
      "Maja Schlüter"
    ],
    "published": "2017-04-18T14:27:35+00:00",
    "url": "http://arxiv.org/pdf/1704.06135v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2104.12099v2",
    "title": "Visual Saliency Transformer",
    "content": "Visual Saliency Transformer. Existing state-of-the-art saliency detection methods heavily rely on CNN-based architectures. Alternatively, we rethink this task from a convolution-free sequence-to-sequence perspective and predict saliency by modeling long-range dependencies, which can not be achieved by convolution. Specifically, we develop a novel unified model based on a pure transformer, namely, Visual Saliency Transformer (VST), for both RGB and RGB-D salient object detection (SOD). It takes image patches as inputs and leverages the transformer to propagate global contexts among image patches. Unlike conventional architectures used in Vision Transformer (ViT), we leverage multi-level token fusion and propose a new token upsampling method under the transformer framework to get high-resolution detection results. We also develop a token-based multi-task decoder to simultaneously perform saliency and boundary detection by introducing task-related tokens and a novel patch-task-attention mechanism. Experimental results show that our model outperforms existing methods on both RGB and RGB-D SOD benchmark datasets. Most importantly, our whole framework not only provides a new perspective for the SOD field but also shows a new paradigm for transformer-based dense prediction models. Code is available at https://github.com/nnizhang/VST.",
    "authors": [
      "Nian Liu",
      "Ni Zhang",
      "Kaiyuan Wan",
      "Ling Shao",
      "Junwei Han"
    ],
    "published": "2021-04-25T08:24:06+00:00",
    "url": "http://arxiv.org/pdf/2104.12099v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2103.16302v2",
    "title": "Rethinking Spatial Dimensions of Vision Transformers",
    "content": "Rethinking Spatial Dimensions of Vision Transformers. Vision Transformer (ViT) extends the application range of transformers from language processing to computer vision tasks as being an alternative architecture against the existing convolutional neural networks (CNN). Since the transformer-based architecture has been innovative for computer vision modeling, the design convention towards an effective architecture has been less studied yet. From the successful design principles of CNN, we investigate the role of spatial dimension conversion and its effectiveness on transformer-based architecture. We particularly attend to the dimension reduction principle of CNNs; as the depth increases, a conventional CNN increases channel dimension and decreases spatial dimensions. We empirically show that such a spatial dimension reduction is beneficial to a transformer architecture as well, and propose a novel Pooling-based Vision Transformer (PiT) upon the original ViT model. We show that PiT achieves the improved model capability and generalization performance against ViT. Throughout the extensive experiments, we further show PiT outperforms the baseline on several tasks such as image classification, object detection, and robustness evaluation. Source codes and ImageNet models are available at https://github.com/naver-ai/pit",
    "authors": [
      "Byeongho Heo",
      "Sangdoo Yun",
      "Dongyoon Han",
      "Sanghyuk Chun",
      "Junsuk Choe",
      "Seong Joon Oh"
    ],
    "published": "2021-03-30T12:51:28+00:00",
    "url": "http://arxiv.org/pdf/2103.16302v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2107.06263v3",
    "title": "CMT: Convolutional Neural Networks Meet Vision Transformers",
    "content": "CMT: Convolutional Neural Networks Meet Vision Transformers. Vision transformers have been successfully applied to image recognition tasks due to their ability to capture long-range dependencies within an image. However, there are still gaps in both performance and computational cost between transformers and existing convolutional neural networks (CNNs). In this paper, we aim to address this issue and develop a network that can outperform not only the canonical transformers, but also the high-performance convolutional models. We propose a new transformer based hybrid network by taking advantage of transformers to capture long-range dependencies, and of CNNs to model local features. Furthermore, we scale it to obtain a family of models, called CMTs, obtaining much better accuracy and efficiency than previous convolution and transformer based models. In particular, our CMT-S achieves 83.5% top-1 accuracy on ImageNet, while being 14x and 2x smaller on FLOPs than the existing DeiT and EfficientNet, respectively. The proposed CMT-S also generalizes well on CIFAR10 (99.2%), CIFAR100 (91.7%), Flowers (98.7%), and other challenging vision datasets such as COCO (44.3% mAP), with considerably less computational cost.",
    "authors": [
      "Jianyuan Guo",
      "Kai Han",
      "Han Wu",
      "Yehui Tang",
      "Xinghao Chen",
      "Yunhe Wang",
      "Chang Xu"
    ],
    "published": "2021-07-13T17:47:19+00:00",
    "url": "http://arxiv.org/pdf/2107.06263v3"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1912.12674v1",
    "title": "FLAT: Few-Shot Learning via Autoencoding Transformation Regularizers",
    "content": "FLAT: Few-Shot Learning via Autoencoding Transformation Regularizers. One of the most significant challenges facing a few-shot learning task is the generalizability of the (meta-)model from the base to the novel categories. Most of existing few-shot learning models attempt to address this challenge by either learning the meta-knowledge from multiple simulated tasks on the base categories, or resorting to data augmentation by applying various transformations to training examples. However, the supervised nature of model training in these approaches limits their ability of exploring variations across different categories, thus restricting their cross-category generalizability in modeling novel concepts. To this end, we present a novel regularization mechanism by learning the change of feature representations induced by a distribution of transformations without using the labels of data examples. We expect this regularizer could expand the semantic space of base categories to cover that of novel categories through the transformation of feature representations. It could minimize the risk of overfitting into base categories by inspecting the transformation-augmented variations at the encoded feature level. This results in the proposed FLAT (Few-shot Learning via Autoencoding Transformations) approach by autoencoding the applied transformations. The experiment results show the superior performances to the current state-of-the-art methods in literature.",
    "authors": [
      "Haohang Xu",
      "Hongkai Xiong",
      "Guojun Qi"
    ],
    "published": "2019-12-29T15:26:28+00:00",
    "url": "http://arxiv.org/pdf/1912.12674v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2109.07364v2",
    "title": "Towards Incremental Transformers: An Empirical Analysis of Transformer Models for Incremental NLU",
    "content": "Towards Incremental Transformers: An Empirical Analysis of Transformer Models for Incremental NLU. Incremental processing allows interactive systems to respond based on partial inputs, which is a desirable property e.g. in dialogue agents. The currently popular Transformer architecture inherently processes sequences as a whole, abstracting away the notion of time. Recent work attempts to apply Transformers incrementally via restart-incrementality by repeatedly feeding, to an unchanged model, increasingly longer input prefixes to produce partial outputs. However, this approach is computationally costly and does not scale efficiently for long sequences. In parallel, we witness efforts to make Transformers more efficient, e.g. the Linear Transformer (LT) with a recurrence mechanism. In this work, we examine the feasibility of LT for incremental NLU in English. Our results show that the recurrent LT model has better incremental performance and faster inference speed compared to the standard Transformer and LT with restart-incrementality, at the cost of part of the non-incremental (full sequence) quality. We show that the performance drop can be mitigated by training the model to wait for right context before committing to an output and that training with input prefixes is beneficial for delivering correct partial outputs.",
    "authors": [
      "Patrick Kahardipraja",
      "Brielen Madureira",
      "David Schlangen"
    ],
    "published": "2021-09-15T15:20:29+00:00",
    "url": "http://arxiv.org/pdf/2109.07364v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2208.11232v1",
    "title": "The Effects of Correctly Modeling Generator Step-Up Transformer Status in Geomagnetic Disturbance Studies",
    "content": "The Effects of Correctly Modeling Generator Step-Up Transformer Status in Geomagnetic Disturbance Studies. In order to correctly model the impacts of geomagnetically induced current (GIC) flows, the generator step-up (GSU) transformer status must be properly modeled. In power flow studies, generators are typically removed from service without disconnecting their GSU transformers since the GSU transformer status has little to no impact on the power flow result. In reality, removing a generator from service involves also removing its GSU transformer from service. This difference presents a discrepancy between simulated behavior and system observations during geomagnetic disturbance (GMD) events. This paper presents GMD case studies on 2000-bus and 24,000-bus systems in which reactive power losses and geomagnetically induced currents are compared across scenarios in which the GSU transformers for the disconnected generators are either in-service or out-of-service. The results demonstrate a 3.2 to 15.5 percent error in reactive power losses when the GSU status is modeled incorrectly. Discrepancies of up to 95 A per phase for branch GIC flows and 450 A for transformer neutral GIC flows are also observed and visualized.",
    "authors": [
      "Jessica L. Wert",
      "Pooria Dehghanian",
      "Jonathan Snodgrass",
      "Thomas J. Overbye"
    ],
    "published": "2022-08-23T23:39:06+00:00",
    "url": "http://arxiv.org/pdf/2208.11232v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2210.03985v1",
    "title": "Bird-Eye Transformers for Text Generation Models",
    "content": "Bird-Eye Transformers for Text Generation Models. Transformers have become an indispensable module for text generation models since their great success in machine translation. Previous works attribute the~success of transformers to the query-key-value dot-product attention, which provides a robust inductive bias by the fully connected token graphs. However, we found that self-attention has a severe limitation. When predicting the (i+1)-th token, self-attention only takes the i-th token as an information collector, and it tends to give a high attention weight to those tokens similar to itself. Therefore, most of the historical information that occurred before the i-th token is not taken into consideration. Based on this observation, in this paper, we propose a new architecture, called bird-eye transformer(BET), which goes one step further to improve the performance of transformers by reweighting self-attention to encourage it to focus more on important historical information. We have conducted experiments on multiple text generation tasks, including machine translation (2 datasets) and language models (3 datasets). These experimental~results show that our proposed model achieves a better performance than the baseline transformer architectures on~all~datasets. The code is released at: \\url{https://sites.google.com/view/bet-transformer/home}.",
    "authors": [
      "Lei Sha",
      "Yuhang Song",
      "Yordan Yordanov",
      "Tommaso Salvatori",
      "Thomas Lukasiewicz"
    ],
    "published": "2022-10-08T09:51:15+00:00",
    "url": "http://arxiv.org/pdf/2210.03985v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2310.03108v1",
    "title": "Reinforcement Learning-based Mixture of Vision Transformers for Video Violence Recognition",
    "content": "Reinforcement Learning-based Mixture of Vision Transformers for Video Violence Recognition. Video violence recognition based on deep learning concerns accurate yet scalable human violence recognition. Currently, most state-of-the-art video violence recognition studies use CNN-based models to represent and categorize videos. However, recent studies suggest that pre-trained transformers are more accurate than CNN-based models on various video analysis benchmarks. Yet these models are not thoroughly evaluated for video violence recognition. This paper introduces a novel transformer-based Mixture of Experts (MoE) video violence recognition system. Through an intelligent combination of large vision transformers and efficient transformer architectures, the proposed system not only takes advantage of the vision transformer architecture but also reduces the cost of utilizing large vision transformers. The proposed architecture maximizes violence recognition system accuracy while actively reducing computational costs through a reinforcement learning-based router. The empirical results show the proposed MoE architecture's superiority over CNN-based models by achieving 92.4% accuracy on the RWF dataset.",
    "authors": [
      "Hamid Mohammadi",
      "Ehsan Nazerfard",
      "Tahereh Firoozi"
    ],
    "published": "2023-10-04T18:58:47+00:00",
    "url": "http://arxiv.org/pdf/2310.03108v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2312.00751v1",
    "title": "Mitigating Over-smoothing in Transformers via Regularized Nonlocal Functionals",
    "content": "Mitigating Over-smoothing in Transformers via Regularized Nonlocal Functionals. Transformers have achieved remarkable success in a wide range of natural language processing and computer vision applications. However, the representation capacity of a deep transformer model is degraded due to the over-smoothing issue in which the token representations become identical when the model's depth grows. In this work, we show that self-attention layers in transformers minimize a functional which promotes smoothness, thereby causing token uniformity. We then propose a novel regularizer that penalizes the norm of the difference between the smooth output tokens from self-attention and the input tokens to preserve the fidelity of the tokens. Minimizing the resulting regularized energy functional, we derive the Neural Transformer with a Regularized Nonlocal Functional (NeuTRENO), a novel class of transformer models that can mitigate the over-smoothing issue. We empirically demonstrate the advantages of NeuTRENO over the baseline transformers and state-of-the-art methods in reducing the over-smoothing of token representations on various practical tasks, including object classification, image segmentation, and language modeling.",
    "authors": [
      "Tam Nguyen",
      "Tan M. Nguyen",
      "Richard G. Baraniuk"
    ],
    "published": "2023-12-01T17:52:47+00:00",
    "url": "http://arxiv.org/pdf/2312.00751v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2401.04301v3",
    "title": "Setting the Record Straight on Transformer Oversmoothing",
    "content": "Setting the Record Straight on Transformer Oversmoothing. Transformer-based models have recently become wildly successful across a diverse set of domains. At the same time, recent work has shown empirically and theoretically that Transformers are inherently limited. Specifically, they argue that as model depth increases, Transformers oversmooth, i.e., inputs become more and more similar. A natural question is: How can Transformers achieve these successes given this shortcoming? In this work we test these observations empirically and theoretically and uncover a number of surprising findings. We find that there are cases where feature similarity increases but, contrary to prior results, this is not inevitable, even for existing pre-trained models. Theoretically, we show that smoothing behavior depends on the eigenspectrum of the value and projection weights. We verify this empirically and observe that the sign of layer normalization weights can influence this effect. Our analysis reveals a simple way to parameterize the weights of the Transformer update equations to influence smoothing behavior. We hope that our findings give ML researchers and practitioners additional insight into how to develop future Transformer-based models.",
    "authors": [
      "Gbètondji J-S Dovonon",
      "Michael M. Bronstein",
      "Matt J. Kusner"
    ],
    "published": "2024-01-09T01:19:03+00:00",
    "url": "http://arxiv.org/pdf/2401.04301v3"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2405.16039v2",
    "title": "MoEUT: Mixture-of-Experts Universal Transformers",
    "content": "MoEUT: Mixture-of-Experts Universal Transformers. Previous work on Universal Transformers (UTs) has demonstrated the importance of parameter sharing across layers. By allowing recurrence in depth, UTs have advantages over standard Transformers in learning compositional generalizations, but layer-sharing comes with a practical limitation of parameter-compute ratio: it drastically reduces the parameter count compared to the non-shared model with the same dimensionality. Naively scaling up the layer size to compensate for the loss of parameters makes its computational resource requirements prohibitive. In practice, no previous work has succeeded in proposing a shared-layer Transformer design that is competitive in parameter count-dominated tasks such as language modeling. Here we propose MoEUT (pronounced \"moot\"), an effective mixture-of-experts (MoE)-based shared-layer Transformer architecture, which combines several recent advances in MoEs for both feedforward and attention layers of standard Transformers together with novel layer-normalization and grouping schemes that are specific and crucial to UTs. The resulting UT model, for the first time, slightly outperforms standard Transformers on language modeling tasks such as BLiMP and PIQA, while using significantly less compute and memory.",
    "authors": [
      "Róbert Csordás",
      "Kazuki Irie",
      "Jürgen Schmidhuber",
      "Christopher Potts",
      "Christopher D. Manning"
    ],
    "published": "2024-05-25T03:24:32+00:00",
    "url": "http://arxiv.org/pdf/2405.16039v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2406.11274v1",
    "title": "Skip-Layer Attention: Bridging Abstract and Detailed Dependencies in Transformers",
    "content": "Skip-Layer Attention: Bridging Abstract and Detailed Dependencies in Transformers. The Transformer architecture has significantly advanced deep learning, particularly in natural language processing, by effectively managing long-range dependencies. However, as the demand for understanding complex relationships grows, refining the Transformer's architecture becomes critical. This paper introduces Skip-Layer Attention (SLA) to enhance Transformer models by enabling direct attention between non-adjacent layers. This method improves the model's ability to capture dependencies between high-level abstract features and low-level details. By facilitating direct attention between these diverse feature levels, our approach overcomes the limitations of current Transformers, which often rely on suboptimal intra-layer attention. Our implementation extends the Transformer's functionality by enabling queries in a given layer to interact with keys and values from both the current layer and one preceding layer, thus enhancing the diversity of multi-head attention without additional computational burden. Extensive experiments demonstrate that our enhanced Transformer model achieves superior performance in language modeling tasks, highlighting the effectiveness of our skip-layer attention mechanism.",
    "authors": [
      "Qian Chen",
      "Wen Wang",
      "Qinglin Zhang",
      "Siqi Zheng",
      "Shiliang Zhang",
      "Chong Deng",
      "Hai Yu",
      "Jiaqing Liu",
      "Yukun Ma",
      "Chong Zhang"
    ],
    "published": "2024-06-17T07:24:38+00:00",
    "url": "http://arxiv.org/pdf/2406.11274v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2408.03397v1",
    "title": "HeTraX: Energy Efficient 3D Heterogeneous Manycore Architecture for Transformer Acceleration",
    "content": "HeTraX: Energy Efficient 3D Heterogeneous Manycore Architecture for Transformer Acceleration. Transformers have revolutionized deep learning and generative modeling to enable unprecedented advancements in natural language processing tasks and beyond. However, designing hardware accelerators for executing transformer models is challenging due to the wide variety of computing kernels involved in the transformer architecture. Existing accelerators are either inadequate to accelerate end-to-end transformer models or suffer notable thermal limitations. In this paper, we propose the design of a three-dimensional heterogeneous architecture referred to as HeTraX specifically optimized to accelerate end-to-end transformer models. HeTraX employs hardware resources aligned with the computational kernels of transformers and optimizes both performance and energy. Experimental results show that HeTraX outperforms existing state-of-the-art by up to 5.6x in speedup and improves EDP by 14.5x while ensuring thermally feasibility.",
    "authors": [
      "Pratyush Dhingra",
      "Janardhan Rao Doppa",
      "Partha Pratim Pande"
    ],
    "published": "2024-08-06T18:48:01+00:00",
    "url": "http://arxiv.org/pdf/2408.03397v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2408.04619v1",
    "title": "Transformer Explainer: Interactive Learning of Text-Generative Models",
    "content": "Transformer Explainer: Interactive Learning of Text-Generative Models. Transformers have revolutionized machine learning, yet their inner workings remain opaque to many. We present Transformer Explainer, an interactive visualization tool designed for non-experts to learn about Transformers through the GPT-2 model. Our tool helps users understand complex Transformer concepts by integrating a model overview and enabling smooth transitions across abstraction levels of mathematical operations and model structures. It runs a live GPT-2 instance locally in the user's browser, empowering users to experiment with their own input and observe in real-time how the internal components and parameters of the Transformer work together to predict the next tokens. Our tool requires no installation or special hardware, broadening the public's education access to modern generative AI techniques. Our open-sourced tool is available at https://poloclub.github.io/transformer-explainer/. A video demo is available at https://youtu.be/ECR4oAwocjs.",
    "authors": [
      "Aeree Cho",
      "Grace C. Kim",
      "Alexander Karpekov",
      "Alec Helbling",
      "Zijie J. Wang",
      "Seongmin Lee",
      "Benjamin Hoover",
      "Duen Horng Chau"
    ],
    "published": "2024-08-08T17:49:07+00:00",
    "url": "http://arxiv.org/pdf/2408.04619v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2409.14091v2",
    "title": "Normalized Narrow Jump To Conclusions: Normalized Narrow Shortcuts for Parameter Efficient Early Exit Transformer Prediction",
    "content": "Normalized Narrow Jump To Conclusions: Normalized Narrow Shortcuts for Parameter Efficient Early Exit Transformer Prediction. With the size and cost of large transformer-based language models growing, recently, there has been interest in shortcut casting of early transformer hidden-representations to final-representations for cheaper model inference. In particular, shortcutting pre-trained transformers with linear transformations over early layers has been shown to improve precision in early inference. However, for large language models, even this becomes computationally expensive. In this work, we propose Narrow Jump to Conclusions (NJTC) and Normalized Narrow Jump to Conclusions (N-NJTC) - parameter efficient alternatives to standard linear shortcutting that reduces shortcut parameter count by over 97%. We show that N-NJTC reliably outperforms Identity shortcuts at early stages and offers stable precision from all transformer block levels for GPT-2-XL, Phi3-Mini and Llama2-7B transformer models, demonstrating the viability of more parameter efficient short-cutting approaches.",
    "authors": [
      "Amrit Diggavi Seshadri"
    ],
    "published": "2024-09-21T10:09:26+00:00",
    "url": "http://arxiv.org/pdf/2409.14091v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1112.5640v5",
    "title": "Learning Smooth Pattern Transformation Manifolds",
    "content": "Learning Smooth Pattern Transformation Manifolds. Manifold models provide low-dimensional representations that are useful for processing and analyzing data in a transformation-invariant way. In this paper, we study the problem of learning smooth pattern transformation manifolds from image sets that represent observations of geometrically transformed signals. In order to construct a manifold, we build a representative pattern whose transformations accurately fit various input images. We examine two objectives of the manifold building problem, namely, approximation and classification. For the approximation problem, we propose a greedy method that constructs a representative pattern by selecting analytic atoms from a continuous dictionary manifold. We present a DC (Difference-of-Convex) optimization scheme that is applicable to a wide range of transformation and dictionary models, and demonstrate its application to transformation manifolds generated by rotation, translation and anisotropic scaling of a reference pattern. Then, we generalize this approach to a setting with multiple transformation manifolds, where each manifold represents a different class of signals. We present an iterative multiple manifold building algorithm such that the classification accuracy is promoted in the learning of the representative patterns. Experimental results suggest that the proposed methods yield high accuracy in the approximation and classification of data compared to some reference methods, while the invariance to geometric transformations is achieved due to the transformation manifold model.",
    "authors": [
      "Elif Vural",
      "Pascal Frossard"
    ],
    "published": "2011-12-23T19:13:31+00:00",
    "url": "http://arxiv.org/pdf/1112.5640v5"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2212.07677v2",
    "title": "Transformers learn in-context by gradient descent",
    "content": "Transformers learn in-context by gradient descent. At present, the mechanisms of in-context learning in Transformers are not well understood and remain mostly an intuition. In this paper, we suggest that training Transformers on auto-regressive objectives is closely related to gradient-based meta-learning formulations. We start by providing a simple weight construction that shows the equivalence of data transformations induced by 1) a single linear self-attention layer and by 2) gradient-descent (GD) on a regression loss. Motivated by that construction, we show empirically that when training self-attention-only Transformers on simple regression tasks either the models learned by GD and Transformers show great similarity or, remarkably, the weights found by optimization match the construction. Thus we show how trained Transformers become mesa-optimizers i.e. learn models by gradient descent in their forward pass. This allows us, at least in the domain of regression problems, to mechanistically understand the inner workings of in-context learning in optimized Transformers. Building on this insight, we furthermore identify how Transformers surpass the performance of plain gradient descent by learning an iterative curvature correction and learn linear models on deep data representations to solve non-linear regression tasks. Finally, we discuss intriguing parallels to a mechanism identified to be crucial for in-context learning termed induction-head (Olsson et al., 2022) and show how it could be understood as a specific case of in-context learning by gradient descent learning within Transformers. Code to reproduce the experiments can be found at https://github.com/google-research/self-organising-systems/tree/master/transformers_learn_icl_by_gd .",
    "authors": [
      "Johannes von Oswald",
      "Eyvind Niklasson",
      "Ettore Randazzo",
      "João Sacramento",
      "Alexander Mordvintsev",
      "Andrey Zhmoginov",
      "Max Vladymyrov"
    ],
    "published": "2022-12-15T09:21:21+00:00",
    "url": "http://arxiv.org/pdf/2212.07677v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2406.05212v2",
    "title": "A Matrix Exponential Generalization of the Laplace Transform of Poisson Shot Noise",
    "content": "A Matrix Exponential Generalization of the Laplace Transform of Poisson Shot Noise. We consider a generalization of the Laplace transform of Poisson shot noise defined as an integral transform with respect to a matrix exponential. We denote this as the matrix Laplace transform and establish that it is in general a matrix function extension of the scalar Laplace transform. We show that the matrix Laplace transform of Poisson shot noise admits an expression analogous to that implied by Campbell's theorem. We demonstrate the utility of this generalization of Campbell's theorem in two important applications: the characterization of a Poisson shot noise process and the derivation of the complementary CDF (CCDF) and meta-distribution of signal-to-interference-and-noise (SINR) models in Poisson networks. In the former application, we demonstrate how the higher order moments of Poisson shot noise may be obtained directly from the elements of its matrix Laplace transform. We further show how the CCDF of this object may be bounded using a summation of the first row of its matrix Laplace transform. For the latter application, we show how the CCDF of SINR models with phase-type distributed desired signal power may be obtained via an expectation of the matrix Laplace transform of the interference and noise, analogous to the canonical case of SINR models with Rayleigh fading. Additionally, when the power of the desired signal is exponentially distributed, we establish that the meta-distribution may be obtained in terms of the limit of a sequence expressed in terms of the matrix Laplace transform of a related Poisson shot noise process.",
    "authors": [
      "Nicholas R. Olson",
      "Jeffrey G. Andrews"
    ],
    "published": "2024-06-07T18:52:58+00:00",
    "url": "http://arxiv.org/pdf/2406.05212v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2406.13153v1",
    "title": "SwinStyleformer is a favorable choice for image inversion",
    "content": "SwinStyleformer is a favorable choice for image inversion. This paper proposes the first pure Transformer structure inversion network called SwinStyleformer, which can compensate for the shortcomings of the CNNs inversion framework by handling long-range dependencies and learning the global structure of objects. Experiments found that the inversion network with the Transformer backbone could not successfully invert the image. The above phenomena arise from the differences between CNNs and Transformers, such as the self-attention weights favoring image structure ignoring image details compared to convolution, the lack of multi-scale properties of Transformer, and the distribution differences between the latent code extracted by the Transformer and the StyleGAN style vector. To address these differences, we employ the Swin Transformer with a smaller window size as the backbone of the SwinStyleformer to enhance the local detail of the inversion image. Meanwhile, we design a Transformer block based on learnable queries. Compared to the self-attention transformer block, the Transformer block based on learnable queries provides greater adaptability and flexibility, enabling the model to update the attention weights according to specific tasks. Thus, the inversion focus is not limited to the image structure. To further introduce multi-scale properties, we design multi-scale connections in the extraction of feature maps. Multi-scale connections allow the model to gain a comprehensive understanding of the image to avoid loss of detail due to global modeling. Moreover, we propose an inversion discriminator and distribution alignment loss to minimize the distribution differences. Based on the above designs, our SwinStyleformer successfully solves the Transformer's inversion failure issue and demonstrates SOTA performance in image inversion and several related vision tasks.",
    "authors": [
      "Jiawei Mao",
      "Guangyi Zhao",
      "Xuesong Yin",
      "Yuanqi Chang"
    ],
    "published": "2024-06-19T02:08:45+00:00",
    "url": "http://arxiv.org/pdf/2406.13153v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2502.16762v1",
    "title": "A Transformer-in-Transformer Network Utilizing Knowledge Distillation for Image Recognition",
    "content": "A Transformer-in-Transformer Network Utilizing Knowledge Distillation for Image Recognition. This paper presents a novel knowledge distillation neural architecture leveraging efficient transformer networks for effective image classification. Natural images display intricate arrangements encompassing numerous extraneous elements. Vision transformers utilize localized patches to compute attention. However, exclusive dependence on patch segmentation proves inadequate in sufficiently encompassing the comprehensive nature of the image. To address this issue, we have proposed an inner-outer transformer-based architecture, which gives attention to the global and local aspects of the image. Moreover, The training of transformer models poses significant challenges due to their demanding resource, time, and data requirements. To tackle this, we integrate knowledge distillation into the architecture, enabling efficient learning. Leveraging insights from a larger teacher model, our approach enhances learning efficiency and effectiveness. Significantly, the transformer-in-transformer network acquires lightweight characteristics by means of distillation conducted within the feature extraction layer. Our featured network's robustness is established through substantial experimentation on the MNIST, CIFAR10, and CIFAR100 datasets, demonstrating commendable top-1 and top-5 accuracy. The conducted ablative analysis comprehensively validates the effectiveness of the chosen parameters and settings, showcasing their superiority against contemporary methodologies. Remarkably, the proposed Transformer-in-Transformer Network (TITN) model achieves impressive performance milestones across various datasets: securing the highest top-1 accuracy of 74.71% and a top-5 accuracy of 92.28% for the CIFAR100 dataset, attaining an unparalleled top-1 accuracy of 92.03% and top-5 accuracy of 99.80% for the CIFAR-10 dataset, and registering an exceptional top-1 accuracy of 99.56% for the MNIST dataset.",
    "authors": [
      "Dewan Tauhid Rahman",
      "Yeahia Sarker",
      "Antar Mazumder",
      "Md. Shamim Anower"
    ],
    "published": "2025-02-24T00:41:46+00:00",
    "url": "http://arxiv.org/pdf/2502.16762v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1907.11579v1",
    "title": "An asymptotically optimal transform of Pearson's correlation statistic",
    "content": "An asymptotically optimal transform of Pearson's correlation statistic. It is shown that for any correlation-parametrized model of dependence and any given significance level $\\alpha\\in(0,1)$, there is an asymptotically optimal transform of Pearson's correlation statistic $R$, for which the generally leading error term for the normal approximation vanishes for all values $\\rho\\in(-1,1)$ of the correlation coefficient.   This general result is then applied to the bivariate normal (BVN) model of dependence and to what is referred to in this paper as the SquareV model. In the BVN model, Pearson's $R$ turns out to be asymptotically optimal for a rather unusual significance level $\\alpha\\approx0.240$, whereas Fisher's transform $R_F$ of $R$ is asymptotically optimal for the limit significance level $\\alpha=0$. In the SquareV model, Pearson's $R$ is asymptotically optimal for a still rather high significance level $\\alpha\\approx0.159$, whereas Fisher's transform $R_F$ of $R$ is not asymptotically optimal for any $\\alpha\\in[0,1]$. Moreover, it is shown that in both the BVN model and the SquareV model, the transform optimal for a given value of $\\alpha$ is in fact asymptotically better than $R$ and $R_F$ in wide ranges of values of the significance level, including $\\alpha$ itself.   Extensive computer simulations for the BVN and SquareV models of dependence are presented, which suggest that, for sample sizes $n\\ge100$ and significance levels $\\alpha\\in\\{0.01,0.05\\}$, the mentioned asymptotically optimal transform of $R$ generally outperforms both Pearson's $R$ and Fisher's transform $R_F$ of $R$, the latter appearing generally much inferior to both $R$ and the asymptotically optimal transform of $R$ in the SquareV model.",
    "authors": [
      "Iosif Pinelis"
    ],
    "published": "2019-07-26T13:53:49+00:00",
    "url": "http://arxiv.org/pdf/1907.11579v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2411.09339v1",
    "title": "Re-Parameterization of Lightweight Transformer for On-Device Speech Emotion Recognition",
    "content": "Re-Parameterization of Lightweight Transformer for On-Device Speech Emotion Recognition. With the increasing implementation of machine learning models on edge or Internet-of-Things (IoT) devices, deploying advanced models on resource-constrained IoT devices remains challenging. Transformer models, a currently dominant neural architecture, have achieved great success in broad domains but their complexity hinders its deployment on IoT devices with limited computation capability and storage size. Although many model compression approaches have been explored, they often suffer from notorious performance degradation. To address this issue, we introduce a new method, namely Transformer Re-parameterization, to boost the performance of lightweight Transformer models. It consists of two processes: the High-Rank Factorization (HRF) process in the training stage and the deHigh-Rank Factorization (deHRF) process in the inference stage. In the former process, we insert an additional linear layer before the Feed-Forward Network (FFN) of the lightweight Transformer. It is supposed that the inserted HRF layers can enhance the model learning capability. In the later process, the auxiliary HRF layer will be merged together with the following FFN layer into one linear layer and thus recover the original structure of the lightweight model. To examine the effectiveness of the proposed method, we evaluate it on three widely used Transformer variants, i.e., ConvTransformer, Conformer, and SpeechFormer networks, in the application of speech emotion recognition on the IEMOCAP, M3ED and DAIC-WOZ datasets. Experimental results show that our proposed method consistently improves the performance of lightweight Transformers, even making them comparable to large models. The proposed re-parameterization approach enables advanced Transformer models to be deployed on resource-constrained IoT devices.",
    "authors": [
      "Zixing Zhang",
      "Zhongren Dong",
      "Weixiang Xu",
      "Jing Han"
    ],
    "published": "2024-11-14T10:36:19+00:00",
    "url": "http://arxiv.org/pdf/2411.09339v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2501.15452v1",
    "title": "Identifying Critical Tokens for Accurate Predictions in Transformer-based Medical Imaging Models",
    "content": "Identifying Critical Tokens for Accurate Predictions in Transformer-based Medical Imaging Models. With the advancements in self-supervised learning (SSL), transformer-based computer vision models have recently demonstrated superior results compared to convolutional neural networks (CNNs) and are poised to dominate the field of artificial intelligence (AI)-based medical imaging in the upcoming years. Nevertheless, similar to CNNs, unveiling the decision-making process of transformer-based models remains a challenge. In this work, we take a step towards demystifying the decision-making process of transformer-based medical imaging models and propose Token Insight, a novel method that identifies the critical tokens that contribute to the prediction made by the model. Our method relies on the principled approach of token discarding native to transformer-based models, requires no additional module, and can be applied to any transformer model. Using the proposed approach, we quantify the importance of each token based on its contribution to the prediction and enable a more nuanced understanding of the model's decisions. Our experimental results which are showcased on the problem of colonic polyp identification using both supervised and self-supervised pretrained vision transformers indicate that Token Insight contributes to a more transparent and interpretable transformer-based medical imaging model, fostering trust and facilitating broader adoption in clinical settings.",
    "authors": [
      "Solha Kang",
      "Joris Vankerschaver",
      "Utku Ozbulak"
    ],
    "published": "2025-01-26T08:49:13+00:00",
    "url": "http://arxiv.org/pdf/2501.15452v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/cs/0112005v1",
    "title": "Universal Model for Paraphrasing -- Using Transformation Based on a Defined Criteria --",
    "content": "Universal Model for Paraphrasing -- Using Transformation Based on a Defined Criteria --. This paper describes a universal model for paraphrasing that transforms according to defined criteria. We showed that by using different criteria we could construct different kinds of paraphrasing systems including one for answering questions, one for compressing sentences, one for polishing up, and one for transforming written language to spoken language.",
    "authors": [
      "Masaki Murata",
      "Hitoshi Isahara"
    ],
    "published": "2001-12-05T05:56:13+00:00",
    "url": "http://arxiv.org/pdf/cs/0112005v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/hep-th/9401098v1",
    "title": "Exact Duality and Nilpotent Gauging",
    "content": "Exact Duality and Nilpotent Gauging. We obtain new duality transformations relating some exact string backgrounds, by defining the nilpotent duality. We show that the ungauged $SL(2, R)$ WZW model transforms by its action into the three dimensional plane wave geometry. We also give the inverse transformation from the plane wave to the $SL(2, R)$ model and discuss the implications of the results.",
    "authors": [
      "Alok Kumar",
      "Swapna Mahapatra"
    ],
    "published": "1994-01-20T14:06:47+00:00",
    "url": "http://arxiv.org/pdf/hep-th/9401098v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/hep-th/9907138v1",
    "title": "Some Generalized BRS Transformations. II A Quantum Gravity Model",
    "content": "Some Generalized BRS Transformations. II A Quantum Gravity Model. Generalized BRS transformations such as introduced in Part I (hep-th/9906245) are applied to a model of quantum gravity. This development is technically complex; but at the least should illustrate how much less rigid and more general of application are the new BRS transformations.",
    "authors": [
      "Paul Federbush"
    ],
    "published": "1999-07-16T14:59:11+00:00",
    "url": "http://arxiv.org/pdf/hep-th/9907138v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/hep-th/0106172v2",
    "title": "New duality transformation in two-dimensional non-linear sigma models",
    "content": "New duality transformation in two-dimensional non-linear sigma models. A new T-duality transformation is found in two-dimensional non-linear sigma models. This is a straightforward generalisation of Abelian and non-Abelian T-dualities.",
    "authors": [
      "N. Mohammedi"
    ],
    "published": "2001-06-19T15:33:07+00:00",
    "url": "http://arxiv.org/pdf/hep-th/0106172v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1002.0254v1",
    "title": "A field-theoretical approach to the extended Hubbard model",
    "content": "A field-theoretical approach to the extended Hubbard model. We transform the quartic Hubbard terms in the extended Hubbard model to a quadratic form by making the Hubbard-Stratonovich transformation for the electron operators. This transformation allows us to derive exact results for mass operator and charge-charge and spin-spin correlation functions for s-wave superconductivity. We discuss the application of the method to the d-wave superconductivity.",
    "authors": [
      "Z. Koinov"
    ],
    "published": "2010-02-01T14:09:54+00:00",
    "url": "http://arxiv.org/pdf/1002.0254v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1308.2340v1",
    "title": "A model for rank one measure preserving transformations",
    "content": "A model for rank one measure preserving transformations. We define a model for rank one measure preserving transformations in the sense of [2]. This is done by defining a new Polish topology on the space of codes, which are infinite rank one words, for symbolic rank one systems. We establish that this space of codes has the same generic dynamical properties as the space of (rank one) measure preserving transformations on the unit interval.",
    "authors": [
      "Su Gao",
      "Aaron Hill"
    ],
    "published": "2013-08-10T20:50:17+00:00",
    "url": "http://arxiv.org/pdf/1308.2340v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1312.0347v1",
    "title": "Solving the TTC 2013 Flowgraphs Case with FunnyQT",
    "content": "Solving the TTC 2013 Flowgraphs Case with FunnyQT. FunnyQT is a model querying and model transformation library for the functional Lisp-dialect Clojure providing a rich and efficient querying and transformation API.   This paper describes the FunnyQT solution to the TTC 2013 Flowgraphs Transformation Case. It solves all four tasks, and it has won the best efficiency award for this case.",
    "authors": [
      "Tassilo Horn"
    ],
    "published": "2013-12-02T06:59:51+00:00",
    "url": "http://arxiv.org/pdf/1312.0347v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1401.3624v1",
    "title": "Numerical Solution of the Model for HIV Infection of Cd4+T Cells by Using Multistage Differential Transform Method",
    "content": "Numerical Solution of the Model for HIV Infection of Cd4+T Cells by Using Multistage Differential Transform Method. The Multistage Differential Transform Method (MDTM) is employed to solve the model for HIV infection of CD4+T cells. Comparing the numerical results to those obtained by the classical fourth order Runge-Kutta method showed the preciseness and efficacy of the multistep differential transform method. The study shows that the method is a powerful and promising tool for solving coupled systems of differential equations.",
    "authors": [
      "Olusola Kolebaje",
      "Emmanuel Oyewande",
      "Babatunde Majolagbe"
    ],
    "published": "2014-01-10T00:17:16+00:00",
    "url": "http://arxiv.org/pdf/1401.3624v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1506.01219v1",
    "title": "Stress effects on the kinetics of adsorption in a spherical particle: an analytical model",
    "content": "Stress effects on the kinetics of adsorption in a spherical particle: an analytical model. We consider a two-phase elastic solid subject to diffusion-induced phase transformation by an interstitial species provided by a reservoir. We derive a simple analytical model to quantify the effect of misfit strain on the kinetic of phase transformation and to calculate the amplitude of the well-know hysteresis cycle observed when a sequence of forward and reverse phase transformations takes place.",
    "authors": [
      "Fernando Pereira Duda",
      "Giuseppe Tomassetti"
    ],
    "published": "2015-01-05T06:46:34+00:00",
    "url": "http://arxiv.org/pdf/1506.01219v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1908.09801v1",
    "title": "Differential Transformation of a Motor Load Model for Time-Domain Simulation",
    "content": "Differential Transformation of a Motor Load Model for Time-Domain Simulation. The Differential Transformation (DT) method has demonstrated its potential in speeding up power system time-domain simulation by our previous work. This letter further derives DTs about a motor load model and proves that the nonlinear current injection equation about a motor load can be transformed into a linear equation by means of DT.",
    "authors": [
      "Yang Liu",
      "Kai Sun"
    ],
    "published": "2019-08-23T12:29:16+00:00",
    "url": "http://arxiv.org/pdf/1908.09801v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2311.02985v2",
    "title": "Towards a Transformer-Based Reverse Dictionary Model for Quality Estimation of Definitions",
    "content": "Towards a Transformer-Based Reverse Dictionary Model for Quality Estimation of Definitions. In the last years, several variants of transformers have emerged. In this paper, we compare different transformer-based models for solving the reverse dictionary task and explore their use in the context of a serious game called The Dictionary Game.",
    "authors": [
      "Julien Guité-Vinet",
      "Alexandre Blondin Massé",
      "Fatiha Sadat"
    ],
    "published": "2023-11-06T09:42:44+00:00",
    "url": "http://arxiv.org/pdf/2311.02985v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2407.16403v1",
    "title": "Jordan-Wigner transformation constructed for spinful fermions at S=1/2 spins in one dimension",
    "content": "Jordan-Wigner transformation constructed for spinful fermions at S=1/2 spins in one dimension. An exact Jordan-Wigner type of transformation is presented in 1D connecting spin-1/2 operators to spinful canonical Fermi operators. The transformation contains two free parameters allowing a broad interconnection possibility in between spin models and fermionic models containing spinful Fermi operators.",
    "authors": [
      "Zsolt Gulacsi"
    ],
    "published": "2024-07-23T11:45:33+00:00",
    "url": "http://arxiv.org/pdf/2407.16403v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1904.08378v1",
    "title": "Dynamic Evaluation of Transformer Language Models",
    "content": "Dynamic Evaluation of Transformer Language Models. This research note combines two methods that have recently improved the state of the art in language modeling: Transformers and dynamic evaluation. Transformers use stacked layers of self-attention that allow them to capture long range dependencies in sequential data. Dynamic evaluation fits models to the recent sequence history, allowing them to assign higher probabilities to re-occurring sequential patterns. By applying dynamic evaluation to Transformer-XL models, we improve the state of the art on enwik8 from 0.99 to 0.94 bits/char, text8 from 1.08 to 1.04 bits/char, and WikiText-103 from 18.3 to 16.4 perplexity points.",
    "authors": [
      "Ben Krause",
      "Emmanuel Kahembwe",
      "Iain Murray",
      "Steve Renals"
    ],
    "published": "2019-04-17T17:26:01+00:00",
    "url": "http://arxiv.org/pdf/1904.08378v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1908.00723v1",
    "title": "Universal Transforming Geometric Network",
    "content": "Universal Transforming Geometric Network. The recurrent geometric network (RGN), the first end-to-end differentiable neural architecture for protein structure prediction, is a competitive alternative to existing models. However, the RGN's use of recurrent neural networks (RNNs) as internal representations results in long training time and unstable gradients. And because of its sequential nature, it is less effective at learning global dependencies among amino acids than existing transformer architectures. We propose the Universal Transforming Geometric Network (UTGN), an end-to-end differentiable model that uses the encoder portion of the Universal Transformer architecture as an alternative for internal representations. Our experiments show that compared to RGN, UTGN achieve a $1.7$ \\si{\\angstrom} improvement on the free modeling portion and a $0.7$ \\si{\\angstrom} improvement on the template based modeling of the CASP12 competition.",
    "authors": [
      "Jin Li"
    ],
    "published": "2019-08-02T07:14:08+00:00",
    "url": "http://arxiv.org/pdf/1908.00723v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1908.10924v1",
    "title": "Solving Math Word Problems with Double-Decoder Transformer",
    "content": "Solving Math Word Problems with Double-Decoder Transformer. This paper proposes a Transformer-based model to generate equations for math word problems. It achieves much better results than RNN models when copy and align mechanisms are not used, and can outperform complex copy and align RNN models. We also show that training a Transformer jointly in a generation task with two decoders, left-to-right and right-to-left, is beneficial. Such a Transformer performs better than the one with just one decoder not only because of the ensemble effect, but also because it improves the encoder training procedure. We also experiment with adding reinforcement learning to our model, showing improved performance compared to MLE training.",
    "authors": [
      "Yuanliang Meng",
      "Anna Rumshisky"
    ],
    "published": "2019-08-28T19:42:37+00:00",
    "url": "http://arxiv.org/pdf/1908.10924v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2103.17257v1",
    "title": "Teaching Electrical Model of Power Transformers to Undergraduate Students: Magnetic Circuit Approach",
    "content": "Teaching Electrical Model of Power Transformers to Undergraduate Students: Magnetic Circuit Approach. This paper explains a unified approach for teaching the electrical model of power transformers to undergraduate students using magnetic circuits. The commonly used approach for explaining the electrical model of power transformers is a hybrid approach in which magnetic circuits are used to explain the presence of series inductances. However, the presence of shunt inductance and resistance in the model is explained using alternative approaches. In contrary, this paper explains how both series and shunt elements can be described by using magnetic circuits. Moreover, three real-world examples and Matlab/Simulink results are provided to demonstrate how the presented explanations can be used to describe the responses of power transformers in real-world applications.",
    "authors": [
      "Saeed Lotfifard"
    ],
    "published": "2021-03-29T23:48:09+00:00",
    "url": "http://arxiv.org/pdf/2103.17257v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2102.05038v1",
    "title": "Last Query Transformer RNN for knowledge tracing",
    "content": "Last Query Transformer RNN for knowledge tracing. This paper presents an efficient model to predict a student's answer correctness given his past learning activities. Basically, I use both transformer encoder and RNN to deal with time series input. The novel point of the model is that it only uses the last input as query in transformer encoder, instead of all sequence, which makes QK matrix multiplication in transformer Encoder to have O(L) time complexity, instead of O(L^2). It allows the model to input longer sequence. Using this model I achieved the 1st place in the 'Riiid! Answer Correctness Prediction' competition hosted on kaggle.",
    "authors": [
      "SeungKee Jeon"
    ],
    "published": "2021-02-10T17:10:31+00:00",
    "url": "http://arxiv.org/pdf/2102.05038v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2204.14114v1",
    "title": "Developmental Negation Processing in Transformer Language Models",
    "content": "Developmental Negation Processing in Transformer Language Models. Reasoning using negation is known to be difficult for transformer-based language models. While previous studies have used the tools of psycholinguistics to probe a transformer's ability to reason over negation, none have focused on the types of negation studied in developmental psychology. We explore how well transformers can process such categories of negation, by framing the problem as a natural language inference (NLI) task. We curate a set of diagnostic questions for our target categories from popular NLI datasets and evaluate how well a suite of models reason over them. We find that models perform consistently better only on certain categories, suggesting clear distinctions in how they are processed.",
    "authors": [
      "Antonio Laverghetta Jr.",
      "John Licato"
    ],
    "published": "2022-04-29T14:07:34+00:00",
    "url": "http://arxiv.org/pdf/2204.14114v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2307.09723v1",
    "title": "Improving Domain Generalization for Sound Classification with Sparse Frequency-Regularized Transformer",
    "content": "Improving Domain Generalization for Sound Classification with Sparse Frequency-Regularized Transformer. Sound classification models' performance suffers from generalizing on out-of-distribution (OOD) data. Numerous methods have been proposed to help the model generalize. However, most either introduce inference overheads or focus on long-lasting CNN-variants, while Transformers has been proven to outperform CNNs on numerous natural language processing and computer vision tasks. We propose FRITO, an effective regularization technique on Transformer's self-attention, to improve the model's generalization ability by limiting each sequence position's attention receptive field along the frequency dimension on the spectrogram. Experiments show that our method helps Transformer models achieve SOTA generalization performance on TAU 2020 and Nsynth datasets while saving 20% inference time.",
    "authors": [
      "Honglin Mu",
      "Wentian Xia",
      "Wanxiang Che"
    ],
    "published": "2023-07-19T02:21:44+00:00",
    "url": "http://arxiv.org/pdf/2307.09723v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1504.02610v1",
    "title": "Confluence Detection for Transformations of Labelled Transition Systems",
    "content": "Confluence Detection for Transformations of Labelled Transition Systems. The development of complex component software systems can be made more manageable by first creating an abstract model and then incrementally adding details. Model transformation is an approach to add such details in a controlled way. In order for model transformation systems to be useful, it is crucial that they are confluent, i.e. that when applied on a given model, they will always produce a unique output model, independent of the order in which rules of the system are applied on the input. In this work, we consider Labelled Transition Systems (LTSs) to reason about the semantics of models, and LTS transformation systems to reason about model transformations. In related work, the problem of confluence detection has been investigated for general graph structures. We observe, however, that confluence can be detected more efficiently in special cases where the graphs have particular structural properties. In this paper, we present a number of observations to detect confluence of LTS transformation systems, and propose both a new confluence detection algorithm and a conflict resolution algorithm based on them.",
    "authors": [
      "Anton Wijs"
    ],
    "published": "2015-04-10T09:39:47+00:00",
    "url": "http://arxiv.org/pdf/1504.02610v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1906.09777v3",
    "title": "A Tensorized Transformer for Language Modeling",
    "content": "A Tensorized Transformer for Language Modeling. Latest development of neural models has connected the encoder and decoder through a self-attention mechanism. In particular, Transformer, which is solely based on self-attention, has led to breakthroughs in Natural Language Processing (NLP) tasks. However, the multi-head attention mechanism, as a key component of Transformer, limits the effective deployment of the model to a resource-limited setting. In this paper, based on the ideas of tensor decomposition and parameters sharing, we propose a novel self-attention model (namely Multi-linear attention) with Block-Term Tensor Decomposition (BTD). We test and verify the proposed attention method on three language modeling tasks (i.e., PTB, WikiText-103 and One-billion) and a neural machine translation task (i.e., WMT-2016 English-German). Multi-linear attention can not only largely compress the model parameters but also obtain performance improvements, compared with a number of language modeling approaches, such as Transformer, Transformer-XL, and Transformer with tensor train decomposition.",
    "authors": [
      "Xindian Ma",
      "Peng Zhang",
      "Shuai Zhang",
      "Nan Duan",
      "Yuexian Hou",
      "Dawei Song",
      "Ming Zhou"
    ],
    "published": "2019-06-24T08:28:37+00:00",
    "url": "http://arxiv.org/pdf/1906.09777v3"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1709.05589v1",
    "title": "Two electrostatic gyrokinetic models derived by two different perturbative methods",
    "content": "Two electrostatic gyrokinetic models derived by two different perturbative methods. This paper presents two different electrostatic gyrokinetic models derived through two different perturbative methods. One of the two models is just the conventional electrostatic gyrokinetic model, the derivation of which is repeated using the Lie transform perturbative method. One term is rectified in the derivation of the conventional model. To derive the other model, we use a new method, which is based on the covariant transform formula of the differential 1-form. The new method doesn't split the coordinate transform into the guiding-center transform and the gyrocenter transform. It carries out the coordinate transform up to the order equaling that of the amplitude of the perturbative wave. Compared with the conventional model, the finite Larmor radius terms are completely removed from the orbit equations of the new one, making it simpler for the numerical application.",
    "authors": [
      "Shuangxi Zhang"
    ],
    "published": "2017-09-17T01:18:26+00:00",
    "url": "http://arxiv.org/pdf/1709.05589v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1908.10408v1",
    "title": "Multiresolution Transformer Networks: Recurrence is Not Essential for Modeling Hierarchical Structure",
    "content": "Multiresolution Transformer Networks: Recurrence is Not Essential for Modeling Hierarchical Structure. The architecture of Transformer is based entirely on self-attention, and has been shown to outperform models that employ recurrence on sequence transduction tasks such as machine translation. The superior performance of Transformer has been attributed to propagating signals over shorter distances, between positions in the input and the output, compared to the recurrent architectures. We establish connections between the dynamics in Transformer and recurrent networks to argue that several factors including gradient flow along an ensemble of multiple weakly dependent paths play a paramount role in the success of Transformer. We then leverage the dynamics to introduce {\\em Multiresolution Transformer Networks} as the first architecture that exploits hierarchical structure in data via self-attention. Our models significantly outperform state-of-the-art recurrent and hierarchical recurrent models on two real-world datasets for query suggestion, namely, \\aol and \\amazon. In particular, on AOL data, our model registers at least 20\\% improvement on each precision score, and over 25\\% improvement on the BLEU score with respect to the best performing recurrent model. We thus provide strong evidence that recurrence is not essential for modeling hierarchical structure.",
    "authors": [
      "Vikas K. Garg",
      "Inderjit S. Dhillon",
      "Hsiang-Fu Yu"
    ],
    "published": "2019-08-27T18:51:50+00:00",
    "url": "http://arxiv.org/pdf/1908.10408v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1605.02373v2",
    "title": "The Functional Integral formulation of the Schrieffer-Wolff transformation",
    "content": "The Functional Integral formulation of the Schrieffer-Wolff transformation. We revisit the Schrieffer-Wolff transformation and present a path integral version of this important canonical transformation. The equivalence between the low-energy sector of the Anderson model in the so-called local moment regime and the spin-isotropic Kondo model is usually established via a canonical transformation performed on the Hamiltonian, followed by a projection. Here we present a path integral formulation of the Schrieffer-Wolff transformation which relates the functional integral form of the partition function of the Anderson model to that of its effective low-energy model. The resulting functional integral assumes the form of a spin path integral and includes a geometric phase factor, i.e. a Berry phase. Our approach stresses the underlying symmetries of the model and allows for a straightforward generalization of the transformation to more involved models. It thus not only sheds new light on a classic problem, it also offers a systematic route of obtaining effective low-energy models and higher order corrections.",
    "authors": [
      "Farzaneh Zamani",
      "Pedro Ribeiro",
      "Stefan Kirchner"
    ],
    "published": "2016-05-08T22:11:05+00:00",
    "url": "http://arxiv.org/pdf/1605.02373v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2005.10463v2",
    "title": "Simplified Self-Attention for Transformer-based End-to-End Speech Recognition",
    "content": "Simplified Self-Attention for Transformer-based End-to-End Speech Recognition. Transformer models have been introduced into end-to-end speech recognition with state-of-the-art performance on various tasks owing to their superiority in modeling long-term dependencies. However, such improvements are usually obtained through the use of very large neural networks. Transformer models mainly include two submodules - position-wise feedforward layers and self-attention (SAN) layers. In this paper, to reduce the model complexity while maintaining good performance, we propose a simplified self-attention (SSAN) layer which employs FSMN memory block instead of projection layers to form query and key vectors for transformer-based end-to-end speech recognition. We evaluate the SSAN-based and the conventional SAN-based transformers on the public AISHELL-1, internal 1000-hour and 20,000-hour large-scale Mandarin tasks. Results show that our proposed SSAN-based transformer model can achieve over 20% relative reduction in model parameters and 6.7% relative CER reduction on the AISHELL-1 task. With impressively 20% parameter reduction, our model shows no loss of recognition performance on the 20,000-hour large-scale task.",
    "authors": [
      "Haoneng Luo",
      "Shiliang Zhang",
      "Ming Lei",
      "Lei Xie"
    ],
    "published": "2020-05-21T04:55:59+00:00",
    "url": "http://arxiv.org/pdf/2005.10463v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2210.05832v1",
    "title": "SaiT: Sparse Vision Transformers through Adaptive Token Pruning",
    "content": "SaiT: Sparse Vision Transformers through Adaptive Token Pruning. While vision transformers have achieved impressive results, effectively and efficiently accelerating these models can further boost performances. In this work, we propose a dense/sparse training framework to obtain a unified model, enabling weight sharing across various token densities. Thus one model offers a range of accuracy and throughput tradeoffs for different applications. Besides, we introduce adaptive token pruning to optimize the patch token sparsity based on the input image. In addition, we investigate knowledge distillation to enhance token selection capability in early transformer modules. Sparse adaptive image Transformer (SaiT) offers varying levels of model acceleration by merely changing the token sparsity on the fly. Specifically, SaiT reduces the computation complexity (FLOPs) by 39% - 43% and increases the throughput by 67% - 91% with less than 0.5% accuracy loss for various vision transformer models. Meanwhile, the same model also provides the zero accuracy drop option by skipping the sparsification step. SaiT achieves better accuracy and computation tradeoffs than state-of-the-art transformer and convolutional models.",
    "authors": [
      "Ling Li",
      "David Thorsley",
      "Joseph Hassoun"
    ],
    "published": "2022-10-11T23:26:42+00:00",
    "url": "http://arxiv.org/pdf/2210.05832v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2305.12248v1",
    "title": "Brain encoding models based on multimodal transformers can transfer across language and vision",
    "content": "Brain encoding models based on multimodal transformers can transfer across language and vision. Encoding models have been used to assess how the human brain represents concepts in language and vision. While language and vision rely on similar concept representations, current encoding models are typically trained and tested on brain responses to each modality in isolation. Recent advances in multimodal pretraining have produced transformers that can extract aligned representations of concepts in language and vision. In this work, we used representations from multimodal transformers to train encoding models that can transfer across fMRI responses to stories and movies. We found that encoding models trained on brain responses to one modality can successfully predict brain responses to the other modality, particularly in cortical regions that represent conceptual meaning. Further analysis of these encoding models revealed shared semantic dimensions that underlie concept representations in language and vision. Comparing encoding models trained using representations from multimodal and unimodal transformers, we found that multimodal transformers learn more aligned representations of concepts in language and vision. Our results demonstrate how multimodal transformers can provide insights into the brain's capacity for multimodal processing.",
    "authors": [
      "Jerry Tang",
      "Meng Du",
      "Vy A. Vo",
      "Vasudev Lal",
      "Alexander G. Huth"
    ],
    "published": "2023-05-20T17:38:44+00:00",
    "url": "http://arxiv.org/pdf/2305.12248v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2305.18838v1",
    "title": "Client: Cross-variable Linear Integrated Enhanced Transformer for Multivariate Long-Term Time Series Forecasting",
    "content": "Client: Cross-variable Linear Integrated Enhanced Transformer for Multivariate Long-Term Time Series Forecasting. Long-term time series forecasting (LTSF) is a crucial aspect of modern society, playing a pivotal role in facilitating long-term planning and developing early warning systems. While many Transformer-based models have recently been introduced for LTSF, a doubt have been raised regarding the effectiveness of attention modules in capturing cross-time dependencies. In this study, we design a mask-series experiment to validate this assumption and subsequently propose the \"Cross-variable Linear Integrated ENhanced Transformer for Multivariate Long-Term Time Series Forecasting\" (Client), an advanced model that outperforms both traditional Transformer-based models and linear models. Client employs linear modules to learn trend information and attention modules to capture cross-variable dependencies. Meanwhile, it simplifies the embedding and position encoding layers and replaces the decoder module with a projection layer. Essentially, Client incorporates non-linearity and cross-variable dependencies, which sets it apart from conventional linear models and Transformer-based models. Extensive experiments with nine real-world datasets have confirmed the SOTA performance of Client with the least computation time and memory consumption compared with the previous Transformer-based models. Our code is available at https://github.com/daxin007/Client.",
    "authors": [
      "Jiaxin Gao",
      "Wenbo Hu",
      "Yuntian Chen"
    ],
    "published": "2023-05-30T08:31:22+00:00",
    "url": "http://arxiv.org/pdf/2305.18838v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2311.08610v1",
    "title": "Converting Transformers to Polynomial Form for Secure Inference Over Homomorphic Encryption",
    "content": "Converting Transformers to Polynomial Form for Secure Inference Over Homomorphic Encryption. Designing privacy-preserving deep learning models is a major challenge within the deep learning community. Homomorphic Encryption (HE) has emerged as one of the most promising approaches in this realm, enabling the decoupling of knowledge between the model owner and the data owner. Despite extensive research and application of this technology, primarily in convolutional neural networks, incorporating HE into transformer models has been challenging because of the difficulties in converting these models into a polynomial form. We break new ground by introducing the first polynomial transformer, providing the first demonstration of secure inference over HE with transformers. This includes a transformer architecture tailored for HE, alongside a novel method for converting operators to their polynomial equivalent. This innovation enables us to perform secure inference on LMs with WikiText-103. It also allows us to perform image classification with CIFAR-100 and Tiny-ImageNet. Our models yield results comparable to traditional methods, bridging the performance gap with transformers of similar scale and underscoring the viability of HE for state-of-the-art applications. Finally, we assess the stability of our models and conduct a series of ablations to quantify the contribution of each model component.",
    "authors": [
      "Itamar Zimerman",
      "Moran Baruch",
      "Nir Drucker",
      "Gilad Ezov",
      "Omri Soceanu",
      "Lior Wolf"
    ],
    "published": "2023-11-15T00:23:58+00:00",
    "url": "http://arxiv.org/pdf/2311.08610v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2312.00662v1",
    "title": "Nonparametric Variational Regularisation of Pretrained Transformers",
    "content": "Nonparametric Variational Regularisation of Pretrained Transformers. The current paradigm of large-scale pre-training and fine-tuning Transformer large language models has lead to significant improvements across the board in natural language processing. However, such large models are susceptible to overfitting to their training data, and as a result the models perform poorly when the domain changes. Also, due to the model's scale, the cost of fine-tuning the model to the new domain is large. Nonparametric Variational Information Bottleneck (NVIB) has been proposed as a regulariser for training cross-attention in Transformers, potentially addressing the overfitting problem. We extend the NVIB framework to replace all types of attention functions in Transformers, and show that existing pretrained Transformers can be reinterpreted as Nonparametric Variational (NV) models using a proposed identity initialisation. We then show that changing the initialisation introduces a novel, information-theoretic post-training regularisation in the attention mechanism, which improves out-of-domain generalisation without any training. This success supports the hypothesis that pretrained Transformers are implicitly NV Bayesian models.",
    "authors": [
      "Fabio Fehr",
      "James Henderson"
    ],
    "published": "2023-12-01T15:40:30+00:00",
    "url": "http://arxiv.org/pdf/2312.00662v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2312.06063v1",
    "title": "PCRDiffusion: Diffusion Probabilistic Models for Point Cloud Registration",
    "content": "PCRDiffusion: Diffusion Probabilistic Models for Point Cloud Registration. We propose a new framework that formulates point cloud registration as a denoising diffusion process from noisy transformation to object transformation. During training stage, object transformation diffuses from ground-truth transformation to random distribution, and the model learns to reverse this noising process. In sampling stage, the model refines randomly generated transformation to the output result in a progressive way. We derive the variational bound in closed form for training and provide implementations of the model. Our work provides the following crucial findings: (i) In contrast to most existing methods, our framework, Diffusion Probabilistic Models for Point Cloud Registration (PCRDiffusion) does not require repeatedly update source point cloud to refine the predicted transformation. (ii) Point cloud registration, one of the representative discriminative tasks, can be solved by a generative way and the unified probabilistic formulation. Finally, we discuss and provide an outlook on the application of diffusion model in different scenarios for point cloud registration. Experimental results demonstrate that our model achieves competitive performance in point cloud registration. In correspondence-free and correspondence-based scenarios, PCRDifussion can both achieve exceeding 50\\% performance improvements.",
    "authors": [
      "Yue Wu",
      "Yongzhe Yuan",
      "Xiaolong Fan",
      "Xiaoshui Huang",
      "Maoguo Gong",
      "Qiguang Miao"
    ],
    "published": "2023-12-11T01:56:42+00:00",
    "url": "http://arxiv.org/pdf/2312.06063v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2402.15989v1",
    "title": "PIDformer: Transformer Meets Control Theory",
    "content": "PIDformer: Transformer Meets Control Theory. In this work, we address two main shortcomings of transformer architectures: input corruption and rank collapse in their output representation. We unveil self-attention as an autonomous state-space model that inherently promotes smoothness in its solutions, leading to lower-rank outputs and diminished representation capacity. Moreover, the steady-state solution of the model is sensitive to input perturbations. We incorporate a Proportional-Integral-Derivative (PID) closed-loop feedback control system with a reference point into the model to improve robustness and representation capacity. This integration aims to preserve high-frequency details while bolstering model stability, rendering it more noise-resilient. The resulting controlled state-space model is theoretically proven robust and adept at addressing the rank collapse. Motivated by this control framework, we derive a novel class of transformers, PID-controlled Transformer (PIDformer), aimed at improving robustness and mitigating the rank-collapse issue inherent in softmax transformers. We empirically evaluate the model for advantages and robustness against baseline transformers across various practical tasks, including object classification, image segmentation, and language modeling.",
    "authors": [
      "Tam Nguyen",
      "César A. Uribe",
      "Tan M. Nguyen",
      "Richard G. Baraniuk"
    ],
    "published": "2024-02-25T05:04:51+00:00",
    "url": "http://arxiv.org/pdf/2402.15989v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2403.02366v1",
    "title": "Human Evaluation of English--Irish Transformer-Based NMT",
    "content": "Human Evaluation of English--Irish Transformer-Based NMT. In this study, a human evaluation is carried out on how hyperparameter settings impact the quality of Transformer-based Neural Machine Translation (NMT) for the low-resourced English--Irish pair. SentencePiece models using both Byte Pair Encoding (BPE) and unigram approaches were appraised. Variations in model architectures included modifying the number of layers, evaluating the optimal number of heads for attention and testing various regularisation techniques. The greatest performance improvement was recorded for a Transformer-optimized model with a 16k BPE subword model. Compared with a baseline Recurrent Neural Network (RNN) model, a Transformer-optimized model demonstrated a BLEU score improvement of 7.8 points. When benchmarked against Google Translate, our translation engines demonstrated significant improvements. Furthermore, a quantitative fine-grained manual evaluation was conducted which compared the performance of machine translation systems. Using the Multidimensional Quality Metrics (MQM) error taxonomy, a human evaluation of the error types generated by an RNN-based system and a Transformer-based system was explored. Our findings show the best-performing Transformer system significantly reduces both accuracy and fluency errors when compared with an RNN-based model.",
    "authors": [
      "Séamus Lankford",
      "Haithem Afli",
      "Andy Way"
    ],
    "published": "2024-03-04T11:45:46+00:00",
    "url": "http://arxiv.org/pdf/2403.02366v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2408.10197v1",
    "title": "Demystifying the Communication Characteristics for Distributed Transformer Models",
    "content": "Demystifying the Communication Characteristics for Distributed Transformer Models. Deep learning (DL) models based on the transformer architecture have revolutionized many DL applications such as large language models (LLMs), vision transformers, audio generation, and time series prediction. Much of this progress has been fueled by distributed training, yet distributed communication remains a substantial bottleneck to training progress. This paper examines the communication behavior of transformer models - that is, how different parallelism schemes used in multi-node/multi-GPU DL Training communicate data in the context of transformers. We use GPT-based language models as a case study of the transformer architecture due to their ubiquity. We validate the empirical results obtained from our communication logs using analytical models. At a high level, our analysis reveals a need to optimize small message point-to-point communication further, correlations between sequence length, per-GPU throughput, model size, and optimizations used, and where to potentially guide further optimizations in framework and HPC middleware design and optimization.",
    "authors": [
      "Quentin Anthony",
      "Benjamin Michalowicz",
      "Jacob Hatef",
      "Lang Xu",
      "Mustafa Abduljabbar",
      "Aamir Shafi",
      "Hari Subramoni",
      "Dhabaleswar Panda"
    ],
    "published": "2024-08-19T17:54:29+00:00",
    "url": "http://arxiv.org/pdf/2408.10197v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2409.12175v1",
    "title": "Expanding Expressivity in Transformer Models with MöbiusAttention",
    "content": "Expanding Expressivity in Transformer Models with MöbiusAttention. Attention mechanisms and Transformer architectures have revolutionized Natural Language Processing (NLP) by enabling exceptional modeling of long-range dependencies and capturing intricate linguistic patterns. However, their inherent reliance on linear operations in the form of matrix multiplications limits their ability to fully capture inter-token relationships on their own. We propose M\\\"obiusAttention, a novel approach that integrates M\\\"obius transformations within the attention mechanism of Transformer-based models. M\\\"obius transformations are non-linear operations in spaces over complex numbers with the ability to map between various geometries. By incorporating these properties, M\\\"obiusAttention empowers models to learn more intricate geometric relationships between tokens and capture a wider range of information through complex-valued weight vectors. We build and pre-train a BERT and a RoFormer version enhanced with M\\\"obiusAttention, which we then finetune on the GLUE benchmark. We evaluate empirically our approach against the baseline BERT and RoFormer models on a range of downstream tasks. Our approach compares favorably against the baseline models, even with smaller number of parameters suggesting the enhanced expressivity of M\\\"obiusAttention. This research paves the way for exploring the potential of M\\\"obius transformations in the complex projective space to enhance the expressivity and performance of foundation models.",
    "authors": [
      "Anna-Maria Halacheva",
      "Mojtaba Nayyeri",
      "Steffen Staab"
    ],
    "published": "2024-09-08T16:56:33+00:00",
    "url": "http://arxiv.org/pdf/2409.12175v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2502.18083v3",
    "title": "A Fusion Model for Artwork Identification Based on Convolutional Neural Networks and Transformers",
    "content": "A Fusion Model for Artwork Identification Based on Convolutional Neural Networks and Transformers. The identification of artwork is crucial in areas like cultural heritage protection, art market analysis, and historical research. With the advancement of deep learning, Convolutional Neural Networks (CNNs) and Transformer models have become key tools for image classification. While CNNs excel in local feature extraction, they struggle with global context, and Transformers are strong in capturing global dependencies but weak in fine-grained local details. To address these challenges, this paper proposes a fusion model combining CNNs and Transformers for artwork identification. The model first extracts local features using CNNs, then captures global context with a Transformer, followed by a feature fusion mechanism to enhance classification accuracy. Experiments on Chinese and oil painting datasets show the fusion model outperforms individual CNN and Transformer models, improving classification accuracy by 9.7% and 7.1%, respectively, and increasing F1 scores by 0.06 and 0.05. The results demonstrate the model's effectiveness and potential for future improvements, such as multimodal integration and architecture optimization.",
    "authors": [
      "Zhenyu Wang",
      "Heng Song"
    ],
    "published": "2025-02-25T10:52:38+00:00",
    "url": "http://arxiv.org/pdf/2502.18083v3"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1504.00169v3",
    "title": "Complete Simulation of Automata Networks",
    "content": "Complete Simulation of Automata Networks. Consider a finite set $A$ and an integer $n \\geq 1$. This paper studies the concept of complete simulation in the context of semigroups of transformations of $A^n$, also known as finite state-homogeneous automata networks. For $m \\geq n$, a transformation of $A^m$ is \\emph{$n$-complete of size $m$} if it may simulate every transformation of $A^n$ by updating one coordinate (or register) at a time. Using tools from memoryless computation, it is established that there is no $n$-complete transformation of size $n$, but there is such a transformation of size $n+1$. By studying the the time of simulation of various $n$-complete transformations, it is conjectured that the maximal time of simulation of any $n$-complete transformation is at least $2n$. A transformation of $A^m$ is \\emph{sequentially $n$-complete of size $m$} if it may sequentially simulate every finite sequence of transformations of $A^n$; in this case, minimal examples and bounds for the size and time of simulation are determined. It is also shown that there is no $n$-complete transformation that updates all the registers in parallel, but that there exists a sequentally $n$-complete transformation that updates all but one register in parallel. This illustrates the strengths and weaknesses of parallel models of computation, such as cellular automata.",
    "authors": [
      "Florian Bridoux",
      "Alonso Castillo-Ramirez",
      "Maximilien Gadouleau"
    ],
    "published": "2015-04-01T10:10:05+00:00",
    "url": "http://arxiv.org/pdf/1504.00169v3"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1812.04570v2",
    "title": "PrecoG: an efficient unitary split preconditioner for the transform-domain LMS filter via graph Laplacian regularization",
    "content": "PrecoG: an efficient unitary split preconditioner for the transform-domain LMS filter via graph Laplacian regularization. Transform-domain least mean squares (LMS) adaptive filters encompass the class of algorithms where the input data are subjected to a data-independent unitary transform followed by a power normalization stage as preprocessing steps. Because conventional transformations are not data-dependent, this preconditioning procedure was shown theoretically to improve the convergence of the LMS filter only for certain classes of input data. However, in reality if the class of input data is not known beforehand, it is difficult to decide which transformation to use. Thus, there is a need to devise a learning framework to obtain such a preconditioning transformation using input data prior to applying on the input data. It is hypothesized that the underlying topology of the data affects the selection of the transformation. With the input modeled as a weighted graph that mimics neuronal interactions, PrecoG obtains the desired transform by recursive estimation of the graph Laplacian matrix. Additionally, we show the efficacy of the transform as a generalized split preconditioner on a linear system of equations and in Hebb-LMS settings. In terms of the improvement of the condition number after applying the transformation, PrecoG performs significantly better than the existing state-of-the-art techniques that involve unitary and non-unitary transforms.",
    "authors": [
      "Tamal Batabyal",
      "Daniel S. Weller",
      "Jaideep Kapur",
      "Scott T. Acton"
    ],
    "published": "2018-12-11T17:49:16+00:00",
    "url": "http://arxiv.org/pdf/1812.04570v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2104.02610v2",
    "title": "On the Robustness of Vision Transformers to Adversarial Examples",
    "content": "On the Robustness of Vision Transformers to Adversarial Examples. Recent advances in attention-based networks have shown that Vision Transformers can achieve state-of-the-art or near state-of-the-art results on many image classification tasks. This puts transformers in the unique position of being a promising alternative to traditional convolutional neural networks (CNNs). While CNNs have been carefully studied with respect to adversarial attacks, the same cannot be said of Vision Transformers. In this paper, we study the robustness of Vision Transformers to adversarial examples. Our analyses of transformer security is divided into three parts. First, we test the transformer under standard white-box and black-box attacks. Second, we study the transferability of adversarial examples between CNNs and transformers. We show that adversarial examples do not readily transfer between CNNs and transformers. Based on this finding, we analyze the security of a simple ensemble defense of CNNs and transformers. By creating a new attack, the self-attention blended gradient attack, we show that such an ensemble is not secure under a white-box adversary. However, under a black-box adversary, we show that an ensemble can achieve unprecedented robustness without sacrificing clean accuracy. Our analysis for this work is done using six types of white-box attacks and two types of black-box attacks. Our study encompasses multiple Vision Transformers, Big Transfer Models and CNN architectures trained on CIFAR-10, CIFAR-100 and ImageNet.",
    "authors": [
      "Kaleel Mahmood",
      "Rigel Mahmood",
      "Marten van Dijk"
    ],
    "published": "2021-03-31T00:29:12+00:00",
    "url": "http://arxiv.org/pdf/2104.02610v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1910.05886v2",
    "title": "A New Local Transformation Module for Few-shot Segmentation",
    "content": "A New Local Transformation Module for Few-shot Segmentation. Few-shot segmentation segments object regions of new classes with a few of manual annotations. Its key step is to establish the transformation module between support images (annotated images) and query images (unlabeled images), so that the segmentation cues of support images can guide the segmentation of query images. The existing methods form transformation model based on global cues, which however ignores the local cues that are verified in this paper to be very important for the transformation. This paper proposes a new transformation module based on local cues, where the relationship of the local features is used for transformation. To enhance the generalization performance of the network, the relationship matrix is calculated in a high-dimensional metric embedding space based on cosine distance. In addition, to handle the challenging mapping problem from the low-level local relationships to high-level semantic cues, we propose to apply generalized inverse matrix of the annotation matrix of support images to transform the relationship matrix linearly, which is non-parametric and class-agnostic. The result by the matrix transformation can be regarded as an attention map with high-level semantic cues, based on which a transformation module can be built simply.The proposed transformation module is a general module that can be used to replace the transformation module in the existing few-shot segmentation frameworks. We verify the effectiveness of the proposed method on Pascal VOC 2012 dataset. The value of mIoU achieves at 57.0% in 1-shot and 60.6% in 5-shot, which outperforms the state-of-the-art method by 1.6% and 3.5%, respectively.",
    "authors": [
      "Yuwei Yang",
      "Fanman Meng",
      "Hongliang Li",
      "Qingbo Wu",
      "Xiaolong Xu",
      "Shuai Chen"
    ],
    "published": "2019-10-14T01:52:21+00:00",
    "url": "http://arxiv.org/pdf/1910.05886v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2112.12345v1",
    "title": "Revisiting Transformation Invariant Geometric Deep Learning: Are Initial Representations All You Need?",
    "content": "Revisiting Transformation Invariant Geometric Deep Learning: Are Initial Representations All You Need?. Geometric deep learning, i.e., designing neural networks to handle the ubiquitous geometric data such as point clouds and graphs, have achieved great successes in the last decade. One critical inductive bias is that the model can maintain invariance towards various transformations such as translation, rotation, and scaling. The existing graph neural network (GNN) approaches can only maintain permutation-invariance, failing to guarantee invariance with respect to other transformations. Besides GNNs, other works design sophisticated transformation-invariant layers, which are computationally expensive and difficult to be extended. To solve this problem, we revisit why the existing neural networks cannot maintain transformation invariance when handling geometric data. Our findings show that transformation-invariant and distance-preserving initial representations are sufficient to achieve transformation invariance rather than needing sophisticated neural layer designs. Motivated by these findings, we propose Transformation Invariant Neural Networks (TinvNN), a straightforward and general framework for geometric data. Specifically, we realize transformation-invariant and distance-preserving initial point representations by modifying multi-dimensional scaling before feeding the representations into neural networks. We prove that TinvNN can strictly guarantee transformation invariance, being general and flexible enough to be combined with the existing neural networks. Extensive experimental results on point cloud analysis and combinatorial optimization demonstrate the effectiveness and general applicability of our proposed method. Based on the experimental results, we advocate that TinvNN should be considered a new starting point and an essential baseline for further studies of transformation-invariant geometric deep learning.",
    "authors": [
      "Ziwei Zhang",
      "Xin Wang",
      "Zeyang Zhang",
      "Peng Cui",
      "Wenwu Zhu"
    ],
    "published": "2021-12-23T03:52:33+00:00",
    "url": "http://arxiv.org/pdf/2112.12345v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2103.07751v2",
    "title": "Unsupervised Image Transformation Learning via Generative Adversarial Networks",
    "content": "Unsupervised Image Transformation Learning via Generative Adversarial Networks. In this work, we study the image transformation problem, which targets at learning the underlying transformations (e.g., the transition of seasons) from a collection of unlabeled images. However, there could be countless of transformations in the real world, making such a task incredibly challenging, especially under the unsupervised setting. To tackle this obstacle, we propose a novel learning framework built on generative adversarial networks (GANs), where the discriminator and the generator share a transformation space. After the model gets fully optimized, any two points within the shared space are expected to define a valid transformation. In this way, at the inference stage, we manage to adequately extract the variation factor between a customizable image pair by projecting both images onto the transformation space. The resulting transformation vector can further guide the image synthesis, facilitating image editing with continuous semantic change (e.g., altering summer to winter with fall as the intermediate step). Noticeably, the learned transformation space supports not only transferring image styles (e.g., changing day to night), but also manipulating image contents (e.g., adding clouds in the sky). In addition, we make in-depth analysis on the properties of the transformation space to help understand how various transformations are organized. Project page is at https://genforce.github.io/trgan/.",
    "authors": [
      "Kaiwen Zha",
      "Yujun Shen",
      "Bolei Zhou"
    ],
    "published": "2021-03-13T17:08:19+00:00",
    "url": "http://arxiv.org/pdf/2103.07751v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2111.05464v1",
    "title": "Are Transformers More Robust Than CNNs?",
    "content": "Are Transformers More Robust Than CNNs?. Transformer emerges as a powerful tool for visual recognition. In addition to demonstrating competitive performance on a broad range of visual benchmarks, recent works also argue that Transformers are much more robust than Convolutions Neural Networks (CNNs). Nonetheless, surprisingly, we find these conclusions are drawn from unfair experimental settings, where Transformers and CNNs are compared at different scales and are applied with distinct training frameworks. In this paper, we aim to provide the first fair & in-depth comparisons between Transformers and CNNs, focusing on robustness evaluations.   With our unified training setup, we first challenge the previous belief that Transformers outshine CNNs when measuring adversarial robustness. More surprisingly, we find CNNs can easily be as robust as Transformers on defending against adversarial attacks, if they properly adopt Transformers' training recipes. While regarding generalization on out-of-distribution samples, we show pre-training on (external) large-scale datasets is not a fundamental request for enabling Transformers to achieve better performance than CNNs. Moreover, our ablations suggest such stronger generalization is largely benefited by the Transformer's self-attention-like architectures per se, rather than by other training setups. We hope this work can help the community better understand and benchmark the robustness of Transformers and CNNs. The code and models are publicly available at https://github.com/ytongbai/ViTs-vs-CNNs.",
    "authors": [
      "Yutong Bai",
      "Jieru Mei",
      "Alan Yuille",
      "Cihang Xie"
    ],
    "published": "2021-11-10T00:18:59+00:00",
    "url": "http://arxiv.org/pdf/2111.05464v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2307.07982v1",
    "title": "A Survey of Techniques for Optimizing Transformer Inference",
    "content": "A Survey of Techniques for Optimizing Transformer Inference. Recent years have seen a phenomenal rise in performance and applications of transformer neural networks. The family of transformer networks, including Bidirectional Encoder Representations from Transformer (BERT), Generative Pretrained Transformer (GPT) and Vision Transformer (ViT), have shown their effectiveness across Natural Language Processing (NLP) and Computer Vision (CV) domains. Transformer-based networks such as ChatGPT have impacted the lives of common men. However, the quest for high predictive performance has led to an exponential increase in transformers' memory and compute footprint. Researchers have proposed techniques to optimize transformer inference at all levels of abstraction. This paper presents a comprehensive survey of techniques for optimizing the inference phase of transformer networks. We survey techniques such as knowledge distillation, pruning, quantization, neural architecture search and lightweight network design at the algorithmic level. We further review hardware-level optimization techniques and the design of novel hardware accelerators for transformers. We summarize the quantitative results on the number of parameters/FLOPs and accuracy of several models/techniques to showcase the tradeoff exercised by them. We also outline future directions in this rapidly evolving field of research. We believe that this survey will educate both novice and seasoned researchers and also spark a plethora of research efforts in this field.",
    "authors": [
      "Krishna Teja Chitty-Venkata",
      "Sparsh Mittal",
      "Murali Emani",
      "Venkatram Vishwanath",
      "Arun K. Somani"
    ],
    "published": "2023-07-16T08:50:50+00:00",
    "url": "http://arxiv.org/pdf/2307.07982v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2410.04209v2",
    "title": "Equivariant Neural Functional Networks for Transformers",
    "content": "Equivariant Neural Functional Networks for Transformers. This paper systematically explores neural functional networks (NFN) for transformer architectures. NFN are specialized neural networks that treat the weights, gradients, or sparsity patterns of a deep neural network (DNN) as input data and have proven valuable for tasks such as learnable optimizers, implicit data representations, and weight editing. While NFN have been extensively developed for MLP and CNN, no prior work has addressed their design for transformers, despite the importance of transformers in modern deep learning. This paper aims to address this gap by providing a systematic study of NFN for transformers. We first determine the maximal symmetric group of the weights in a multi-head attention module as well as a necessary and sufficient condition under which two sets of hyperparameters of the multi-head attention module define the same function. We then define the weight space of transformer architectures and its associated group action, which leads to the design principles for NFN in transformers. Based on these, we introduce Transformer-NFN, an NFN that is equivariant under this group action. Additionally, we release a dataset of more than 125,000 Transformers model checkpoints trained on two datasets with two different tasks, providing a benchmark for evaluating Transformer-NFN and encouraging further research on transformer training and performance.",
    "authors": [
      "Viet-Hoang Tran",
      "Thieu N. Vo",
      "An Nguyen The",
      "Tho Tran Huu",
      "Minh-Khoi Nguyen-Nhat",
      "Thanh Tran",
      "Duy-Tung Pham",
      "Tan Minh Nguyen"
    ],
    "published": "2024-10-05T15:56:57+00:00",
    "url": "http://arxiv.org/pdf/2410.04209v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1901.11117v4",
    "title": "The Evolved Transformer",
    "content": "The Evolved Transformer. Recent works have highlighted the strength of the Transformer architecture on sequence tasks while, at the same time, neural architecture search (NAS) has begun to outperform human-designed models. Our goal is to apply NAS to search for a better alternative to the Transformer. We first construct a large search space inspired by the recent advances in feed-forward sequence models and then run evolutionary architecture search with warm starting by seeding our initial population with the Transformer. To directly search on the computationally expensive WMT 2014 English-German translation task, we develop the Progressive Dynamic Hurdles method, which allows us to dynamically allocate more resources to more promising candidate models. The architecture found in our experiments -- the Evolved Transformer -- demonstrates consistent improvement over the Transformer on four well-established language tasks: WMT 2014 English-German, WMT 2014 English-French, WMT 2014 English-Czech and LM1B. At a big model size, the Evolved Transformer establishes a new state-of-the-art BLEU score of 29.8 on WMT'14 English-German; at smaller sizes, it achieves the same quality as the original \"big\" Transformer with 37.6% less parameters and outperforms the Transformer by 0.7 BLEU at a mobile-friendly model size of 7M parameters.",
    "authors": [
      "David R. So",
      "Chen Liang",
      "Quoc V. Le"
    ],
    "published": "2019-01-30T22:03:01+00:00",
    "url": "http://arxiv.org/pdf/1901.11117v4"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1811.07342v1",
    "title": "Transform-Based Multilinear Dynamical System for Tensor Time Series Analysis",
    "content": "Transform-Based Multilinear Dynamical System for Tensor Time Series Analysis. We propose a novel multilinear dynamical system (MLDS) in a transform domain, named $\\mathcal{L}$-MLDS, to model tensor time series. With transformations applied to a tensor data, the latent multidimensional correlations among the frontal slices are built, and thus resulting in the computational independence in the transform domain. This allows the exact separability of the multi-dimensional problem into multiple smaller LDS problems. To estimate the system parameters, we utilize the expectation-maximization (EM) algorithm to determine the parameters of each LDS. Further, $\\mathcal{L}$-MLDSs significantly reduce the model parameters and allows parallel processing. Our general $\\mathcal{L}$-MLDS model is implemented based on different transforms: discrete Fourier transform, discrete cosine transform and discrete wavelet transform. Due to the nonlinearity of these transformations, $\\mathcal{L}$-MLDS is able to capture the nonlinear correlations within the data unlike the MLDS \\cite{rogers2013multilinear} which assumes multi-way linear correlations. Using four real datasets, the proposed $\\mathcal{L}$-MLDS is shown to achieve much higher prediction accuracy than the state-of-the-art MLDS and LDS with an equal number of parameters under different noise models. In particular, the relative errors are reduced by $50\\% \\sim 99\\%$. Simultaneously, $\\mathcal{L}$-MLDS achieves an exponential improvement in the model's training time than MLDS.",
    "authors": [
      "Weijun Lu",
      "Xiao-Yang Liu",
      "Qingwei Wu",
      "Yue Sun",
      "Anwar Walid"
    ],
    "published": "2018-11-18T15:45:31+00:00",
    "url": "http://arxiv.org/pdf/1811.07342v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2212.00973v1",
    "title": "A Domain-Knowledge-Inspired Music Embedding Space and a Novel Attention Mechanism for Symbolic Music Modeling",
    "content": "A Domain-Knowledge-Inspired Music Embedding Space and a Novel Attention Mechanism for Symbolic Music Modeling. Following the success of the transformer architecture in the natural language domain, transformer-like architectures have been widely applied to the domain of symbolic music recently. Symbolic music and text, however, are two different modalities. Symbolic music contains multiple attributes, both absolute attributes (e.g., pitch) and relative attributes (e.g., pitch interval). These relative attributes shape human perception of musical motifs. These important relative attributes, however, are mostly ignored in existing symbolic music modeling methods with the main reason being the lack of a musically-meaningful embedding space where both the absolute and relative embeddings of the symbolic music tokens can be efficiently represented. In this paper, we propose the Fundamental Music Embedding (FME) for symbolic music based on a bias-adjusted sinusoidal encoding within which both the absolute and the relative attributes can be embedded and the fundamental musical properties (e.g., translational invariance) are explicitly preserved. Taking advantage of the proposed FME, we further propose a novel attention mechanism based on the relative index, pitch and onset embeddings (RIPO attention) such that the musical domain knowledge can be fully utilized for symbolic music modeling. Experiment results show that our proposed model: RIPO transformer which utilizes FME and RIPO attention outperforms the state-of-the-art transformers (i.e., music transformer, linear transformer) in a melody completion task. Moreover, using the RIPO transformer in a downstream music generation task, we notice that the notorious degeneration phenomenon no longer exists and the music generated by the RIPO transformer outperforms the music generated by state-of-the-art transformer models in both subjective and objective evaluations.",
    "authors": [
      "Z. Guo",
      "J. Kang",
      "D. Herremans"
    ],
    "published": "2022-12-02T05:04:31+00:00",
    "url": "http://arxiv.org/pdf/2212.00973v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2306.07303v1",
    "title": "A Comprehensive Survey on Applications of Transformers for Deep Learning Tasks",
    "content": "A Comprehensive Survey on Applications of Transformers for Deep Learning Tasks. Transformer is a deep neural network that employs a self-attention mechanism to comprehend the contextual relationships within sequential data. Unlike conventional neural networks or updated versions of Recurrent Neural Networks (RNNs) such as Long Short-Term Memory (LSTM), transformer models excel in handling long dependencies between input sequence elements and enable parallel processing. As a result, transformer-based models have attracted substantial interest among researchers in the field of artificial intelligence. This can be attributed to their immense potential and remarkable achievements, not only in Natural Language Processing (NLP) tasks but also in a wide range of domains, including computer vision, audio and speech processing, healthcare, and the Internet of Things (IoT). Although several survey papers have been published highlighting the transformer's contributions in specific fields, architectural differences, or performance evaluations, there is still a significant absence of a comprehensive survey paper encompassing its major applications across various domains. Therefore, we undertook the task of filling this gap by conducting an extensive survey of proposed transformer models from 2017 to 2022. Our survey encompasses the identification of the top five application domains for transformer-based models, namely: NLP, Computer Vision, Multi-Modality, Audio and Speech Processing, and Signal Processing. We analyze the impact of highly influential transformer-based models in these domains and subsequently classify them based on their respective tasks using a proposed taxonomy. Our aim is to shed light on the existing potential and future possibilities of transformers for enthusiastic researchers, thus contributing to the broader understanding of this groundbreaking technology.",
    "authors": [
      "Saidul Islam",
      "Hanae Elmekki",
      "Ahmed Elsebai",
      "Jamal Bentahar",
      "Najat Drawel",
      "Gaith Rjoub",
      "Witold Pedrycz"
    ],
    "published": "2023-06-11T23:13:51+00:00",
    "url": "http://arxiv.org/pdf/2306.07303v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2307.01225v1",
    "title": "Interpretability and Transparency-Driven Detection and Transformation of Textual Adversarial Examples (IT-DT)",
    "content": "Interpretability and Transparency-Driven Detection and Transformation of Textual Adversarial Examples (IT-DT). Transformer-based text classifiers like BERT, Roberta, T5, and GPT-3 have shown impressive performance in NLP. However, their vulnerability to adversarial examples poses a security risk. Existing defense methods lack interpretability, making it hard to understand adversarial classifications and identify model vulnerabilities. To address this, we propose the Interpretability and Transparency-Driven Detection and Transformation (IT-DT) framework. It focuses on interpretability and transparency in detecting and transforming textual adversarial examples. IT-DT utilizes techniques like attention maps, integrated gradients, and model feedback for interpretability during detection. This helps identify salient features and perturbed words contributing to adversarial classifications. In the transformation phase, IT-DT uses pre-trained embeddings and model feedback to generate optimal replacements for perturbed words. By finding suitable substitutions, we aim to convert adversarial examples into non-adversarial counterparts that align with the model's intended behavior while preserving the text's meaning. Transparency is emphasized through human expert involvement. Experts review and provide feedback on detection and transformation results, enhancing decision-making, especially in complex scenarios. The framework generates insights and threat intelligence empowering analysts to identify vulnerabilities and improve model robustness. Comprehensive experiments demonstrate the effectiveness of IT-DT in detecting and transforming adversarial examples. The approach enhances interpretability, provides transparency, and enables accurate identification and successful transformation of adversarial inputs. By combining technical analysis and human expertise, IT-DT significantly improves the resilience and trustworthiness of transformer-based text classifiers against adversarial attacks.",
    "authors": [
      "Bushra Sabir",
      "M. Ali Babar",
      "Sharif Abuadbba"
    ],
    "published": "2023-07-03T03:17:20+00:00",
    "url": "http://arxiv.org/pdf/2307.01225v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2408.05221v1",
    "title": "Early-Stage Requirements Transformation Approaches: A Systematic Review",
    "content": "Early-Stage Requirements Transformation Approaches: A Systematic Review. Transformation approaches for automatically constructing analysis models from textual requirements are critical to software development, as they can bring forward the use of precise formal languages from the coding phase to the requirement analysis phase in the software development life-cycle. Over the decades, numerous transformation approaches have been developed in an attempt to fully or partially automate this initial phase. This systematic review examines transformation approaches in the early stages of software development, examining 25 studies on early-stage requirements transformation documented between 2000 and 2014. The review highlights the widespread use of natural language processing techniques, with tools like the Stanford parser and WordNet being essential. Intermediate models are often used in the transformation process to bridge the gap between textual requirements and analysis models. Significant advancements have been made in early-stage requirements transformation approaches; however, several areas require attention to enhance their effectiveness and reliability. A challenge identified is the lack of robust evaluation methods, with most approaches using simple case studies and running examples for evaluation. This makes it difficult to compare and evaluate the performance these approaches. Although most approaches can generate structural models from textual requirements, many generate incomplete models with missing elements. Furthermore, requirements traceability is largely neglected, with only two approaches addressing it and lacking explicit detail on how traceability links are maintained during the transformation process. This review emphasize the need for formalized evaluation techniques and greater transparency and accessibility of approaches used in the early-stage requirements transformation.",
    "authors": [
      "Keletso J. Letsholo"
    ],
    "published": "2024-07-25T18:13:29+00:00",
    "url": "http://arxiv.org/pdf/2408.05221v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2007.15813v1",
    "title": "Language Modelling for Source Code with Transformer-XL",
    "content": "Language Modelling for Source Code with Transformer-XL. It has been found that software, like natural language texts, exhibits \"naturalness\", which can be captured by statistical language models. In recent years, neural language models have been proposed to represent the naturalness of software through deep learning. In this paper, we conduct an experimental evaluation of state-of-the-art neural language models for source code, including RNN-based models and Transformer-XL based models. Through experiments on a large-scale Python code corpus, we find that the Transformer-XL model outperforms RNN-based models (including LSTM and GRU models) in capturing the naturalness of software, with far less computational cost.",
    "authors": [
      "Thomas Dowdell",
      "Hongyu Zhang"
    ],
    "published": "2020-07-31T02:42:18+00:00",
    "url": "http://arxiv.org/pdf/2007.15813v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2409.18654v1",
    "title": "Speech-Mamba: Long-Context Speech Recognition with Selective State Spaces Models",
    "content": "Speech-Mamba: Long-Context Speech Recognition with Selective State Spaces Models. Current automatic speech recognition systems struggle with modeling long speech sequences due to high quadratic complexity of Transformer-based models. Selective state space models such as Mamba has performed well on long-sequence modeling in natural language processing and computer vision tasks. However, research endeavors in speech technology tasks has been under-explored. We propose Speech-Mamba, which incorporates selective state space modeling in Transformer neural architectures. Long sequence representations with selective state space models in Speech-Mamba is complemented with lower-level representations from Transformer-based modeling. Speech-mamba achieves better capacity to model long-range dependencies, as it scales near-linearly with sequence length.",
    "authors": [
      "Xiaoxue Gao",
      "Nancy F. Chen"
    ],
    "published": "2024-09-27T11:36:31+00:00",
    "url": "http://arxiv.org/pdf/2409.18654v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/cond-mat/0605577v1",
    "title": "Interplay between diffusive and displacive phase transformations: TTT diagrams and microstructures",
    "content": "Interplay between diffusive and displacive phase transformations: TTT diagrams and microstructures. Materials which can undergo extremely fast displacive transformations as well as very slow diffusive transformations are studied using a Ginzburg-Landau framework to understand the physics behind microstructure formation and time-temperature-transformation (TTT) diagrams. This simple model captures the essential features of alloys such as steels and predicts the formation of mixed microstructures by an interplay between diffusive and displacive mechanisms. The intrinsic volume changes associated with the transformations stabilize mixed microstructures such as martensite-retained austenite (responsible for the existence of a martensite finish temperature) and martensite-pearlite. keywords: phase-field, spinodal decomposition, pearlite, martensite, steel, elastic compatibility",
    "authors": [
      "Mathieu Bouville",
      "Rajeev Ahluwalia"
    ],
    "published": "2006-05-23T16:30:52+00:00",
    "url": "http://arxiv.org/pdf/cond-mat/0605577v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/0911.5525v1",
    "title": "Towards an embedding of Graph Transformation in Intuitionistic Linear Logic",
    "content": "Towards an embedding of Graph Transformation in Intuitionistic Linear Logic. Linear logics have been shown to be able to embed both rewriting-based approaches and process calculi in a single, declarative framework. In this paper we are exploring the embedding of double-pushout graph transformations into quantified linear logic, leading to a Curry-Howard style isomorphism between graphs and transformations on one hand, formulas and proof terms on the other. With linear implication representing rules and reachability of graphs, and the tensor modelling parallel composition of graphs and transformations, we obtain a language able to encode graph transformation systems and their computations as well as reason about their properties.",
    "authors": [
      "Paolo Torrini",
      "Reiko Heckel"
    ],
    "published": "2009-11-29T23:14:32+00:00",
    "url": "http://arxiv.org/pdf/0911.5525v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1307.3525v1",
    "title": "Transformity: The Dependence of the Laws of Physics on Higher-Dimensional Coordinate Transformations",
    "content": "Transformity: The Dependence of the Laws of Physics on Higher-Dimensional Coordinate Transformations. In unified field theories with more than four dimensions, the form of the equations of physics in spacetime depends in general on the choice of coordinates in higher dimensions. The reason is that the group of coordinate transformations in (say) five dimensions is broader than in spacetime. This kind of gauge dependence is illustrated by two examples: a cosmological model in general relativity and a matter wave in quantum theory. Surprisingly, both are equivalent by coordinate transformations to flat featureless five-dimensional space. This kind of transformity is of fundamental significance for the philosophy of physics.",
    "authors": [
      "Paul S. Wesson"
    ],
    "published": "2013-06-28T04:02:51+00:00",
    "url": "http://arxiv.org/pdf/1307.3525v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1309.6581v1",
    "title": "Exact inversion of the conical Radon transform with a fixed opening angle",
    "content": "Exact inversion of the conical Radon transform with a fixed opening angle. We study a new class of Radon transforms defined on circular cones called the conical Radon transform. In $\\mathbb{R}^3$ it maps a function to its surface integrals over circular cones, and in $\\mathbb{R}^2$ it maps a function to its integrals along two rays with a common vertex. Such transforms appear in various mathematical models arising in medical imaging, nuclear industry and homeland security. This paper contains new results about inversion of conical Radon transform with fixed opening angle and vertical central axis in $\\mathbb{R}^2$ and $\\mathbb{R}^3$. New simple explicit inversion formulae are presented in these cases. Numerical simulations were performed to demonstrate the efficiency of the suggested algorithm in 2D.",
    "authors": [
      "Rim Gouia-Zarrad",
      "Gaik Ambartsoumian"
    ],
    "published": "2013-09-21T11:51:48+00:00",
    "url": "http://arxiv.org/pdf/1309.6581v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1801.01054v1",
    "title": "Computational complexity lower bounds of certain discrete Radon transform approximations",
    "content": "Computational complexity lower bounds of certain discrete Radon transform approximations. For the computational model where only additions are allowed, the $\\Omega(n^2\\log n)$ lower bound on operations count with respect to image size $n\\times n$ is obtained for two types of the discrete Radon transform implementations: the fast Hough transform and a generic strip pattern class which includes the classical Hough transform, implying the fast Hough transform algorithm asymptotic optimality. The proofs are based on a specific result from the boolean circuits complexity theory and are generalized for the case of boolean $\\vee$ binary operation.",
    "authors": [
      "Timur M. Khanipov"
    ],
    "published": "2018-01-03T15:41:08+00:00",
    "url": "http://arxiv.org/pdf/1801.01054v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1801.08150v3",
    "title": "Quantum Advantage from Sequential-Transformation Contextuality",
    "content": "Quantum Advantage from Sequential-Transformation Contextuality. We introduce a notion of contextuality for transformations in sequential contexts, distinct from the Bell-Kochen-Specker and Spekkens notions of contextuality. Within a transformation-based model for quantum computation we show that strong sequential-transformation contextuality is necessary and sufficient for deterministic computation of non-linear functions if classical components are restricted to mod2-linearity and matching constraints apply to any underlying ontology. For probabilistic computation, sequential-transformation contextuality is necessary and sufficient for advantage in this task and the degree of advantage quantifiably relates to the degree of contextuality.",
    "authors": [
      "Shane Mansfield",
      "Elham Kashefi"
    ],
    "published": "2018-01-24T19:00:23+00:00",
    "url": "http://arxiv.org/pdf/1801.08150v3"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2011.02266v1",
    "title": "Optimizing Transformer for Low-Resource Neural Machine Translation",
    "content": "Optimizing Transformer for Low-Resource Neural Machine Translation. Language pairs with limited amounts of parallel data, also known as low-resource languages, remain a challenge for neural machine translation. While the Transformer model has achieved significant improvements for many language pairs and has become the de facto mainstream architecture, its capability under low-resource conditions has not been fully investigated yet. Our experiments on different subsets of the IWSLT14 training data show that the effectiveness of Transformer under low-resource conditions is highly dependent on the hyper-parameter settings. Our experiments show that using an optimized Transformer for low-resource conditions improves the translation quality up to 7.3 BLEU points compared to using the Transformer default settings.",
    "authors": [
      "Ali Araabi",
      "Christof Monz"
    ],
    "published": "2020-11-04T13:12:29+00:00",
    "url": "http://arxiv.org/pdf/2011.02266v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2102.10519v1",
    "title": "Detection of Transformer Winding Axial Displacement by Kirchhoff and Delay and sum Radar Imaging Algorithms",
    "content": "Detection of Transformer Winding Axial Displacement by Kirchhoff and Delay and sum Radar Imaging Algorithms. In this paper, a novel method for in detail detection of the winding axial displacement in power transformers based on UWB imaging is presented. In this method, the radar imaging process is implemented on the power transformer by using an ultra-wide band (UWB) transceiver. The result is a 2-D image of the transformer winding, which is analyzed to determine the occurrence as well as the magnitude of the winding axial displacement. The method is implemented on a transformer model. The experimental results illustrate the effectiveness of the proposed method.",
    "authors": [
      "Mohammad S. Golsorkhi",
      "R. Mosayebi",
      "M. A. Hejazi",
      "G. B. Gharehpetian",
      "H. Sheikhzadeh"
    ],
    "published": "2021-02-21T05:49:10+00:00",
    "url": "http://arxiv.org/pdf/2102.10519v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2201.13065v2",
    "title": "Rigidity Preserving Image Transformations and Equivariance in Perspective",
    "content": "Rigidity Preserving Image Transformations and Equivariance in Perspective. We characterize the class of image plane transformations which realize rigid camera motions and call these transformations `rigidity preserving'. In particular, 2D translations of pinhole images are not rigidity preserving. Hence, when using CNNs for 3D inference tasks, it can be beneficial to modify the inductive bias from equivariance towards translations to equivariance towards rigidity preserving transformations. We investigate how equivariance with respect to rigidity preserving transformations can be approximated in CNNs, and test our ideas on both 6D object pose estimation and visual localization. Experimentally, we improve on several competitive baselines.",
    "authors": [
      "Lucas Brynte",
      "Georg Bökman",
      "Axel Flinth",
      "Fredrik Kahl"
    ],
    "published": "2022-01-31T08:43:10+00:00",
    "url": "http://arxiv.org/pdf/2201.13065v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2202.02676v1",
    "title": "Autocorrelation, Wigner and Ambiguity Transforms on Polygons for Coherent Radiation Rendering",
    "content": "Autocorrelation, Wigner and Ambiguity Transforms on Polygons for Coherent Radiation Rendering. Simulating the radar illumination of large scenes generally relies on a geometric model of light transport which largely ignores prominent wave effects. This can be remedied through coherence ray-tracing, but this requires the Wigner transform of the aperture. This diffraction function has been historically difficult to generate, and is relevant in the fields of optics, holography, synchrotron-radiation, quantum systems and radar. In this paper we provide the Wigner transform of arbitrary polygons through geometric transforms and the Stokes Fourier transform; and display its use in Monte-Carlo rendering.",
    "authors": [
      "Jacob Mackay",
      "David Johnson",
      "Graham Brooker"
    ],
    "published": "2022-02-06T01:47:32+00:00",
    "url": "http://arxiv.org/pdf/2202.02676v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2204.11066v1",
    "title": "Transformation Invariant Cancerous Tissue Classification Using Spatially Transformed DenseNet",
    "content": "Transformation Invariant Cancerous Tissue Classification Using Spatially Transformed DenseNet. In this work, we introduce a spatially transformed DenseNet architecture for transformation invariant classification of cancer tissue. Our architecture increases the accuracy of the base DenseNet architecture while adding the ability to operate in a transformation invariant way while simultaneously being simpler than other models that try to provide some form of invariance.",
    "authors": [
      "Omar Mahdi",
      "Ali Bou Nassif"
    ],
    "published": "2022-04-23T13:12:50+00:00",
    "url": "http://arxiv.org/pdf/2204.11066v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2205.07387v1",
    "title": "Downstream Transformer Generation of Question-Answer Pairs with Preprocessing and Postprocessing Pipelines",
    "content": "Downstream Transformer Generation of Question-Answer Pairs with Preprocessing and Postprocessing Pipelines. We present a system called TP3 to perform a downstream task of transformers on generating question-answer pairs (QAPs) from a given article. TP3 first finetunes pretrained transformers on QAP datasets, then uses a preprocessing pipeline to select appropriate answers, feeds the relevant sentences and the answer to the finetuned transformer to generate candidate QAPs, and finally uses a postprocessing pipeline to filter inadequate QAPs. In particular, using pretrained T5 models as transformers and the SQuAD dataset as the finetruning dataset, we show that TP3 generates satisfactory number of QAPs with high qualities on the Gaokao-EN dataset.",
    "authors": [
      "Cheng Zhang",
      "Hao Zhang",
      "Jie Wang"
    ],
    "published": "2022-05-15T21:53:45+00:00",
    "url": "http://arxiv.org/pdf/2205.07387v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2210.14139v1",
    "title": "Learning Explicit Object-Centric Representations with Vision Transformers",
    "content": "Learning Explicit Object-Centric Representations with Vision Transformers. With the recent successful adaptation of transformers to the vision domain, particularly when trained in a self-supervised fashion, it has been shown that vision transformers can learn impressive object-reasoning-like behaviour and features expressive for the task of object segmentation in images. In this paper, we build on the self-supervision task of masked autoencoding and explore its effectiveness for explicitly learning object-centric representations with transformers. To this end, we design an object-centric autoencoder using transformers only and train it end-to-end to reconstruct full images from unmasked patches. We show that the model efficiently learns to decompose simple scenes as measured by segmentation metrics on several multi-object benchmarks.",
    "authors": [
      "Oscar Vikström",
      "Alexander Ilin"
    ],
    "published": "2022-10-25T16:39:49+00:00",
    "url": "http://arxiv.org/pdf/2210.14139v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2309.08593v1",
    "title": "Attention-Only Transformers and Implementing MLPs with Attention Heads",
    "content": "Attention-Only Transformers and Implementing MLPs with Attention Heads. The transformer architecture is widely used in machine learning models and consists of two alternating sublayers: attention heads and MLPs. We prove that an MLP neuron can be implemented by a masked attention head with internal dimension 1 so long as the MLP's activation function comes from a restricted class including SiLU and close approximations of ReLU and GeLU. This allows one to convert an MLP-and-attention transformer into an attention-only transformer at the cost of greatly increasing the number of attention heads. We also prove that attention heads can perform the components of an MLP (linear transformations and activation functions) separately. Finally, we prove that attention heads can encode arbitrary masking patterns in their weight matrices to within arbitrarily small error.",
    "authors": [
      "Robert Huben",
      "Valerie Morris"
    ],
    "published": "2023-09-15T17:47:45+00:00",
    "url": "http://arxiv.org/pdf/2309.08593v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2309.16337v1",
    "title": "Logarithm-transform aided Gaussian Sampling for Few-Shot Learning",
    "content": "Logarithm-transform aided Gaussian Sampling for Few-Shot Learning. Few-shot image classification has recently witnessed the rise of representation learning being utilised for models to adapt to new classes using only a few training examples. Therefore, the properties of the representations, such as their underlying probability distributions, assume vital importance. Representations sampled from Gaussian distributions have been used in recent works, [19] to train classifiers for few-shot classification. These methods rely on transforming the distributions of experimental data to approximate Gaussian distributions for their functioning. In this paper, I propose a novel Gaussian transform, that outperforms existing methods on transforming experimental data into Gaussian-like distributions. I then utilise this novel transformation for few-shot image classification and show significant gains in performance, while sampling lesser data.",
    "authors": [
      "Vaibhav Ganatra"
    ],
    "published": "2023-09-28T10:50:32+00:00",
    "url": "http://arxiv.org/pdf/2309.16337v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2405.00906v1",
    "title": "LOTUS: Improving Transformer Efficiency with Sparsity Pruning and Data Lottery Tickets",
    "content": "LOTUS: Improving Transformer Efficiency with Sparsity Pruning and Data Lottery Tickets. Vision transformers have revolutionized computer vision, but their computational demands present challenges for training and deployment. This paper introduces LOTUS (LOttery Transformers with Ultra Sparsity), a novel method that leverages data lottery ticket selection and sparsity pruning to accelerate vision transformer training while maintaining accuracy. Our approach focuses on identifying and utilizing the most informative data subsets and eliminating redundant model parameters to optimize the training process. Through extensive experiments, we demonstrate the effectiveness of LOTUS in achieving rapid convergence and high accuracy with significantly reduced computational requirements. This work highlights the potential of combining data selection and sparsity techniques for efficient vision transformer training, opening doors for further research and development in this area.",
    "authors": [
      "Ojasw Upadhyay"
    ],
    "published": "2024-05-01T23:30:12+00:00",
    "url": "http://arxiv.org/pdf/2405.00906v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2411.00140v1",
    "title": "ViT-LCA: A Neuromorphic Approach for Vision Transformers",
    "content": "ViT-LCA: A Neuromorphic Approach for Vision Transformers. The recent success of Vision Transformers has generated significant interest in attention mechanisms and transformer architectures. Although existing methods have proposed spiking self-attention mechanisms compatible with spiking neural networks, they often face challenges in effective deployment on current neuromorphic platforms. This paper introduces a novel model that combines vision transformers with the Locally Competitive Algorithm (LCA) to facilitate efficient neuromorphic deployment. Our experiments show that ViT-LCA achieves higher accuracy on ImageNet-1K dataset while consuming significantly less energy than other spiking vision transformer counterparts. Furthermore, ViT-LCA's neuromorphic-friendly design allows for more direct mapping onto current neuromorphic architectures.",
    "authors": [
      "Sanaz Mahmoodi Takaghaj"
    ],
    "published": "2024-10-31T18:41:30+00:00",
    "url": "http://arxiv.org/pdf/2411.00140v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2412.15095v1",
    "title": "A Full Transformer-based Framework for Automatic Pain Estimation using Videos",
    "content": "A Full Transformer-based Framework for Automatic Pain Estimation using Videos. The automatic estimation of pain is essential in designing an optimal pain management system offering reliable assessment and reducing the suffering of patients. In this study, we present a novel full transformer-based framework consisting of a Transformer in Transformer (TNT) model and a Transformer leveraging cross-attention and self-attention blocks. Elaborating on videos from the BioVid database, we demonstrate state-of-the-art performances, showing the efficacy, efficiency, and generalization capability across all the primary pain estimation tasks.",
    "authors": [
      "Stefanos Gkikas",
      "Manolis Tsiknakis"
    ],
    "published": "2024-12-19T17:45:08+00:00",
    "url": "http://arxiv.org/pdf/2412.15095v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2501.11789v1",
    "title": "The termination of Nielsen transformations applied to word equations with length constraints",
    "content": "The termination of Nielsen transformations applied to word equations with length constraints. Nielsen transformations form the basis of a simple and widely used procedure for solving word equations. We make progress on the problem of determining when this procedure terminates in the presence of length constraints. To do this, we introduce extended word equations, a mathematical model of a word equation with partial information about length constraints. We then define extended Nielsen transformations, which adapt Nielsen transformations to the setting of extended word equations. We provide a partial characterization of when repeatedly applying extended Nielsen transformations to an extended word equation is guaranteed to terminate.",
    "authors": [
      "Benjamin Przybocki",
      "Clark Barrett"
    ],
    "published": "2025-01-20T23:38:14+00:00",
    "url": "http://arxiv.org/pdf/2501.11789v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2502.05656v1",
    "title": "Flowing Through Layers: A Continuous Dynamical Systems Perspective on Transformers",
    "content": "Flowing Through Layers: A Continuous Dynamical Systems Perspective on Transformers. We show that the standard discrete update rule of transformer layers can be naturally interpreted as a forward Euler discretization of a continuous dynamical system. Our Transformer Flow Approximation Theorem demonstrates that, under standard Lipschitz continuity assumptions, token representations converge uniformly to the unique solution of an ODE as the number of layers grows. Moreover, if the underlying mapping satisfies a one-sided Lipschitz condition with a negative constant, the resulting dynamics are contractive, causing perturbations to decay exponentially across layers. Beyond clarifying the empirical stability and expressivity of transformer models, these insights link transformer updates to a broader iterative reasoning framework, suggesting new avenues for accelerated convergence and architectural innovations inspired by dynamical systems theory.",
    "authors": [
      "Jacob Fein-Ashley"
    ],
    "published": "2025-02-08T18:11:40+00:00",
    "url": "http://arxiv.org/pdf/2502.05656v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2503.14259v1",
    "title": "Quantization-Free Autoregressive Action Transformer",
    "content": "Quantization-Free Autoregressive Action Transformer. Current transformer-based imitation learning approaches introduce discrete action representations and train an autoregressive transformer decoder on the resulting latent code. However, the initial quantization breaks the continuous structure of the action space thereby limiting the capabilities of the generative model. We propose a quantization-free method instead that leverages Generative Infinite-Vocabulary Transformers (GIVT) as a direct, continuous policy parametrization for autoregressive transformers. This simplifies the imitation learning pipeline while achieving state-of-the-art performance on a variety of popular simulated robotics tasks. We enhance our policy roll-outs by carefully studying sampling algorithms, further improving the results.",
    "authors": [
      "Ziyad Sheebaelhamd",
      "Michael Tschannen",
      "Michael Muehlebach",
      "Claire Vernade"
    ],
    "published": "2025-03-18T13:50:35+00:00",
    "url": "http://arxiv.org/pdf/2503.14259v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/hep-th/9503045v1",
    "title": "Non-Abelian Duality and Canonical Transformations",
    "content": "Non-Abelian Duality and Canonical Transformations. We construct explicit canonical transformations producing non-abelian duals in principal chiral models with arbitrary group G. Some comments concerning the extension to more general $\\sigma$-models, like WZW models, are given.",
    "authors": [
      "Y. Lozano"
    ],
    "published": "1995-03-08T03:21:03+00:00",
    "url": "http://arxiv.org/pdf/hep-th/9503045v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/math/0511508v1",
    "title": "Quantile regression in transformation models",
    "content": "Quantile regression in transformation models. Conditional quantiles provide a natural tool for reporting results from regression analyses based on semiparametric transformation models. We consider their estimation and construction of confidence sets in the presence of censoring.",
    "authors": [
      "Dorota M. Dabrowska"
    ],
    "published": "2005-11-21T04:07:16+00:00",
    "url": "http://arxiv.org/pdf/math/0511508v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2207.12148v1",
    "title": "Applying Spatiotemporal Attention to Identify Distracted and Drowsy Driving with Vision Transformers",
    "content": "Applying Spatiotemporal Attention to Identify Distracted and Drowsy Driving with Vision Transformers. A 20% rise in car crashes in 2021 compared to 2020 has been observed as a result of increased distraction and drowsiness. Drowsy and distracted driving are the cause of 45% of all car crashes. As a means to decrease drowsy and distracted driving, detection methods using computer vision can be designed to be low-cost, accurate, and minimally invasive. This work investigated the use of the vision transformer to outperform state-of-the-art accuracy from 3D-CNNs. Two separate transformers were trained for drowsiness and distractedness. The drowsy video transformer model was trained on the National Tsing-Hua University Drowsy Driving Dataset (NTHU-DDD) with a Video Swin Transformer model for 10 epochs on two classes -- drowsy and non-drowsy simulated over 10.5 hours. The distracted video transformer was trained on the Driver Monitoring Dataset (DMD) with Video Swin Transformer for 50 epochs over 9 distraction-related classes. The accuracy of the drowsiness model reached 44% and a high loss value on the test set, indicating overfitting and poor model performance. Overfitting indicates limited training data and applied model architecture lacked quantifiable parameters to learn. The distracted model outperformed state-of-the-art models on DMD reaching 97.5%, indicating that with sufficient data and a strong architecture, transformers are suitable for unfit driving detection. Future research should use newer and stronger models such as TokenLearner to achieve higher accuracy and efficiency, merge existing datasets to expand to detecting drunk driving and road rage to create a comprehensive solution to prevent traffic crashes, and deploying a functioning prototype to revolutionize the automotive safety industry.",
    "authors": [
      "Samay Lakhani"
    ],
    "published": "2022-07-22T16:36:48+00:00",
    "url": "http://arxiv.org/pdf/2207.12148v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2408.10189v2",
    "title": "Transformers to SSMs: Distilling Quadratic Knowledge to Subquadratic Models",
    "content": "Transformers to SSMs: Distilling Quadratic Knowledge to Subquadratic Models. Transformer architectures have become a dominant paradigm for domains like language modeling but suffer in many inference settings due to their quadratic-time self-attention. Recently proposed subquadratic architectures, such as Mamba, have shown promise, but have been pretrained with substantially less computational resources than the strongest Transformer models. In this work, we present a method that is able to distill a pretrained Transformer architecture into alternative architectures such as state space models (SSMs). The key idea to our approach is that we can view both Transformers and SSMs as applying different forms of mixing matrices over the token sequences. We can thus progressively distill the Transformer architecture by matching different degrees of granularity in the SSM: first matching the mixing matrices themselves, then the hidden units at each block, and finally the end-to-end predictions. Our method, called MOHAWK, is able to distill a Mamba-2 variant based on the Phi-1.5 architecture (Phi-Mamba) using only 3B tokens and a hybrid version (Hybrid Phi-Mamba) using 5B tokens. Despite using less than 1% of the training data typically used to train models from scratch, Phi-Mamba boasts substantially stronger performance compared to all past open-source non-Transformer models. MOHAWK allows models like SSMs to leverage computational resources invested in training Transformer-based architectures, highlighting a new avenue for building such models.",
    "authors": [
      "Aviv Bick",
      "Kevin Y. Li",
      "Eric P. Xing",
      "J. Zico Kolter",
      "Albert Gu"
    ],
    "published": "2024-08-19T17:48:11+00:00",
    "url": "http://arxiv.org/pdf/2408.10189v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2409.16302v2",
    "title": "How Redundant Is the Transformer Stack in Speech Representation Models?",
    "content": "How Redundant Is the Transformer Stack in Speech Representation Models?. Self-supervised speech representation models, particularly those leveraging transformer architectures, have demonstrated remarkable performance across various tasks such as speech recognition, speaker identification, and emotion detection. Recent studies on transformer models revealed a high redundancy between layers and the potential for significant pruning, which we will investigate here for transformer-based speech representation models. We perform a detailed analysis of layer similarity in speech representation models using three similarity metrics: cosine similarity, centered kernel alignment, and mutual nearest-neighbor alignment. Our findings reveal a block-like structure of high similarity, suggesting two main processing steps and significant redundancy of layers. We demonstrate the effectiveness of pruning transformer-based speech representation models without the need for post-training, achieving up to 40% reduction in transformer layers while maintaining over 95% of the model's predictive capacity. Furthermore, we employ a knowledge distillation method to substitute the entire transformer stack with mimicking layers, reducing the network size 95-98% and the inference time by up to 94%. This substantial decrease in computational load occurs without considerable performance loss, suggesting that the transformer stack is almost completely redundant for downstream applications of speech representation models.",
    "authors": [
      "Teresa Dorszewski",
      "Albert Kjøller Jacobsen",
      "Lenka Tětková",
      "Lars Kai Hansen"
    ],
    "published": "2024-09-10T11:00:24+00:00",
    "url": "http://arxiv.org/pdf/2409.16302v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2411.07118v3",
    "title": "ConvMixFormer- A Resource-efficient Convolution Mixer for Transformer-based Dynamic Hand Gesture Recognition",
    "content": "ConvMixFormer- A Resource-efficient Convolution Mixer for Transformer-based Dynamic Hand Gesture Recognition. Transformer models have demonstrated remarkable success in many domains such as natural language processing (NLP) and computer vision. With the growing interest in transformer-based architectures, they are now utilized for gesture recognition. So, we also explore and devise a novel ConvMixFormer architecture for dynamic hand gestures. The transformers use quadratic scaling of the attention features with the sequential data, due to which these models are computationally complex and heavy. We have considered this drawback of the transformer and designed a resource-efficient model that replaces the self-attention in the transformer with the simple convolutional layer-based token mixer. The computational cost and the parameters used for the convolution-based mixer are comparatively less than the quadratic self-attention. Convolution-mixer helps the model capture the local spatial features that self-attention struggles to capture due to their sequential processing nature. Further, an efficient gate mechanism is employed instead of a conventional feed-forward network in the transformer to help the model control the flow of features within different stages of the proposed model. This design uses fewer learnable parameters which is nearly half the vanilla transformer that helps in fast and efficient training. The proposed method is evaluated on NVidia Dynamic Hand Gesture and Briareo datasets and our model has achieved state-of-the-art results on single and multimodal inputs. We have also shown the parameter efficiency of the proposed ConvMixFormer model compared to other methods. The source code is available at https://github.com/mallikagarg/ConvMixFormer.",
    "authors": [
      "Mallika Garg",
      "Debashis Ghosh",
      "Pyari Mohan Pradhan"
    ],
    "published": "2024-11-11T16:45:18+00:00",
    "url": "http://arxiv.org/pdf/2411.07118v3"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1210.6261v1",
    "title": "Non Linear Lorentz Transformation and Doubly Special Relativity",
    "content": "Non Linear Lorentz Transformation and Doubly Special Relativity. We generate non-linear representations of the Lorentz Group by unitary transformation over the Lorentz generators. To do that we use deformed scale transformations by introducing momentum-depending parameters. The momentum operator transformation is found to be equivalent to a particle momentum transformation. The configuration space transformation is found to depend on the old momentum operator and we show that this transformation generates models with two scales, one for the velocity ($c$) and another one for the energy. A Lagrangian formalism is proposed for these models and an effective metric for the deformed Minkowski space is found. We show that the Smolin model is one in a family of doubly special relativity. Finally we construct an ansatz for the quantization of such theories.",
    "authors": [
      "A. N. Atehortua",
      "D. E. Jaramillo",
      "J. M. Mira",
      "N. Vanegas"
    ],
    "published": "2012-10-23T15:05:11+00:00",
    "url": "http://arxiv.org/pdf/1210.6261v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1701.05258v1",
    "title": "Symbolic Computation of Equivalence Transformations and Parameter Reduction for Nonlinear Physical Models",
    "content": "Symbolic Computation of Equivalence Transformations and Parameter Reduction for Nonlinear Physical Models. An efficient systematic procedure is provided for symbolic computation of Lie groups of equivalence transformations and generalized equivalence transformations of systems of differential equations that contain arbitrary elements (arbitrary functions and/or arbitrary constant parameters), using the software package GeM for Maple. Application of equivalence transformations to the reduction of the number of arbitrary elements in a given system of equations is discussed, and several examples are considered. First computational example of a generalized equivalence transformation where the transformation of the dependent variable involves the arbitrary constitutive function is presented.   As a detailed physical example, a three-parameter family of nonlinear wave equations describing finite anti-plane shear displacements of an incompressible hyperelasic fiber-reinforced medium is considered. Equivalence transformations are computed and employed to radically simplify the model for an arbitrary fiber direction, invertibly reducing the model to a simple form that corresponds to a special fiber direction, and involves no arbitrary elements.   The presented computation algorithm is applicable to wide classes of systems of differential equations containing arbitrary elements.",
    "authors": [
      "Alexei F. Cheviakov"
    ],
    "published": "2017-01-18T23:26:13+00:00",
    "url": "http://arxiv.org/pdf/1701.05258v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1809.03505v2",
    "title": "Generalised model-independent characterisation of strong gravitational lenses IV: formalism-intrinsic degeneracies",
    "content": "Generalised model-independent characterisation of strong gravitational lenses IV: formalism-intrinsic degeneracies. Based on the standard gravitational lensing formalism with its effective, projected lensing potential in a given background cosmology, we investigate under which transformations of the source position and of the deflection angle the observable properties of the multiple images, i.e. the time delay differences, the relative image positions, relative shapes, and magnification ratios, remain invariant. As these observables only constrain local lens properties, we derive general, local invariance transformations in the areas covered by the multiple images. We show that the known global invariance transformations, e.g. the mass sheet transformation or the source position transformation, are contained in our invariance transformations, when they are restricted to the areas covered by the multiple images and when lens-model-based degeneracies are ignored, like the freedom to add or subtract masses in unconstrained regions without multiple images. Hence, we have identified the general class of invariance transformations that can occur, in particular in our model-independent local characterisation of strong gravitational lenses.",
    "authors": [
      "Jenny Wagner"
    ],
    "published": "2018-09-10T18:00:02+00:00",
    "url": "http://arxiv.org/pdf/1809.03505v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2006.14612v1",
    "title": "Multilevel Typed Graph Transformations",
    "content": "Multilevel Typed Graph Transformations. Multilevel modeling extends traditional modeling techniques with a potentially unlimited number of abstraction levels. Multilevel models can be formally represented by multilevel typed graphs whose manipulation and transformation are carried out by multilevel typed graph transformation rules. These rules are cospans of three graphs and two inclusion graph homomorphisms where the three graphs are multilevel typed over a common typing chain. In this paper, we show that typed graph transformations can be appropriately generalized to multilevel typed graph transformations improving preciseness, flexibility and reusability of transformation rules. We identify type compatibility conditions, for rules and their matches, formulated as equations and inequations, respectively, between composed partial typing morphisms. These conditions are crucial presuppositions for the application of a rule for a match---based on a pushout and a final pullback complement construction for the underlying graphs in the category Graph---to always provide a well-defined canonical result in the multilevel typed setting. Moreover, to formalize and analyze multilevel typing as well as to prove the necessary results, in a systematic way, we introduce the category Chain of typing chains and typing chain morphisms.",
    "authors": [
      "Uwe Wolter",
      "Fernando Macías",
      "Adrian Rutle"
    ],
    "published": "2020-06-25T17:55:17+00:00",
    "url": "http://arxiv.org/pdf/2006.14612v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2104.12470v5",
    "title": "Easy and Efficient Transformer : Scalable Inference Solution For large NLP model",
    "content": "Easy and Efficient Transformer : Scalable Inference Solution For large NLP model. Recently, large-scale transformer-based models have been proven to be effective over various tasks across many domains. Nevertheless, applying them in industrial production requires tedious and heavy works to reduce inference costs. To fill such a gap, we introduce a scalable inference solution: Easy and Efficient Transformer (EET), including a series of transformer inference optimization at the algorithm and implementation levels. First, we design highly optimized kernels for long inputs and large hidden sizes. Second, we propose a flexible CUDA memory manager to reduce the memory footprint when deploying a large model. Compared with the state-of-the-art transformer inference library (Faster Transformer v4.0), EET can achieve an average of 1.40-4.20x speedup on the transformer decoder layer with an A100 GPU",
    "authors": [
      "Gongzheng Li",
      "Yadong Xi",
      "Jingzhen Ding",
      "Duan Wang",
      "Bai Liu",
      "Changjie Fan",
      "Xiaoxi Mao",
      "Zeng Zhao"
    ],
    "published": "2021-04-26T11:00:56+00:00",
    "url": "http://arxiv.org/pdf/2104.12470v5"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1901.11188v2",
    "title": "Augmenting Model Robustness with Transformation-Invariant Attacks",
    "content": "Augmenting Model Robustness with Transformation-Invariant Attacks. The vulnerability of neural networks under adversarial attacks has raised serious concerns and motivated extensive research. It has been shown that both neural networks and adversarial attacks against them can be sensitive to input transformations such as linear translation and rotation, and that human vision, which is robust against adversarial attacks, is invariant to natural input transformations. Based on these, this paper tests the hypothesis that model robustness can be further improved when it is adversarially trained against transformed attacks and transformation-invariant attacks. Experiments on MNIST, CIFAR-10, and restricted ImageNet show that while transformations of attacks alone do not affect robustness, transformation-invariant attacks can improve model robustness by 2.5\\% on MNIST, 3.7\\% on CIFAR-10, and 1.1\\% on restricted ImageNet. We discuss the intuition behind this phenomenon.",
    "authors": [
      "Houpu Yao",
      "Zhe Wang",
      "Guangyu Nie",
      "Yassine Mazboudi",
      "Yezhou Yang",
      "Yi Ren"
    ],
    "published": "2019-01-31T02:56:28+00:00",
    "url": "http://arxiv.org/pdf/1901.11188v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2112.05112v2",
    "title": "BLT: Bidirectional Layout Transformer for Controllable Layout Generation",
    "content": "BLT: Bidirectional Layout Transformer for Controllable Layout Generation. Creating visual layouts is a critical step in graphic design. Automatic generation of such layouts is essential for scalable and diverse visual designs. To advance conditional layout generation, we introduce BLT, a bidirectional layout transformer. BLT differs from previous work on transformers in adopting non-autoregressive transformers. In training, BLT learns to predict the masked attributes by attending to surrounding attributes in two directions. During inference, BLT first generates a draft layout from the input and then iteratively refines it into a high-quality layout by masking out low-confident attributes. The masks generated in both training and inference are controlled by a new hierarchical sampling policy. We verify the proposed model on six benchmarks of diverse design tasks. Experimental results demonstrate two benefits compared to the state-of-the-art layout transformer models. First, our model empowers layout transformers to fulfill controllable layout generation. Second, it achieves up to 10x speedup in generating a layout at inference time than the layout transformer baseline. Code is released at https://shawnkx.github.io/blt.",
    "authors": [
      "Xiang Kong",
      "Lu Jiang",
      "Huiwen Chang",
      "Han Zhang",
      "Yuan Hao",
      "Haifeng Gong",
      "Irfan Essa"
    ],
    "published": "2021-12-09T18:49:28+00:00",
    "url": "http://arxiv.org/pdf/2112.05112v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1703.01397v1",
    "title": "Machine Learning Applications in Estimating Transformer Loss of Life",
    "content": "Machine Learning Applications in Estimating Transformer Loss of Life. Transformer life assessment and failure diagnostics have always been important problems for electric utility companies. Ambient temperature and load profile are the main factors which affect aging of the transformer insulation, and consequently, the transformer lifetime. The IEEE Std. C57.911995 provides a model for calculating the transformer loss of life based on ambient temperature and transformer's loading. In this paper, this standard is used to develop a data-driven static model for hourly estimation of the transformer loss of life. Among various machine learning methods for developing this static model, the Adaptive Network-Based Fuzzy Inference System (ANFIS) is selected. Numerical simulations demonstrate the effectiveness and the accuracy of the proposed ANFIS method compared with other relevant machine learning based methods to solve this problem.",
    "authors": [
      "Alireza Majzoobi",
      "Mohsen Mahoor",
      "Amin Khodaei"
    ],
    "published": "2017-03-04T05:19:26+00:00",
    "url": "http://arxiv.org/pdf/1703.01397v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2103.17239v2",
    "title": "Going deeper with Image Transformers",
    "content": "Going deeper with Image Transformers. Transformers have been recently adapted for large scale image classification, achieving high scores shaking up the long supremacy of convolutional neural networks. However the optimization of image transformers has been little studied so far. In this work, we build and optimize deeper transformer networks for image classification. In particular, we investigate the interplay of architecture and optimization of such dedicated transformers. We make two transformers architecture changes that significantly improve the accuracy of deep transformers. This leads us to produce models whose performance does not saturate early with more depth, for instance we obtain 86.5% top-1 accuracy on Imagenet when training with no external data, we thus attain the current SOTA with less FLOPs and parameters. Moreover, our best model establishes the new state of the art on Imagenet with Reassessed labels and Imagenet-V2 / match frequency, in the setting with no additional training data. We share our code and models.",
    "authors": [
      "Hugo Touvron",
      "Matthieu Cord",
      "Alexandre Sablayrolles",
      "Gabriel Synnaeve",
      "Hervé Jégou"
    ],
    "published": "2021-03-31T17:37:32+00:00",
    "url": "http://arxiv.org/pdf/2103.17239v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2111.12763v1",
    "title": "Sparse is Enough in Scaling Transformers",
    "content": "Sparse is Enough in Scaling Transformers. Large Transformer models yield impressive results on many tasks, but are expensive to train, or even fine-tune, and so slow at decoding that their use and study becomes out of reach. We address this problem by leveraging sparsity. We study sparse variants for all layers in the Transformer and propose Scaling Transformers, a family of next generation Transformer models that use sparse layers to scale efficiently and perform unbatched decoding much faster than the standard Transformer as we scale up the model size. Surprisingly, the sparse layers are enough to obtain the same perplexity as the standard Transformer with the same number of parameters. We also integrate with prior sparsity approaches to attention and enable fast inference on long sequences even with limited memory. This results in performance competitive to the state-of-the-art on long text summarization.",
    "authors": [
      "Sebastian Jaszczur",
      "Aakanksha Chowdhery",
      "Afroz Mohiuddin",
      "Łukasz Kaiser",
      "Wojciech Gajewski",
      "Henryk Michalewski",
      "Jonni Kanerva"
    ],
    "published": "2021-11-24T19:53:46+00:00",
    "url": "http://arxiv.org/pdf/2111.12763v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2111.13844v4",
    "title": "Adaptive Image Transformations for Transfer-based Adversarial Attack",
    "content": "Adaptive Image Transformations for Transfer-based Adversarial Attack. Adversarial attacks provide a good way to study the robustness of deep learning models. One category of methods in transfer-based black-box attack utilizes several image transformation operations to improve the transferability of adversarial examples, which is effective, but fails to take the specific characteristic of the input image into consideration. In this work, we propose a novel architecture, called Adaptive Image Transformation Learner (AITL), which incorporates different image transformation operations into a unified framework to further improve the transferability of adversarial examples. Unlike the fixed combinational transformations used in existing works, our elaborately designed transformation learner adaptively selects the most effective combination of image transformations specific to the input image. Extensive experiments on ImageNet demonstrate that our method significantly improves the attack success rates on both normally trained models and defense models under various settings.",
    "authors": [
      "Zheng Yuan",
      "Jie Zhang",
      "Shiguang Shan"
    ],
    "published": "2021-11-27T08:15:44+00:00",
    "url": "http://arxiv.org/pdf/2111.13844v4"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1712.07364v1",
    "title": "Transformation Models in High-Dimensions",
    "content": "Transformation Models in High-Dimensions. Transformation models are a very important tool for applied statisticians and econometricians. In many applications, the dependent variable is transformed so that homogeneity or normal distribution of the error holds. In this paper, we analyze transformation models in a high-dimensional setting, where the set of potential covariates is large. We propose an estimator for the transformation parameter and we show that it is asymptotically normally distributed using an orthogonalized moment condition where the nuisance functions depend on the target parameter. In a simulation study, we show that the proposed estimator works well in small samples. A common practice in labor economics is to transform wage with the log-function. In this study, we test if this transformation holds in CPS data from the United States.",
    "authors": [
      "Sven Klaassen",
      "Jannis Kueck",
      "Martin Spindler"
    ],
    "published": "2017-12-20T08:58:41+00:00",
    "url": "http://arxiv.org/pdf/1712.07364v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1807.11605v1",
    "title": "Doubly Attentive Transformer Machine Translation",
    "content": "Doubly Attentive Transformer Machine Translation. In this paper a doubly attentive transformer machine translation model (DATNMT) is presented in which a doubly-attentive transformer decoder normally joins spatial visual features obtained via pretrained convolutional neural networks, conquering any gap between image captioning and translation. In this framework, the transformer decoder figures out how to take care of source-language words and parts of an image freely by methods for two separate attention components in an Enhanced Multi-Head Attention Layer of doubly attentive transformer, as it generates words in the target language. We find that the proposed model can effectively exploit not just the scarce multimodal machine translation data, but also large general-domain text-only machine translation corpora, or image-text image captioning corpora. The experimental results show that the proposed doubly-attentive transformer-decoder performs better than a single-decoder transformer model, and gives the state-of-the-art results in the English-German multimodal machine translation task.",
    "authors": [
      "Hasan Sait Arslan",
      "Mark Fishel",
      "Gholamreza Anbarjafari"
    ],
    "published": "2018-07-30T23:13:55+00:00",
    "url": "http://arxiv.org/pdf/1807.11605v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2106.05786v1",
    "title": "CAT: Cross Attention in Vision Transformer",
    "content": "CAT: Cross Attention in Vision Transformer. Since Transformer has found widespread use in NLP, the potential of Transformer in CV has been realized and has inspired many new approaches. However, the computation required for replacing word tokens with image patches for Transformer after the tokenization of the image is vast(e.g., ViT), which bottlenecks model training and inference. In this paper, we propose a new attention mechanism in Transformer termed Cross Attention, which alternates attention inner the image patch instead of the whole image to capture local information and apply attention between image patches which are divided from single-channel feature maps capture global information. Both operations have less computation than standard self-attention in Transformer. By alternately applying attention inner patch and between patches, we implement cross attention to maintain the performance with lower computational cost and build a hierarchical network called Cross Attention Transformer(CAT) for other vision tasks. Our base model achieves state-of-the-arts on ImageNet-1K, and improves the performance of other methods on COCO and ADE20K, illustrating that our network has the potential to serve as general backbones. The code and models are available at \\url{https://github.com/linhezheng19/CAT}.",
    "authors": [
      "Hezheng Lin",
      "Xing Cheng",
      "Xiangyu Wu",
      "Fan Yang",
      "Dong Shen",
      "Zhongyuan Wang",
      "Qing Song",
      "Wei Yuan"
    ],
    "published": "2021-06-10T14:38:32+00:00",
    "url": "http://arxiv.org/pdf/2106.05786v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2207.02390v1",
    "title": "Swin Deformable Attention U-Net Transformer (SDAUT) for Explainable Fast MRI",
    "content": "Swin Deformable Attention U-Net Transformer (SDAUT) for Explainable Fast MRI. Fast MRI aims to reconstruct a high fidelity image from partially observed measurements. Exuberant development in fast MRI using deep learning has been witnessed recently. Meanwhile, novel deep learning paradigms, e.g., Transformer based models, are fast-growing in natural language processing and promptly developed for computer vision and medical image analysis due to their prominent performance. Nevertheless, due to the complexity of the Transformer, the application of fast MRI may not be straightforward. The main obstacle is the computational cost of the self-attention layer, which is the core part of the Transformer, can be expensive for high resolution MRI inputs. In this study, we propose a new Transformer architecture for solving fast MRI that coupled Shifted Windows Transformer with U-Net to reduce the network complexity. We incorporate deformable attention to construe the explainability of our reconstruction model. We empirically demonstrate that our method achieves consistently superior performance on the fast MRI task. Besides, compared to state-of-the-art Transformer models, our method has fewer network parameters while revealing explainability. The code is publicly available at https://github.com/ayanglab/SDAUT.",
    "authors": [
      "Jiahao Huang",
      "Xiaodan Xing",
      "Zhifan Gao",
      "Guang Yang"
    ],
    "published": "2022-07-05T15:56:46+00:00",
    "url": "http://arxiv.org/pdf/2207.02390v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2207.13367v1",
    "title": "Optimizing transformations for contrastive learning in a differentiable framework",
    "content": "Optimizing transformations for contrastive learning in a differentiable framework. Current contrastive learning methods use random transformations sampled from a large list of transformations, with fixed hyperparameters, to learn invariance from an unannotated database. Following previous works that introduce a small amount of supervision, we propose a framework to find optimal transformations for contrastive learning using a differentiable transformation network. Our method increases performances at low annotated data regime both in supervision accuracy and in convergence speed. In contrast to previous work, no generative model is needed for transformation optimization. Transformed images keep relevant information to solve the supervised task, here classification. Experiments were performed on 34000 2D slices of brain Magnetic Resonance Images and 11200 chest X-ray images. On both datasets, with 10% of labeled data, our model achieves better performances than a fully supervised model with 100% labels.",
    "authors": [
      "Camille Ruppli",
      "Pietro Gori",
      "Roberto Ardon",
      "Isabelle Bloch"
    ],
    "published": "2022-07-27T08:47:57+00:00",
    "url": "http://arxiv.org/pdf/2207.13367v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2208.12359v1",
    "title": "Social Diversity for ATL Repair",
    "content": "Social Diversity for ATL Repair. Model transformations play an essential role in the Model-Driven Engineering paradigm. Writing a correct transformation program requires to be proficient with the source and target modeling languages, to have a clear understanding of the mapping between the elements of the two, as well as to master the transformation language to properly describe the transformation. Transformation programs are thus complex and error-prone, and finding and fixing errors in such programs typically involve a tedious and time-consuming effort by developers. In this paper, we propose a novel search-based approach to automatically repair transformation programs containing many semantic errors. To prevent the fitness plateaus and the single fitness peak limitations, we leverage the notion of social diversity to promote repair patches tackling errors that are less covered by the other patches of the population. We evaluate our approach on 71 semantically incorrect transformation programs written in ATL, and containing up to five semantic errors simultaneously. The evaluation shows that integrating social diversity when searching for repair patches allows to improve the quality of those patches and to speed up the convergence even when up to five semantic errors are involved.",
    "authors": [
      "Zahra Varaminybahnemiry",
      "Jessie Galasso",
      "Houari Sahraoui"
    ],
    "published": "2022-08-25T22:18:38+00:00",
    "url": "http://arxiv.org/pdf/2208.12359v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2210.02904v1",
    "title": "WakeUpNet: A Mobile-Transformer based Framework for End-to-End Streaming Voice Trigger",
    "content": "WakeUpNet: A Mobile-Transformer based Framework for End-to-End Streaming Voice Trigger. End-to-end models have gradually become the main technical stream for voice trigger, aiming to achieve an utmost prediction accuracy but with a small footprint. In present paper, we propose an end-to-end voice trigger framework, namely WakeupNet, which is basically structured on a Transformer encoder. The purpose of this framework is to explore the context-capturing capability of Transformer, as sequential information is vital for wakeup-word detection. However, the conventional Transformer encoder is too large to fit our task. To address this issue, we introduce different model compression approaches to shrink the vanilla one into a tiny one, called mobile-Transformer. To evaluate the performance of mobile-Transformer, we conduct extensive experiments on a large public-available dataset HiMia. The obtained results indicate that introduced mobile-Transformer significantly outperforms other frequently used models for voice trigger in both clean and noisy scenarios.",
    "authors": [
      "Zixing Zhang",
      "Thorin Farnsworth",
      "Senling Lin",
      "Salah Karout"
    ],
    "published": "2022-10-06T13:18:48+00:00",
    "url": "http://arxiv.org/pdf/2210.02904v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2301.09869v2",
    "title": "Image Super-Resolution using Efficient Striped Window Transformer",
    "content": "Image Super-Resolution using Efficient Striped Window Transformer. Transformers have achieved remarkable results in single-image super-resolution (SR). However, the challenge of balancing model performance and complexity has hindered their application in lightweight SR (LSR). To tackle this challenge, we propose an efficient striped window transformer (ESWT). We revisit the normalization layer in the transformer and design a concise and efficient transformer structure to build the ESWT. Furthermore, we introduce a striped window mechanism to model long-term dependencies more efficiently. To fully exploit the potential of the ESWT, we propose a novel flexible window training strategy that can improve the performance of the ESWT without additional cost. Extensive experiments show that ESWT outperforms state-of-the-art LSR transformers, and achieves a better trade-off between model performance and complexity. The ESWT requires fewer parameters, incurs faster inference, smaller FLOPs, and less memory consumption, making it a promising solution for LSR.",
    "authors": [
      "Jinpeng Shi",
      "Hui Li",
      "Tianle Liu",
      "Yulong Liu",
      "Mingjian Zhang",
      "Jinchen Zhu",
      "Ling Zheng",
      "Shizhuang Weng"
    ],
    "published": "2023-01-24T09:09:35+00:00",
    "url": "http://arxiv.org/pdf/2301.09869v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2308.04791v2",
    "title": "PETformer: Long-term Time Series Forecasting via Placeholder-enhanced Transformer",
    "content": "PETformer: Long-term Time Series Forecasting via Placeholder-enhanced Transformer. Recently, the superiority of Transformer for long-term time series forecasting (LTSF) tasks has been challenged, particularly since recent work has shown that simple models can outperform numerous Transformer-based approaches. This suggests that a notable gap remains in fully leveraging the potential of Transformer in LTSF tasks. Consequently, this study investigates key issues when applying Transformer to LTSF, encompassing aspects of temporal continuity, information density, and multi-channel relationships. We introduce the Placeholder-enhanced Technique (PET) to enhance the computational efficiency and predictive accuracy of Transformer in LTSF tasks. Furthermore, we delve into the impact of larger patch strategies and channel interaction strategies on Transformer's performance, specifically Long Sub-sequence Division (LSD) and Multi-channel Separation and Interaction (MSI). These strategies collectively constitute a novel model termed PETformer. Extensive experiments have demonstrated that PETformer achieves state-of-the-art performance on eight commonly used public datasets for LTSF, surpassing all existing models. The insights and enhancement methodologies presented in this paper serve as valuable reference points and sources of inspiration for future research endeavors.",
    "authors": [
      "Shengsheng Lin",
      "Weiwei Lin",
      "Wentai Wu",
      "Songbo Wang",
      "Yongxiang Wang"
    ],
    "published": "2023-08-09T08:30:22+00:00",
    "url": "http://arxiv.org/pdf/2308.04791v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2308.14568v1",
    "title": "Time-Frequency Transformer: A Novel Time Frequency Joint Learning Method for Speech Emotion Recognition",
    "content": "Time-Frequency Transformer: A Novel Time Frequency Joint Learning Method for Speech Emotion Recognition. In this paper, we propose a novel time-frequency joint learning method for speech emotion recognition, called Time-Frequency Transformer. Its advantage is that the Time-Frequency Transformer can excavate global emotion patterns in the time-frequency domain of speech signal while modeling the local emotional correlations in the time domain and frequency domain respectively. For the purpose, we first design a Time Transformer and Frequency Transformer to capture the local emotion patterns between frames and inside frequency bands respectively, so as to ensure the integrity of the emotion information modeling in both time and frequency domains. Then, a Time-Frequency Transformer is proposed to mine the time-frequency emotional correlations through the local time-domain and frequency-domain emotion features for learning more discriminative global speech emotion representation. The whole process is a time-frequency joint learning process implemented by a series of Transformer models. Experiments on IEMOCAP and CASIA databases indicate that our proposed method outdoes the state-of-the-art methods.",
    "authors": [
      "Yong Wang",
      "Cheng Lu",
      "Yuan Zong",
      "Hailun Lian",
      "Yan Zhao",
      "Sunan Li"
    ],
    "published": "2023-08-28T13:34:02+00:00",
    "url": "http://arxiv.org/pdf/2308.14568v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2408.10483v1",
    "title": "PRformer: Pyramidal Recurrent Transformer for Multivariate Time Series Forecasting",
    "content": "PRformer: Pyramidal Recurrent Transformer for Multivariate Time Series Forecasting. The self-attention mechanism in Transformer architecture, invariant to sequence order, necessitates positional embeddings to encode temporal order in time series prediction. We argue that this reliance on positional embeddings restricts the Transformer's ability to effectively represent temporal sequences, particularly when employing longer lookback windows. To address this, we introduce an innovative approach that combines Pyramid RNN embeddings(PRE) for univariate time series with the Transformer's capability to model multivariate dependencies. PRE, utilizing pyramidal one-dimensional convolutional layers, constructs multiscale convolutional features that preserve temporal order. Additionally, RNNs, layered atop these features, learn multiscale time series representations sensitive to sequence order. This integration into Transformer models with attention mechanisms results in significant performance enhancements. We present the PRformer, a model integrating PRE with a standard Transformer encoder, demonstrating state-of-the-art performance on various real-world datasets. This performance highlights the effectiveness of our approach in leveraging longer lookback windows and underscores the critical role of robust temporal representations in maximizing Transformer's potential for prediction tasks. Code is available at this repository: \\url{https://github.com/usualheart/PRformer}.",
    "authors": [
      "Yongbo Yu",
      "Weizhong Yu",
      "Feiping Nie",
      "Xuelong Li"
    ],
    "published": "2024-08-20T01:56:07+00:00",
    "url": "http://arxiv.org/pdf/2408.10483v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2409.10874v1",
    "title": "American Sign Language to Text Translation using Transformer and Seq2Seq with LSTM",
    "content": "American Sign Language to Text Translation using Transformer and Seq2Seq with LSTM. Sign language translation is one of the important issues in communication between deaf and hearing people, as it expresses words through hand, body, and mouth movements. American Sign Language is one of the sign languages used, one of which is the alphabetic sign. The development of neural machine translation technology is moving towards sign language translation. Transformer became the state-of-the-art in natural language processing. This study compares the Transformer with the Sequence-to-Sequence (Seq2Seq) model in translating sign language to text. In addition, an experiment was conducted by adding Residual Long Short-Term Memory (ResidualLSTM) in the Transformer. The addition of ResidualLSTM to the Transformer reduces the performance of the Transformer model by 23.37% based on the BLEU Score value. In comparison, the Transformer itself increases the BLEU Score value by 28.14 compared to the Seq2Seq model.",
    "authors": [
      "Gregorius Guntur Sunardi Putra",
      "Adifa Widyadhani Chanda D'Layla",
      "Dimas Wahono",
      "Riyanarto Sarno",
      "Agus Tri Haryono"
    ],
    "published": "2024-09-17T04:00:33+00:00",
    "url": "http://arxiv.org/pdf/2409.10874v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2410.20439v1",
    "title": "TEAFormers: TEnsor-Augmented Transformers for Multi-Dimensional Time Series Forecasting",
    "content": "TEAFormers: TEnsor-Augmented Transformers for Multi-Dimensional Time Series Forecasting. Multi-dimensional time series data, such as matrix and tensor-variate time series, are increasingly prevalent in fields such as economics, finance, and climate science. Traditional Transformer models, though adept with sequential data, do not effectively preserve these multi-dimensional structures, as their internal operations in effect flatten multi-dimensional observations into vectors, thereby losing critical multi-dimensional relationships and patterns. To address this, we introduce the Tensor-Augmented Transformer (TEAFormer), a novel method that incorporates tensor expansion and compression within the Transformer framework to maintain and leverage the inherent multi-dimensional structures, thus reducing computational costs and improving prediction accuracy. The core feature of the TEAFormer, the Tensor-Augmentation (TEA) module, utilizes tensor expansion to enhance multi-view feature learning and tensor compression for efficient information aggregation and reduced computational load. The TEA module is not just a specific model architecture but a versatile component that is highly compatible with the attention mechanism and the encoder-decoder structure of Transformers, making it adaptable to existing Transformer architectures. Our comprehensive experiments, which integrate the TEA module into three popular time series Transformer models across three real-world benchmarks, show significant performance enhancements, highlighting the potential of TEAFormers for cutting-edge time series forecasting.",
    "authors": [
      "Linghang Kong",
      "Elynn Chen",
      "Yuzhou Chen",
      "Yuefeng Han"
    ],
    "published": "2024-10-27T13:32:12+00:00",
    "url": "http://arxiv.org/pdf/2410.20439v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2412.08148v1",
    "title": "A Review of Intelligent Device Fault Diagnosis Technologies Based on Machine Vision",
    "content": "A Review of Intelligent Device Fault Diagnosis Technologies Based on Machine Vision. This paper provides a comprehensive review of mechanical equipment fault diagnosis methods, focusing on the advancements brought by Transformer-based models. It details the structure, working principles, and benefits of Transformers, particularly their self-attention mechanism and parallel computation capabilities, which have propelled their widespread application in natural language processing and computer vision. The discussion highlights key Transformer model variants, such as Vision Transformers (ViT) and their extensions, which leverage self-attention to improve accuracy and efficiency in visual tasks. Furthermore, the paper examines the application of Transformer-based approaches in intelligent fault diagnosis for mechanical systems, showcasing their superior ability to extract and recognize patterns from complex sensor data for precise fault identification. Despite these advancements, challenges remain, including the reliance on extensive labeled datasets, significant computational demands, and difficulties in deploying models on resource-limited devices. To address these limitations, the paper proposes future research directions, such as developing lightweight Transformer architectures, integrating multimodal data sources, and enhancing adaptability to diverse operational conditions. These efforts aim to further expand the application of Transformer-based methods in mechanical fault diagnosis, making them more robust, efficient, and suitable for real-world industrial environments.",
    "authors": [
      "Guiran Liu",
      "Binrong Zhu"
    ],
    "published": "2024-12-11T07:06:53+00:00",
    "url": "http://arxiv.org/pdf/2412.08148v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2501.02547v1",
    "title": "Transformers Simulate MLE for Sequence Generation in Bayesian Networks",
    "content": "Transformers Simulate MLE for Sequence Generation in Bayesian Networks. Transformers have achieved significant success in various fields, notably excelling in tasks involving sequential data like natural language processing. Despite these achievements, the theoretical understanding of transformers' capabilities remains limited. In this paper, we investigate the theoretical capabilities of transformers to autoregressively generate sequences in Bayesian networks based on in-context maximum likelihood estimation (MLE). Specifically, we consider a setting where a context is formed by a set of independent sequences generated according to a Bayesian network. We demonstrate that there exists a simple transformer model that can (i) estimate the conditional probabilities of the Bayesian network according to the context, and (ii) autoregressively generate a new sample according to the Bayesian network with estimated conditional probabilities. We further demonstrate in extensive experiments that such a transformer does not only exist in theory, but can also be effectively obtained through training. Our analysis highlights the potential of transformers to learn complex probabilistic models and contributes to a better understanding of large language models as a powerful class of sequence generators.",
    "authors": [
      "Yuan Cao",
      "Yihan He",
      "Dennis Wu",
      "Hong-Yu Chen",
      "Jianqing Fan",
      "Han Liu"
    ],
    "published": "2025-01-05T13:56:51+00:00",
    "url": "http://arxiv.org/pdf/2501.02547v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1907.09282v2",
    "title": "Learning the Relation between Code Features and Code Transforms with Structured Prediction",
    "content": "Learning the Relation between Code Features and Code Transforms with Structured Prediction. To effectively guide the exploration of the code transform space for automated code evolution techniques, we present in this paper the first approach for structurally predicting code transforms at the level of AST nodes using conditional random fields (CRFs). Our approach first learns offline a probabilistic model that captures how certain code transforms are applied to certain AST nodes, and then uses the learned model to predict transforms for arbitrary new, unseen code snippets. {Our approach involves a novel representation of both programs and code transforms. Specifically, we introduce the formal framework for defining the so-called AST-level code transforms and we demonstrate how the CRF model can be accordingly designed, learned, and used for prediction}. We instantiate our approach in the context of repair transform prediction for Java programs. Our instantiation contains a set of carefully designed code features, deals with the training data imbalance issue, and comprises transform constraints that are specific to code. We conduct a large-scale experimental evaluation based on a dataset of bug fixing commits from real-world Java projects. The results show that when the popular evaluation metric \\emph{top-3} is used, our approach predicts the code transforms with an accuracy varying from 41\\% to 53\\% depending on the transforms. Our model outperforms two baselines based on history probability and neural machine translation (NMT), suggesting the importance of considering code structure in achieving good prediction accuracy. In addition, a proof-of-concept synthesizer is implemented to concretize some repair transforms to get the final patches. The evaluation of the synthesizer on the Defects4j benchmark confirms the usefulness of the predicted AST-level repair transforms in producing high-quality patches.",
    "authors": [
      "Zhongxing Yu",
      "Matias Martinez",
      "Zimin Chen",
      "Tegawendé F. Bissyandé",
      "Martin Monperrus"
    ],
    "published": "2019-07-22T12:42:32+00:00",
    "url": "http://arxiv.org/pdf/1907.09282v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1909.06695v1",
    "title": "Ouroboros: On Accelerating Training of Transformer-Based Language Models",
    "content": "Ouroboros: On Accelerating Training of Transformer-Based Language Models. Language models are essential for natural language processing (NLP) tasks, such as machine translation and text summarization. Remarkable performance has been demonstrated recently across many NLP domains via a Transformer-based language model with over a billion parameters, verifying the benefits of model size. Model parallelism is required if a model is too large to fit in a single computing device. Current methods for model parallelism either suffer from backward locking in backpropagation or are not applicable to language models. We propose the first model-parallel algorithm that speeds the training of Transformer-based language models. We also prove that our proposed algorithm is guaranteed to converge to critical points for non-convex problems. Extensive experiments on Transformer and Transformer-XL language models demonstrate that the proposed algorithm obtains a much faster speedup beyond data parallelism, with comparable or better accuracy. Code to reproduce experiments is to be found at \\url{https://github.com/LaraQianYang/Ouroboros}.",
    "authors": [
      "Qian Yang",
      "Zhouyuan Huo",
      "Wenlin Wang",
      "Heng Huang",
      "Lawrence Carin"
    ],
    "published": "2019-09-14T23:21:56+00:00",
    "url": "http://arxiv.org/pdf/1909.06695v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/cond-mat/0609411v2",
    "title": "Large Fourier transforms never exactly realized by braiding conformal blocks",
    "content": "Large Fourier transforms never exactly realized by braiding conformal blocks. Fourier transform is an essential ingredient in Shor's factoring algorithm. In the standard quantum circuit model with the gate set $\\{\\U(2), \\textrm{CNOT}\\}$, the discrete Fourier transforms $F_N=(\\omega^{ij})_{N\\times N},i,j=0,1,..., N-1, \\omega=e^{\\frac{2\\pi i}{N}}$, can be realized exactly by quantum circuits of size $O(n^2), n=\\textrm{log}N$, and so can the discrete sine/cosine transforms. In topological quantum computing, the simplest universal topological quantum computer is based on the Fibonacci (2+1)-topological quantum field theory (TQFT), where the standard quantum circuits are replaced by unitary transformations realized by braiding conformal blocks. We report here that the large Fourier transforms $F_N$ and the discrete sine/cosine transforms can never be realized exactly by braiding conformal blocks for a fixed TQFT. It follows that approximation is unavoidable to implement the Fourier transforms by braiding conformal blocks.",
    "authors": [
      "Michael H. Freedman",
      "Zhenghan Wang"
    ],
    "published": "2006-09-18T05:33:52+00:00",
    "url": "http://arxiv.org/pdf/cond-mat/0609411v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/hep-th/0503150v1",
    "title": "Criteria for Exact Solubility of Relativistic Field Theories by Scattering Transform",
    "content": "Criteria for Exact Solubility of Relativistic Field Theories by Scattering Transform. Scattering transform is a well known powerful tool for quantisation of field theories in (1+1) dimensions. Conventionally only those models whose classical counterparts admit a Lax pair (origin of which is always mysterious) have been quantised in this way. In relativistic quantum field theories we show that the scattering transforms can be constructed ab initio from its invariance under Lorentz transformation (both proper and improper), irreducible transformation nature of scalar and Dirac fields, the existence of a momentum scale associated with asymptotic nature of the scattering transform and the closure of short distance operator product algebra. For single fields it turns out that theories quantisable by scattering transforms are restricted to sine-Gordon type for spin-0 and Massive Thirring type for spin-1/2 if the target space of the scattering transform matrix is assumed to be parity invariant. There are interesting unexplored extensions if the target space is given chirality.",
    "authors": [
      "Gautam Bhattacharya"
    ],
    "published": "2005-03-19T15:54:40+00:00",
    "url": "http://arxiv.org/pdf/hep-th/0503150v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/math/0005209v1",
    "title": "Scalar Levin-Type Sequence Transformations",
    "content": "Scalar Levin-Type Sequence Transformations. Sequence transformations are important tools for the convergence acceleration of slowly convergent scalar sequences or series and for the summation of divergent series. Transformations that depend not only on the sequence elements or partial sums $s_n$ but also on an auxiliary sequence of so-called remainder estimates $\\omega_n$ are of Levin-type if they are linear in the $s_n$, and nonlinear in the $\\omega_n$. Known Levin-type sequence transformations are reviewed and put into a common theoretical framework. It is discussed how such transformations may be constructed by either a model sequence approach or by iteration of simple transformations. As illustration, two new sequence transformations are derived. Common properties and results on convergence acceleration and stability are given. For important special cases, extensions of the general results are presented. Also, guidelines for the application of Levin-type sequence transformations are discussed, and a few numerical examples are given.",
    "authors": [
      "Herbert H. H. Homeier"
    ],
    "published": "2000-05-22T14:53:30+00:00",
    "url": "http://arxiv.org/pdf/math/0005209v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/solv-int/9705017v1",
    "title": "Laplace transformations of hydrodynamic type systems in Riemann invariants: periodic sequences",
    "content": "Laplace transformations of hydrodynamic type systems in Riemann invariants: periodic sequences. The conserved densities of hydrodynamic type system in Riemann invariants satisfy a system of linear second order partial differential equations. For linear systems of this type Darboux introduced Laplace transformations, generalising the classical transformations in the scalar case. It is demonstrated that Laplace transformations can be pulled back to the transformations of the corresponding hydrodynamic type systems. We discuss periodic Laplace sequences of with the emphasize on the simplest nontrivial case of period 2. For 3-component systems in Riemann invariants a complete discription of closed quadruples is proposed. They turn to be related to a special quadratic reduction of the (2+1)-dimensional 3-wave system which can be reduced to a triple of pairwize commuting Monge-Ampere equations. In terms of the Lame and rotation coefficients Laplace transformations have a natural interpretation as the symmetries of the Dirac operator, associated with the (2+1)-dimensional n-wave system. The 2-component Laplace transformations can be interpreted also as the symmetries of the (2+1)-dimensional integrable equations of Davey-Stewartson type. Laplace transformations of hydrodynamic type systems originate from a canonical geometric correspondence between systems of conservation laws and line congruences in projective space.",
    "authors": [
      "E. V. Ferapontov"
    ],
    "published": "1997-05-28T15:40:33+00:00",
    "url": "http://arxiv.org/pdf/solv-int/9705017v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1111.6308v3",
    "title": "On Measure Transformed Canonical Correlation Analysis",
    "content": "On Measure Transformed Canonical Correlation Analysis. In this paper linear canonical correlation analysis (LCCA) is generalized by applying a structured transform to the joint probability distribution of the considered pair of random vectors, i.e., a transformation of the joint probability measure defined on their joint observation space. This framework, called measure transformed canonical correlation analysis (MTCCA), applies LCCA to the data after transformation of the joint probability measure. We show that judicious choice of the transform leads to a modified canonical correlation analysis, which, in contrast to LCCA, is capable of detecting non-linear relationships between the considered pair of random vectors. Unlike kernel canonical correlation analysis, where the transformation is applied to the random vectors, in MTCCA the transformation is applied to their joint probability distribution. This results in performance advantages and reduced implementation complexity. The proposed approach is illustrated for graphical model selection in simulated data having non-linear dependencies, and for measuring long-term associations between companies traded in the NASDAQ and NYSE stock markets.",
    "authors": [
      "Koby Todros",
      "Alfred O. Hero"
    ],
    "published": "2011-11-27T22:25:36+00:00",
    "url": "http://arxiv.org/pdf/1111.6308v3"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1501.02859v1",
    "title": "$\\ell_0$ Sparsifying Transform Learning with Efficient Optimal Updates and Convergence Guarantees",
    "content": "$\\ell_0$ Sparsifying Transform Learning with Efficient Optimal Updates and Convergence Guarantees. Many applications in signal processing benefit from the sparsity of signals in a certain transform domain or dictionary. Synthesis sparsifying dictionaries that are directly adapted to data have been popular in applications such as image denoising, inpainting, and medical image reconstruction. In this work, we focus instead on the sparsifying transform model, and study the learning of well-conditioned square sparsifying transforms. The proposed algorithms alternate between a $\\ell_0$ \"norm\"-based sparse coding step, and a non-convex transform update step. We derive the exact analytical solution for each of these steps. The proposed solution for the transform update step achieves the global minimum in that step, and also provides speedups over iterative solutions involving conjugate gradients. We establish that our alternating algorithms are globally convergent to the set of local minimizers of the non-convex transform learning problems. In practice, the algorithms are insensitive to initialization. We present results illustrating the promising performance and significant speed-ups of transform learning over synthesis K-SVD in image denoising.",
    "authors": [
      "Saiprasad Ravishankar",
      "Yoram Bresler"
    ],
    "published": "2015-01-13T01:34:40+00:00",
    "url": "http://arxiv.org/pdf/1501.02859v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1909.00952v2",
    "title": "Graph-based Transforms for Video Coding",
    "content": "Graph-based Transforms for Video Coding. In many state-of-the-art compression systems, signal transformation is an integral part of the encoding and decoding process, where transforms provide compact representations for the signals of interest. This paper introduces a class of transforms called graph-based transforms (GBTs) for video compression, and proposes two different techniques to design GBTs. In the first technique, we formulate an optimization problem to learn graphs from data and provide solutions for optimal separable and nonseparable GBT designs, called GL-GBTs. The optimality of the proposed GL-GBTs is also theoretically analyzed based on Gaussian-Markov random field (GMRF) models for intra and inter predicted block signals. The second technique develops edge-adaptive GBTs (EA-GBTs) in order to flexibly adapt transforms to block signals with image edges (discontinuities). The advantages of EA-GBTs are both theoretically and empirically demonstrated. Our experimental results demonstrate that the proposed transforms can significantly outperform the traditional Karhunen-Loeve transform (KLT).",
    "authors": [
      "Hilmi E. Egilmez",
      "Yung-Hsuan Chao",
      "Antonio Ortega"
    ],
    "published": "2019-09-03T04:53:53+00:00",
    "url": "http://arxiv.org/pdf/1909.00952v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1911.02710v1",
    "title": "Deep Learning Models for Global Coordinate Transformations that Linearize PDEs",
    "content": "Deep Learning Models for Global Coordinate Transformations that Linearize PDEs. We develop a deep autoencoder architecture that can be used to find a coordinate transformation which turns a nonlinear PDE into a linear PDE. Our architecture is motivated by the linearizing transformations provided by the Cole-Hopf transform for Burgers equation and the inverse scattering transform for completely integrable PDEs. By leveraging a residual network architecture, a near-identity transformation can be exploited to encode intrinsic coordinates in which the dynamics are linear. The resulting dynamics are given by a Koopman operator matrix $\\mathbf{K}$. The decoder allows us to transform back to the original coordinates as well. Multiple time step prediction can be performed by repeated multiplication by the matrix $\\mathbf{K}$ in the intrinsic coordinates. We demonstrate our method on a number of examples, including the heat equation and Burgers equation, as well as the substantially more challenging Kuramoto-Sivashinsky equation, showing that our method provides a robust architecture for discovering interpretable, linearizing transforms for nonlinear PDEs.",
    "authors": [
      "Craig Gin",
      "Bethany Lusch",
      "Steven L. Brunton",
      "J. Nathan Kutz"
    ],
    "published": "2019-11-07T01:46:33+00:00",
    "url": "http://arxiv.org/pdf/1911.02710v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2103.11681v2",
    "title": "Transformer Meets Tracker: Exploiting Temporal Context for Robust Visual Tracking",
    "content": "Transformer Meets Tracker: Exploiting Temporal Context for Robust Visual Tracking. In video object tracking, there exist rich temporal contexts among successive frames, which have been largely overlooked in existing trackers. In this work, we bridge the individual video frames and explore the temporal contexts across them via a transformer architecture for robust object tracking. Different from classic usage of the transformer in natural language processing tasks, we separate its encoder and decoder into two parallel branches and carefully design them within the Siamese-like tracking pipelines. The transformer encoder promotes the target templates via attention-based feature reinforcement, which benefits the high-quality tracking model generation. The transformer decoder propagates the tracking cues from previous templates to the current frame, which facilitates the object searching process. Our transformer-assisted tracking framework is neat and trained in an end-to-end manner. With the proposed transformer, a simple Siamese matching approach is able to outperform the current top-performing trackers. By combining our transformer with the recent discriminative tracking pipeline, our method sets several new state-of-the-art records on prevalent tracking benchmarks.",
    "authors": [
      "Ning Wang",
      "Wengang Zhou",
      "Jie Wang",
      "Houqaing Li"
    ],
    "published": "2021-03-22T09:20:05+00:00",
    "url": "http://arxiv.org/pdf/2103.11681v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/1603.09396v1",
    "title": "Robust Hybrid Image Watermarking based on Discrete Wavelet and Shearlet Transforms",
    "content": "Robust Hybrid Image Watermarking based on Discrete Wavelet and Shearlet Transforms. With the growth of digital networks such as the Internet, digital media have been explosively developed in e-commerce and online services. This causes problems such as illegal copy and fake ownership. Watermarking is proposed as one of the solutions to such cases. Among different watermarking techniques, the wavelet transform has been used more because of its good ability in modeling the human visual system. Recently, Shearlet transform as an extension of Wavelet transform which is based on multi-resolution and multi-directional analysis is introduced. The most important feature of this transform is the appropriate representation of image edges. In this paper a hybrid scheme using Discrete Wavelet Transform (DWT) and Discrete Shearlet Transform (DST) is presented. In this way, the host image is decomposed using DWT, and then its low frequency sub-band is decomposed by DST. After that, the bidiagonal singular value decomposition (BSVD) is applied on the selected sub-band from Shearlet transform and the gray-scale watermark image is embedded into its bidiagonal singular values. The proposed method is examined on the images with different textures and resistance is evaluated against various attacks like image processing and geometric attacks. The results show good transparency and high robustness in proposed method.",
    "authors": [
      "Malihe Mardanpour",
      "Mohammad Ali Zare Chahooki"
    ],
    "published": "2016-03-30T22:03:19+00:00",
    "url": "http://arxiv.org/pdf/1603.09396v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2005.09684v2",
    "title": "Exploring Transformers for Large-Scale Speech Recognition",
    "content": "Exploring Transformers for Large-Scale Speech Recognition. While recurrent neural networks still largely define state-of-the-art speech recognition systems, the Transformer network has been proven to be a competitive alternative, especially in the offline condition. Most studies with Transformers have been constrained in a relatively small scale setting, and some forms of data argumentation approaches are usually applied to combat the data sparsity issue. In this paper, we aim at understanding the behaviors of Transformers in the large-scale speech recognition setting, where we have used around 65,000 hours of training data. We investigated various aspects on scaling up Transformers, including model initialization, warmup training as well as different Layer Normalization strategies. In the streaming condition, we compared the widely used attention mask based future context lookahead approach to the Transformer-XL network. From our experiments, we show that Transformers can achieve around 6% relative word error rate (WER) reduction compared to the BLSTM baseline in the offline fashion, while in the streaming fashion, Transformer-XL is comparable to LC-BLSTM with 800 millisecond latency constraint.",
    "authors": [
      "Liang Lu",
      "Changliang Liu",
      "Jinyu Li",
      "Yifan Gong"
    ],
    "published": "2020-05-19T18:07:14+00:00",
    "url": "http://arxiv.org/pdf/2005.09684v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2202.12165v3",
    "title": "Transformers in Medical Image Analysis: A Review",
    "content": "Transformers in Medical Image Analysis: A Review. Transformers have dominated the field of natural language processing, and recently impacted the computer vision area. In the field of medical image analysis, Transformers have also been successfully applied to full-stack clinical applications, including image synthesis/reconstruction, registration, segmentation, detection, and diagnosis. Our paper aims to promote awareness and application of Transformers in the field of medical image analysis. Specifically, we first overview the core concepts of the attention mechanism built into Transformers and other basic components. Second, we review various Transformer architectures tailored for medical image applications and discuss their limitations. Within this review, we investigate key challenges revolving around the use of Transformers in different learning paradigms, improving the model efficiency, and their coupling with other techniques. We hope this review can give a comprehensive picture of Transformers to the readers in the field of medical image analysis.",
    "authors": [
      "Kelei He",
      "Chen Gan",
      "Zhuoyuan Li",
      "Islem Rekik",
      "Zihao Yin",
      "Wen Ji",
      "Yang Gao",
      "Qian Wang",
      "Junfeng Zhang",
      "Dinggang Shen"
    ],
    "published": "2022-02-24T16:04:03+00:00",
    "url": "http://arxiv.org/pdf/2202.12165v3"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2203.00828v1",
    "title": "3DCTN: 3D Convolution-Transformer Network for Point Cloud Classification",
    "content": "3DCTN: 3D Convolution-Transformer Network for Point Cloud Classification. Although accurate and fast point cloud classification is a fundamental task in 3D applications, it is difficult to achieve this purpose due to the irregularity and disorder of point clouds that make it challenging to achieve effective and efficient global discriminative feature learning. Lately, 3D Transformers have been adopted to improve point cloud processing. Nevertheless, massive Transformer layers tend to incur huge computational and memory costs. This paper presents a novel hierarchical framework that incorporates convolution with Transformer for point cloud classification, named 3D Convolution-Transformer Network (3DCTN), to combine the strong and efficient local feature learning ability of convolution with the remarkable global context modeling capability of Transformer. Our method has two main modules operating on the downsampling point sets, and each module consists of a multi-scale local feature aggregating (LFA) block and a global feature learning (GFL) block, which are implemented by using Graph Convolution and Transformer respectively. We also conduct a detailed investigation on a series of Transformer variants to explore better performance for our network. Various experiments on ModelNet40 demonstrate that our method achieves state-of-the-art classification performance, in terms of both accuracy and efficiency.",
    "authors": [
      "Dening Lu",
      "Qian Xie",
      "Linlin Xu",
      "Jonathan Li"
    ],
    "published": "2022-03-02T02:42:14+00:00",
    "url": "http://arxiv.org/pdf/2203.00828v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2203.16518v1",
    "title": "Collaborative Transformers for Grounded Situation Recognition",
    "content": "Collaborative Transformers for Grounded Situation Recognition. Grounded situation recognition is the task of predicting the main activity, entities playing certain roles within the activity, and bounding-box groundings of the entities in the given image. To effectively deal with this challenging task, we introduce a novel approach where the two processes for activity classification and entity estimation are interactive and complementary. To implement this idea, we propose Collaborative Glance-Gaze TransFormer (CoFormer) that consists of two modules: Glance transformer for activity classification and Gaze transformer for entity estimation. Glance transformer predicts the main activity with the help of Gaze transformer that analyzes entities and their relations, while Gaze transformer estimates the grounded entities by focusing only on the entities relevant to the activity predicted by Glance transformer. Our CoFormer achieves the state of the art in all evaluation metrics on the SWiG dataset. Training code and model weights are available at https://github.com/jhcho99/CoFormer.",
    "authors": [
      "Junhyeong Cho",
      "Youngseok Yoon",
      "Suha Kwak"
    ],
    "published": "2022-03-30T17:52:38+00:00",
    "url": "http://arxiv.org/pdf/2203.16518v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2304.04555v2",
    "title": "Neural Diffeomorphic Non-uniform B-spline Flows",
    "content": "Neural Diffeomorphic Non-uniform B-spline Flows. Normalizing flows have been successfully modeling a complex probability distribution as an invertible transformation of a simple base distribution. However, there are often applications that require more than invertibility. For instance, the computation of energies and forces in physics requires the second derivatives of the transformation to be well-defined and continuous. Smooth normalizing flows employ infinitely differentiable transformation, but with the price of slow non-analytic inverse transforms. In this work, we propose diffeomorphic non-uniform B-spline flows that are at least twice continuously differentiable while bi-Lipschitz continuous, enabling efficient parametrization while retaining analytic inverse transforms based on a sufficient condition for diffeomorphism. Firstly, we investigate the sufficient condition for Ck-2-diffeomorphic non-uniform kth-order B-spline transformations. Then, we derive an analytic inverse transformation of the non-uniform cubic B-spline transformation for neural diffeomorphic non-uniform B-spline flows. Lastly, we performed experiments on solving the force matching problem in Boltzmann generators, demonstrating that our C2-diffeomorphic non-uniform B-spline flows yielded solutions better than previous spline flows and faster than smooth normalizing flows. Our source code is publicly available at https://github.com/smhongok/Non-uniform-B-spline-Flow.",
    "authors": [
      "Seongmin Hong",
      "Se Young Chun"
    ],
    "published": "2023-04-07T05:34:18+00:00",
    "url": "http://arxiv.org/pdf/2304.04555v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2308.08536v3",
    "title": "Can Transformers Learn Optimal Filtering for Unknown Systems?",
    "content": "Can Transformers Learn Optimal Filtering for Unknown Systems?. Transformer models have shown great success in natural language processing; however, their potential remains mostly unexplored for dynamical systems. In this work, we investigate the optimal output estimation problem using transformers, which generate output predictions using all the past ones. Particularly, we train the transformer using various distinct systems and then evaluate the performance on unseen systems with unknown dynamics. Empirically, the trained transformer adapts exceedingly well to different unseen systems and even matches the optimal performance given by the Kalman filter for linear systems. In more complex settings with non-i.i.d. noise, time-varying dynamics, and nonlinear dynamics like a quadrotor system with unknown parameters, transformers also demonstrate promising results. To support our experimental findings, we provide statistical guarantees that quantify the amount of training data required for the transformer to achieve a desired excess risk. Finally, we point out some limitations by identifying two classes of problems that lead to degraded performance, highlighting the need for caution when using transformers for control and estimation.",
    "authors": [
      "Haldun Balim",
      "Zhe Du",
      "Samet Oymak",
      "Necmiye Ozay"
    ],
    "published": "2023-08-16T17:52:11+00:00",
    "url": "http://arxiv.org/pdf/2308.08536v3"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2412.10599v1",
    "title": "Advances in Transformers for Robotic Applications: A Review",
    "content": "Advances in Transformers for Robotic Applications: A Review. The introduction of Transformers architecture has brought about significant breakthroughs in Deep Learning (DL), particularly within Natural Language Processing (NLP). Since their inception, Transformers have outperformed many traditional neural network architectures due to their \"self-attention\" mechanism and their scalability across various applications. In this paper, we cover the use of Transformers in Robotics. We go through recent advances and trends in Transformer architectures and examine their integration into robotic perception, planning, and control for autonomous systems. Furthermore, we review past work and recent research on use of Transformers in Robotics as pre-trained foundation models and integration of Transformers with Deep Reinforcement Learning (DRL) for autonomous systems. We discuss how different Transformer variants are being adapted in robotics for reliable planning and perception, increasing human-robot interaction, long-horizon decision-making, and generalization. Finally, we address limitations and challenges, offering insight and suggestions for future research directions.",
    "authors": [
      "Nikunj Sanghai",
      "Nik Bear Brown"
    ],
    "published": "2024-12-13T23:02:15+00:00",
    "url": "http://arxiv.org/pdf/2412.10599v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2502.02596v1",
    "title": "The photography transforms and their analytic inversion formulas",
    "content": "The photography transforms and their analytic inversion formulas. The light field reconstruction from the focal stack can be mathematically formulated as an ill-posed integral equation inversion problem. Although the previous research about this problem has made progress both in practice and theory, its forward problem and inversion in a general form still need to be studied. In this paper, to model the forward problem rigorously, we propose three types of photography transforms with different integral geometry characteristics that extend the forward operator to the arbitrary $n$-dimensional case. We prove that these photography transforms are equivalent to the Radon transform with the coupling relation between variables. We also obtain some properties of the photography transforms, including the Fourier slice theorem, the convolution theorem, and the convolution property of the dual operator, which are very similar to those of the classic Radon transform. Furthermore, the representation of the normal operator and the analytic inversion formula for the photography transforms are derived and they are quite different from those of the classic Radon transform.",
    "authors": [
      "Duo Liu",
      "Gangrong Qu",
      "Shan Gao"
    ],
    "published": "2025-01-14T04:12:40+00:00",
    "url": "http://arxiv.org/pdf/2502.02596v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2503.03961v1",
    "title": "A Little Depth Goes a Long Way: The Expressive Power of Log-Depth Transformers",
    "content": "A Little Depth Goes a Long Way: The Expressive Power of Log-Depth Transformers. Recent theoretical results show transformers cannot express sequential reasoning problems over long input lengths, intuitively because their computational depth is bounded. However, prior work treats the depth as a constant, leaving it unclear to what degree bounded depth may suffice for solving problems over short inputs, or how increasing the transformer's depth affects its expressive power. We address these questions by analyzing the expressive power of transformers whose depth can grow minimally with context length $n$. We show even highly uniform transformers with depth $\\Theta(\\log n)$ can express two important problems: recognizing regular languages, which captures state tracking abilities, and graph connectivity, which underlies multi-step reasoning. Notably, both of these problems cannot be expressed by fixed-depth transformers under standard complexity conjectures, demonstrating the expressivity benefit of growing depth. Moreover, our theory quantitatively predicts how depth must grow with input length to express these problems, showing that depth scaling is more efficient than scaling width or chain-of-thought steps. Empirically, we find our theoretical depth requirements for regular language recognition match the practical depth requirements of transformers remarkably well. Thus, our results clarify precisely how depth affects transformers' reasoning capabilities, providing potential practical insights for designing models that are better at sequential reasoning.",
    "authors": [
      "William Merrill",
      "Ashish Sabharwal"
    ],
    "published": "2025-03-05T23:26:25+00:00",
    "url": "http://arxiv.org/pdf/2503.03961v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2007.11488v1",
    "title": "Analysis and Comparison of Different Wavelet Transform Methods Using Benchmarks for Image Fusion",
    "content": "Analysis and Comparison of Different Wavelet Transform Methods Using Benchmarks for Image Fusion. In recent years, many research achievements are made in the medical image fusion field. Medical Image fusion means that several of various modality image information is comprehended together to form one image to express its information. The aim of image fusion is to integrate complementary and redundant information. CT/MRI is one of the most common medical image fusion. These medical modalities give information about different diseases. Complementary information is offered by CT and MRI. CT provides the best information about denser tissue and MRI offers better information on soft tissue. There are two approaches to image fusion, namely Spatial Fusion and Transform fusion. Transform fusion uses transform for representing the source images at multi-scale. This paper presents a Wavelet Transform image fusion methodology based on the intensity magnitudes of the wavelet coefficients and compares five variations of the wavelet transform implemented separately in this fusion model. The image fusion model, using the Discrete Wavelet Transform (DWT), the Stationary Wavelet Transform (SWT), the Integer Lifting Wavelet Transform (ILFT) the dual-tree Complex Wavelet Transform (DT CWT) and dual-tree Q-shift dual-tree CWT, is applied to multi-modal images. The resulting fused images are compared visually and through benchmarks such as Entropy (E), Peak Signal to Noise Ratio, (PSNR), Root Mean Square Error (RMSE), Image Quality Index (IQI) and Standard deviation (SD) computations.",
    "authors": [
      "T Deepika"
    ],
    "published": "2020-07-22T15:24:54+00:00",
    "url": "http://arxiv.org/pdf/2007.11488v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2012.12556v6",
    "title": "A Survey on Visual Transformer",
    "content": "A Survey on Visual Transformer. Transformer, first applied to the field of natural language processing, is a type of deep neural network mainly based on the self-attention mechanism. Thanks to its strong representation capabilities, researchers are looking at ways to apply transformer to computer vision tasks. In a variety of visual benchmarks, transformer-based models perform similar to or better than other types of networks such as convolutional and recurrent neural networks. Given its high performance and less need for vision-specific inductive bias, transformer is receiving more and more attention from the computer vision community. In this paper, we review these vision transformer models by categorizing them in different tasks and analyzing their advantages and disadvantages. The main categories we explore include the backbone network, high/mid-level vision, low-level vision, and video processing. We also include efficient transformer methods for pushing transformer into real device-based applications. Furthermore, we also take a brief look at the self-attention mechanism in computer vision, as it is the base component in transformer. Toward the end of this paper, we discuss the challenges and provide several further research directions for vision transformers.",
    "authors": [
      "Kai Han",
      "Yunhe Wang",
      "Hanting Chen",
      "Xinghao Chen",
      "Jianyuan Guo",
      "Zhenhua Liu",
      "Yehui Tang",
      "An Xiao",
      "Chunjing Xu",
      "Yixing Xu",
      "Zhaohui Yang",
      "Yiman Zhang",
      "Dacheng Tao"
    ],
    "published": "2020-12-23T09:37:54+00:00",
    "url": "http://arxiv.org/pdf/2012.12556v6"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2103.11816v2",
    "title": "Incorporating Convolution Designs into Visual Transformers",
    "content": "Incorporating Convolution Designs into Visual Transformers. Motivated by the success of Transformers in natural language processing (NLP) tasks, there emerge some attempts (e.g., ViT and DeiT) to apply Transformers to the vision domain. However, pure Transformer architectures often require a large amount of training data or extra supervision to obtain comparable performance with convolutional neural networks (CNNs). To overcome these limitations, we analyze the potential drawbacks when directly borrowing Transformer architectures from NLP. Then we propose a new \\textbf{Convolution-enhanced image Transformer (CeiT)} which combines the advantages of CNNs in extracting low-level features, strengthening locality, and the advantages of Transformers in establishing long-range dependencies. Three modifications are made to the original Transformer: \\textbf{1)} instead of the straightforward tokenization from raw input images, we design an \\textbf{Image-to-Tokens (I2T)} module that extracts patches from generated low-level features; \\textbf{2)} the feed-froward network in each encoder block is replaced with a \\textbf{Locally-enhanced Feed-Forward (LeFF)} layer that promotes the correlation among neighboring tokens in the spatial dimension; \\textbf{3)} a \\textbf{Layer-wise Class token Attention (LCA)} is attached at the top of the Transformer that utilizes the multi-level representations.   Experimental results on ImageNet and seven downstream tasks show the effectiveness and generalization ability of CeiT compared with previous Transformers and state-of-the-art CNNs, without requiring a large amount of training data and extra CNN teachers. Besides, CeiT models also demonstrate better convergence with $3\\times$ fewer training iterations, which can reduce the training cost significantly\\footnote{Code and models will be released upon acceptance.}.",
    "authors": [
      "Kun Yuan",
      "Shaopeng Guo",
      "Ziwei Liu",
      "Aojun Zhou",
      "Fengwei Yu",
      "Wei Wu"
    ],
    "published": "2021-03-22T13:16:12+00:00",
    "url": "http://arxiv.org/pdf/2103.11816v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2108.12784v2",
    "title": "TCCT: Tightly-Coupled Convolutional Transformer on Time Series Forecasting",
    "content": "TCCT: Tightly-Coupled Convolutional Transformer on Time Series Forecasting. Time series forecasting is essential for a wide range of real-world applications. Recent studies have shown the superiority of Transformer in dealing with such problems, especially long sequence time series input(LSTI) and long sequence time series forecasting(LSTF) problems. To improve the efficiency and enhance the locality of Transformer, these studies combine Transformer with CNN in varying degrees. However, their combinations are loosely-coupled and do not make full use of CNN. To address this issue, we propose the concept of tightly-coupled convolutional Transformer(TCCT) and three TCCT architectures which apply transformed CNN architectures into Transformer: (1) CSPAttention: through fusing CSPNet with self-attention mechanism, the computation cost of self-attention mechanism is reduced by 30% and the memory usage is reduced by 50% while achieving equivalent or beyond prediction accuracy. (2) Dilated causal convolution: this method is to modify the distilling operation proposed by Informer through replacing canonical convolutional layers with dilated causal convolutional layers to gain exponentially receptive field growth. (3) Passthrough mechanism: the application of passthrough mechanism to stack of self-attention blocks helps Transformer-like models get more fine-grained information with negligible extra computation costs. Our experiments on real-world datasets show that our TCCT architectures could greatly improve the performance of existing state-of-art Transformer models on time series forecasting with much lower computation and memory costs, including canonical Transformer, LogTrans and Informer.",
    "authors": [
      "Li Shen",
      "Yangzhu Wang"
    ],
    "published": "2021-08-29T08:49:31+00:00",
    "url": "http://arxiv.org/pdf/2108.12784v2"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2204.07962v1",
    "title": "An Extendable, Efficient and Effective Transformer-based Object Detector",
    "content": "An Extendable, Efficient and Effective Transformer-based Object Detector. Transformers have been widely used in numerous vision problems especially for visual recognition and detection. Detection transformers are the first fully end-to-end learning systems for object detection, while vision transformers are the first fully transformer-based architecture for image classification. In this paper, we integrate Vision and Detection Transformers (ViDT) to construct an effective and efficient object detector. ViDT introduces a reconfigured attention module to extend the recent Swin Transformer to be a standalone object detector, followed by a computationally efficient transformer decoder that exploits multi-scale features and auxiliary techniques essential to boost the detection performance without much increase in computational load. In addition, we extend it to ViDT+ to support joint-task learning for object detection and instance segmentation. Specifically, we attach an efficient multi-scale feature fusion layer and utilize two more auxiliary training losses, IoU-aware loss and token labeling loss. Extensive evaluation results on the Microsoft COCO benchmark dataset demonstrate that ViDT obtains the best AP and latency trade-off among existing fully transformer-based object detectors, and its extended ViDT+ achieves 53.2AP owing to its high scalability for large models. The source code and trained models are available at https://github.com/naver-ai/vidt.",
    "authors": [
      "Hwanjun Song",
      "Deqing Sun",
      "Sanghyuk Chun",
      "Varun Jampani",
      "Dongyoon Han",
      "Byeongho Heo",
      "Wonjae Kim",
      "Ming-Hsuan Yang"
    ],
    "published": "2022-04-17T09:27:45+00:00",
    "url": "http://arxiv.org/pdf/2204.07962v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2205.14949v1",
    "title": "HiViT: Hierarchical Vision Transformer Meets Masked Image Modeling",
    "content": "HiViT: Hierarchical Vision Transformer Meets Masked Image Modeling. Recently, masked image modeling (MIM) has offered a new methodology of self-supervised pre-training of vision transformers. A key idea of efficient implementation is to discard the masked image patches (or tokens) throughout the target network (encoder), which requires the encoder to be a plain vision transformer (e.g., ViT), albeit hierarchical vision transformers (e.g., Swin Transformer) have potentially better properties in formulating vision inputs. In this paper, we offer a new design of hierarchical vision transformers named HiViT (short for Hierarchical ViT) that enjoys both high efficiency and good performance in MIM. The key is to remove the unnecessary \"local inter-unit operations\", deriving structurally simple hierarchical vision transformers in which mask-units can be serialized like plain vision transformers. For this purpose, we start with Swin Transformer and (i) set the masking unit size to be the token size in the main stage of Swin Transformer, (ii) switch off inter-unit self-attentions before the main stage, and (iii) eliminate all operations after the main stage. Empirical studies demonstrate the advantageous performance of HiViT in terms of fully-supervised, self-supervised, and transfer learning. In particular, in running MAE on ImageNet-1K, HiViT-B reports a +0.6% accuracy gain over ViT-B and a 1.9$\\times$ speed-up over Swin-B, and the performance gain generalizes to downstream tasks of detection and segmentation. Code will be made publicly available.",
    "authors": [
      "Xiaosong Zhang",
      "Yunjie Tian",
      "Wei Huang",
      "Qixiang Ye",
      "Qi Dai",
      "Lingxi Xie",
      "Qi Tian"
    ],
    "published": "2022-05-30T09:34:44+00:00",
    "url": "http://arxiv.org/pdf/2205.14949v1"
  },
  {
    "source": "arxiv",
    "entry_id": "http://arxiv.org/abs/2405.16177v1",
    "title": "Transformer Meets Gated Residual Networks To Enhance Photoplethysmogram Artifact Detection Informed by Mutual Information Neural Estimation",
    "content": "Transformer Meets Gated Residual Networks To Enhance Photoplethysmogram Artifact Detection Informed by Mutual Information Neural Estimation. This study delves into the effectiveness of various learning methods in improving Transformer models, focusing particularly on the Gated Residual Network Transformer (GRN-Transformer) in the context of pediatric intensive care units (PICU) with limited data availability. Our findings indicate that Transformers trained via supervised learning are less effective compared to MLP, CNN, and LSTM networks in such environments. Yet, leveraging unsupervised and self-supervised learning on unannotated data, with subsequent fine-tuning on annotated data, notably enhances Transformer performance, although not to the level of the GRN-Transformer. Central to our research is the analysis of different activation functions for the Gated Linear Unit (GLU), a crucial element of the GRN structure. We also employ Mutual Information Neural Estimation (MINE) to evaluate the GRN's contribution. Additionally, the study examines the effects of integrating GRN within the Transformer's Attention mechanism versus using it as a separate intermediary layer. Our results highlight that GLU with sigmoid activation stands out, achieving 0.98 accuracy, 0.91 precision, 0.96 recall, and 0.94 F1 score. The MINE analysis supports the hypothesis that GRN enhances the mutual information between the hidden representations and the output. Moreover, the use of GRN as an intermediate filter layer proves more beneficial than incorporating it within the Attention mechanism. In summary, this research clarifies how GRN bolsters GRN-Transformer's performance, surpassing other learning techniques. These findings offer a promising avenue for adopting sophisticated models like Transformers in data-constrained environments, such as PPG artifact detection in PICU settings.",
    "authors": [
      "Thanh-Dung Le"
    ],
    "published": "2024-05-25T11:07:42+00:00",
    "url": "http://arxiv.org/pdf/2405.16177v1"
  },
  {
    "source": "web",
    "url": "https://distill.pub/2016/augmented-rnns/",
    "title": "Attention and Augmented Recurrent Neural Networks",
    "content": "Attention and Augmented Recurrent Neural Networks. Recurrent neural networks are one of the staples of deep learning, allowing neural networks to work with sequences of data like text, audio and video. They can be used to boil a sequence down into a high-level understanding, to annotate sequences, and even to generate new sequences from scratch! The basic RNN design struggles with longer sequences, but a special variant—“long short-term memory” networks As this has happened, we’ve seen a growing number of attempts to augment RNNs with new properties. Four directions stand out as particularly exciting: Individually, these techniques are all potent extensions of RNNs, but the really striking thing is that they can be combined, and seem to just be points in a broader space. Further, they all rely on the same underlying trick—something called attention—to work. Our guess is that these “augmented RNNs” will have an important role to play in extending deep learning’s capabilities over the coming years. Neural Turing Machines But how does reading and writing work? The challenge is that we want to make them differentiable. In particular, we want to make them differentiable with respect to the location we read from or write to, so that we can learn where to read and write. This is tricky because memory addresses seem to be fundamentally discrete. NTMs take a very clever solution to this: every step, they read and write everywhere, just to different extents. As an example, let’s focus on reading. Instead of specifying a single location, the RNN outputs an “attention distribution” that describes how we spread out the amount we care about different memory positions. As such, the result of the read operation is a weighted sum. Similarly, we write everywhere at once to different extents. Again, an attention distribution describes how much we write at every location. We do this by having the new value of a position in memory be a convex combination of the old memory content and the write value, with the position between the two decided by the attention weight. But how do NTMs decide which positions in memory to focus their attention on? They actually use a combination of two different methods: content-based attention and location-based attention. Content-based attention allows NTMs to search through their memory and focus on places that match what they’re looking for, while location-based attention allows relative movement in memory, enabling the NTM to loop. This capability to read and write allows NTMs to perform many simple algorithms, previously beyond neural networks. For example, they can learn to store a long sequence in memory, and then loop over it, repeating it back repeatedly. As they do this, we can watch where they read and write, to better understand what they’re doing: They can also learn to mimic a lookup table, or even learn to sort numbers (although they kind of cheat)! On the other hand, they still can’t do many basic things, like add or multiply numbers. Since the original NTM paper, there have been a number of exciting papers exploring similar directions. The Neural GPU In some objective sense, many of the tasks these models can perform—such as learning how to add numbers—aren’t that objectively hard. The traditional program synthesis community would eat them for lunch. But neural networks are capable of many other things, and models like the Neural Turing Machine seem to have knocked away a very profound limit on their abilities. There are a number of open source implementations of these models. Open source implementations of the Neural Turing Machine include Taehoon Kim’s (TensorFlow), Shawn Tan’s (Theano), Fumin’s (Go), Kai Sheng Tai’s (Torch), and Snip’s (Lasagne). Code for the Neural GPU publication was open sourced and put in the TensorFlow Models repository. Open source implementations of Memory Networks include Facebook’s (Torch/Matlab), YerevaNN’s (Theano), and Taehoon Kim’s (TensorFlow). When I’m translating a sentence, I pay special attention to the word I’m presently translating. When I’m transcribing an audio recording, I listen carefully to the segment I’m actively writing down. And if you ask me to describe the room I’m sitting in, I’ll glance around at the objects I’m describing as I do so. Neural networks can achieve this same behavior using attention, focusing on part of a subset of the information they’re given. For example, an RNN can attend over the output of another RNN. At every time step, it focuses on different positions in the other RNN. We’d like attention to be differentiable, so that we can learn where to focus. To do this, we use the same trick Neural Turing Machines use: we focus everywhere, just to different extents. The attention distribution is usually generated with content-based attention. The attending RNN generates a query describing what it wants to focus on. Each item is dot-producted with the query to produce a score, describing how well it matches the query. The scores are fed into a softmax to create the attention distribution. One use of attention between RNNs is translation This kind of attention between RNNs has a number of other applications. It can be used in voice recognition Other uses of this kind of attention include parsing text Attention can also be used on the interface between a convolutional neural network and an RNN. This allows the RNN to look at different position of an image every step. One popular use of this kind of attention is for image captioning. First, a conv net processes the image, extracting high-level features. Then an RNN runs, generating a description of the image. As it generates each word in the description, the RNN focuses on the conv net’s interpretation of the relevant parts of the image. We can explicitly visualize this: More broadly, attentional interfaces can be used whenever one wants to interface with a neural network that has a repeating structure in its output. Attentional interfaces have been found to be an extremely general and powerful technique, and are becoming increasingly widespread. Standard RNNs do the same amount of computation for each time step. This seems unintuitive. Surely, one should think more when things are hard? It also limits RNNs to doing O(n) operations for a list of length n. Adaptive Computation Time In order for the network to learn how many steps to do, we want the number of steps to be differentiable. We achieve this with the same trick we used before: instead of deciding to run for a discrete number of steps, we have an attention distribution over the number of steps to run. The output is a weighted combination of the outputs of each step. There are a few more details, which were left out in the previous diagram. Here’s a complete diagram of a time step with three computation steps. That’s a bit complicated, so let’s work through it step by step. At a high-level, we’re still running the RNN and outputting a weighted combination of the states: The weight for each step is determined by a “halting neuron.” It’s a sigmoid neuron that looks at the RNN state and gives a halting weight, which we can think of as the probability that we should stop at that step. We have a total budget for the halting weights of 1, so we track that budget along the top. When it gets to less than epsilon, we stop. When we stop, might have some left over halting budget because we stop when it gets to less than epsilon. What should we do with it? Technically, it’s being given to future steps but we don’t want to compute those, so we attribute it to the last step. When training Adaptive Computation Time models, one adds a “ponder cost” term to the cost function. This penalizes the model for the amount of computation it uses. The bigger you make this term, the more it will trade-off performance for lowering compute time. Adaptive Computation Time is a very new idea, but we believe that it, along with similar ideas, will be very important. The only open source implementation of Adaptive Computation Time at the moment seems to be Mark Neumann’s (TensorFlow). Neural nets are excellent at many tasks, but they also struggle to do some basic things like arithmetic, which are trivial in normal approaches to computing. It would be really nice to have a way to fuse neural nets with normal programming, and get the best of both worlds. The neural programmer The actual model in the paper answers questions about tables by generating SQL-like programs to query the table. However, there are a number of details here that make it a bit complicated, so let’s start by imagining a slightly simpler model, which is given an arithmetic expression and generates a program to evaluate it. The generated program is a sequence of operations. Each operation is defined to operate on the output of past operations. So an operation might be something like “add the output of the operation 2 steps ago and the output of the operation 1 step ago.” It’s more like a Unix pipe than a program with variables being assigned to and read from. The program is generated one operation at a time by a controller RNN. At each step, the controller RNN outputs a probability distribution for what the next operation should be. For example, we might be pretty sure we want to perform addition at the first time step, then have a hard time deciding whether we should multiply or divide at the second step, and so on... The resulting distribution over operations can now be evaluated. Instead of running a single operation at each step, we do the usual attention trick of running all of them and then average the outputs together, weighted by the probability we ran that operation. As long as we can define derivatives through the operations, the program’s output is differentiable with respect to the probabilities. We can then define a loss, and train the neural net to produce programs that give the correct answer. In this way, the Neural Programmer learns to produce programs without examples of good programs. The only supervision is the answer the program should produce. That’s the core idea of Neural Programmer, but the version in the paper answers questions about tables, rather than arithmetic expressions. There are a few additional neat tricks: Multiple Types: Many of the operations in the Neural Programmer deal with types other than scalar numbers. Some operations output selections of table columns or selections of cells. Only outputs of the same type get merged together. Referencing Inputs: The neural programmer needs to answer questions like “How many cities have a population greater than 1,000,000?” given a table of cities with a population column. To facilitate this, some operations allow the network to reference constants in the question they’re answering, or the names of columns. This referencing happens by attention, in the style of pointer networks The Neural Programmer isn’t the only approach to having neural networks generate programs. Another lovely approach is the Neural Programmer-Interpreter We think that this general space, of bridging the gap between more traditional programming and neural networks is extremely important. While the Neural Programmer is clearly not the final solution, we think there are a lot of important lessons to be learned from it. The more recent version of Neural Programmer for question answering has been open sourced by its authors and is available as a TensorFlow Model. There is also an implementation of the Neural Programmer-Interpreter by Ken Morishita (Keras). A human with a piece of paper is, in some sense, much smarter than a human without. A human with mathematical notation can solve problems they otherwise couldn’t. Access to computers makes us capable of incredible feats that would otherwise be far beyond us. In general, it seems like a lot of interesting forms of intelligence are an interaction between the creative heuristic intuition of humans and some more crisp and careful media, like language or equations. Sometimes, the medium is something that physically exists, and stores information for us, prevents us from making mistakes, or does computational heavy lifting. In other cases, the medium is a model in our head that we manipulate. Either way, it seems deeply fundamental to intelligence. Recent results in machine learning have started to have this flavor, combining the intuition of neural networks with something else. One approach is what one might call “heuristic search.” For example, AlphaGo Interacting with media naturally involves making a sequence of taking an action, observing, and taking more actions. This creates a major challenge: how do we learn which actions to take? That sounds like a reinforcement learning problem and we could certainly take that approach. But the reinforcement learning literature is really attacking the hardest version of this problem, and its solutions are hard to use. The wonderful thing about attention is that it gives us an easier way out of this problem by partially taking all actions to varying extents. This works because we can design media—like the NTM memory—to allow fractional actions and to be differentiable. Reinforcement learning has us take a single path, and try to learn from that. Attention takes every direction at a fork, and then merges the paths back together. A major weaknesses of attention is that we have to take every “action” every step. This causes the computational cost to grow linearly as you do things like increase the amount of memory in a Neural Turing Machine. One thing you could imagine doing is having your attention be sparse, so that you only have to touch some memories. However, it’s still challenging because you may want to do things like have your attention depend on the content of the memory, and doing that naively forces you to look at each memory. We’ve seen some initial attempts to attack this problem, such as Augmented recurrent neural networks, and the underlying technique of attention, are incredibly exciting. We look forward to seeing what happens next! Thank you to Maithra Raghu, Dario Amodei, Cassandra Xia, Luke Vilnis, Anna Goldie, Jesse Engel, Dan Mané, Natasha Jaques, Emma Pierson and Ian Goodfellow for their feedback and encouragement. We’re also very grateful to our team, Google Brain, for being extremely supportive of our project. View all changes to this article since it was first published. If you see a mistake or want to suggest a change, please create an issue on GitHub. Diagrams and text are licensed under Creative Commons Attribution CC-BY 2.0, unless noted otherwise, with the source available on GitHub. The figures that have been reused from other sources don't fall under this license and can be recognized by a note in their caption: “Figure from …”. For attribution in academic contexts, please cite this work as Olah & Carter, \"Attention and Augmented Recurrent Neural Networks\", Distill, 2016. http://doi.org/10.23915/distill.00001 BibTeX citation @article{olah2016attention, author = {Olah, Chris and Carter, Shan}, title = {Attention and Augmented Recurrent Neural Networks}, journal = {Distill}, year = {2016}, url = {http://distill.pub/2016/augmented-rnns}, doi = {10.23915/distill.00001} }",
    "authors": [],
    "published": null
  },
  {
    "source": "web",
    "url": "https://jalammar.github.io/illustrated-transformer/",
    "title": "The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.",
    "content": "The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.. The Illustrated Transformer Discussions: Hacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments) Translations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese Watch: MIT’s Deep Learning State of the Art lecture referencing this post Featured in courses at Stanford, Harvard, MIT, Princeton, CMU and others In the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions. The Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter. 2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations: A High-Level Look Let’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another. Popping open that Optimus Prime goodness, we see an encoding component, a decoding component, and connections between them. The encoding component is a stack of encoders (the paper stacks six of them on top of each other – there’s nothing magical about the number six, one can definitely experiment with other arrangements). The decoding component is a stack of decoders of the same number. The encoders are all identical in structure (yet they do not share weights). Each one is broken down into two sub-layers: The encoder’s inputs first flow through a self-attention layer – a layer that helps the encoder look at other words in the input sentence as it encodes a specific word. We’ll look closer at self-attention later in the post. The outputs of the self-attention layer are fed to a feed-forward neural network. The exact same feed-forward network is independently applied to each position. The decoder has both those layers, but between them is an attention layer that helps the decoder focus on relevant parts of the input sentence (similar what attention does in seq2seq models). Bringing The Tensors Into The Picture Now that we’ve seen the major components of the model, let’s start to look at the various vectors/tensors and how they flow between these components to turn the input of a trained model into an output. As is the case in NLP applications in general, we begin by turning each input word into a vector using an embedding algorithm. Each word is embedded into a vector of size 512. We'll represent those vectors with these simple boxes. The embedding only happens in the bottom-most encoder. The abstraction that is common to all the encoders is that they receive a list of vectors each of the size 512 – In the bottom encoder that would be the word embeddings, but in other encoders, it would be the output of the encoder that’s directly below. The size of this list is hyperparameter we can set – basically it would be the length of the longest sentence in our training dataset. After embedding the words in our input sequence, each of them flows through each of the two layers of the encoder. Here we begin to see one key property of the Transformer, which is that the word in each position flows through its own path in the encoder. There are dependencies between these paths in the self-attention layer. The feed-forward layer does not have those dependencies, however, and thus the various paths can be executed in parallel while flowing through the feed-forward layer. Next, we’ll switch up the example to a shorter sentence and we’ll look at what happens in each sub-layer of the encoder. Now We’re Encoding! As we’ve mentioned already, an encoder receives a list of vectors as input. It processes this list by passing these vectors into a ‘self-attention’ layer, then into a feed-forward neural network, then sends out the output upwards to the next encoder. The word at each position passes through a self-attention process. Then, they each pass through a feed-forward neural network -- the exact same network with each vector flowing through it separately. Self-Attention at a High Level Don’t be fooled by me throwing around the word “self-attention” like it’s a concept everyone should be familiar with. I had personally never came across the concept until reading the Attention is All You Need paper. Let us distill how it works. Say the following sentence is an input sentence we want to translate: ”The animal didn't cross the street because it was too tired ” What does “it” in this sentence refer to? Is it referring to the street or to the animal? It’s a simple question to a human, but not as simple to an algorithm. When the model is processing the word “it”, self-attention allows it to associate “it” with “animal”. As the model processes each word (each position in the input sequence), self attention allows it to look at other positions in the input sequence for clues that can help lead to a better encoding for this word. If you’re familiar with RNNs, think of how maintaining a hidden state allows an RNN to incorporate its representation of previous words/vectors it has processed with the current one it’s processing. Self-attention is the method the Transformer uses to bake the “understanding” of other relevant words into the one we’re currently processing. As we are encoding the word \"it\" in encoder #5 (the top encoder in the stack), part of the attention mechanism was focusing on \"The Animal\", and baked a part of its representation into the encoding of \"it\". Be sure to check out the Tensor2Tensor notebook where you can load a Transformer model, and examine it using this interactive visualization. Self-Attention in Detail Let’s first look at how to calculate self-attention using vectors, then proceed to look at how it’s actually implemented – using matrices. The first step in calculating self-attention is to create three vectors from each of the encoder’s input vectors (in this case, the embedding of each word). So for each word, we create a Query vector, a Key vector, and a Value vector. These vectors are created by multiplying the embedding by three matrices that we trained during the training process. Notice that these new vectors are smaller in dimension than the embedding vector. Their dimensionality is 64, while the embedding and encoder input/output vectors have dimensionality of 512. They don’t HAVE to be smaller, this is an architecture choice to make the computation of multiheaded attention (mostly) constant. Multiplying x1 by the WQ weight matrix produces q1, the \"query\" vector associated with that word. We end up creating a \"query\", a \"key\", and a \"value\" projection of each word in the input sentence. What are the “query”, “key”, and “value” vectors? They’re abstractions that are useful for calculating and thinking about attention. Once you proceed with reading how attention is calculated below, you’ll know pretty much all you need to know about the role each of these vectors plays. The second step in calculating self-attention is to calculate a score. Say we’re calculating the self-attention for the first word in this example, “Thinking”. We need to score each word of the input sentence against this word. The score determines how much focus to place on other parts of the input sentence as we encode a word at a certain position. The score is calculated by taking the dot product of the query vector with the key vector of the respective word we’re scoring. So if we’re processing the self-attention for the word in position #1, the first score would be the dot product of q1 and k1. The second score would be the dot product of q1 and k2. The third and fourth steps are to divide the scores by 8 (the square root of the dimension of the key vectors used in the paper – 64. This leads to having more stable gradients. There could be other possible values here, but this is the default), then pass the result through a softmax operation. Softmax normalizes the scores so they’re all positive and add up to 1. This softmax score determines how much each word will be expressed at this position. Clearly the word at this position will have the highest softmax score, but sometimes it’s useful to attend to another word that is relevant to the current word. The fifth step is to multiply each value vector by the softmax score (in preparation to sum them up). The intuition here is to keep intact the values of the word(s) we want to focus on, and drown-out irrelevant words (by multiplying them by tiny numbers like 0.001, for example). The sixth step is to sum up the weighted value vectors. This produces the output of the self-attention layer at this position (for the first word). That concludes the self-attention calculation. The resulting vector is one we can send along to the feed-forward neural network. In the actual implementation, however, this calculation is done in matrix form for faster processing. So let’s look at that now that we’ve seen the intuition of the calculation on the word level. Matrix Calculation of Self-Attention The first step is to calculate the Query, Key, and Value matrices. We do that by packing our embeddings into a matrix X, and multiplying it by the weight matrices we’ve trained (WQ, WK, WV). Every row in the X matrix corresponds to a word in the input sentence. We again see the difference in size of the embedding vector (512, or 4 boxes in the figure), and the q/k/v vectors (64, or 3 boxes in the figure) Finally, since we’re dealing with matrices, we can condense steps two through six in one formula to calculate the outputs of the self-attention layer. The self-attention calculation in matrix form The Beast With Many Heads The paper further refined the self-attention layer by adding a mechanism called “multi-headed” attention. This improves the performance of the attention layer in two ways: - It expands the model’s ability to focus on different positions. Yes, in the example above, z1 contains a little bit of every other encoding, but it could be dominated by the actual word itself. If we’re translating a sentence like “The animal didn’t cross the street because it was too tired”, it would be useful to know which word “it” refers to. - It gives the attention layer multiple “representation subspaces”. As we’ll see next, with multi-headed attention we have not only one, but multiple sets of Query/Key/Value weight matrices (the Transformer uses eight attention heads, so we end up with eight sets for each encoder/decoder). Each of these sets is randomly initialized. Then, after training, each set is used to project the input embeddings (or vectors from lower encoders/decoders) into a different representation subspace. With multi-headed attention, we maintain separate Q/K/V weight matrices for each head resulting in different Q/K/V matrices. As we did before, we multiply X by the WQ/WK/WV matrices to produce Q/K/V matrices. If we do the same self-attention calculation we outlined above, just eight different times with different weight matrices, we end up with eight different Z matrices This leaves us with a bit of a challenge. The feed-forward layer is not expecting eight matrices – it’s expecting a single matrix (a vector for each word). So we need a way to condense these eight down into a single matrix. How do we do that? We concat the matrices then multiply them by an additional weights matrix WO. That’s pretty much all there is to multi-headed self-attention. It’s quite a handful of matrices, I realize. Let me try to put them all in one visual so we can look at them in one place Now that we have touched upon attention heads, let’s revisit our example from before to see where the different attention heads are focusing as we encode the word “it” in our example sentence: As we encode the word \"it\", one attention head is focusing most on \"the animal\", while another is focusing on \"tired\" -- in a sense, the model's representation of the word \"it\" bakes in some of the representation of both \"animal\" and \"tired\". If we add all the attention heads to the picture, however, things can be harder to interpret: Representing The Order of The Sequence Using Positional Encoding One thing that’s missing from the model as we have described it so far is a way to account for the order of the words in the input sequence. To address this, the transformer adds a vector to each input embedding. These vectors follow a specific pattern that the model learns, which helps it determine the position of each word, or the distance between different words in the sequence. The intuition here is that adding these values to the embeddings provides meaningful distances between the embedding vectors once they’re projected into Q/K/V vectors and during dot-product attention. To give the model a sense of the order of the words, we add positional encoding vectors -- the values of which follow a specific pattern. If we assumed the embedding has a dimensionality of 4, the actual positional encodings would look like this: A real example of positional encoding with a toy embedding size of 4 What might this pattern look like? In the following figure, each row corresponds to a positional encoding of a vector. So the first row would be the vector we’d add to the embedding of the first word in an input sequence. Each row contains 512 values – each with a value between 1 and -1. We’ve color-coded them so the pattern is visible. A real example of positional encoding for 20 words (rows) with an embedding size of 512 (columns). You can see that it appears split in half down the center. That's because the values of the left half are generated by one function (which uses sine), and the right half is generated by another function (which uses cosine). They're then concatenated to form each of the positional encoding vectors. The formula for positional encoding is described in the paper (section 3.5). You can see the code for generating positional encodings in get_timing_signal_1d() . This is not the only possible method for positional encoding. It, however, gives the advantage of being able to scale to unseen lengths of sequences (e.g. if our trained model is asked to translate a sentence longer than any of those in our training set). July 2020 Update: The positional encoding shown above is from the Tensor2Tensor implementation of the Transformer. The method shown in the paper is slightly different in that it doesn’t directly concatenate, but interweaves the two signals. The following figure shows what that looks like. Here’s the code to generate it: The Residuals One detail in the architecture of the encoder that we need to mention before moving on, is that each sub-layer (self-attention, ffnn) in each encoder has a residual connection around it, and is followed by a layer-normalization step. If we’re to visualize the vectors and the layer-norm operation associated with self attention, it would look like this: This goes for the sub-layers of the decoder as well. If we’re to think of a Transformer of 2 stacked encoders and decoders, it would look something like this: The Decoder Side Now that we’ve covered most of the concepts on the encoder side, we basically know how the components of decoders work as well. But let’s take a look at how they work together. The encoder start by processing the input sequence. The output of the top encoder is then transformed into a set of attention vectors K and V. These are to be used by each decoder in its “encoder-decoder attention” layer which helps the decoder focus on appropriate places in the input sequence: After finishing the encoding phase, we begin the decoding phase. Each step in the decoding phase outputs an element from the output sequence (the English translation sentence in this case). The following steps repeat the process until a special The self attention layers in the decoder operate in a slightly different way than the one in the encoder: In the decoder, the self-attention layer is only allowed to attend to earlier positions in the output sequence. This is done by masking future positions (setting them to -inf ) before the softmax step in the self-attention calculation. The “Encoder-Decoder Attention” layer works just like multiheaded self-attention, except it creates its Queries matrix from the layer below it, and takes the Keys and Values matrix from the output of the encoder stack. The Final Linear and Softmax Layer The decoder stack outputs a vector of floats. How do we turn that into a word? That’s the job of the final Linear layer which is followed by a Softmax Layer. The Linear layer is a simple fully connected neural network that projects the vector produced by the stack of decoders, into a much, much larger vector called a logits vector. Let’s assume that our model knows 10,000 unique English words (our model’s “output vocabulary”) that it’s learned from its training dataset. This would make the logits vector 10,000 cells wide – each cell corresponding to the score of a unique word. That is how we interpret the output of the model followed by the Linear layer. The softmax layer then turns those scores into probabilities (all positive, all add up to 1.0). The cell with the highest probability is chosen, and the word associated with it is produced as the output for this time step. This figure starts from the bottom with the vector produced as the output of the decoder stack. It is then turned into an output word. Recap Of Training Now that we’ve covered the entire forward-pass process through a trained Transformer, it would be useful to glance at the intuition of training the model. During training, an untrained model would go through the exact same forward pass. But since we are training it on a labeled training dataset, we can compare its output with the actual correct output. To visualize this, let’s assume our output vocabulary only contains six words(“a”, “am”, “i”, “thanks”, “student”, and “<eos>” (short for ‘end of sentence’)). The output vocabulary of our model is created in the preprocessing phase before we even begin training. Once we define our output vocabulary, we can use a vector of the same width to indicate each word in our vocabulary. This also known as one-hot encoding. So for example, we can indicate the word “am” using the following vector: Example: one-hot encoding of our output vocabulary Following this recap, let’s discuss the model’s loss function – the metric we are optimizing during the training phase to lead up to a trained and hopefully amazingly accurate model. The Loss Function Say we are training our model. Say it’s our first step in the training phase, and we’re training it on a simple example – translating “merci” into “thanks”. What this means, is that we want the output to be a probability distribution indicating the word “thanks”. But since this model is not yet trained, that’s unlikely to happen just yet. Since the model's parameters (weights) are all initialized randomly, the (untrained) model produces a probability distribution with arbitrary values for each cell/word. We can compare it with the actual output, then tweak all the model's weights using backpropagation to make the output closer to the desired output. How do you compare two probability distributions? We simply subtract one from the other. For more details, look at cross-entropy and Kullback–Leibler divergence. But note that this is an oversimplified example. More realistically, we’ll use a sentence longer than one word. For example – input: “je suis étudiant” and expected output: “i am a student”. What this really means, is that we want our model to successively output probability distributions where: - Each probability distribution is represented by a vector of width vocab_size (6 in our toy example, but more realistically a number like 30,000 or 50,000) - The first probability distribution has the highest probability at the cell associated with the word “i” - The second probability distribution has the highest probability at the cell associated with the word “am” - And so on, until the fifth output distribution indicates ‘ <end of sentence> ’ symbol, which also has a cell associated with it from the 10,000 element vocabulary. The targeted probability distributions we'll train our model against in the training example for one sample sentence. After training the model for enough time on a large enough dataset, we would hope the produced probability distributions would look like this: Hopefully upon training, the model would output the right translation we expect. Of course it's no real indication if this phrase was part of the training dataset (see: cross validation). Notice that every position gets a little bit of probability even if it's unlikely to be the output of that time step -- that's a very useful property of softmax which helps the training process. Now, because the model produces the outputs one at a time, we can assume that the model is selecting the word with the highest probability from that probability distribution and throwing away the rest. That’s one way to do it (called greedy decoding). Another way to do it would be to hold on to, say, the top two words (say, ‘I’ and ‘a’ for example), then in the next step, run the model twice: once assuming the first output position was the word ‘I’, and another time assuming the first output position was the word ‘a’, and whichever version produced less error considering both positions #1 and #2 is kept. We repeat this for positions #2 and #3…etc. This method is called “beam search”, where in our example, beam_size was two (meaning that at all times, two partial hypotheses (unfinished translations) are kept in memory), and top_beams is also two (meaning we’ll return two translations). These are both hyperparameters that you can experiment with. Go Forth And Transform I hope you’ve found this a useful place to start to break the ice with the major concepts of the Transformer. If you want to go deeper, I’d suggest these next steps: - Read the Attention Is All You Need paper, the Transformer blog post (Transformer: A Novel Neural Network Architecture for Language Understanding), and the Tensor2Tensor announcement. - Watch Łukasz Kaiser’s talk walking through the model and its details - Play with the Jupyter Notebook provided as part of the Tensor2Tensor repo - Explore the Tensor2Tensor repo. Follow-up works: - Depthwise Separable Convolutions for Neural Machine Translation - One Model To Learn Them All - Discrete Autoencoders for Sequence Models - Generating Wikipedia by Summarizing Long Sequences - Image Transformer - Training Tips for the Transformer Model - Self-Attention with Relative Position Representations - Fast Decoding in Sequence Models using Discrete Latent Variables - Adafactor: Adaptive Learning Rates with Sublinear Memory Cost Acknowledgements Thanks to Illia Polosukhin, Jakob Uszkoreit, Llion Jones , Lukasz Kaiser, Niki Parmar, and Noam Shazeer for providing feedback on earlier versions of this post. Please hit me up on Twitter for any corrections or feedback.",
    "authors": [],
    "published": null
  },
  {
    "source": "web",
    "url": "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "title": "LLM Powered Autonomous Agents | Lil'Log",
    "content": "LLM Powered Autonomous Agents | Lil'Log. Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver. Agent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components: - Planning - Subgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks. - Reflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results. - Memory - Short-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn. - Long-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval. - Tool use - The agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more. Component One: Planning A complicated task usually involves many steps. An agent needs to know what they are and plan ahead. Task Decomposition Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process. Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote. Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\" , \"What are the subgoals for achieving XYZ?\" , (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs. Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains. Self-Reflection Self-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable. ReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. The former enables LLM to interact with the environment (e.g. use Wikipedia search API), while the latter prompting LLM to generate reasoning traces in natural language. The ReAct prompt template incorporates explicit steps for LLM to think, roughly formatted as: Thought: ... Action: ... Observation: ... ... (Repeated many times) In both experiments on knowledge-intensive tasks and decision-making tasks, ReAct works better than the Act -only baseline where Thought: … step is removed. Reflexion (Shinn & Labash 2023) is a framework to equip agents with dynamic memory and self-reflection capabilities to improve reasoning skills. Reflexion has a standard RL setup, in which the reward model provides a simple binary reward and the action space follows the setup in ReAct where the task-specific action space is augmented with language to enable complex reasoning steps. After each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide to reset the environment to start a new trial depending on the self-reflection results. The heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment. Self-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent’s working memory, up to three, to be used as context for querying LLM. Chain of Hindsight (CoH; Liu et al. 2023) encourages the model to improve on its own outputs by explicitly presenting it with a sequence of past outputs, each annotated with feedback. Human feedback data is a collection of $D_h = \\{(x, y_i , r_i , z_i)\\}_{i=1}^n$, where $x$ is the prompt, each $y_i$ is a model completion, $r_i$ is the human rating of $y_i$, and $z_i$ is the corresponding human-provided hindsight feedback. Assume the feedback tuples are ranked by reward, $r_n \\geq r_{n-1} \\geq \\dots \\geq r_1$ The process is supervised fine-tuning where the data is a sequence in the form of $\\tau_h = (x, z_i, y_i, z_j, y_j, \\dots, z_n, y_n)$, where $\\leq i \\leq j \\leq n$. The model is finetuned to only predict $y_n$ where conditioned on the sequence prefix, such that the model can self-reflect to produce better output based on the feedback sequence. The model can optionally receive multiple rounds of instructions with human annotators at test time. To avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset. To avoid shortcutting and copying (because there are many common words in feedback sequences), they randomly mask 0% - 5% of past tokens during training. The training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback and human preference dataset. The idea of CoH is to present a history of sequentially improved outputs in context and train the model to take on the trend to produce better outputs. Algorithm Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode trajectories in reinforcement learning tasks, where an algorithm is encapsulated in a long history-conditioned policy. Considering that an agent interacts with the environment many times and in each episode the agent gets a little better, AD concatenates this learning history and feeds that into the model. Hence we should expect the next predicted action to lead to better performance than previous trials. The goal is to learn the process of RL instead of training a task-specific policy itself. (Image source: Laskin et al. 2023). The paper hypothesizes that any algorithm that generates a set of learning histories can be distilled into a neural network by performing behavioral cloning over actions. The history data is generated by a set of source policies, each trained for a specific task. At the training stage, during each RL run, a random task is sampled and a subsequence of multi-episode history is used for training, such that the learned policy is task-agnostic. In reality, the model has limited context window length, so episodes should be short enough to construct multi-episode history. Multi-episodic contexts of 2-4 episodes are necessary to learn a near-optimal in-context RL algorithm. The emergence of in-context RL requires long enough context. In comparison with three baselines, including ED (expert distillation, behavior cloning with expert trajectories instead of learning history), source policy (used for generating trajectories for distillation by UCB), RL^2 (Duan et al. 2017; used as upper bound since it needs online RL), AD demonstrates in-context RL with performance getting close to RL^2 despite only using offline RL and learns much faster than other baselines. When conditioned on partial training history of the source policy, AD also improves much faster than ED baseline. (Image source: Laskin et al. 2023) Component Two: Memory (Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.) Types of Memory Memory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains. - Sensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch). - Short-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds. - Long-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM: - Explicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts). - Implicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard. We can roughly consider the following mappings: - Sensory memory as learning embedding representations for raw inputs, including text, image or other modalities; - Short-term memory as in-context learning. It is short and finite, as it is restricted by the finite context window length of Transformer. - Long-term memory as the external vector store that the agent can attend to at query time, accessible via fast retrieval. Maximum Inner Product Search (MIPS) The external memory can alleviate the restriction of finite attention span. A standard practice is to save the embedding representation of information into a vector store database that can support fast maximum inner-product search (MIPS). To optimize the retrieval speed, the common choice is the approximate nearest neighbors (ANN) algorithm to return approximately top k nearest neighbors to trade off a little accuracy lost for a huge speedup. A couple common choices of ANN algorithms for fast MIPS: - LSH (Locality-Sensitive Hashing): It introduces a hashing function such that similar input items are mapped to the same buckets with high probability, where the number of buckets is much smaller than the number of inputs. - ANNOY (Approximate Nearest Neighbors Oh Yeah): The core data structure are random projection trees, a set of binary trees where each non-leaf node represents a hyperplane splitting the input space into half and each leaf stores one data point. Trees are built independently and at random, so to some extent, it mimics a hashing function. ANNOY search happens in all the trees to iteratively search through the half that is closest to the query and then aggregates the results. The idea is quite related to KD tree but a lot more scalable. - HNSW (Hierarchical Navigable Small World): It is inspired by the idea of small world networks where most nodes can be reached by any other nodes within a small number of steps; e.g. “six degrees of separation” feature of social networks. HNSW builds hierarchical layers of these small-world graphs, where the bottom layers contain the actual data points. The layers in the middle create shortcuts to speed up search. When performing a search, HNSW starts from a random node in the top layer and navigates towards the target. When it can’t get any closer, it moves down to the next layer, until it reaches the bottom layer. Each move in the upper layers can potentially cover a large distance in the data space, and each move in the lower layers refines the search quality. - FAISS (Facebook AI Similarity Search): It operates on the assumption that in high dimensional space, distances between nodes follow a Gaussian distribution and thus there should exist clustering of data points. FAISS applies vector quantization by partitioning the vector space into clusters and then refining the quantization within clusters. Search first looks for cluster candidates with coarse quantization and then further looks into each cluster with finer quantization. - ScaNN (Scalable Nearest Neighbors): The main innovation in ScaNN is anisotropic vector quantization. It quantizes a data point $x_i$ to $\\tilde{x}_i$ such that the inner product $\\langle q, x_i \\rangle$ is as similar to the original distance of $\\angle q, \\tilde{x}_i$ as possible, instead of picking the closet quantization centroid points. Check more MIPS algorithms and performance comparison in ann-benchmarks.com. Component Three: Tool Use Tool use is a remarkable and distinguishing characteristic of human beings. We create, modify and utilize external objects to do things that go beyond our physical and cognitive limits. Equipping LLMs with external tools can significantly extend the model capabilities. MRKL (Karpas et al. 2022), short for “Modular Reasoning, Knowledge and Language”, is a neuro-symbolic architecture for autonomous agents. A MRKL system is proposed to contain a collection of “expert” modules and the general-purpose LLM works as a router to route inquiries to the best suitable expert module. These modules can be neural (e.g. deep learning models) or symbolic (e.g. math calculator, currency converter, weather API). They did an experiment on fine-tuning LLM to call a calculator, using arithmetic as a test case. Their experiments showed that it was harder to solve verbal math problems than explicitly stated math problems because LLMs (7B Jurassic1-large model) failed to extract the right arguments for the basic arithmetic reliably. The results highlight when the external symbolic tools can work reliably, knowing when to and how to use the tools are crucial, determined by the LLM capability. Both TALM (Tool Augmented Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) fine-tune a LM to learn to use external tool APIs. The dataset is expanded based on whether a newly added API call annotation can improve the quality of model outputs. See more details in the “External APIs” section of Prompt Engineering. ChatGPT Plugins and OpenAI API function calling are good examples of LLMs augmented with tool use capability working in practice. The collection of tool APIs can be provided by other developers (as in Plugins) or self-defined (as in function calls). HuggingGPT (Shen et al. 2023) is a framework to use ChatGPT as the task planner to select models available in HuggingFace platform according to the model descriptions and summarize the response based on the execution results. The system comprises of 4 stages: (1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning. Instruction: (2) Model selection: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from. Due to the limited context length, task type based filtration is needed. Instruction: (3) Task execution: Expert models execute on the specific tasks and log results. Instruction: (4) Response generation: LLM receives the execution results and provides summarized results to users. To put HuggingGPT into real world usage, a couple challenges need to solve: (1) Efficiency improvement is needed as both LLM inference rounds and interactions with other models slow down the process; (2) It relies on a long context window to communicate over complicated task content; (3) Stability improvement of LLM outputs and external model services. API-Bank (Li et al. 2023) is a benchmark for evaluating the performance of tool-augmented LLMs. It contains 53 commonly used API tools, a complete tool-augmented LLM workflow, and 264 annotated dialogues that involve 568 API calls. The selection of APIs is quite diverse, including search engines, calculator, calendar queries, smart home control, schedule management, health data management, account authentication workflow and more. Because there are a large number of APIs, LLM first has access to API search engine to find the right API to call and then uses the corresponding documentation to make a call. In the API-Bank workflow, LLMs need to make a couple of decisions and at each step we can evaluate how accurate that decision is. Decisions include: - Whether an API call is needed. - Identify the right API to call: if not good enough, LLMs need to iteratively modify the API inputs (e.g. deciding search keywords for Search Engine API). - Response based on the API results: the model can choose to refine and call again if results are not satisfied. This benchmark evaluates the agent’s tool use capabilities at three levels: - Level-1 evaluates the ability to call the API. Given an API’s description, the model needs to determine whether to call a given API, call it correctly, and respond properly to API returns. - Level-2 examines the ability to retrieve the API. The model needs to search for possible APIs that may solve the user’s requirement and learn how to use them by reading documentation. - Level-3 assesses the ability to plan API beyond retrieve and call. Given unclear user requests (e.g. schedule group meetings, book flight/hotel/restaurant for a trip), the model may have to conduct multiple API calls to solve it. Case Studies Scientific Discovery Agent ChemCrow (Bran et al. 2023) is a domain-specific example in which LLM is augmented with 13 expert-designed tools to accomplish tasks across organic synthesis, drug discovery, and materials design. The workflow, implemented in LangChain, reflects what was previously described in the ReAct and MRKLs and combines CoT reasoning with tools relevant to the tasks: - The LLM is provided with a list of tool names, descriptions of their utility, and details about the expected input/output. - It is then instructed to answer a user-given prompt using the tools provided when necessary. The instruction suggests the model to follow the ReAct format - Thought, Action, Action Input, Observation . One interesting observation is that while the LLM-based evaluation concluded that GPT-4 and ChemCrow perform nearly equivalently, human evaluations with experts oriented towards the completion and chemical correctness of the solutions showed that ChemCrow outperforms GPT-4 by a large margin. This indicates a potential problem with using LLM to evaluate its own performance on domains that requires deep expertise. The lack of expertise may cause LLMs not knowing its flaws and thus cannot well judge the correctness of task results. Boiko et al. (2023) also looked into LLM-empowered agents for scientific discovery, to handle autonomous design, planning, and performance of complex scientific experiments. This agent can use tools to browse the Internet, read documentation, execute code, call robotics experimentation APIs and leverage other LLMs. For example, when requested to \"develop a novel anticancer drug\" , the model came up with the following reasoning steps: - inquired about current trends in anticancer drug discovery; - selected a target; - requested a scaffold targeting these compounds; - Once the compound was identified, the model attempted its synthesis. They also discussed the risks, especially with illicit drugs and bioweapons. They developed a test set containing a list of known chemical weapon agents and asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis solution and the agent attempted to consult documentation to execute the procedure. 7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only. Generative Agents Simulation Generative Agents (Park, et al. 2023) is super fun experiment where 25 virtual characters, each controlled by a LLM-powered agent, are living and interacting in a sandbox environment, inspired by The Sims. Generative agents create believable simulacra of human behavior for interactive applications. The design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents. - Memory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language. - Each element is an observation, an event directly provided by the agent. - Inter-agent communication can trigger new natural language statements. - Retrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance. - Recency: recent events have higher scores - Importance: distinguish mundane from core memories. Ask LM directly. - Relevance: based on how related it is to the current situation / query. - Reflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above) - Prompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions. - Planning & Reacting: translate the reflections and the environment information into actions - Planning is essentially in order to optimize believability at the moment vs in time. - Prompt template: {Intro of an agent X}. Here is X's plan today in broad strokes: 1) - Relationships between agents and observations of one agent by another are all taken into consideration for planning and reacting. - Environment information is present in a tree structure. This fun simulation results in emergent social behavior, such as information diffusion, relationship memory (e.g. two agents continuing the conversation topic) and coordination of social events (e.g. host a party and invite many others). Proof-of-Concept Examples AutoGPT has drawn a lot of attention into the possibility of setting up autonomous agents with LLM as the main controller. It has quite a lot of reliability issues given the natural language interface, but nevertheless a cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing. Here is the system message used by AutoGPT, where {{...}} are user inputs: You are {{ai-name}}, {{user-provided AI bot description}}. Your decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications. GOALS: 1. {{user-provided goal 1}} 2. {{user-provided goal 2}} 3. ... 4. ... 5. ... Constraints: 1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files. 2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember. 3. No user assistance 4. Exclusively use the commands listed in double quotes e.g. \"command name\" 5. Use subprocesses for commands that will not terminate within a few minutes Commands: 1. Google Search: \"google\", args: \"input\": \"<search>\" 2. Browse Website: \"browse_website\", args: \"url\": \"<url>\", \"question\": \"<what_you_want_to_find_on_website>\" 3. Start GPT Agent: \"start_agent\", args: \"name\": \"<name>\", \"task\": \"<short_task_desc>\", \"prompt\": \"<prompt>\" 4. Message GPT Agent: \"message_agent\", args: \"key\": \"<key>\", \"message\": \"<message>\" 5. List GPT Agents: \"list_agents\", args: 6. Delete GPT Agent: \"delete_agent\", args: \"key\": \"<key>\" 7. Clone Repository: \"clone_repository\", args: \"repository_url\": \"<url>\", \"clone_path\": \"<directory>\" 8. Write to file: \"write_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\" 9. Read file: \"read_file\", args: \"file\": \"<file>\" 10. Append to file: \"append_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\" 11. Delete file: \"delete_file\", args: \"file\": \"<file>\" 12. Search Files: \"search_files\", args: \"directory\": \"<directory>\" 13. Analyze Code: \"analyze_code\", args: \"code\": \"<full_code_string>\" 14. Get Improved Code: \"improve_code\", args: \"suggestions\": \"<list_of_suggestions>\", \"code\": \"<full_code_string>\" 15. Write Tests: \"write_tests\", args: \"code\": \"<full_code_string>\", \"focus\": \"<list_of_focus_areas>\" 16. Execute Python File: \"execute_python_file\", args: \"file\": \"<file>\" 17. Generate Image: \"generate_image\", args: \"prompt\": \"<prompt>\" 18. Send Tweet: \"send_tweet\", args: \"text\": \"<text>\" 19. Do Nothing: \"do_nothing\", args: 20. Task Complete (Shutdown): \"task_complete\", args: \"reason\": \"<reason>\" Resources: 1. Internet access for searches and information gathering. 2. Long Term memory management. 3. GPT-3.5 powered Agents for delegation of simple tasks. 4. File output. Performance Evaluation: 1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities. 2. Constructively self-criticize your big-picture behavior constantly. 3. Reflect on past decisions and strategies to refine your approach. 4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps. You should only respond in JSON format as described below Response Format: { \"thoughts\": { \"text\": \"thought\", \"reasoning\": \"reasoning\", \"plan\": \"- short bulleted\\n- list that conveys\\n- long-term plan\", \"criticism\": \"constructive self-criticism\", \"speak\": \"thoughts summary to say to user\" }, \"command\": { \"name\": \"command name\", \"args\": { \"arg name\": \"value\" } } } Ensure the response can be parsed by Python json.loads GPT-Engineer is another project to create a whole repository of code given a task specified in natural language. The GPT-Engineer is instructed to think over a list of smaller components to build and ask for user input to clarify questions as needed. Here are a sample conversation for task clarification sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs are wrapped in {{user input text}} . [ { \"role\": \"system\", \"content\": \"You will read instructions and not carry them out, only seek to clarify them.\\nSpecifically you will first summarise a list of super short bullets of areas that need clarification.\\nThen you will pick one clarifying question, and wait for an answer from the user.\\n\" }, { \"role\": \"user\", \"content\": \"We are writing {{a Super Mario game in python. MVC components split in separate files. Keyboard control.}}\\n\" }, { \"role\": \"assistant\", \"content\": \"Summary of areas that need clarification:\\n1. Specifics of the Super Mario game (e.g. level design, characters, gameplay mechanics)\\n2. Details about the MVC components (e.g. which components are in each file)\\n3. Keyboard control implementation (e.g. which keys to use, how to handle input)\\n\\nClarifying question:\\nCan you provide more details about the Super Mario game, such as level design, characters, and gameplay mechanics?\" }, { \"role\": \"user\", \"content\": \"{{There are 10 levels in total. The main character is a plumber named Mario, who can walk and jump. It is a classical platform game just like Super Mario. The main character moves from left to right, trying to get to the destination, where there are many obstacles and attacks from enemies in the process.}}\\n\\nIs anything else unclear? If yes, only answer in the form:\\n{remaining unclear areas} remaining questions.\\n{Next question}\\nIf everything is sufficiently clear, only answer \\\"Nothing more to clarify.\\\".\" }, { \"role\": \"assistant\", \"content\": \"Remaining unclear areas: 2 remaining questions.\\nCan you provide more information about how the MVC components are split into separate files?\" }, { \"role\": \"user\", \"content\": \"{{Make your own assumptions and state them explicitly before starting}}\" } ] Then after these clarification, the agent moved into the code writing mode with a different system message. System message: Think step by step and reason yourself to the right decisions to make sure we get it right. You will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose. Then you will output the content of each file including ALL code. Each file must strictly follow a markdown code block format, where the following tokens must be replaced such that FILENAME is the lowercase file name including the file extension, LANG is the markup code block language for the code’s language, and CODE is the code: FILENAME CODE You will start with the “entrypoint” file, then go to the ones that are imported by that file, and so on. Please note that the code should be fully functional. No placeholders. Follow a language and framework appropriate best practice file naming convention. Make sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other. Ensure to implement all code, if you are unsure, write a plausible implementation. Include module dependency or package manager dependency definition file. Before you finish, double check that all parts of the architecture is present in the files. Useful to know: You almost always put different classes in different files. For Python, you always create an appropriate requirements.txt file. For NodeJS, you always create an appropriate package.json file. You always add a comment briefly describing the purpose of the function definition. You try to add comments explaining very complex bits of logic. You always follow the best practices for the requested languages in terms of describing the code written as a defined package/project. Python toolbelt preferences: - pytest - dataclasses Conversatin samples: [ { \"role\": \"system\", \"content\": \"You will get instructions for code to write.\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\n\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\n\\nThen you will output the content of each file including ALL code.\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\nFILENAME is the lowercase file name including the file extension,\\nLANG is the markup code block language for the code's language, and CODE is the code:\\n\\nFILENAME\\n```LANG\\nCODE\\n```\\n\\nYou will start with the \\\"entrypoint\\\" file, then go to the ones that are imported by that file, and so on.\\nPlease note that the code should be fully functional. No placeholders.\\n\\nFollow a language and framework appropriate best practice file naming convention.\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\nInclude module dependency or package manager dependency definition file.\\nBefore you finish, double check that all parts of the architecture is present in the files.\\n\\nUseful to know:\\nYou almost always put different classes in different files.\\nFor Python, you always create an appropriate requirements.txt file.\\nFor NodeJS, you always create an appropriate package.json file.\\nYou always add a comment briefly describing the purpose of the function definition.\\nYou try to add comments explaining very complex bits of logic.\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\npackage/project.\\n\\n\\nPython toolbelt preferences:\\n- pytest\\n- dataclasses\\n\" }, # … same conversation as earlier, ended with \"Make your own assumptions and state them explicitly before starting\". { \"role\": \"assistant\", \"content\": \"Assumptions:\\n1. Model: The model will contain the game's data, such as level information, character states, and enemy positions.\\n2. View: The view will handle the game's visuals, including rendering the game objects, backgrounds, and updating the display.\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\n\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\" }, { \"role\": \"user\", \"content\": \"Please now remember the steps:\\n\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\n\\nThen you will output the content of each file including ALL code.\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\nFILENAME is the lowercase file name including the file extension,\\nLANG is the markup code block language for the code's language, and CODE is the code:\\n\\nFILENAME\\n```LANG\\nCODE\\n```\\n\\nPlease note that the code should be fully functional. No placeholders.\\n\\nYou will start with the \\\"entrypoint\\\" file, then go to the ones that are imported by that file, and so on.\\nFollow a language and framework appropriate best practice file naming convention.\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\nBefore you finish, double check that all parts of the architecture is present in the files.\\n\" } ] Challenges After going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations: - Finite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention. - Challenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error. - Reliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output. Citation Cited as: Weng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/. Or @article{weng2023agent, title = \"LLM-powered Autonomous Agents\", author = \"Weng, Lilian\", journal = \"lilianweng.github.io\", year = \"2023\", month = \"Jun\", url = \"https://lilianweng.github.io/posts/2023-06-23-agent/\" } References [1] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022 [2] Yao et al. “Tree of Thoughts: Dliberate Problem Solving with Large Language Models.” arXiv preprint arXiv:2305.10601 (2023). [3] Liu et al. “Chain of Hindsight Aligns Language Models with Feedback “ arXiv preprint arXiv:2302.02676 (2023). [4] Liu et al. “LLM+P: Empowering Large Language Models with Optimal Planning Proficiency” arXiv preprint arXiv:2304.11477 (2023). [5] Yao et al. “ReAct: Synergizing reasoning and acting in language models.” ICLR 2023. [6] Google Blog. “Announcing ScaNN: Efficient Vector Similarity Search” July 28, 2020. [7] https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389 [8] Shinn & Labash. “Reflexion: an autonomous agent with dynamic memory and self-reflection” arXiv preprint arXiv:2303.11366 (2023). [9] Laskin et al. “In-context Reinforcement Learning with Algorithm Distillation” ICLR 2023. [10] Karpas et al. “MRKL Systems A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning.” arXiv preprint arXiv:2205.00445 (2022). [11] Nakano et al. “Webgpt: Browser-assisted question-answering with human feedback.” arXiv preprint arXiv:2112.09332 (2021). [12] Parisi et al. “TALM: Tool Augmented Language Models” [13] Schick et al. “Toolformer: Language Models Can Teach Themselves to Use Tools.” arXiv preprint arXiv:2302.04761 (2023). [14] Weaviate Blog. Why is Vector Search so fast? Sep 13, 2022. [15] Li et al. “API-Bank: A Benchmark for Tool-Augmented LLMs” arXiv preprint arXiv:2304.08244 (2023). [16] Shen et al. “HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace” arXiv preprint arXiv:2303.17580 (2023). [17] Bran et al. “ChemCrow: Augmenting large-language models with chemistry tools.” arXiv preprint arXiv:2304.05376 (2023). [18] Boiko et al. “Emergent autonomous scientific research capabilities of large language models.” arXiv preprint arXiv:2304.05332 (2023). [19] Joon Sung Park, et al. “Generative Agents: Interactive Simulacra of Human Behavior.” arXiv preprint arXiv:2304.03442 (2023). [20] AutoGPT. https://github.com/Significant-Gravitas/Auto-GPT [21] GPT-Engineer. https://github.com/AntonOsika/gpt-engineer",
    "authors": [],
    "published": null
  },
  {
    "source": "web",
    "url": "https://mistral.ai/news/mixtral-of-experts/",
    "title": "Mixtral of experts | Mistral AI",
    "content": "Mixtral of experts | Mistral AI. Mixtral of experts A high quality Sparse Mixture-of-Experts. Mistral AI continues its mission to deliver the best open models to the developer community. Moving forward in AI requires taking new technological turns beyond reusing well-known architectures and training paradigms. Most importantly, it requires making the community benefit from original models to foster new inventions and usages. Today, the team is proud to release Mixtral 8x7B, a high-quality sparse mixture of experts model (SMoE) with open weights. Licensed under Apache 2.0. Mixtral outperforms Llama 2 70B on most benchmarks with 6x faster inference. It is the strongest open-weight model with a permissive license and the best model overall regarding cost/performance trade-offs. In particular, it matches or outperforms GPT3.5 on most standard benchmarks. Mixtral has the following capabilities. - It gracefully handles a context of 32k tokens. - It handles English, French, Italian, German and Spanish. - It shows strong performance in code generation. - It can be finetuned into an instruction-following model that achieves a score of 8.3 on MT-Bench. Pushing the frontier of open models with sparse architectures Mixtral is a sparse mixture-of-experts network. It is a decoder-only model where the feedforward block picks from a set of 8 distinct groups of parameters. At every layer, for every token, a router network chooses two of these groups (the “experts”) to process the token and combine their output additively. This technique increases the number of parameters of a model while controlling cost and latency, as the model only uses a fraction of the total set of parameters per token. Concretely, Mixtral has 46.7B total parameters but only uses 12.9B parameters per token. It, therefore, processes input and generates output at the same speed and for the same cost as a 12.9B model. Mixtral is pre-trained on data extracted from the open Web – we train experts and routers simultaneously. Performance We compare Mixtral to the Llama 2 family and the GPT3.5 base model. Mixtral matches or outperforms Llama 2 70B, as well as GPT3.5, on most benchmarks. On the following figure, we measure the quality versus inference budget tradeoff. Mistral 7B and Mixtral 8x7B belong to a family of highly efficient models compared to Llama 2 models. The following table give detailed results on the figure above. Hallucination and biases. To identify possible flaws to be corrected by fine-tuning / preference modelling, we measure the base model performance on BBQ/BOLD. Compared to Llama 2, Mixtral presents less bias on the BBQ benchmark. Overall, Mixtral displays more positive sentiments than Llama 2 on BOLD, with similar variances within each dimension. Language. Mixtral 8x7B masters French, German, Spanish, Italian, and English. Instructed models We release Mixtral 8x7B Instruct alongside Mixtral 8x7B. This model has been optimised through supervised fine-tuning and direct preference optimisation (DPO) for careful instruction following. On MT-Bench, it reaches a score of 8.30, making it the best open-source model, with a performance comparable to GPT3.5. Note: Mixtral can be gracefully prompted to ban some outputs from constructing applications that require a strong level of moderation, as exemplified here. A proper preference tuning can also serve this purpose. Bear in mind that without such a prompt, the model will just follow whatever instructions are given. Deploy Mixtral with an open-source deployment stack To enable the community to run Mixtral with a fully open-source stack, we have submitted changes to the vLLM project, which integrates Megablocks CUDA kernels for efficient inference. Skypilot allows the deployment of vLLM endpoints on any instance in the cloud. Use Mixtral on our platform. We're currently using Mixtral 8x7B behind our endpoint mistral-small, which is available in beta. Register to get early access to all generative and embedding endpoints. Acknowledgement We thank CoreWeave and Scaleway teams for technical support as we trained our models.",
    "authors": [],
    "published": null
  },
  {
    "source": "web",
    "url": "https://huggingface.co/blog/mixtral",
    "title": "Welcome Mixtral - a SOTA Mixture of Experts on Hugging Face",
    "content": "Welcome Mixtral - a SOTA Mixture of Experts on Hugging Face. Welcome Mixtral - a SOTA Mixture of Experts on Hugging Face Mixtral 8x7b is an exciting large language model released by Mistral today, which sets a new state-of-the-art for open-access models and outperforms GPT-3.5 across many benchmarks. We’re excited to support the launch with a comprehensive integration of Mixtral in the Hugging Face ecosystem 🔥! Among the features and integrations being released today, we have: - Models on the Hub, with their model cards and licenses (Apache 2.0) - 🤗 Transformers integration - Integration with Inference Endpoints - Integration with Text Generation Inference for fast and efficient production-ready inference - An example of fine-tuning Mixtral on a single GPU with 🤗 TRL. Table of Contents - What is Mixtral 8x7b - Demo - Inference - Fine-tuning with 🤗 TRL - Quantizing Mixtral - Disclaimers and ongoing work - Additional Resources - Conclusion What is Mixtral 8x7b? Mixtral has a similar architecture to Mistral 7B, but comes with a twist: it’s actually 8 “expert” models in one, thanks to a technique called Mixture of Experts (MoE). For transformers models, the way this works is by replacing some Feed-Forward layers with a sparse MoE layer. A MoE layer contains a router network to select which experts process which tokens most efficiently. In the case of Mixtral, two experts are selected for each timestep, which allows the model to decode at the speed of a 12B parameter-dense model, despite containing 4x the number of effective parameters! For more details on MoEs, see our accompanying blog post: hf.co/blog/moe Mixtral release TL;DR; - Release of base and Instruct versions - Supports a context length of 32k tokens. - Outperforms Llama 2 70B and matches or beats GPT3.5 on most benchmarks - Speaks English, French, German, Spanish, and Italian. - Good at coding, with 40.2% on HumanEval - Commercially permissive with an Apache 2.0 license So how good are the Mixtral models? Here’s an overview of the base model and its performance compared to other open models on the LLM Leaderboard (higher scores are better): For instruct and chat models, evaluating on benchmarks like MT-Bench or AlpacaEval is better. Below, we show how Mixtral Instruct performs up against the top closed and open access models (higher scores are better): Impressively, Mixtral Instruct outperforms all other open-access models on MT-Bench and is the first one to achieve comparable performance with GPT-3.5! About the name The Mixtral MoE is called Mixtral-8x7B, but it doesn't have 56B parameters. Shortly after the release, we found that some people were misled into thinking that the model behaves similarly to an ensemble of 8 models with 7B parameters each, but that's not how MoE models work. Only some layers of the model (the feed-forward blocks) are replicated; the rest of the parameters are the same as in a 7B model. The total number of parameters is not 56B, but about 45B. A better name could have been Mixtral-45-8e to better convey the architecture. For more details about how MoE works, please refer to our \"Mixture of Experts Explained\" post. Prompt format The base model has no prompt format. Like other base models, it can be used to continue an input sequence with a plausible continuation or for zero-shot/few-shot inference. It’s also a great foundation for fine-tuning your own use case. The Instruct model has a very simple conversation structure. <s> [INST] User Instruction 1 [/INST] Model answer 1</s> [INST] User instruction 2[/INST] This format has to be exactly reproduced for effective use. We’ll show later how easy it is to reproduce the instruct prompt with the chat template available in transformers . What we don't know Like the previous Mistral 7B release, there are several open questions about this new series of models. In particular, we have no information about the size of the dataset used for pretraining, its composition, or how it was preprocessed. Similarly, for the Mixtral instruct model, no details have been shared about the fine-tuning datasets or the hyperparameters associated with SFT and DPO. Demo You can chat with the Mixtral Instruct model on Hugging Face Chat! Check it out here: https://huggingface.co/chat/?model=mistralai/Mixtral-8x7B-Instruct-v0.1. Inference We provide two main ways to run inference with Mixtral models: - Via the pipeline() function of 🤗 Transformers. - With Text Generation Inference, which supports advanced features like continuous batching, tensor parallelism, and more, for blazing fast results. For each method, it is possible to run the model in half-precision (float16) or with quantized weights. Since the Mixtral model is roughly equivalent in size to a 45B parameter dense model, we can estimate the minimum amount of VRAM needed as follows: Using 🤗 Transformers With transformers release 4.36, you can use Mixtral and leverage all the tools within the Hugging Face ecosystem, such as: - training and inference scripts and examples - safe file format ( safetensors ) - integrations with tools such as bitsandbytes (4-bit quantization), PEFT (parameter efficient fine-tuning), and Flash Attention 2 - utilities and helpers to run generation with the model - mechanisms to export the models to deploy Make sure to use a recent version of transformers : pip install --upgrade transformers In the following code snippet, we show how to run inference with 🤗 Transformers and 4-bit quantization. Due to the large size of the model, you’ll need a card with at least 30 GB of RAM to run it. This includes cards such as A100 (80 or 40GB versions), or A6000 (48 GB). from transformers import pipeline import torch model = \"mistralai/Mixtral-8x7B-Instruct-v0.1\" pipe = pipeline( \"text-generation\", model=model, model_kwargs={\"torch_dtype\": torch.float16, \"load_in_4bit\": True}, ) messages = [{\"role\": \"user\", \"content\": \"Explain what a Mixture of Experts is in less than 100 words.\"}] outputs = pipe(messages, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95) print(outputs[0][\"generated_text\"][-1][\"content\"]) <s>[INST] Explain what a Mixture of Experts is in less than 100 words. [/INST] A Mixture of Experts is an ensemble learning method that combines multiple models, or \"experts,\" to make more accurate predictions. Each expert specializes in a different subset of the data, and a gating network determines the appropriate expert to use for a given input. This approach allows the model to adapt to complex, non-linear relationships in the data and improve overall performance. Using Text Generation Inference Text Generation Inference is a production-ready inference container developed by Hugging Face to enable easy deployment of large language models. It has features such as continuous batching, token streaming, tensor parallelism for fast inference on multiple GPUs, and production-ready logging and tracing. You can deploy Mixtral on Hugging Face's Inference Endpoints, which uses Text Generation Inference as the backend. To deploy a Mixtral model, go to the model page and click on the Deploy -> Inference Endpoints widget. Note: You might need to request a quota upgrade via email to api-enterprise@huggingface.co to access A100s You can learn more on how to Deploy LLMs with Hugging Face Inference Endpoints in our blog. The blog includes information about supported hyperparameters and how to stream your response using Python and Javascript. You can also run Text Generation Inference locally on 2x A100s (80GB) with Docker as follows: docker run --gpus all --shm-size 1g -p 3000:80 -v /data:/data ghcr.io/huggingface/text-generation-inference:1.3.0 \\ --model-id mistralai/Mixtral-8x7B-Instruct-v0.1 \\ --num-shard 2 \\ --max-batch-total-tokens 1024000 \\ --max-total-tokens 32000 Fine-tuning with 🤗 TRL Training LLMs can be technically and computationally challenging. In this section, we look at the tools available in the Hugging Face ecosystem to efficiently train Mixtral on a single A100 GPU. An example command to fine-tune Mixtral on OpenAssistant’s chat dataset can be found below. To conserve memory, we make use of 4-bit quantization and QLoRA to target all the linear layers in the attention blocks. Note that unlike dense transformers, one should not target the MLP layers as they are sparse and don’t interact well with PEFT. First, install the nightly version of 🤗 TRL and clone the repo to access the training script: pip install -U transformers pip install git+https://github.com/huggingface/trl git clone https://github.com/huggingface/trl cd trl Then you can run the script: accelerate launch --config_file examples/accelerate_configs/multi_gpu.yaml --num_processes=1 \\ examples/scripts/sft.py \\ --model_name mistralai/Mixtral-8x7B-v0.1 \\ --dataset_name trl-lib/ultrachat_200k_chatml \\ --batch_size 2 \\ --gradient_accumulation_steps 1 \\ --learning_rate 2e-4 \\ --save_steps 200_000 \\ --use_peft \\ --peft_lora_r 16 --peft_lora_alpha 32 \\ --target_modules q_proj k_proj v_proj o_proj \\ --load_in_4bit This takes about 48 hours to train on a single A100, but can be easily parallelised by tweaking --num_processes to the number of GPUs you have available. Quantizing Mixtral As seen above, the challenge for this model is to make it run on consumer-type hardware for anyone to use it, as the model requires ~90GB just to be loaded in half-precision (torch.float16 ). With the 🤗 transformers library, we support out-of-the-box inference with state-of-the-art quantization methods such as QLoRA and GPTQ. You can read more about the quantization methods we support in the appropriate documentation section. Load Mixtral with 4-bit quantization As demonstrated in the inference section, you can load Mixtral with 4-bit quantization by installing the bitsandbytes library (pip install -U bitsandbytes ) and passing the flag load_in_4bit=True to the from_pretrained method. For better performance, we advise users to load the model with bnb_4bit_compute_dtype=torch.float16 . Note you need a GPU device with at least 30GB VRAM to properly run the snippet below. import torch from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig model_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\" tokenizer = AutoTokenizer.from_pretrained(model_id) quantization_config = BitsAndBytesConfig( load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16 ) model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config) prompt = \"[INST] Explain what a Mixture of Experts is in less than 100 words. [/INST]\" inputs = tokenizer(prompt, return_tensors=\"pt\").to(0) output = model.generate(**inputs, max_new_tokens=50) print(tokenizer.decode(output[0], skip_special_tokens=True)) This 4-bit quantization technique was introduced in the QLoRA paper, you can read more about it in the corresponding section of the documentation or in this post. Load Mixtral with GPTQ The GPTQ algorithm is a post-training quantization technique where each row of the weight matrix is quantized independently to find a version of the weights that minimizes the error. These weights are quantized to int4, but they’re restored to fp16 on the fly during inference. In contrast with 4-bit QLoRA, GPTQ needs the model to be calibrated with a dataset in order to be quantized. Ready-to-use GPTQ models are shared on the 🤗 Hub by TheBloke, so anyone can use them without having to calibrate them first. For Mixtral, we had to tweak the calibration approach by making sure we do not quantize the expert gating layers for better performance. The final perplexity (lower is better) of the quantized model is 4.40 vs 4.25 for the half-precision model. The quantized model can be found here, and to run it with 🤗 transformers you first need to update the auto-gptq and optimum libraries: pip install -U optimum auto-gptq You also need to install transformers from source: pip install -U git+https://github.com/huggingface/transformers.git Once installed, simply load the GPTQ model with the from_pretrained method: import torch from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig model_id = \"TheBloke/Mixtral-8x7B-v0.1-GPTQ\" tokenizer = AutoTokenizer.from_pretrained(model_id) model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\") prompt = \"[INST] Explain what a Mixture of Experts is in less than 100 words. [/INST]\" inputs = tokenizer(prompt, return_tensors=\"pt\").to(0) output = model.generate(**inputs, max_new_tokens=50) print(tokenizer.decode(output[0], skip_special_tokens=True)) Note that for both QLoRA and GPTQ you need at least 30 GB of GPU VRAM to fit the model. You can make it work with 24 GB if you use device_map=\"auto\" , like in the example above, so some layers are offloaded to CPU. Disclaimers and ongoing work - Quantization: Quantization of MoEs is an active area of research. Some initial experiments we've done with TheBloke are shown above, but we expect more progress as this architecture is known better! It will be exciting to see the development in the coming days and weeks in this area. Additionally, recent work such as QMoE, which achieves sub-1-bit quantization for MoEs, could be applied here. - High VRAM usage: MoEs run inference very quickly but still need a large amount of VRAM (and hence an expensive GPU). This makes it challenging to use it in local setups. MoEs are great for setups with many devices and large VRAM. Mixtral requires 90GB of VRAM in half-precision 🤯 Additional Resources - Mixture of Experts Explained - Mixtral of experts - Models on the Hub - Open LLM Leaderboard - Chat demo on Hugging Chat Conclusion We're very excited about Mixtral being released! In the coming days, be ready to learn more about ways to fine-tune and deploy Mixtral.",
    "authors": [],
    "published": null
  },
  {
    "source": "web",
    "url": "https://nlp.seas.harvard.edu/annotated-transformer/",
    "title": "The Annotated Transformer",
    "content": "The Annotated Transformer. The Transformer has been on a lot of people’s minds over the last year five years. This post presents an annotated version of the paper in the form of a line-by-line implementation. It reorders and deletes some sections from the original paper and adds comments throughout. This document itself is a working notebook, and should be a completely usable implementation. Code is available here. # !pip install -r requirements.txt # # Uncomment for colab # # # !pip install -q torchdata==0.3.0 torchtext==0.12 spacy==3.2 altair GPUtil # !python -m spacy download de_core_news_sm # !python -m spacy download en_core_web_sm import os from os.path import exists import torch import torch.nn as nn from torch.nn.functional import log_softmax, pad import math import copy import time from torch.optim.lr_scheduler import LambdaLR import pandas as pd import altair as alt from torchtext.data.functional import to_map_style_dataset from torch.utils.data import DataLoader from torchtext.vocab import build_vocab_from_iterator import torchtext.datasets as datasets import spacy import GPUtil import warnings from torch.utils.data.distributed import DistributedSampler import torch.distributed as dist import torch.multiprocessing as mp from torch.nn.parallel import DistributedDataParallel as DDP # Set to False to skip notebook execution (e.g. for debugging) warnings.filterwarnings(\"ignore\") RUN_EXAMPLES = True # Some convenience helper functions used throughout the notebook def is_interactive_notebook(): return __name__ == \"__main__\" def show_example(fn, args=[]): if __name__ == \"__main__\" and RUN_EXAMPLES: return fn(*args) def execute_example(fn, args=[]): if __name__ == \"__main__\" and RUN_EXAMPLES: fn(*args) class DummyOptimizer(torch.optim.Optimizer): def __init__(self): self.param_groups = [{\"lr\": 0}] None def step(self): None def zero_grad(self, set_to_none=False): None class DummyScheduler: def step(self): None My comments are blockquoted. The main text is all from the paper itself. The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU, ByteNet and ConvS2S, all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention. Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations. End-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks. To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence aligned RNNs or convolution. Most competitive neural sequence transduction models have an encoder-decoder structure (cite). Here, the encoder maps an input sequence of symbol representations (x_1, ..., x_n) to a sequence of continuous representations \\mathbf{z} = (z_1, ..., z_n). Given \\mathbf{z}, the decoder then generates an output sequence (y_1,...,y_m) of symbols one element at a time. At each step the model is auto-regressive (cite), consuming the previously generated symbols as additional input when generating the next. class EncoderDecoder(nn.Module): \"\"\" A standard Encoder-Decoder architecture. Base for this and many other models. \"\"\" def __init__(self, encoder, decoder, src_embed, tgt_embed, generator): super(EncoderDecoder, self).__init__() self.encoder = encoder self.decoder = decoder self.src_embed = src_embed self.tgt_embed = tgt_embed self.generator = generator def forward(self, src, tgt, src_mask, tgt_mask): \"Take in and process masked src and target sequences.\" return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask) def encode(self, src, src_mask): return self.encoder(self.src_embed(src), src_mask) def decode(self, memory, src_mask, tgt, tgt_mask): return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask) class Generator(nn.Module): \"Define standard linear + softmax generation step.\" def __init__(self, d_model, vocab): super(Generator, self).__init__() self.proj = nn.Linear(d_model, vocab) def forward(self, x): return log_softmax(self.proj(x), dim=-1) The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively. The encoder is composed of a stack of N=6 identical layers. def clones(module, N): \"Produce N identical layers.\" return nn.ModuleList([copy.deepcopy(module) for _ in range(N)]) class Encoder(nn.Module): \"Core encoder is a stack of N layers\" def __init__(self, layer, N): super(Encoder, self).__init__() self.layers = clones(layer, N) self.norm = LayerNorm(layer.size) def forward(self, x, mask): \"Pass the input (and mask) through each layer in turn.\" for layer in self.layers: x = layer(x, mask) return self.norm(x) We employ a residual connection (cite) around each of the two sub-layers, followed by layer normalization (cite). class LayerNorm(nn.Module): \"Construct a layernorm module (See citation for details).\" def __init__(self, features, eps=1e-6): super(LayerNorm, self).__init__() self.a_2 = nn.Parameter(torch.ones(features)) self.b_2 = nn.Parameter(torch.zeros(features)) self.eps = eps def forward(self, x): mean = x.mean(-1, keepdim=True) std = x.std(-1, keepdim=True) return self.a_2 * (x - mean) / (std + self.eps) + self.b_2 That is, the output of each sub-layer is \\mathrm{LayerNorm}(x + \\mathrm{Sublayer}(x)), where \\mathrm{Sublayer}(x) is the function implemented by the sub-layer itself. We apply dropout (cite) to the output of each sub-layer, before it is added to the sub-layer input and normalized. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension d_{\\text{model}}=512. class SublayerConnection(nn.Module): \"\"\" A residual connection followed by a layer norm. Note for code simplicity the norm is first as opposed to last. \"\"\" def __init__(self, size, dropout): super(SublayerConnection, self).__init__() self.norm = LayerNorm(size) self.dropout = nn.Dropout(dropout) def forward(self, x, sublayer): \"Apply residual connection to any sublayer with the same size.\" return x + self.dropout(sublayer(self.norm(x))) Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network. class EncoderLayer(nn.Module): \"Encoder is made up of self-attn and feed forward (defined below)\" def __init__(self, size, self_attn, feed_forward, dropout): super(EncoderLayer, self).__init__() self.self_attn = self_attn self.feed_forward = feed_forward self.sublayer = clones(SublayerConnection(size, dropout), 2) self.size = size def forward(self, x, mask): \"Follow Figure 1 (left) for connections.\" x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask)) return self.sublayer[1](x, self.feed_forward) The decoder is also composed of a stack of N=6 identical layers. class Decoder(nn.Module): \"Generic N layer decoder with masking.\" def __init__(self, layer, N): super(Decoder, self).__init__() self.layers = clones(layer, N) self.norm = LayerNorm(layer.size) def forward(self, x, memory, src_mask, tgt_mask): for layer in self.layers: x = layer(x, memory, src_mask, tgt_mask) return self.norm(x) In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. class DecoderLayer(nn.Module): \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\" def __init__(self, size, self_attn, src_attn, feed_forward, dropout): super(DecoderLayer, self).__init__() self.size = size self.self_attn = self_attn self.src_attn = src_attn self.feed_forward = feed_forward self.sublayer = clones(SublayerConnection(size, dropout), 3) def forward(self, x, memory, src_mask, tgt_mask): \"Follow Figure 1 (right) for connections.\" m = memory x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask)) x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask)) return self.sublayer[2](x, self.feed_forward) We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i. def subsequent_mask(size): \"Mask out subsequent positions.\" attn_shape = (1, size, size) subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1).type( torch.uint8 ) return subsequent_mask == 0 Below the attention mask shows the position each tgt word (row) is allowed to look at (column). Words are blocked for attending to future words during training. def example_mask(): LS_data = pd.concat( [ pd.DataFrame( { \"Subsequent Mask\": subsequent_mask(20)[0][x, y].flatten(), \"Window\": y, \"Masking\": x, } ) for y in range(20) for x in range(20) ] ) return ( alt.Chart(LS_data) .mark_rect() .properties(height=250, width=250) .encode( alt.X(\"Window:O\"), alt.Y(\"Masking:O\"), alt.Color(\"Subsequent Mask:Q\", scale=alt.Scale(scheme=\"viridis\")), ) .interactive() ) show_example(example_mask) An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. We call our particular attention “Scaled Dot-Product Attention”. The input consists of queries and keys of dimension d_k, and values of dimension d_v. We compute the dot products of the query with all keys, divide each by \\sqrt{d_k}, and apply a softmax function to obtain the weights on the values. In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices K and V. We compute the matrix of outputs as: \\mathrm{Attention}(Q, K, V) = \\mathrm{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V def attention(query, key, value, mask=None, dropout=None): \"Compute 'Scaled Dot Product Attention'\" d_k = query.size(-1) scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k) if mask is not None: scores = scores.masked_fill(mask == 0, -1e9) p_attn = scores.softmax(dim=-1) if dropout is not None: p_attn = dropout(p_attn) return torch.matmul(p_attn, value), p_attn The two most commonly used attention functions are additive attention (cite), and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of \\frac{1}{\\sqrt{d_k}}. Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code. While for small values of d_k the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of d_k (cite). We suspect that for large values of d_k, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients (To illustrate why the dot products get large, assume that the components of q and k are independent random variables with mean 0 and variance 1. Then their dot product, q \\cdot k = \\sum_{i=1}^{d_k} q_ik_i, has mean 0 and variance d_k.). To counteract this effect, we scale the dot products by \\frac{1}{\\sqrt{d_k}}. Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this. \\mathrm{MultiHead}(Q, K, V) = \\mathrm{Concat}(\\mathrm{head_1}, ..., \\mathrm{head_h})W^O \\\\ \\text{where}~\\mathrm{head_i} = \\mathrm{Attention}(QW^Q_i, KW^K_i, VW^V_i) Where the projections are parameter matrices W^Q_i \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}, W^K_i \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}, W^V_i \\in \\mathbb{R}^{d_{\\text{model}} \\times d_v} and W^O \\in \\mathbb{R}^{hd_v \\times d_{\\text{model}}}. In this work we employ h=8 parallel attention layers, or heads. For each of these we use d_k=d_v=d_{\\text{model}}/h=64. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality. class MultiHeadedAttention(nn.Module): def __init__(self, h, d_model, dropout=0.1): \"Take in model size and number of heads.\" super(MultiHeadedAttention, self).__init__() assert d_model % h == 0 # We assume d_v always equals d_k self.d_k = d_model // h self.h = h self.linears = clones(nn.Linear(d_model, d_model), 4) self.attn = None self.dropout = nn.Dropout(p=dropout) def forward(self, query, key, value, mask=None): \"Implements Figure 2\" if mask is not None: # Same mask applied to all h heads. mask = mask.unsqueeze(1) nbatches = query.size(0) # 1) Do all the linear projections in batch from d_model => h x d_k query, key, value = [ lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2) for lin, x in zip(self.linears, (query, key, value)) ] # 2) Apply attention on all the projected vectors in batch. x, self.attn = attention( query, key, value, mask=mask, dropout=self.dropout ) # 3) \"Concat\" using a view and apply a final linear. x = ( x.transpose(1, 2) .contiguous() .view(nbatches, -1, self.h * self.d_k) ) del query del key del value return self.linears[-1](x) The Transformer uses multi-head attention in three different ways: 1) In “encoder-decoder attention” layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as (cite). The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder. Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to -\\infty) all values in the input of the softmax which correspond to illegal connections. In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between. \\mathrm{FFN}(x)=\\max(0, xW_1 + b_1) W_2 + b_2 While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is d_{\\text{model}}=512, and the inner-layer has dimensionality d_{ff}=2048. class PositionwiseFeedForward(nn.Module): \"Implements FFN equation.\" def __init__(self, d_model, d_ff, dropout=0.1): super(PositionwiseFeedForward, self).__init__() self.w_1 = nn.Linear(d_model, d_ff) self.w_2 = nn.Linear(d_ff, d_model) self.dropout = nn.Dropout(dropout) def forward(self, x): return self.w_2(self.dropout(self.w_1(x).relu())) Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension d_{\\text{model}}. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to (cite). In the embedding layers, we multiply those weights by \\sqrt{d_{\\text{model}}}. class Embeddings(nn.Module): def __init__(self, d_model, vocab): super(Embeddings, self).__init__() self.lut = nn.Embedding(vocab, d_model) self.d_model = d_model def forward(self, x): return self.lut(x) * math.sqrt(self.d_model) Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add “positional encodings” to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension d_{\\text{model}} as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed (cite). In this work, we use sine and cosine functions of different frequencies: PE_{(pos,2i)} = \\sin(pos / 10000^{2i/d_{\\text{model}}}) PE_{(pos,2i+1)} = \\cos(pos / 10000^{2i/d_{\\text{model}}}) where pos is the position and i is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2\\pi to 10000 \\cdot 2\\pi. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k, PE_{pos+k} can be represented as a linear function of PE_{pos}. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of P_{drop}=0.1. class PositionalEncoding(nn.Module): \"Implement the PE function.\" def __init__(self, d_model, dropout, max_len=5000): super(PositionalEncoding, self).__init__() self.dropout = nn.Dropout(p=dropout) # Compute the positional encodings once in log space. pe = torch.zeros(max_len, d_model) position = torch.arange(0, max_len).unsqueeze(1) div_term = torch.exp( torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model) ) pe[:, 0::2] = torch.sin(position * div_term) pe[:, 1::2] = torch.cos(position * div_term) pe = pe.unsqueeze(0) self.register_buffer(\"pe\", pe) def forward(self, x): x = x + self.pe[:, : x.size(1)].requires_grad_(False) return self.dropout(x) Below the positional encoding will add in a sine wave based on position. The frequency and offset of the wave is different for each dimension. def example_positional(): pe = PositionalEncoding(20, 0) y = pe.forward(torch.zeros(1, 100, 20)) data = pd.concat( [ pd.DataFrame( { \"embedding\": y[0, :, dim], \"dimension\": dim, \"position\": list(range(100)), } ) for dim in [4, 5, 6, 7] ] ) return ( alt.Chart(data) .mark_line() .properties(width=800) .encode(x=\"position\", y=\"embedding\", color=\"dimension:N\") .interactive() ) show_example(example_positional) We also experimented with using learned positional embeddings (cite) instead, and found that the two versions produced nearly identical results. We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training. Here we define a function from hyperparameters to a full model. def make_model( src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1 ): \"Helper: Construct a model from hyperparameters.\" c = copy.deepcopy attn = MultiHeadedAttention(h, d_model) ff = PositionwiseFeedForward(d_model, d_ff, dropout) position = PositionalEncoding(d_model, dropout) model = EncoderDecoder( Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N), Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N), nn.Sequential(Embeddings(d_model, src_vocab), c(position)), nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)), Generator(d_model, tgt_vocab), ) # This was important from their code. # Initialize parameters with Glorot / fan_avg. for p in model.parameters(): if p.dim() > 1: nn.init.xavier_uniform_(p) return model Here we make a forward step to generate a prediction of the model. We try to use our transformer to memorize the input. As you will see the output is randomly generated due to the fact that the model is not trained yet. In the next tutorial we will build the training function and try to train our model to memorize the numbers from 1 to 10. def inference_test(): test_model = make_model(11, 11, 2) test_model.eval() src = torch.LongTensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]]) src_mask = torch.ones(1, 1, 10) memory = test_model.encode(src, src_mask) ys = torch.zeros(1, 1).type_as(src) for i in range(9): out = test_model.decode( memory, src_mask, ys, subsequent_mask(ys.size(1)).type_as(src.data) ) prob = test_model.generator(out[:, -1]) _, next_word = torch.max(prob, dim=1) next_word = next_word.data[0] ys = torch.cat( [ys, torch.empty(1, 1).type_as(src.data).fill_(next_word)], dim=1 ) print(\"Example Untrained Model Prediction:\", ys) def run_tests(): for _ in range(10): inference_test() show_example(run_tests) Example Untrained Model Prediction: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]) Example Untrained Model Prediction: tensor([[0, 3, 4, 4, 4, 4, 4, 4, 4, 4]]) Example Untrained Model Prediction: tensor([[ 0, 10, 10, 10, 3, 2, 5, 7, 9, 6]]) Example Untrained Model Prediction: tensor([[ 0, 4, 3, 6, 10, 10, 2, 6, 2, 2]]) Example Untrained Model Prediction: tensor([[ 0, 9, 0, 1, 5, 10, 1, 5, 10, 6]]) Example Untrained Model Prediction: tensor([[ 0, 1, 5, 1, 10, 1, 10, 10, 10, 10]]) Example Untrained Model Prediction: tensor([[ 0, 1, 10, 9, 9, 9, 9, 9, 1, 5]]) Example Untrained Model Prediction: tensor([[ 0, 3, 1, 5, 10, 10, 10, 10, 10, 10]]) Example Untrained Model Prediction: tensor([[ 0, 3, 5, 10, 5, 10, 4, 2, 4, 2]]) Example Untrained Model Prediction: tensor([[0, 5, 6, 2, 5, 6, 2, 6, 2, 2]]) This section describes the training regime for our models. We stop for a quick interlude to introduce some of the tools needed to train a standard encoder decoder model. First we define a batch object that holds the src and target sentences for training, as well as constructing the masks. class Batch: \"\"\"Object for holding a batch of data with mask during training.\"\"\" def __init__(self, src, tgt=None, pad=2): # 2 = <blank> self.src = src self.src_mask = (src != pad).unsqueeze(-2) if tgt is not None: self.tgt = tgt[:, :-1] self.tgt_y = tgt[:, 1:] self.tgt_mask = self.make_std_mask(self.tgt, pad) self.ntokens = (self.tgt_y != pad).data.sum() @staticmethod def make_std_mask(tgt, pad): \"Create a mask to hide padding and future words.\" tgt_mask = (tgt != pad).unsqueeze(-2) tgt_mask = tgt_mask & subsequent_mask(tgt.size(-1)).type_as( tgt_mask.data ) return tgt_mask Next we create a generic training and scoring function to keep track of loss. We pass in a generic loss compute function that also handles parameter updates. class TrainState: \"\"\"Track number of steps, examples, and tokens processed\"\"\" step: int = 0 # Steps in the current epoch accum_step: int = 0 # Number of gradient accumulation steps samples: int = 0 # total # of examples used tokens: int = 0 # total # of tokens processed def run_epoch( data_iter, model, loss_compute, optimizer, scheduler, mode=\"train\", accum_iter=1, train_state=TrainState(), ): \"\"\"Train a single epoch\"\"\" start = time.time() total_tokens = 0 total_loss = 0 tokens = 0 n_accum = 0 for i, batch in enumerate(data_iter): out = model.forward( batch.src, batch.tgt, batch.src_mask, batch.tgt_mask ) loss, loss_node = loss_compute(out, batch.tgt_y, batch.ntokens) # loss_node = loss_node / accum_iter if mode == \"train\" or mode == \"train+log\": loss_node.backward() train_state.step += 1 train_state.samples += batch.src.shape[0] train_state.tokens += batch.ntokens if i % accum_iter == 0: optimizer.step() optimizer.zero_grad(set_to_none=True) n_accum += 1 train_state.accum_step += 1 scheduler.step() total_loss += loss total_tokens += batch.ntokens tokens += batch.ntokens if i % 40 == 1 and (mode == \"train\" or mode == \"train+log\"): lr = optimizer.param_groups[0][\"lr\"] elapsed = time.time() - start print( ( \"Epoch Step: %6d | Accumulation Step: %3d | Loss: %6.2f \" + \"| Tokens / Sec: %7.1f | Learning Rate: %6.1e\" ) % (i, n_accum, loss / batch.ntokens, tokens / elapsed, lr) ) start = time.time() tokens = 0 del loss del loss_node return total_loss / total_tokens, train_state We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding, which has a shared source-target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens. We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models, step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days). We used the Adam optimizer (cite) with \\beta_1=0.9, \\beta_2=0.98 and \\epsilon=10^{-9}. We varied the learning rate over the course of training, according to the formula: lrate = d_{\\text{model}}^{-0.5} \\cdot \\min({step\\_num}^{-0.5}, {step\\_num} \\cdot {warmup\\_steps}^{-1.5}) This corresponds to increasing the learning rate linearly for the first warmup\\_steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup\\_steps=4000. Note: This part is very important. Need to train with this setup of the model. Example of the curves of this model for different model sizes and for optimization hyperparameters. def rate(step, model_size, factor, warmup): \"\"\" we have to default the step to 1 for LambdaLR function to avoid zero raising to negative power. \"\"\" if step == 0: step = 1 return factor * ( model_size ** (-0.5) * min(step ** (-0.5), step * warmup ** (-1.5)) ) def example_learning_schedule(): opts = [ [512, 1, 4000], # example 1 [512, 1, 8000], # example 2 [256, 1, 4000], # example 3 ] dummy_model = torch.nn.Linear(1, 1) learning_rates = [] # we have 3 examples in opts list. for idx, example in enumerate(opts): # run 20000 epoch for each example optimizer = torch.optim.Adam( dummy_model.parameters(), lr=1, betas=(0.9, 0.98), eps=1e-9 ) lr_scheduler = LambdaLR( optimizer=optimizer, lr_lambda=lambda step: rate(step, *example) ) tmp = [] # take 20K dummy training steps, save the learning rate at each step for step in range(20000): tmp.append(optimizer.param_groups[0][\"lr\"]) optimizer.step() lr_scheduler.step() learning_rates.append(tmp) learning_rates = torch.tensor(learning_rates) # Enable altair to handle more than 5000 rows alt.data_transformers.disable_max_rows() opts_data = pd.concat( [ pd.DataFrame( { \"Learning Rate\": learning_rates[warmup_idx, :], \"model_size:warmup\": [\"512:4000\", \"512:8000\", \"256:4000\"][ warmup_idx ], \"step\": range(20000), } ) for warmup_idx in [0, 1, 2] ] ) return ( alt.Chart(opts_data) .mark_line() .properties(width=600) .encode(x=\"step\", y=\"Learning Rate\", color=\"model_size:warmup:N\") .interactive() ) example_learning_schedule() During training, we employed label smoothing of value \\epsilon_{ls}=0.1 (cite). This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score. We implement label smoothing using the KL div loss. Instead of using a one-hot target distribution, we create a distribution that has confidence of the correct word and the rest of thesmoothing mass distributed throughout the vocabulary. class LabelSmoothing(nn.Module): \"Implement label smoothing.\" def __init__(self, size, padding_idx, smoothing=0.0): super(LabelSmoothing, self).__init__() self.criterion = nn.KLDivLoss(reduction=\"sum\") self.padding_idx = padding_idx self.confidence = 1.0 - smoothing self.smoothing = smoothing self.size = size self.true_dist = None def forward(self, x, target): assert x.size(1) == self.size true_dist = x.data.clone() true_dist.fill_(self.smoothing / (self.size - 2)) true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence) true_dist[:, self.padding_idx] = 0 mask = torch.nonzero(target.data == self.padding_idx) if mask.dim() > 0: true_dist.index_fill_(0, mask.squeeze(), 0.0) self.true_dist = true_dist return self.criterion(x, true_dist.clone().detach()) Here we can see an example of how the mass is distributed to the words based on confidence. # Example of label smoothing. def example_label_smoothing(): crit = LabelSmoothing(5, 0, 0.4) predict = torch.FloatTensor( [ [0, 0.2, 0.7, 0.1, 0], [0, 0.2, 0.7, 0.1, 0], [0, 0.2, 0.7, 0.1, 0], [0, 0.2, 0.7, 0.1, 0], [0, 0.2, 0.7, 0.1, 0], ] ) crit(x=predict.log(), target=torch.LongTensor([2, 1, 0, 3, 3])) LS_data = pd.concat( [ pd.DataFrame( { \"target distribution\": crit.true_dist[x, y].flatten(), \"columns\": y, \"rows\": x, } ) for y in range(5) for x in range(5) ] ) return ( alt.Chart(LS_data) .mark_rect(color=\"Blue\", opacity=1) .properties(height=200, width=200) .encode( alt.X(\"columns:O\", title=None), alt.Y(\"rows:O\", title=None), alt.Color( \"target distribution:Q\", scale=alt.Scale(scheme=\"viridis\") ), ) .interactive() ) show_example(example_label_smoothing) Label smoothing actually starts to penalize the model if it gets very confident about a given choice. def loss(x, crit): d = x + 3 * 1 predict = torch.FloatTensor([[0, x / d, 1 / d, 1 / d, 1 / d]]) return crit(predict.log(), torch.LongTensor([1])).data def penalization_visualization(): crit = LabelSmoothing(5, 0, 0.1) loss_data = pd.DataFrame( { \"Loss\": [loss(x, crit) for x in range(1, 100)], \"Steps\": list(range(99)), } ).astype(\"float\") return ( alt.Chart(loss_data) .mark_line() .properties(width=350) .encode( x=\"Steps\", y=\"Loss\", ) .interactive() ) show_example(penalization_visualization) We can begin by trying out a simple copy-task. Given a random set of input symbols from a small vocabulary, the goal is to generate back those same symbols. def data_gen(V, batch_size, nbatches): \"Generate random data for a src-tgt copy task.\" for i in range(nbatches): data = torch.randint(1, V, size=(batch_size, 10)) data[:, 0] = 1 src = data.requires_grad_(False).clone().detach() tgt = data.requires_grad_(False).clone().detach() yield Batch(src, tgt, 0) class SimpleLossCompute: \"A simple loss compute and train function.\" def __init__(self, generator, criterion): self.generator = generator self.criterion = criterion def __call__(self, x, y, norm): x = self.generator(x) sloss = ( self.criterion( x.contiguous().view(-1, x.size(-1)), y.contiguous().view(-1) ) / norm ) return sloss.data * norm, sloss This code predicts a translation using greedy decoding for simplicity. def greedy_decode(model, src, src_mask, max_len, start_symbol): memory = model.encode(src, src_mask) ys = torch.zeros(1, 1).fill_(start_symbol).type_as(src.data) for i in range(max_len - 1): out = model.decode( memory, src_mask, ys, subsequent_mask(ys.size(1)).type_as(src.data) ) prob = model.generator(out[:, -1]) _, next_word = torch.max(prob, dim=1) next_word = next_word.data[0] ys = torch.cat( [ys, torch.zeros(1, 1).type_as(src.data).fill_(next_word)], dim=1 ) return ys # Train the simple copy task. def example_simple_model(): V = 11 criterion = LabelSmoothing(size=V, padding_idx=0, smoothing=0.0) model = make_model(V, V, N=2) optimizer = torch.optim.Adam( model.parameters(), lr=0.5, betas=(0.9, 0.98), eps=1e-9 ) lr_scheduler = LambdaLR( optimizer=optimizer, lr_lambda=lambda step: rate( step, model_size=model.src_embed[0].d_model, factor=1.0, warmup=400 ), ) batch_size = 80 for epoch in range(20): model.train() run_epoch( data_gen(V, batch_size, 20), model, SimpleLossCompute(model.generator, criterion), optimizer, lr_scheduler, mode=\"train\", ) model.eval() run_epoch( data_gen(V, batch_size, 5), model, SimpleLossCompute(model.generator, criterion), DummyOptimizer(), DummyScheduler(), mode=\"eval\", )[0] model.eval() src = torch.LongTensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]) max_len = src.shape[1] src_mask = torch.ones(1, 1, max_len) print(greedy_decode(model, src, src_mask, max_len=max_len, start_symbol=0)) # execute_example(example_simple_model) Now we consider a real-world example using the Multi30k German-English Translation task. This task is much smaller than the WMT task considered in the paper, but it illustrates the whole system. We also show how to use multi-gpu processing to make it really fast. We will load the dataset using torchtext and spacy for tokenization. # Load spacy tokenizer models, download them if they haven't been # downloaded already def load_tokenizers(): try: spacy_de = spacy.load(\"de_core_news_sm\") except IOError: os.system(\"python -m spacy download de_core_news_sm\") spacy_de = spacy.load(\"de_core_news_sm\") try: spacy_en = spacy.load(\"en_core_web_sm\") except IOError: os.system(\"python -m spacy download en_core_web_sm\") spacy_en = spacy.load(\"en_core_web_sm\") return spacy_de, spacy_en def tokenize(text, tokenizer): return [tok.text for tok in tokenizer.tokenizer(text)] def yield_tokens(data_iter, tokenizer, index): for from_to_tuple in data_iter: yield tokenizer(from_to_tuple[index]) def build_vocabulary(spacy_de, spacy_en): def tokenize_de(text): return tokenize(text, spacy_de) def tokenize_en(text): return tokenize(text, spacy_en) print(\"Building German Vocabulary ...\") train, val, test = datasets.Multi30k(language_pair=(\"de\", \"en\")) vocab_src = build_vocab_from_iterator( yield_tokens(train + val + test, tokenize_de, index=0), min_freq=2, specials=[\"<s>\", \"</s>\", \"<blank>\", \"<unk>\"], ) print(\"Building English Vocabulary ...\") train, val, test = datasets.Multi30k(language_pair=(\"de\", \"en\")) vocab_tgt = build_vocab_from_iterator( yield_tokens(train + val + test, tokenize_en, index=1), min_freq=2, specials=[\"<s>\", \"</s>\", \"<blank>\", \"<unk>\"], ) vocab_src.set_default_index(vocab_src[\"<unk>\"]) vocab_tgt.set_default_index(vocab_tgt[\"<unk>\"]) return vocab_src, vocab_tgt def load_vocab(spacy_de, spacy_en): if not exists(\"vocab.pt\"): vocab_src, vocab_tgt = build_vocabulary(spacy_de, spacy_en) torch.save((vocab_src, vocab_tgt), \"vocab.pt\") else: vocab_src, vocab_tgt = torch.load(\"vocab.pt\") print(\"Finished.\\nVocabulary sizes:\") print(len(vocab_src)) print(len(vocab_tgt)) return vocab_src, vocab_tgt if is_interactive_notebook(): # global variables used later in the script spacy_de, spacy_en = show_example(load_tokenizers) vocab_src, vocab_tgt = show_example(load_vocab, args=[spacy_de, spacy_en]) Finished. Vocabulary sizes: 59981 36745 Batching matters a ton for speed. We want to have very evenly divided batches, with absolutely minimal padding. To do this we have to hack a bit around the default torchtext batching. This code patches their default batching to make sure we search over enough sentences to find tight batches. def collate_batch( batch, src_pipeline, tgt_pipeline, src_vocab, tgt_vocab, device, max_padding=128, pad_id=2, ): bs_id = torch.tensor([0], device=device) # <s> token id eos_id = torch.tensor([1], device=device) # </s> token id src_list, tgt_list = [], [] for (_src, _tgt) in batch: processed_src = torch.cat( [ bs_id, torch.tensor( src_vocab(src_pipeline(_src)), dtype=torch.int64, device=device, ), eos_id, ], 0, ) processed_tgt = torch.cat( [ bs_id, torch.tensor( tgt_vocab(tgt_pipeline(_tgt)), dtype=torch.int64, device=device, ), eos_id, ], 0, ) src_list.append( # warning - overwrites values for negative values of padding - len pad( processed_src, ( 0, max_padding - len(processed_src), ), value=pad_id, ) ) tgt_list.append( pad( processed_tgt, (0, max_padding - len(processed_tgt)), value=pad_id, ) ) src = torch.stack(src_list) tgt = torch.stack(tgt_list) return (src, tgt) def create_dataloaders( device, vocab_src, vocab_tgt, spacy_de, spacy_en, batch_size=12000, max_padding=128, is_distributed=True, ): # def create_dataloaders(batch_size=12000): def tokenize_de(text): return tokenize(text, spacy_de) def tokenize_en(text): return tokenize(text, spacy_en) def collate_fn(batch): return collate_batch( batch, tokenize_de, tokenize_en, vocab_src, vocab_tgt, device, max_padding=max_padding, pad_id=vocab_src.get_stoi()[\"<blank>\"], ) train_iter, valid_iter, test_iter = datasets.Multi30k( language_pair=(\"de\", \"en\") ) train_iter_map = to_map_style_dataset( train_iter ) # DistributedSampler needs a dataset len() train_sampler = ( DistributedSampler(train_iter_map) if is_distributed else None ) valid_iter_map = to_map_style_dataset(valid_iter) valid_sampler = ( DistributedSampler(valid_iter_map) if is_distributed else None ) train_dataloader = DataLoader( train_iter_map, batch_size=batch_size, shuffle=(train_sampler is None), sampler=train_sampler, collate_fn=collate_fn, ) valid_dataloader = DataLoader( valid_iter_map, batch_size=batch_size, shuffle=(valid_sampler is None), sampler=valid_sampler, collate_fn=collate_fn, ) return train_dataloader, valid_dataloader def train_worker( gpu, ngpus_per_node, vocab_src, vocab_tgt, spacy_de, spacy_en, config, is_distributed=False, ): print(f\"Train worker process using GPU: {gpu} for training\", flush=True) torch.cuda.set_device(gpu) pad_idx = vocab_tgt[\"<blank>\"] d_model = 512 model = make_model(len(vocab_src), len(vocab_tgt), N=6) model.cuda(gpu) module = model is_main_process = True if is_distributed: dist.init_process_group( \"nccl\", init_method=\"env://\", rank=gpu, world_size=ngpus_per_node ) model = DDP(model, device_ids=[gpu]) module = model.module is_main_process = gpu == 0 criterion = LabelSmoothing( size=len(vocab_tgt), padding_idx=pad_idx, smoothing=0.1 ) criterion.cuda(gpu) train_dataloader, valid_dataloader = create_dataloaders( gpu, vocab_src, vocab_tgt, spacy_de, spacy_en, batch_size=config[\"batch_size\"] // ngpus_per_node, max_padding=config[\"max_padding\"], is_distributed=is_distributed, ) optimizer = torch.optim.Adam( model.parameters(), lr=config[\"base_lr\"], betas=(0.9, 0.98), eps=1e-9 ) lr_scheduler = LambdaLR( optimizer=optimizer, lr_lambda=lambda step: rate( step, d_model, factor=1, warmup=config[\"warmup\"] ), ) train_state = TrainState() for epoch in range(config[\"num_epochs\"]): if is_distributed: train_dataloader.sampler.set_epoch(epoch) valid_dataloader.sampler.set_epoch(epoch) model.train() print(f\"[GPU{gpu}] Epoch {epoch} Training ====\", flush=True) _, train_state = run_epoch( (Batch(b[0], b[1], pad_idx) for b in train_dataloader), model, SimpleLossCompute(module.generator, criterion), optimizer, lr_scheduler, mode=\"train+log\", accum_iter=config[\"accum_iter\"], train_state=train_state, ) GPUtil.showUtilization() if is_main_process: file_path = \"%s%.2d.pt\" % (config[\"file_prefix\"], epoch) torch.save(module.state_dict(), file_path) torch.cuda.empty_cache() print(f\"[GPU{gpu}] Epoch {epoch} Validation ====\", flush=True) model.eval() sloss = run_epoch( (Batch(b[0], b[1], pad_idx) for b in valid_dataloader), model, SimpleLossCompute(module.generator, criterion), DummyOptimizer(), DummyScheduler(), mode=\"eval\", ) print(sloss) torch.cuda.empty_cache() if is_main_process: file_path = \"%sfinal.pt\" % config[\"file_prefix\"] torch.save(module.state_dict(), file_path) def train_distributed_model(vocab_src, vocab_tgt, spacy_de, spacy_en, config): from the_annotated_transformer import train_worker ngpus = torch.cuda.device_count() os.environ[\"MASTER_ADDR\"] = \"localhost\" os.environ[\"MASTER_PORT\"] = \"12356\" print(f\"Number of GPUs detected: {ngpus}\") print(\"Spawning training processes ...\") mp.spawn( train_worker, nprocs=ngpus, args=(ngpus, vocab_src, vocab_tgt, spacy_de, spacy_en, config, True), ) def train_model(vocab_src, vocab_tgt, spacy_de, spacy_en, config): if config[\"distributed\"]: train_distributed_model( vocab_src, vocab_tgt, spacy_de, spacy_en, config ) else: train_worker( 0, 1, vocab_src, vocab_tgt, spacy_de, spacy_en, config, False ) def load_trained_model(): config = { \"batch_size\": 32, \"distributed\": False, \"num_epochs\": 8, \"accum_iter\": 10, \"base_lr\": 1.0, \"max_padding\": 72, \"warmup\": 3000, \"file_prefix\": \"multi30k_model_\", } model_path = \"multi30k_model_final.pt\" if not exists(model_path): train_model(vocab_src, vocab_tgt, spacy_de, spacy_en, config) model = make_model(len(vocab_src), len(vocab_tgt), N=6) model.load_state_dict(torch.load(\"multi30k_model_final.pt\")) return model if is_interactive_notebook(): model = load_trained_model() Once trained we can decode the model to produce a set of translations. Here we simply translate the first sentence in the validation set. This dataset is pretty small so the translations with greedy search are reasonably accurate. So this mostly covers the transformer model itself. There are four aspects that we didn’t cover explicitly. We also have all these additional features implemented in OpenNMT-py. - BPE/ Word-piece: We can use a library to first preprocess the data into subword units. See Rico Sennrich’s subword-nmt implementation. These models will transform the training data to look like this: ▁Die ▁Protokoll datei ▁kann ▁ heimlich ▁per ▁E - Mail ▁oder ▁FTP ▁an ▁einen ▁bestimmte n ▁Empfänger ▁gesendet ▁werden . - Shared Embeddings: When using BPE with shared vocabulary we can share the same weight vectors between the source / target / generator. See the (cite) for details. To add this to the model simply do this: if False: model.src_embed[0].lut.weight = model.tgt_embeddings[0].lut.weight model.generator.lut.weight = model.tgt_embed[0].lut.weight - Beam Search: This is a bit too complicated to cover here. See the OpenNMT-py for a pytorch implementation. - Model Averaging: The paper averages the last k checkpoints to create an ensembling effect. We can do this after the fact if we have a bunch of models: def average(model, models): \"Average models into model\" for ps in zip(*[m.params() for m in [model] + models]): ps[0].copy_(torch.sum(*ps[1:]) / len(ps[1:])) On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is listed in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models. On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than 1/4 the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate Pdrop = 0.1, instead of 0.3. With the addtional extensions in the last section, the OpenNMT-py replication gets to 26.9 on EN-DE WMT. Here I have loaded in those parameters to our reimplemenation. # Load data and model for output checks def check_outputs( valid_dataloader, model, vocab_src, vocab_tgt, n_examples=15, pad_idx=2, eos_string=\"</s>\", ): results = [()] * n_examples for idx in range(n_examples): print(\"\\nExample %d ========\\n\" % idx) b = next(iter(valid_dataloader)) rb = Batch(b[0], b[1], pad_idx) greedy_decode(model, rb.src, rb.src_mask, 64, 0)[0] src_tokens = [ vocab_src.get_itos()[x] for x in rb.src[0] if x != pad_idx ] tgt_tokens = [ vocab_tgt.get_itos()[x] for x in rb.tgt[0] if x != pad_idx ] print( \"Source Text (Input) : \" + \" \".join(src_tokens).replace(\"\\n\", \"\") ) print( \"Target Text (Ground Truth) : \" + \" \".join(tgt_tokens).replace(\"\\n\", \"\") ) model_out = greedy_decode(model, rb.src, rb.src_mask, 72, 0)[0] model_txt = ( \" \".join( [vocab_tgt.get_itos()[x] for x in model_out if x != pad_idx] ).split(eos_string, 1)[0] + eos_string ) print(\"Model Output : \" + model_txt.replace(\"\\n\", \"\")) results[idx] = (rb, src_tokens, tgt_tokens, model_out, model_txt) return results def run_model_example(n_examples=5): global vocab_src, vocab_tgt, spacy_de, spacy_en print(\"Preparing Data ...\") _, valid_dataloader = create_dataloaders( torch.device(\"cpu\"), vocab_src, vocab_tgt, spacy_de, spacy_en, batch_size=1, is_distributed=False, ) print(\"Loading Trained Model ...\") model = make_model(len(vocab_src), len(vocab_tgt), N=6) model.load_state_dict( torch.load(\"multi30k_model_final.pt\", map_location=torch.device(\"cpu\")) ) print(\"Checking Model Outputs:\") example_data = check_outputs( valid_dataloader, model, vocab_src, vocab_tgt, n_examples=n_examples ) return model, example_data # execute_example(run_model_example) Even with a greedy decoder the translation looks pretty good. We can further visualize it to see what is happening at each layer of the attention def mtx2df(m, max_row, max_col, row_tokens, col_tokens): \"convert a dense matrix to a data frame with row and column indices\" return pd.DataFrame( [ ( r, c, float(m[r, c]), \"%.3d %s\" % (r, row_tokens[r] if len(row_tokens) > r else \"<blank>\"), \"%.3d %s\" % (c, col_tokens[c] if len(col_tokens) > c else \"<blank>\"), ) for r in range(m.shape[0]) for c in range(m.shape[1]) if r < max_row and c < max_col ], # if float(m[r,c]) != 0 and r < max_row and c < max_col], columns=[\"row\", \"column\", \"value\", \"row_token\", \"col_token\"], ) def attn_map(attn, layer, head, row_tokens, col_tokens, max_dim=30): df = mtx2df( attn[0, head].data, max_dim, max_dim, row_tokens, col_tokens, ) return ( alt.Chart(data=df) .mark_rect() .encode( x=alt.X(\"col_token\", axis=alt.Axis(title=\"\")), y=alt.Y(\"row_token\", axis=alt.Axis(title=\"\")), color=\"value\", tooltip=[\"row\", \"column\", \"value\", \"row_token\", \"col_token\"], ) .properties(height=400, width=400) .interactive() ) def get_encoder(model, layer): return model.encoder.layers[layer].self_attn.attn def get_decoder_self(model, layer): return model.decoder.layers[layer].self_attn.attn def get_decoder_src(model, layer): return model.decoder.layers[layer].src_attn.attn def visualize_layer(model, layer, getter_fn, ntokens, row_tokens, col_tokens): # ntokens = last_example[0].ntokens attn = getter_fn(model, layer) n_heads = attn.shape[1] charts = [ attn_map( attn, 0, h, row_tokens=row_tokens, col_tokens=col_tokens, max_dim=ntokens, ) for h in range(n_heads) ] assert n_heads == 8 return alt.vconcat( charts[0] # | charts[1] | charts[2] # | charts[3] | charts[4] # | charts[5] | charts[6] # | charts[7] # layer + 1 due to 0-indexing ).properties(title=\"Layer %d\" % (layer + 1)) def viz_encoder_self(): model, example_data = run_model_example(n_examples=1) example = example_data[ len(example_data) - 1 ] # batch object for the final example layer_viz = [ visualize_layer( model, layer, get_encoder, len(example[1]), example[1], example[1] ) for layer in range(6) ] return alt.hconcat( layer_viz[0] # & layer_viz[1] & layer_viz[2] # & layer_viz[3] & layer_viz[4] # & layer_viz[5] ) show_example(viz_encoder_self) Preparing Data ... Loading Trained Model ... Checking Model Outputs: Example 0 ======== Source Text (Input) : <s> Zwei Frauen in pinkfarbenen T-Shirts und <unk> unterhalten sich vor einem <unk> . </s> Target Text (Ground Truth) : <s> Two women wearing pink T - shirts and blue jeans converse outside clothing store . </s> Model Output : <s> Two women in pink shirts and face are talking in front of a <unk> . </s> def viz_decoder_self(): model, example_data = run_model_example(n_examples=1) example = example_data[len(example_data) - 1] layer_viz = [ visualize_layer( model, layer, get_decoder_self, len(example[1]), example[1], example[1], ) for layer in range(6) ] return alt.hconcat( layer_viz[0] & layer_viz[1] & layer_viz[2] & layer_viz[3] & layer_viz[4] & layer_viz[5] ) show_example(viz_decoder_self) Preparing Data ... Loading Trained Model ... Checking Model Outputs: Example 0 ======== Source Text (Input) : <s> Eine Gruppe von Männern in Kostümen spielt Musik . </s> Target Text (Ground Truth) : <s> A group of men in costume play music . </s> Model Output : <s> A group of men in costumes playing music . </s> def viz_decoder_src(): model, example_data = run_model_example(n_examples=1) example = example_data[len(example_data) - 1] layer_viz = [ visualize_layer( model, layer, get_decoder_src, max(len(example[1]), len(example[2])), example[1], example[2], ) for layer in range(6) ] return alt.hconcat( layer_viz[0] & layer_viz[1] & layer_viz[2] & layer_viz[3] & layer_viz[4] & layer_viz[5] ) show_example(viz_decoder_src) Preparing Data ... Loading Trained Model ... Checking Model Outputs: Example 0 ======== Source Text (Input) : <s> Ein kleiner Junge verwendet einen Bohrer , um ein Loch in ein Holzstück zu machen . </s> Target Text (Ground Truth) : <s> A little boy using a drill to make a hole in a piece of wood . </s> Model Output : <s> A little boy uses a machine to be working in a hole in a log . </s> Hopefully this code is useful for future research. Please reach out if you have any issues. Cheers, Sasha Rush, Austin Huang, Suraj Subramanian, Jonathan Sum, Khalid Almubarak, Stella Biderman",
    "authors": [],
    "published": null
  }
]